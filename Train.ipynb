{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=32\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.05\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=100\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define Parameters\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .05, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 100, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "positive_data_file = ''\n",
    "negative_data_file = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(classId, positive_data_file, negative_data_file):\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(positive_data_file, negative_data_file)\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print( vocab_processor.vocabulary_)\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    \n",
    "# Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", str(classId), timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C3FC3B38>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\n",
      "\n",
      "2017-11-05T07:46:19.582521: step 1, loss 5.1139, acc 0.34375\n",
      "2017-11-05T07:46:24.114741: step 2, loss 4.02802, acc 0.375\n",
      "2017-11-05T07:46:28.818083: step 3, loss 2.1731, acc 0.59375\n",
      "2017-11-05T07:46:32.967038: step 4, loss 0.466138, acc 0.9375\n",
      "2017-11-05T07:46:37.419203: step 5, loss 0.663373, acc 0.8125\n",
      "2017-11-05T07:46:41.898384: step 6, loss 0.546506, acc 0.875\n",
      "2017-11-05T07:46:46.048333: step 7, loss 0.964868, acc 0.9375\n",
      "2017-11-05T07:46:51.206153: step 8, loss 1.63635, acc 0.84375\n",
      "2017-11-05T07:46:56.732080: step 9, loss 1.18846, acc 0.90625\n",
      "2017-11-05T07:47:01.423447: step 10, loss 2.61903, acc 0.8125\n",
      "2017-11-05T07:47:06.189834: step 11, loss 0.573374, acc 0.96875\n",
      "2017-11-05T07:47:10.854648: step 12, loss 2.64662, acc 0.8125\n",
      "2017-11-05T07:47:15.871213: step 13, loss 1.1407, acc 0.9375\n",
      "2017-11-05T07:47:20.381417: step 14, loss 2.98894, acc 0.8125\n",
      "2017-11-05T07:47:24.671466: step 15, loss 0.930751, acc 0.90625\n",
      "2017-11-05T07:47:29.132748: step 16, loss 2.4696, acc 0.84375\n",
      "2017-11-05T07:47:33.904137: step 17, loss 1.42957, acc 0.90625\n",
      "2017-11-05T07:47:38.797614: step 18, loss 4.07868, acc 0.78125\n",
      "2017-11-05T07:47:43.534485: step 19, loss 0.238512, acc 0.96875\n",
      "2017-11-05T07:47:48.584073: step 20, loss 2.75193, acc 0.84375\n",
      "2017-11-05T07:47:53.475549: step 21, loss 0.872607, acc 0.90625\n",
      "2017-11-05T07:47:58.106339: step 22, loss 1.16426, acc 0.90625\n",
      "2017-11-05T07:48:03.186733: step 23, loss 0.985575, acc 0.875\n",
      "2017-11-05T07:48:08.140753: step 24, loss 1.49335, acc 0.75\n",
      "2017-11-05T07:48:13.477051: step 25, loss 0.810078, acc 0.84375\n",
      "2017-11-05T07:48:17.931215: step 26, loss 2.17516, acc 0.8125\n",
      "2017-11-05T07:48:22.643063: step 27, loss 0.433857, acc 0.84375\n",
      "2017-11-05T07:48:27.648320: step 28, loss 1.56838, acc 0.6875\n",
      "2017-11-05T07:48:32.432218: step 29, loss 0.853256, acc 0.78125\n",
      "2017-11-05T07:48:37.251643: step 30, loss 1.46995, acc 0.6875\n",
      "2017-11-05T07:48:42.222175: step 31, loss 2.04183, acc 0.6875\n",
      "2017-11-05T07:48:47.387850: step 32, loss 0.480865, acc 0.8125\n",
      "2017-11-05T07:48:52.221286: step 33, loss 0.722169, acc 0.875\n",
      "2017-11-05T07:48:57.114266: step 34, loss 0.308137, acc 0.90625\n",
      "2017-11-05T07:49:02.108814: step 35, loss 1.71514, acc 0.84375\n",
      "2017-11-05T07:49:05.080926: step 36, loss 3.35441, acc 0.7\n",
      "2017-11-05T07:49:10.043221: step 37, loss 0.776435, acc 0.75\n",
      "2017-11-05T07:49:14.992238: step 38, loss 0.557804, acc 0.875\n",
      "2017-11-05T07:49:19.773134: step 39, loss 1.25262, acc 0.84375\n",
      "2017-11-05T07:49:25.200180: step 40, loss 0.594482, acc 0.84375\n",
      "2017-11-05T07:49:30.116674: step 41, loss 0.890184, acc 0.84375\n",
      "2017-11-05T07:49:35.454996: step 42, loss 0.891261, acc 0.875\n",
      "2017-11-05T07:49:40.567129: step 43, loss 0.248178, acc 0.96875\n",
      "2017-11-05T07:49:45.548167: step 44, loss 0.943911, acc 0.875\n",
      "2017-11-05T07:49:50.331573: step 45, loss 0.856554, acc 0.71875\n",
      "2017-11-05T07:49:55.390668: step 46, loss 0.161203, acc 0.90625\n",
      "2017-11-05T07:50:00.360767: step 47, loss 0.253429, acc 0.9375\n",
      "2017-11-05T07:50:05.183694: step 48, loss 0.664093, acc 0.84375\n",
      "2017-11-05T07:50:09.700904: step 49, loss 0.540148, acc 0.90625\n",
      "2017-11-05T07:50:14.723210: step 50, loss 0.366777, acc 0.84375\n",
      "2017-11-05T07:50:19.328482: step 51, loss 1.21275, acc 0.8125\n",
      "2017-11-05T07:50:24.002309: step 52, loss 1.05612, acc 0.84375\n",
      "2017-11-05T07:50:28.626093: step 53, loss 0.821809, acc 0.875\n",
      "2017-11-05T07:50:33.296412: step 54, loss 0.770298, acc 0.9375\n",
      "2017-11-05T07:50:38.057295: step 55, loss 1.22327, acc 0.875\n",
      "2017-11-05T07:50:42.728113: step 56, loss 0.242845, acc 0.90625\n",
      "2017-11-05T07:50:47.463478: step 57, loss 0.594264, acc 0.9375\n",
      "2017-11-05T07:50:52.295411: step 58, loss 0.141758, acc 0.9375\n",
      "2017-11-05T07:50:57.073839: step 59, loss 0.525097, acc 0.875\n",
      "2017-11-05T07:51:01.871864: step 60, loss 0.869932, acc 0.90625\n",
      "2017-11-05T07:51:06.735078: step 61, loss 1.17511, acc 0.84375\n",
      "2017-11-05T07:51:11.594030: step 62, loss 0.682955, acc 0.84375\n",
      "2017-11-05T07:51:16.648121: step 63, loss 0.744834, acc 0.9375\n",
      "2017-11-05T07:51:21.628660: step 64, loss 0.663917, acc 0.84375\n",
      "2017-11-05T07:51:26.441091: step 65, loss 0.655453, acc 0.84375\n",
      "2017-11-05T07:51:31.417627: step 66, loss 0.4883, acc 0.875\n",
      "2017-11-05T07:51:36.106472: step 67, loss 0.157131, acc 0.90625\n",
      "2017-11-05T07:51:41.461508: step 68, loss 0.435954, acc 0.90625\n",
      "2017-11-05T07:51:46.130356: step 69, loss 2.68782, acc 0.65625\n",
      "2017-11-05T07:51:50.848710: step 70, loss 1.84505, acc 0.75\n",
      "2017-11-05T07:51:55.462988: step 71, loss 1.13185, acc 0.6875\n",
      "2017-11-05T07:51:58.445108: step 72, loss 1.93722, acc 0.7\n",
      "2017-11-05T07:52:03.104418: step 73, loss 0.977984, acc 0.6875\n",
      "2017-11-05T07:52:07.809761: step 74, loss 0.561427, acc 0.84375\n",
      "2017-11-05T07:52:12.429549: step 75, loss 0.995942, acc 0.8125\n",
      "2017-11-05T07:52:17.164154: step 76, loss 0.583692, acc 0.8125\n",
      "2017-11-05T07:52:21.789941: step 77, loss 1.54379, acc 0.75\n",
      "2017-11-05T07:52:26.488784: step 78, loss 0.714958, acc 0.90625\n",
      "2017-11-05T07:52:31.179623: step 79, loss 0.44089, acc 0.90625\n",
      "2017-11-05T07:52:35.803908: step 80, loss 0.565055, acc 0.90625\n",
      "2017-11-05T07:52:40.633842: step 81, loss 0.912825, acc 0.84375\n",
      "2017-11-05T07:52:45.246117: step 82, loss 0.763994, acc 0.90625\n",
      "2017-11-05T07:52:49.902426: step 83, loss 0.576604, acc 0.90625\n",
      "2017-11-05T07:52:54.542036: step 84, loss 1.12037, acc 0.875\n",
      "2017-11-05T07:52:59.139813: step 85, loss 1.30154, acc 0.71875\n",
      "2017-11-05T07:53:03.668547: step 86, loss 0.8053, acc 0.8125\n",
      "2017-11-05T07:53:08.254306: step 87, loss 1.27368, acc 0.875\n",
      "2017-11-05T07:53:13.006696: step 88, loss 0.30421, acc 0.875\n",
      "2017-11-05T07:53:17.679517: step 89, loss 0.893559, acc 0.84375\n",
      "2017-11-05T07:53:22.306809: step 90, loss 1.29079, acc 0.78125\n",
      "2017-11-05T07:53:27.068698: step 91, loss 0.771617, acc 0.875\n",
      "2017-11-05T07:53:31.785549: step 92, loss 0.850048, acc 0.90625\n",
      "2017-11-05T07:53:36.400829: step 93, loss 1.38221, acc 0.8125\n",
      "2017-11-05T07:53:40.981584: step 94, loss 1.21159, acc 0.84375\n",
      "2017-11-05T07:53:45.591361: step 95, loss 0.551378, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T07:53:50.148604: step 96, loss 0.856746, acc 0.84375\n",
      "2017-11-05T07:53:54.802911: step 97, loss 0.357446, acc 0.875\n",
      "2017-11-05T07:53:59.579084: step 98, loss 1.43444, acc 0.78125\n",
      "2017-11-05T07:54:04.368486: step 99, loss 0.673507, acc 0.8125\n",
      "2017-11-05T07:54:08.923222: step 100, loss 0.775227, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T07:54:12.212063: step 100, loss 1.39585, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-100\n",
      "\n",
      "2017-11-05T07:54:21.118976: step 101, loss 0.23491, acc 0.9375\n",
      "2017-11-05T07:54:26.518351: step 102, loss 0.56634, acc 0.84375\n",
      "2017-11-05T07:54:31.240205: step 103, loss 0.0519857, acc 0.96875\n",
      "2017-11-05T07:54:35.872497: step 104, loss 0.778303, acc 0.9375\n",
      "2017-11-05T07:54:40.756967: step 105, loss 0.479218, acc 0.9375\n",
      "2017-11-05T07:54:45.876605: step 106, loss 0.917351, acc 0.78125\n",
      "2017-11-05T07:54:50.452357: step 107, loss 0.739332, acc 0.9375\n",
      "2017-11-05T07:54:53.450487: step 108, loss 1.1575, acc 0.75\n",
      "2017-11-05T07:54:58.009726: step 109, loss 0.363115, acc 0.90625\n",
      "2017-11-05T07:55:02.609001: step 110, loss 1.692, acc 0.71875\n",
      "2017-11-05T07:55:07.118705: step 111, loss 0.633524, acc 0.90625\n",
      "2017-11-05T07:55:11.659933: step 112, loss 0.862246, acc 0.78125\n",
      "2017-11-05T07:55:16.213670: step 113, loss 0.480658, acc 0.875\n",
      "2017-11-05T07:55:20.753909: step 114, loss 0.649251, acc 0.84375\n",
      "2017-11-05T07:55:25.402713: step 115, loss 1.42815, acc 0.8125\n",
      "2017-11-05T07:55:29.982973: step 116, loss 0.119605, acc 0.90625\n",
      "2017-11-05T07:55:34.550717: step 117, loss 0.217773, acc 0.9375\n",
      "2017-11-05T07:55:39.127975: step 118, loss 0.865178, acc 0.78125\n",
      "2017-11-05T07:55:43.720739: step 119, loss 1.08872, acc 0.875\n",
      "2017-11-05T07:55:48.838876: step 120, loss 0.650833, acc 0.875\n",
      "2017-11-05T07:55:53.069384: step 121, loss 0.796174, acc 0.875\n",
      "2017-11-05T07:55:57.263363: step 122, loss 0.704675, acc 0.875\n",
      "2017-11-05T07:56:02.313709: step 123, loss 0.03152, acc 1\n",
      "2017-11-05T07:56:07.443294: step 124, loss 0.32471, acc 0.8125\n",
      "2017-11-05T07:56:12.892665: step 125, loss 1.2799, acc 0.84375\n",
      "2017-11-05T07:56:17.782018: step 126, loss 0.364811, acc 0.90625\n",
      "2017-11-05T07:56:22.255196: step 127, loss 0.527374, acc 0.875\n",
      "2017-11-05T07:56:26.932519: step 128, loss 1.43366, acc 0.8125\n",
      "2017-11-05T07:56:31.397707: step 129, loss 0.665652, acc 0.90625\n",
      "2017-11-05T07:56:35.999486: step 130, loss 0.336028, acc 0.90625\n",
      "2017-11-05T07:56:40.521706: step 131, loss 0.678076, acc 0.9375\n",
      "2017-11-05T07:56:44.911825: step 132, loss 0.216723, acc 0.9375\n",
      "2017-11-05T07:56:49.376998: step 133, loss 0.222333, acc 0.96875\n",
      "2017-11-05T07:56:53.831663: step 134, loss 1.54473, acc 0.84375\n",
      "2017-11-05T07:56:58.335008: step 135, loss 0.630749, acc 0.9375\n",
      "2017-11-05T07:57:02.802182: step 136, loss 0.700674, acc 0.84375\n",
      "2017-11-05T07:57:07.231328: step 137, loss 0.0534577, acc 0.96875\n",
      "2017-11-05T07:57:11.699052: step 138, loss 0.174227, acc 0.9375\n",
      "2017-11-05T07:57:16.244827: step 139, loss 0.522871, acc 0.9375\n",
      "2017-11-05T07:57:20.712508: step 140, loss 0.245011, acc 0.96875\n",
      "2017-11-05T07:57:25.145170: step 141, loss 0.263134, acc 0.90625\n",
      "2017-11-05T07:57:29.603977: step 142, loss 0.0238153, acc 1\n",
      "2017-11-05T07:57:34.110680: step 143, loss 0.929638, acc 0.90625\n",
      "2017-11-05T07:57:36.866637: step 144, loss 1.64417, acc 0.75\n",
      "2017-11-05T07:57:41.286779: step 145, loss 0.586955, acc 0.84375\n",
      "2017-11-05T07:57:45.691556: step 146, loss 0.847473, acc 0.78125\n",
      "2017-11-05T07:57:50.180252: step 147, loss 0.813081, acc 0.8125\n",
      "2017-11-05T07:57:54.539850: step 148, loss 0.632037, acc 0.875\n",
      "2017-11-05T07:57:58.891457: step 149, loss 0.482217, acc 0.90625\n",
      "2017-11-05T07:58:03.326609: step 150, loss 0.981936, acc 0.78125\n",
      "2017-11-05T07:58:07.775620: step 151, loss 0.55632, acc 0.8125\n",
      "2017-11-05T07:58:12.246297: step 152, loss 1.08328, acc 0.84375\n",
      "2017-11-05T07:58:16.712476: step 153, loss 1.16598, acc 0.78125\n",
      "2017-11-05T07:58:21.091588: step 154, loss 0.0925695, acc 0.96875\n",
      "2017-11-05T07:58:25.780002: step 155, loss 0.00714764, acc 1\n",
      "2017-11-05T07:58:30.628952: step 156, loss 0.430594, acc 0.90625\n",
      "2017-11-05T07:58:35.095126: step 157, loss 0.558871, acc 0.9375\n",
      "2017-11-05T07:58:39.690395: step 158, loss 0.854739, acc 0.84375\n",
      "2017-11-05T07:58:44.079517: step 159, loss 0.600292, acc 0.875\n",
      "2017-11-05T07:58:48.422982: step 160, loss 1.65515, acc 0.84375\n",
      "2017-11-05T07:58:52.878159: step 161, loss 1.14685, acc 0.78125\n",
      "2017-11-05T07:58:57.260773: step 162, loss 0.585568, acc 0.90625\n",
      "2017-11-05T07:59:01.690926: step 163, loss 0.983184, acc 0.875\n",
      "2017-11-05T07:59:06.162603: step 164, loss 1.14206, acc 0.8125\n",
      "2017-11-05T07:59:10.718339: step 165, loss 0.46964, acc 0.875\n",
      "2017-11-05T07:59:15.126972: step 166, loss 0.488173, acc 0.875\n",
      "2017-11-05T07:59:19.560622: step 167, loss 1.42877, acc 0.71875\n",
      "2017-11-05T07:59:23.921722: step 168, loss 0.395084, acc 0.875\n",
      "2017-11-05T07:59:28.383440: step 169, loss 0.271346, acc 0.90625\n",
      "2017-11-05T07:59:32.859124: step 170, loss 1.15896, acc 0.78125\n",
      "2017-11-05T07:59:37.363325: step 171, loss 1.90591, acc 0.78125\n",
      "2017-11-05T07:59:41.861021: step 172, loss 0.430481, acc 0.90625\n",
      "2017-11-05T07:59:46.230625: step 173, loss 0.789486, acc 0.84375\n",
      "2017-11-05T07:59:50.605233: step 174, loss 0.514136, acc 0.90625\n",
      "2017-11-05T07:59:55.074909: step 175, loss 0.69774, acc 0.875\n",
      "2017-11-05T07:59:59.704698: step 176, loss 0.253347, acc 0.90625\n",
      "2017-11-05T08:00:04.802828: step 177, loss 1.80038, acc 0.75\n",
      "2017-11-05T08:00:09.760351: step 178, loss 0.919821, acc 0.875\n",
      "2017-11-05T08:00:14.137469: step 179, loss 0.264411, acc 0.875\n",
      "2017-11-05T08:00:17.421808: step 180, loss 0.00730409, acc 1\n",
      "2017-11-05T08:00:22.805770: step 181, loss 1.65809, acc 0.8125\n",
      "2017-11-05T08:00:27.784815: step 182, loss 0.698114, acc 0.84375\n",
      "2017-11-05T08:00:32.147420: step 183, loss 0.920969, acc 0.90625\n",
      "2017-11-05T08:00:36.437468: step 184, loss 0.2713, acc 0.9375\n",
      "2017-11-05T08:00:40.622942: step 185, loss 0.328095, acc 0.90625\n",
      "2017-11-05T08:00:44.690343: step 186, loss 0.217965, acc 0.9375\n",
      "2017-11-05T08:00:48.822310: step 187, loss 1.83284, acc 0.75\n",
      "2017-11-05T08:00:53.325048: step 188, loss 0.994202, acc 0.875\n",
      "2017-11-05T08:00:57.948333: step 189, loss 0.808416, acc 0.84375\n",
      "2017-11-05T08:01:02.629160: step 190, loss 0.472209, acc 0.90625\n",
      "2017-11-05T08:01:07.323995: step 191, loss 0.417852, acc 0.96875\n",
      "2017-11-05T08:01:11.722620: step 192, loss 0.764694, acc 0.90625\n",
      "2017-11-05T08:01:16.169280: step 193, loss 0.169739, acc 0.9375\n",
      "2017-11-05T08:01:20.539885: step 194, loss 0.272473, acc 0.90625\n",
      "2017-11-05T08:01:24.904987: step 195, loss 0.426196, acc 0.9375\n",
      "2017-11-05T08:01:29.222555: step 196, loss 0.623697, acc 0.875\n",
      "2017-11-05T08:01:33.622682: step 197, loss 0.46441, acc 0.90625\n",
      "2017-11-05T08:01:38.761839: step 198, loss 0.360367, acc 0.8125\n",
      "2017-11-05T08:01:43.390628: step 199, loss 0.352528, acc 0.9375\n",
      "2017-11-05T08:01:47.852298: step 200, loss 0.464505, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:01:50.796891: step 200, loss 1.26521, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-200\n",
      "\n",
      "2017-11-05T08:01:56.869209: step 201, loss 0.563222, acc 0.90625\n",
      "2017-11-05T08:02:01.380533: step 202, loss 0.0885074, acc 0.96875\n",
      "2017-11-05T08:02:05.841203: step 203, loss 0.914239, acc 0.84375\n",
      "2017-11-05T08:02:10.339905: step 204, loss 1.32354, acc 0.71875\n",
      "2017-11-05T08:02:14.828594: step 205, loss 0.429614, acc 0.875\n",
      "2017-11-05T08:02:19.560956: step 206, loss 0.119812, acc 0.9375\n",
      "2017-11-05T08:02:24.305328: step 207, loss 1.20906, acc 0.8125\n",
      "2017-11-05T08:02:29.188298: step 208, loss 1.04178, acc 0.875\n",
      "2017-11-05T08:02:34.278414: step 209, loss 0.188928, acc 0.9375\n",
      "2017-11-05T08:02:38.737082: step 210, loss 0.12862, acc 0.96875\n",
      "2017-11-05T08:02:43.140210: step 211, loss 0.787418, acc 0.8125\n",
      "2017-11-05T08:02:47.634904: step 212, loss 0.406281, acc 0.9375\n",
      "2017-11-05T08:02:52.108583: step 213, loss 0.379437, acc 0.875\n",
      "2017-11-05T08:02:56.946204: step 214, loss 0.522739, acc 0.90625\n",
      "2017-11-05T08:03:01.070120: step 215, loss 0.470213, acc 0.84375\n",
      "2017-11-05T08:03:03.712998: step 216, loss 0.282966, acc 0.95\n",
      "2017-11-05T08:03:07.902975: step 217, loss 0.372481, acc 0.875\n",
      "2017-11-05T08:03:12.284088: step 218, loss 0.459487, acc 0.875\n",
      "2017-11-05T08:03:17.574397: step 219, loss 0.572987, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T08:03:22.153150: step 220, loss 0.227559, acc 0.90625\n",
      "2017-11-05T08:03:26.655850: step 221, loss 0.716176, acc 0.84375\n",
      "2017-11-05T08:03:31.385210: step 222, loss 0.371057, acc 0.875\n",
      "2017-11-05T08:03:35.914929: step 223, loss 0.455666, acc 0.875\n",
      "2017-11-05T08:03:40.480411: step 224, loss 1.10929, acc 0.875\n",
      "2017-11-05T08:03:44.950587: step 225, loss 0.514334, acc 0.875\n",
      "2017-11-05T08:03:49.436775: step 226, loss 0.329683, acc 0.9375\n",
      "2017-11-05T08:03:54.009595: step 227, loss 0.286685, acc 0.875\n",
      "2017-11-05T08:03:58.511307: step 228, loss 0.316821, acc 0.90625\n",
      "2017-11-05T08:04:03.023513: step 229, loss 0.735054, acc 0.8125\n",
      "2017-11-05T08:04:07.587269: step 230, loss 0.435044, acc 0.96875\n",
      "2017-11-05T08:04:12.167034: step 231, loss 0.257733, acc 0.90625\n",
      "2017-11-05T08:04:16.654222: step 232, loss 0.794533, acc 0.78125\n",
      "2017-11-05T08:04:21.223534: step 233, loss 1.05216, acc 0.78125\n",
      "2017-11-05T08:04:25.776769: step 234, loss 0.192009, acc 0.9375\n",
      "2017-11-05T08:04:30.324000: step 235, loss 0.523294, acc 0.90625\n",
      "2017-11-05T08:04:35.082381: step 236, loss 0.432512, acc 0.875\n",
      "2017-11-05T08:04:39.668640: step 237, loss 0.167928, acc 0.90625\n",
      "2017-11-05T08:04:44.179348: step 238, loss 0.836675, acc 0.875\n",
      "2017-11-05T08:04:48.878310: step 239, loss 0.334362, acc 0.875\n",
      "2017-11-05T08:04:53.396520: step 240, loss 0.339946, acc 0.9375\n",
      "2017-11-05T08:04:57.857190: step 241, loss 0.57419, acc 0.875\n",
      "2017-11-05T08:05:02.276334: step 242, loss 0.611248, acc 0.90625\n",
      "2017-11-05T08:05:06.772028: step 243, loss 0.756084, acc 0.875\n",
      "2017-11-05T08:05:11.445348: step 244, loss 0.297799, acc 0.90625\n",
      "2017-11-05T08:05:16.281785: step 245, loss 0.381994, acc 0.9375\n",
      "2017-11-05T08:05:20.880553: step 246, loss 0.617307, acc 0.90625\n",
      "2017-11-05T08:05:26.031712: step 247, loss 0.392875, acc 0.9375\n",
      "2017-11-05T08:05:30.653190: step 248, loss 0.216153, acc 0.96875\n",
      "2017-11-05T08:05:35.430587: step 249, loss 0.0701383, acc 0.96875\n",
      "2017-11-05T08:05:40.518713: step 250, loss 0.172367, acc 0.9375\n",
      "2017-11-05T08:05:45.117981: step 251, loss 0.360685, acc 0.90625\n",
      "2017-11-05T08:05:47.920972: step 252, loss 0.470922, acc 0.85\n",
      "2017-11-05T08:05:53.072153: step 253, loss 0.458033, acc 0.84375\n",
      "2017-11-05T08:05:58.290523: step 254, loss 0.856232, acc 0.84375\n",
      "2017-11-05T08:06:02.766203: step 255, loss 0.754678, acc 0.84375\n",
      "2017-11-05T08:06:07.138310: step 256, loss 0.377342, acc 0.90625\n",
      "2017-11-05T08:06:11.520424: step 257, loss 0.351999, acc 0.90625\n",
      "2017-11-05T08:06:16.005611: step 258, loss 0.977352, acc 0.875\n",
      "2017-11-05T08:06:20.582863: step 259, loss 0.821743, acc 0.84375\n",
      "2017-11-05T08:06:25.287281: step 260, loss 0.369445, acc 0.875\n",
      "2017-11-05T08:06:30.752170: step 261, loss 0.802897, acc 0.90625\n",
      "2017-11-05T08:06:36.385673: step 262, loss 0.743569, acc 0.8125\n",
      "2017-11-05T08:06:41.914601: step 263, loss 0.952505, acc 0.84375\n",
      "2017-11-05T08:06:46.668479: step 264, loss 0.396513, acc 0.875\n",
      "2017-11-05T08:06:51.960239: step 265, loss 0.608619, acc 0.90625\n",
      "2017-11-05T08:06:57.220490: step 266, loss 0.344778, acc 0.90625\n",
      "2017-11-05T08:07:02.397211: step 267, loss 0.00588894, acc 1\n",
      "2017-11-05T08:07:06.696266: step 268, loss 0.551055, acc 0.875\n",
      "2017-11-05T08:07:11.041353: step 269, loss 0.101769, acc 0.96875\n",
      "2017-11-05T08:07:15.332402: step 270, loss 0.632511, acc 0.875\n",
      "2017-11-05T08:07:19.921221: step 271, loss 0.00573109, acc 1\n",
      "2017-11-05T08:07:24.376887: step 272, loss 0.595735, acc 0.875\n",
      "2017-11-05T08:07:28.798528: step 273, loss 0.231996, acc 0.9375\n",
      "2017-11-05T08:07:33.992729: step 274, loss 0.512727, acc 0.90625\n",
      "2017-11-05T08:07:38.941943: step 275, loss 0.402313, acc 0.875\n",
      "2017-11-05T08:07:43.486377: step 276, loss 0.637794, acc 0.9375\n",
      "2017-11-05T08:07:48.226245: step 277, loss 0.186277, acc 0.9375\n",
      "2017-11-05T08:07:52.643383: step 278, loss 0.684798, acc 0.84375\n",
      "2017-11-05T08:07:57.428783: step 279, loss 0.531123, acc 0.9375\n",
      "2017-11-05T08:08:02.074818: step 280, loss 0.399356, acc 0.875\n",
      "2017-11-05T08:08:06.506967: step 281, loss 0.877832, acc 0.875\n",
      "2017-11-05T08:08:10.870568: step 282, loss 0.974087, acc 0.875\n",
      "2017-11-05T08:08:15.104075: step 283, loss 0.267253, acc 0.875\n",
      "2017-11-05T08:08:19.516711: step 284, loss 0.115013, acc 0.96875\n",
      "2017-11-05T08:08:23.867803: step 285, loss 0.401059, acc 0.84375\n",
      "2017-11-05T08:08:28.268977: step 286, loss 0.37633, acc 0.9375\n",
      "2017-11-05T08:08:32.950804: step 287, loss 0.721026, acc 0.8125\n",
      "2017-11-05T08:08:36.033494: step 288, loss 0.540149, acc 0.9\n",
      "2017-11-05T08:08:40.430119: step 289, loss 0.869211, acc 0.875\n",
      "2017-11-05T08:08:44.816746: step 290, loss 0.730995, acc 0.90625\n",
      "2017-11-05T08:08:49.818300: step 291, loss 0.139178, acc 0.96875\n",
      "2017-11-05T08:08:54.429577: step 292, loss 1.11892, acc 0.8125\n",
      "2017-11-05T08:08:59.213980: step 293, loss 0.630686, acc 0.90625\n",
      "2017-11-05T08:09:03.731191: step 294, loss 0.361355, acc 0.90625\n",
      "2017-11-05T08:09:08.012173: step 295, loss 0.263853, acc 0.9375\n",
      "2017-11-05T08:09:12.328100: step 296, loss 0.353927, acc 0.9375\n",
      "2017-11-05T08:09:16.885321: step 297, loss 0.40008, acc 0.875\n",
      "2017-11-05T08:09:22.310176: step 298, loss 0.00881774, acc 1\n",
      "2017-11-05T08:09:27.907507: step 299, loss 1.14845, acc 0.875\n",
      "2017-11-05T08:09:32.442229: step 300, loss 0.278307, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:09:35.457872: step 300, loss 1.43104, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-300\n",
      "\n",
      "2017-11-05T08:09:41.737545: step 301, loss 0.377984, acc 0.96875\n",
      "2017-11-05T08:09:45.869481: step 302, loss 0.636826, acc 0.8125\n",
      "2017-11-05T08:09:50.235583: step 303, loss 0.0942368, acc 0.96875\n",
      "2017-11-05T08:09:54.642715: step 304, loss 0.584119, acc 0.90625\n",
      "2017-11-05T08:09:58.756638: step 305, loss 0.152613, acc 0.9375\n",
      "2017-11-05T08:10:03.616095: step 306, loss 0.441664, acc 0.96875\n",
      "2017-11-05T08:10:07.934972: step 307, loss 0.28498, acc 0.875\n",
      "2017-11-05T08:10:12.310079: step 308, loss 0.570903, acc 0.84375\n",
      "2017-11-05T08:10:16.567105: step 309, loss 0.737475, acc 0.84375\n",
      "2017-11-05T08:10:20.732064: step 310, loss 0.895171, acc 0.875\n",
      "2017-11-05T08:10:24.920040: step 311, loss 0.874406, acc 0.8125\n",
      "2017-11-05T08:10:29.178819: step 312, loss 0.470461, acc 0.9375\n",
      "2017-11-05T08:10:33.387809: step 313, loss 0.00536818, acc 1\n",
      "2017-11-05T08:10:37.748966: step 314, loss 0.303741, acc 0.875\n",
      "2017-11-05T08:10:41.873896: step 315, loss 0.213192, acc 0.90625\n",
      "2017-11-05T08:10:46.729346: step 316, loss 0.0569733, acc 0.96875\n",
      "2017-11-05T08:10:51.087443: step 317, loss 0.515572, acc 0.90625\n",
      "2017-11-05T08:10:55.262410: step 318, loss 0.0265225, acc 1\n",
      "2017-11-05T08:10:59.458391: step 319, loss 0.32896, acc 0.90625\n",
      "2017-11-05T08:11:03.840303: step 320, loss 0.621334, acc 0.84375\n",
      "2017-11-05T08:11:07.945219: step 321, loss 1.32576, acc 0.8125\n",
      "2017-11-05T08:11:12.160214: step 322, loss 0.925144, acc 0.875\n",
      "2017-11-05T08:11:16.474279: step 323, loss 0.783253, acc 0.75\n",
      "2017-11-05T08:11:19.164191: step 324, loss 0.449753, acc 0.85\n",
      "2017-11-05T08:11:23.363931: step 325, loss 0.47452, acc 0.875\n",
      "2017-11-05T08:11:27.930175: step 326, loss 0.0612683, acc 0.96875\n",
      "2017-11-05T08:11:33.262966: step 327, loss 0.140269, acc 0.9375\n",
      "2017-11-05T08:11:38.721846: step 328, loss 0.12525, acc 0.9375\n",
      "2017-11-05T08:11:43.599309: step 329, loss 0.623253, acc 0.90625\n",
      "2017-11-05T08:11:48.072491: step 330, loss 0.567614, acc 0.875\n",
      "2017-11-05T08:11:53.249166: step 331, loss 0.212669, acc 0.9375\n",
      "2017-11-05T08:11:57.913480: step 332, loss 0.734059, acc 0.875\n",
      "2017-11-05T08:12:02.602573: step 333, loss 0.263487, acc 0.9375\n",
      "2017-11-05T08:12:06.740512: step 334, loss 0.245534, acc 0.90625\n",
      "2017-11-05T08:12:11.009547: step 335, loss 0.375608, acc 0.9375\n",
      "2017-11-05T08:12:16.314315: step 336, loss 0.426906, acc 0.90625\n",
      "2017-11-05T08:12:20.684178: step 337, loss 0.715169, acc 0.90625\n",
      "2017-11-05T08:12:24.923190: step 338, loss 0.328391, acc 0.84375\n",
      "2017-11-05T08:12:29.174210: step 339, loss 0.771584, acc 0.84375\n",
      "2017-11-05T08:12:34.303856: step 340, loss 0.45177, acc 0.90625\n",
      "2017-11-05T08:12:38.712988: step 341, loss 0.905615, acc 0.90625\n",
      "2017-11-05T08:12:43.342278: step 342, loss 0.544513, acc 0.9375\n",
      "2017-11-05T08:12:47.996584: step 343, loss 0.910443, acc 0.75\n",
      "2017-11-05T08:12:52.216583: step 344, loss 0.0012874, acc 1\n",
      "2017-11-05T08:12:56.491620: step 345, loss 0.464223, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T08:13:00.860484: step 346, loss 0.878916, acc 0.84375\n",
      "2017-11-05T08:13:05.460753: step 347, loss 1.11071, acc 0.8125\n",
      "2017-11-05T08:13:09.695762: step 348, loss 1.14431, acc 0.84375\n",
      "2017-11-05T08:13:14.512654: step 349, loss 0.686643, acc 0.8125\n",
      "2017-11-05T08:13:19.121012: step 350, loss 0.901567, acc 0.875\n",
      "2017-11-05T08:13:23.394197: step 351, loss 0.440077, acc 0.875\n",
      "2017-11-05T08:13:27.846560: step 352, loss 1.14401, acc 0.8125\n",
      "2017-11-05T08:13:32.406835: step 353, loss 0.508842, acc 0.875\n",
      "2017-11-05T08:13:36.516281: step 354, loss 0.232061, acc 0.9375\n",
      "2017-11-05T08:13:40.860172: step 355, loss 0.165082, acc 0.9375\n",
      "2017-11-05T08:13:44.884217: step 356, loss 0.222663, acc 0.90625\n",
      "2017-11-05T08:13:48.972658: step 357, loss 0.388958, acc 0.90625\n",
      "2017-11-05T08:13:53.145287: step 358, loss 0.348915, acc 0.8125\n",
      "2017-11-05T08:13:57.311519: step 359, loss 0.374386, acc 0.875\n",
      "2017-11-05T08:14:00.180401: step 360, loss 0.332965, acc 0.9\n",
      "2017-11-05T08:14:05.524949: step 361, loss 0.341419, acc 0.9375\n",
      "2017-11-05T08:14:09.679902: step 362, loss 0.546887, acc 0.90625\n",
      "2017-11-05T08:14:14.471306: step 363, loss 0.370389, acc 0.84375\n",
      "2017-11-05T08:14:18.811963: step 364, loss 0.700027, acc 0.875\n",
      "2017-11-05T08:14:23.052073: step 365, loss 0.183599, acc 0.9375\n",
      "2017-11-05T08:14:27.511836: step 366, loss 0.274491, acc 0.90625\n",
      "2017-11-05T08:14:31.682375: step 367, loss 0.983742, acc 0.78125\n",
      "2017-11-05T08:14:35.787636: step 368, loss 0.338587, acc 0.84375\n",
      "2017-11-05T08:14:40.053866: step 369, loss 0.6329, acc 0.90625\n",
      "2017-11-05T08:14:44.371199: step 370, loss 0.21686, acc 0.9375\n",
      "2017-11-05T08:14:48.707251: step 371, loss 0.22871, acc 0.84375\n",
      "2017-11-05T08:14:53.057889: step 372, loss 0.105001, acc 0.9375\n",
      "2017-11-05T08:14:57.688900: step 373, loss 0.152992, acc 0.90625\n",
      "2017-11-05T08:15:02.330826: step 374, loss 0.171492, acc 0.96875\n",
      "2017-11-05T08:15:07.437038: step 375, loss 1.10581, acc 0.78125\n",
      "2017-11-05T08:15:11.663040: step 376, loss 0.669745, acc 0.84375\n",
      "2017-11-05T08:15:15.843022: step 377, loss 0.36246, acc 0.90625\n",
      "2017-11-05T08:15:20.043704: step 378, loss 0.817634, acc 0.8125\n",
      "2017-11-05T08:15:24.508128: step 379, loss 0.20326, acc 0.9375\n",
      "2017-11-05T08:15:29.218239: step 380, loss 0.0922589, acc 0.9375\n",
      "2017-11-05T08:15:33.826519: step 381, loss 0.0503689, acc 0.96875\n",
      "2017-11-05T08:15:38.291691: step 382, loss 0.599734, acc 0.8125\n",
      "2017-11-05T08:15:42.573921: step 383, loss 0.435952, acc 0.9375\n",
      "2017-11-05T08:15:46.963062: step 384, loss 1.30931, acc 0.78125\n",
      "2017-11-05T08:15:51.826989: step 385, loss 0.457892, acc 0.84375\n",
      "2017-11-05T08:15:56.202604: step 386, loss 0.144426, acc 0.9375\n",
      "2017-11-05T08:16:00.511164: step 387, loss 0.28375, acc 0.96875\n",
      "2017-11-05T08:16:05.019407: step 388, loss 1.14788, acc 0.84375\n",
      "2017-11-05T08:16:09.945110: step 389, loss 0.778789, acc 0.84375\n",
      "2017-11-05T08:16:14.554886: step 390, loss 0.303789, acc 0.90625\n",
      "2017-11-05T08:16:19.229707: step 391, loss 1.16308, acc 0.78125\n",
      "2017-11-05T08:16:23.729380: step 392, loss 0.167551, acc 0.96875\n",
      "2017-11-05T08:16:28.073997: step 393, loss 0.338801, acc 0.90625\n",
      "2017-11-05T08:16:32.576323: step 394, loss 0.989421, acc 0.84375\n",
      "2017-11-05T08:16:37.062841: step 395, loss 0.0151434, acc 1\n",
      "2017-11-05T08:16:40.097477: step 396, loss 0.346849, acc 0.9\n",
      "2017-11-05T08:16:44.499933: step 397, loss 0.518113, acc 0.8125\n",
      "2017-11-05T08:16:49.630038: step 398, loss 0.0883399, acc 0.96875\n",
      "2017-11-05T08:16:54.727684: step 399, loss 0.634065, acc 0.875\n",
      "2017-11-05T08:16:59.709549: step 400, loss 0.760375, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:17:03.092566: step 400, loss 1.33915, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-400\n",
      "\n",
      "2017-11-05T08:17:09.203375: step 401, loss 0.575924, acc 0.875\n",
      "2017-11-05T08:17:13.688563: step 402, loss 0.333277, acc 0.875\n",
      "2017-11-05T08:17:18.419924: step 403, loss 0.93161, acc 0.8125\n",
      "2017-11-05T08:17:23.118263: step 404, loss 0.348845, acc 0.90625\n",
      "2017-11-05T08:17:28.018572: step 405, loss 0.486013, acc 0.9375\n",
      "2017-11-05T08:17:32.953079: step 406, loss 0.19454, acc 0.96875\n",
      "2017-11-05T08:17:37.665426: step 407, loss 0.670541, acc 0.8125\n",
      "2017-11-05T08:17:42.415802: step 408, loss 0.456287, acc 0.9375\n",
      "2017-11-05T08:17:47.443145: step 409, loss 0.296264, acc 0.90625\n",
      "2017-11-05T08:17:52.341627: step 410, loss 0.132923, acc 0.9375\n",
      "2017-11-05T08:17:57.101008: step 411, loss 0.425863, acc 0.875\n",
      "2017-11-05T08:18:01.781334: step 412, loss 0.00471204, acc 1\n",
      "2017-11-05T08:18:06.642788: step 413, loss 0.137782, acc 0.9375\n",
      "2017-11-05T08:18:11.460211: step 414, loss 0.359327, acc 0.875\n",
      "2017-11-05T08:18:16.028457: step 415, loss 0.808776, acc 0.8125\n",
      "2017-11-05T08:18:20.552671: step 416, loss 1.01697, acc 0.84375\n",
      "2017-11-05T08:18:25.490180: step 417, loss 0.971251, acc 0.875\n",
      "2017-11-05T08:18:30.235551: step 418, loss 0.190847, acc 0.9375\n",
      "2017-11-05T08:18:34.893861: step 419, loss 0.390282, acc 0.90625\n",
      "2017-11-05T08:18:39.640995: step 420, loss 0.877004, acc 0.90625\n",
      "2017-11-05T08:18:44.547481: step 421, loss 0.434375, acc 0.9375\n",
      "2017-11-05T08:18:49.363902: step 422, loss 0.16298, acc 0.9375\n",
      "2017-11-05T08:18:53.886116: step 423, loss 0.0168543, acc 1\n",
      "2017-11-05T08:18:58.317108: step 424, loss 0.512405, acc 0.78125\n",
      "2017-11-05T08:19:03.106511: step 425, loss 0.931988, acc 0.90625\n",
      "2017-11-05T08:19:08.656711: step 426, loss 0.645235, acc 0.8125\n",
      "2017-11-05T08:19:13.708807: step 427, loss 0.633168, acc 0.875\n",
      "2017-11-05T08:19:18.645914: step 428, loss 0.77345, acc 0.84375\n",
      "2017-11-05T08:19:23.562908: step 429, loss 0.659953, acc 0.8125\n",
      "2017-11-05T08:19:28.465894: step 430, loss 0.863276, acc 0.78125\n",
      "2017-11-05T08:19:33.264303: step 431, loss 0.355539, acc 0.90625\n",
      "2017-11-05T08:19:36.482591: step 432, loss 0.915768, acc 0.75\n",
      "2017-11-05T08:19:41.374074: step 433, loss 0.306083, acc 0.9375\n",
      "2017-11-05T08:19:46.493704: step 434, loss 0.384805, acc 0.9375\n",
      "2017-11-05T08:19:51.474033: step 435, loss 0.224404, acc 0.90625\n",
      "2017-11-05T08:19:56.545139: step 436, loss 0.00813074, acc 1\n",
      "2017-11-05T08:20:01.949280: step 437, loss 0.389987, acc 0.90625\n",
      "2017-11-05T08:20:06.766935: step 438, loss 0.581303, acc 0.8125\n",
      "2017-11-05T08:20:11.135538: step 439, loss 0.399382, acc 0.90625\n",
      "2017-11-05T08:20:15.913934: step 440, loss 0.517365, acc 0.90625\n",
      "2017-11-05T08:20:21.329951: step 441, loss 0.505096, acc 0.84375\n",
      "2017-11-05T08:20:26.937785: step 442, loss 0.464511, acc 0.90625\n",
      "2017-11-05T08:20:31.385800: step 443, loss 0.65014, acc 0.90625\n",
      "2017-11-05T08:20:35.803473: step 444, loss 0.907216, acc 0.84375\n",
      "2017-11-05T08:20:40.331168: step 445, loss 0.0243268, acc 1\n",
      "2017-11-05T08:20:44.847184: step 446, loss 0.823546, acc 0.90625\n",
      "2017-11-05T08:20:48.919156: step 447, loss 0.178618, acc 0.96875\n",
      "2017-11-05T08:20:53.050433: step 448, loss 0.467907, acc 0.84375\n",
      "2017-11-05T08:20:57.364744: step 449, loss 0.651289, acc 0.875\n",
      "2017-11-05T08:21:02.056973: step 450, loss 0.362682, acc 0.875\n",
      "2017-11-05T08:21:07.033221: step 451, loss 0.00515363, acc 1\n",
      "2017-11-05T08:21:11.548372: step 452, loss 0.405456, acc 0.875\n",
      "2017-11-05T08:21:16.565540: step 453, loss 0.272485, acc 0.90625\n",
      "2017-11-05T08:21:20.929000: step 454, loss 0.3292, acc 0.90625\n",
      "2017-11-05T08:21:25.306308: step 455, loss 0.642258, acc 0.78125\n",
      "2017-11-05T08:21:29.542723: step 456, loss 0.500443, acc 0.90625\n",
      "2017-11-05T08:21:33.924429: step 457, loss 0.0633766, acc 0.96875\n",
      "2017-11-05T08:21:38.164565: step 458, loss 0.413625, acc 0.84375\n",
      "2017-11-05T08:21:42.505280: step 459, loss 0.340636, acc 0.9375\n",
      "2017-11-05T08:21:47.357391: step 460, loss 0.0854779, acc 0.96875\n",
      "2017-11-05T08:21:51.706982: step 461, loss 0.985194, acc 0.875\n",
      "2017-11-05T08:21:55.881950: step 462, loss 0.631819, acc 0.875\n",
      "2017-11-05T08:22:00.053912: step 463, loss 0.686438, acc 0.875\n",
      "2017-11-05T08:22:05.481210: step 464, loss 0.0789852, acc 0.9375\n",
      "2017-11-05T08:22:10.796489: step 465, loss 0.388703, acc 0.84375\n",
      "2017-11-05T08:22:15.368877: step 466, loss 1.33533, acc 0.78125\n",
      "2017-11-05T08:22:19.449864: step 467, loss 0.0999569, acc 0.96875\n",
      "2017-11-05T08:22:22.093373: step 468, loss 0.350878, acc 0.9\n",
      "2017-11-05T08:22:26.310426: step 469, loss 0.580772, acc 0.78125\n",
      "2017-11-05T08:22:30.329433: step 470, loss 0.25701, acc 0.96875\n",
      "2017-11-05T08:22:34.366511: step 471, loss 0.357086, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T08:22:38.468934: step 472, loss 0.516312, acc 0.90625\n",
      "2017-11-05T08:22:42.570900: step 473, loss 0.71017, acc 0.875\n",
      "2017-11-05T08:22:46.849698: step 474, loss 0.176538, acc 0.96875\n",
      "2017-11-05T08:22:50.922744: step 475, loss 0.374337, acc 0.84375\n",
      "2017-11-05T08:22:55.116427: step 476, loss 0.626938, acc 0.9375\n",
      "2017-11-05T08:22:59.500042: step 477, loss 0.0791407, acc 0.9375\n",
      "2017-11-05T08:23:03.544753: step 478, loss 0.487113, acc 0.90625\n",
      "2017-11-05T08:23:07.658638: step 479, loss 0.546769, acc 0.875\n",
      "2017-11-05T08:23:11.849017: step 480, loss 0.456606, acc 0.9375\n",
      "2017-11-05T08:23:15.997291: step 481, loss 0.51051, acc 0.875\n",
      "2017-11-05T08:23:20.062033: step 482, loss 0.696208, acc 0.875\n",
      "2017-11-05T08:23:24.187590: step 483, loss 0.734337, acc 0.84375\n",
      "2017-11-05T08:23:28.799509: step 484, loss 0.786304, acc 0.875\n",
      "2017-11-05T08:23:32.875961: step 485, loss 0.283144, acc 0.90625\n",
      "2017-11-05T08:23:36.868022: step 486, loss 0.277393, acc 0.9375\n",
      "2017-11-05T08:23:40.965597: step 487, loss 0.400283, acc 0.90625\n",
      "2017-11-05T08:23:45.008755: step 488, loss 0.365743, acc 0.875\n",
      "2017-11-05T08:23:49.151737: step 489, loss 0.356668, acc 0.9375\n",
      "2017-11-05T08:23:53.314333: step 490, loss 0.355653, acc 0.90625\n",
      "2017-11-05T08:23:57.412460: step 491, loss 0.198739, acc 0.96875\n",
      "2017-11-05T08:24:01.540460: step 492, loss 0.538978, acc 0.875\n",
      "2017-11-05T08:24:05.745856: step 493, loss 0.52038, acc 0.8125\n",
      "2017-11-05T08:24:10.039853: step 494, loss 0.247734, acc 0.875\n",
      "2017-11-05T08:24:14.095306: step 495, loss 0.192666, acc 0.9375\n",
      "2017-11-05T08:24:18.279224: step 496, loss 0.434798, acc 0.90625\n",
      "2017-11-05T08:24:22.380237: step 497, loss 0.737622, acc 0.875\n",
      "2017-11-05T08:24:26.457756: step 498, loss 0.126953, acc 0.96875\n",
      "2017-11-05T08:24:30.528298: step 499, loss 0.545192, acc 0.9375\n",
      "2017-11-05T08:24:34.782250: step 500, loss 0.177264, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:24:37.492888: step 500, loss 1.57495, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-500\n",
      "\n",
      "2017-11-05T08:24:43.338699: step 501, loss 0.938498, acc 0.875\n",
      "2017-11-05T08:24:47.676221: step 502, loss 1.18593, acc 0.84375\n",
      "2017-11-05T08:24:51.891360: step 503, loss 0.478004, acc 0.90625\n",
      "2017-11-05T08:24:54.602791: step 504, loss 0.364479, acc 0.85\n",
      "2017-11-05T08:24:58.794447: step 505, loss 0.0282004, acc 1\n",
      "2017-11-05T08:25:02.946359: step 506, loss 0.331087, acc 0.9375\n",
      "2017-11-05T08:25:07.395445: step 507, loss 0.459833, acc 0.96875\n",
      "2017-11-05T08:25:12.883345: step 508, loss 0.55721, acc 0.875\n",
      "2017-11-05T08:25:17.992368: step 509, loss 0.0263339, acc 1\n",
      "2017-11-05T08:25:22.284393: step 510, loss 0.512339, acc 0.875\n",
      "2017-11-05T08:25:26.748196: step 511, loss 0.501379, acc 0.90625\n",
      "2017-11-05T08:25:31.188436: step 512, loss 0.614805, acc 0.84375\n",
      "2017-11-05T08:25:36.051775: step 513, loss 0.114425, acc 0.9375\n",
      "2017-11-05T08:25:40.347034: step 514, loss 0.681316, acc 0.78125\n",
      "2017-11-05T08:25:44.493462: step 515, loss 0.403712, acc 0.875\n",
      "2017-11-05T08:25:49.506921: step 516, loss 0.556719, acc 0.84375\n",
      "2017-11-05T08:25:53.590009: step 517, loss 0.262185, acc 0.9375\n",
      "2017-11-05T08:25:57.691938: step 518, loss 0.659579, acc 0.875\n",
      "2017-11-05T08:26:01.835544: step 519, loss 0.884995, acc 0.875\n",
      "2017-11-05T08:26:06.479876: step 520, loss 0.0562854, acc 0.96875\n",
      "2017-11-05T08:26:10.947433: step 521, loss 0.228775, acc 0.9375\n",
      "2017-11-05T08:26:15.731903: step 522, loss 0.245564, acc 0.875\n",
      "2017-11-05T08:26:19.922198: step 523, loss 0.21589, acc 0.90625\n",
      "2017-11-05T08:26:24.004143: step 524, loss 0.216851, acc 0.9375\n",
      "2017-11-05T08:26:28.146274: step 525, loss 0.0640093, acc 0.96875\n",
      "2017-11-05T08:26:32.253418: step 526, loss 0.493116, acc 0.875\n",
      "2017-11-05T08:26:36.404295: step 527, loss 0.351306, acc 0.90625\n",
      "2017-11-05T08:26:40.478859: step 528, loss 0.109003, acc 0.9375\n",
      "2017-11-05T08:26:45.025949: step 529, loss 0.149474, acc 0.9375\n",
      "2017-11-05T08:26:50.294957: step 530, loss 0.189417, acc 0.96875\n",
      "2017-11-05T08:26:54.680573: step 531, loss 0.81237, acc 0.78125\n",
      "2017-11-05T08:26:59.177769: step 532, loss 0.839665, acc 0.90625\n",
      "2017-11-05T08:27:03.553380: step 533, loss 0.367338, acc 0.90625\n",
      "2017-11-05T08:27:07.921484: step 534, loss 1.17043, acc 0.75\n",
      "2017-11-05T08:27:12.222309: step 535, loss 0.276134, acc 0.9375\n",
      "2017-11-05T08:27:16.514358: step 536, loss 0.268937, acc 0.9375\n",
      "2017-11-05T08:27:20.904478: step 537, loss 0.448712, acc 0.84375\n",
      "2017-11-05T08:27:25.206535: step 538, loss 0.449831, acc 0.875\n",
      "2017-11-05T08:27:29.626675: step 539, loss 0.0971205, acc 0.9375\n",
      "2017-11-05T08:27:32.528515: step 540, loss 0.453148, acc 0.95\n",
      "2017-11-05T08:27:36.809557: step 541, loss 0.338714, acc 0.90625\n",
      "2017-11-05T08:27:41.010542: step 542, loss 0.323773, acc 0.90625\n",
      "2017-11-05T08:27:45.303593: step 543, loss 0.302443, acc 0.9375\n",
      "2017-11-05T08:27:50.308376: step 544, loss 0.100929, acc 0.96875\n",
      "2017-11-05T08:27:55.004222: step 545, loss 0.375258, acc 0.875\n",
      "2017-11-05T08:27:59.575470: step 546, loss 0.622894, acc 0.875\n",
      "2017-11-05T08:28:04.198377: step 547, loss 0.352468, acc 0.875\n",
      "2017-11-05T08:28:08.870126: step 548, loss 0.468108, acc 0.875\n",
      "2017-11-05T08:28:13.032181: step 549, loss 0.319412, acc 0.9375\n",
      "2017-11-05T08:28:17.315532: step 550, loss 0.257546, acc 0.90625\n",
      "2017-11-05T08:28:21.503044: step 551, loss 0.243485, acc 0.90625\n",
      "2017-11-05T08:28:25.968218: step 552, loss 0.327733, acc 0.90625\n",
      "2017-11-05T08:28:30.553619: step 553, loss 0.38257, acc 0.875\n",
      "2017-11-05T08:28:34.976831: step 554, loss 0.78659, acc 0.9375\n",
      "2017-11-05T08:28:39.483033: step 555, loss 0.621645, acc 0.90625\n",
      "2017-11-05T08:28:44.364313: step 556, loss 0.401223, acc 0.9375\n",
      "2017-11-05T08:28:49.255788: step 557, loss 0.623114, acc 0.875\n",
      "2017-11-05T08:28:53.377547: step 558, loss 0.207036, acc 0.90625\n",
      "2017-11-05T08:28:57.786680: step 559, loss 0.734392, acc 0.875\n",
      "2017-11-05T08:29:02.386948: step 560, loss 0.84524, acc 0.90625\n",
      "2017-11-05T08:29:07.727171: step 561, loss 0.696778, acc 0.90625\n",
      "2017-11-05T08:29:12.514109: step 562, loss 0.624937, acc 0.90625\n",
      "2017-11-05T08:29:17.167988: step 563, loss 0.0364851, acc 0.96875\n",
      "2017-11-05T08:29:21.717232: step 564, loss 0.212124, acc 0.9375\n",
      "2017-11-05T08:29:26.263967: step 565, loss 0.426694, acc 0.90625\n",
      "2017-11-05T08:29:30.858771: step 566, loss 0.455718, acc 0.875\n",
      "2017-11-05T08:29:35.439029: step 567, loss 0.471197, acc 0.90625\n",
      "2017-11-05T08:29:40.126360: step 568, loss 0.344091, acc 0.96875\n",
      "2017-11-05T08:29:44.696118: step 569, loss 0.975917, acc 0.84375\n",
      "2017-11-05T08:29:49.313899: step 570, loss 0.601357, acc 0.8125\n",
      "2017-11-05T08:29:53.943689: step 571, loss 0.625147, acc 0.875\n",
      "2017-11-05T08:29:59.010564: step 572, loss 0.563526, acc 0.8125\n",
      "2017-11-05T08:30:04.117216: step 573, loss 0.332443, acc 0.90625\n",
      "2017-11-05T08:30:08.719988: step 574, loss 0.854802, acc 0.8125\n",
      "2017-11-05T08:30:13.329957: step 575, loss 0.438548, acc 0.875\n",
      "2017-11-05T08:30:16.156466: step 576, loss 0.689537, acc 0.85\n",
      "2017-11-05T08:30:20.597120: step 577, loss 0.234148, acc 0.9375\n",
      "2017-11-05T08:30:25.196048: step 578, loss 0.719193, acc 0.84375\n",
      "2017-11-05T08:30:30.889590: step 579, loss 0.0942878, acc 0.96875\n",
      "2017-11-05T08:30:36.383995: step 580, loss 0.206257, acc 0.90625\n",
      "2017-11-05T08:30:41.727792: step 581, loss 0.140754, acc 0.9375\n",
      "2017-11-05T08:30:47.121640: step 582, loss 0.267029, acc 0.9375\n",
      "2017-11-05T08:30:52.448424: step 583, loss 0.258386, acc 0.9375\n",
      "2017-11-05T08:30:57.840756: step 584, loss 0.21327, acc 0.90625\n",
      "2017-11-05T08:31:03.041952: step 585, loss 0.354336, acc 0.90625\n",
      "2017-11-05T08:31:07.765308: step 586, loss 0.177999, acc 0.90625\n",
      "2017-11-05T08:31:12.252002: step 587, loss 0.344279, acc 0.96875\n",
      "2017-11-05T08:31:16.754001: step 588, loss 0.188304, acc 0.90625\n",
      "2017-11-05T08:31:21.256214: step 589, loss 0.0314909, acc 0.96875\n",
      "2017-11-05T08:31:25.689864: step 590, loss 0.0926432, acc 0.96875\n",
      "2017-11-05T08:31:30.221086: step 591, loss 0.119081, acc 0.9375\n",
      "2017-11-05T08:31:34.764312: step 592, loss 0.233253, acc 0.9375\n",
      "2017-11-05T08:31:39.344155: step 593, loss 0.0463844, acc 0.96875\n",
      "2017-11-05T08:31:43.802511: step 594, loss 0.199468, acc 0.875\n",
      "2017-11-05T08:31:48.496078: step 595, loss 0.128304, acc 0.96875\n",
      "2017-11-05T08:31:53.274472: step 596, loss 0.0885635, acc 0.96875\n",
      "2017-11-05T08:31:57.780175: step 597, loss 0.402077, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T08:32:02.557419: step 598, loss 0.812617, acc 0.9375\n",
      "2017-11-05T08:32:07.161712: step 599, loss 0.749426, acc 0.78125\n",
      "2017-11-05T08:32:11.644397: step 600, loss 0.162897, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:32:14.623522: step 600, loss 1.42395, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-600\n",
      "\n",
      "2017-11-05T08:32:20.866480: step 601, loss 1.23343, acc 0.78125\n",
      "2017-11-05T08:32:25.256131: step 602, loss 0.28637, acc 0.9375\n",
      "2017-11-05T08:32:29.808866: step 603, loss 0.266101, acc 0.9375\n",
      "2017-11-05T08:32:34.400629: step 604, loss 0.0755544, acc 0.96875\n",
      "2017-11-05T08:32:39.008902: step 605, loss 0.136412, acc 0.9375\n",
      "2017-11-05T08:32:43.486089: step 606, loss 0.155558, acc 0.90625\n",
      "2017-11-05T08:32:48.073349: step 607, loss 0.416849, acc 0.90625\n",
      "2017-11-05T08:32:52.574547: step 608, loss 0.199543, acc 0.90625\n",
      "2017-11-05T08:32:57.209845: step 609, loss 0.273497, acc 0.84375\n",
      "2017-11-05T08:33:01.738063: step 610, loss 0.474363, acc 0.90625\n",
      "2017-11-05T08:33:06.197231: step 611, loss 0.430268, acc 0.84375\n",
      "2017-11-05T08:33:09.112809: step 612, loss 0.79533, acc 0.9\n",
      "2017-11-05T08:33:13.625021: step 613, loss 0.307313, acc 0.84375\n",
      "2017-11-05T08:33:18.208777: step 614, loss 0.287987, acc 0.90625\n",
      "2017-11-05T08:33:22.637425: step 615, loss 0.336235, acc 0.90625\n",
      "2017-11-05T08:33:27.142125: step 616, loss 0.528101, acc 0.84375\n",
      "2017-11-05T08:33:31.645826: step 617, loss 0.174035, acc 0.9375\n",
      "2017-11-05T08:33:36.170540: step 618, loss 0.0802131, acc 0.96875\n",
      "2017-11-05T08:33:40.664734: step 619, loss 0.464301, acc 0.875\n",
      "2017-11-05T08:33:45.069863: step 620, loss 0.560747, acc 0.90625\n",
      "2017-11-05T08:33:49.620097: step 621, loss 0.161779, acc 0.96875\n",
      "2017-11-05T08:33:54.147313: step 622, loss 0.0644916, acc 0.96875\n",
      "2017-11-05T08:33:58.648512: step 623, loss 0.315341, acc 0.90625\n",
      "2017-11-05T08:34:03.190239: step 624, loss 0.309683, acc 0.875\n",
      "2017-11-05T08:34:07.808520: step 625, loss 0.175207, acc 0.9375\n",
      "2017-11-05T08:34:12.560397: step 626, loss 0.163753, acc 0.96875\n",
      "2017-11-05T08:34:16.690333: step 627, loss 0.830938, acc 0.8125\n",
      "2017-11-05T08:34:21.023412: step 628, loss 0.199208, acc 0.96875\n",
      "2017-11-05T08:34:25.659484: step 629, loss 0.811541, acc 0.8125\n",
      "2017-11-05T08:34:30.314792: step 630, loss 0.443191, acc 0.84375\n",
      "2017-11-05T08:34:34.773315: step 631, loss 0.10071, acc 0.9375\n",
      "2017-11-05T08:34:38.878656: step 632, loss 0.59049, acc 0.90625\n",
      "2017-11-05T08:34:42.939007: step 633, loss 0.368336, acc 0.90625\n",
      "2017-11-05T08:34:47.014449: step 634, loss 0.562423, acc 0.90625\n",
      "2017-11-05T08:34:51.151928: step 635, loss 0.577741, acc 0.8125\n",
      "2017-11-05T08:34:55.413317: step 636, loss 0.135932, acc 0.9375\n",
      "2017-11-05T08:34:59.591189: step 637, loss 0.784742, acc 0.8125\n",
      "2017-11-05T08:35:03.650361: step 638, loss 0.276227, acc 0.9375\n",
      "2017-11-05T08:35:07.707240: step 639, loss 0.230665, acc 0.9375\n",
      "2017-11-05T08:35:11.780252: step 640, loss 0.0873535, acc 0.9375\n",
      "2017-11-05T08:35:15.947852: step 641, loss 0.187063, acc 0.90625\n",
      "2017-11-05T08:35:20.063756: step 642, loss 0.2001, acc 0.9375\n",
      "2017-11-05T08:35:24.104671: step 643, loss 0.277942, acc 0.90625\n",
      "2017-11-05T08:35:28.165697: step 644, loss 0.538287, acc 0.84375\n",
      "2017-11-05T08:35:32.224768: step 645, loss 0.328529, acc 0.875\n",
      "2017-11-05T08:35:36.275381: step 646, loss 0.584699, acc 0.84375\n",
      "2017-11-05T08:35:40.369141: step 647, loss 0.299707, acc 0.96875\n",
      "2017-11-05T08:35:43.027244: step 648, loss 0.0281069, acc 1\n",
      "2017-11-05T08:35:47.085401: step 649, loss 0.267402, acc 0.9375\n",
      "2017-11-05T08:35:51.146226: step 650, loss 0.553546, acc 0.84375\n",
      "2017-11-05T08:35:55.179785: step 651, loss 1.34812, acc 0.75\n",
      "2017-11-05T08:35:59.222657: step 652, loss 1.11018, acc 0.84375\n",
      "2017-11-05T08:36:03.298798: step 653, loss 0.0880709, acc 0.9375\n",
      "2017-11-05T08:36:07.367190: step 654, loss 0.584629, acc 0.90625\n",
      "2017-11-05T08:36:11.495632: step 655, loss 0.664805, acc 0.875\n",
      "2017-11-05T08:36:15.531500: step 656, loss 0.435144, acc 0.9375\n",
      "2017-11-05T08:36:19.562950: step 657, loss 0.428271, acc 0.875\n",
      "2017-11-05T08:36:23.600319: step 658, loss 0.289569, acc 0.9375\n",
      "2017-11-05T08:36:27.636578: step 659, loss 0.173244, acc 0.9375\n",
      "2017-11-05T08:36:31.703766: step 660, loss 0.503081, acc 0.84375\n",
      "2017-11-05T08:36:35.891242: step 661, loss 0.433761, acc 0.875\n",
      "2017-11-05T08:36:40.086222: step 662, loss 0.426491, acc 0.84375\n",
      "2017-11-05T08:36:44.274198: step 663, loss 0.384986, acc 0.90625\n",
      "2017-11-05T08:36:48.402631: step 664, loss 0.485487, acc 0.875\n",
      "2017-11-05T08:36:52.439500: step 665, loss 0.0057663, acc 1\n",
      "2017-11-05T08:36:56.448849: step 666, loss 0.561763, acc 0.875\n",
      "2017-11-05T08:37:00.468704: step 667, loss 0.353843, acc 0.875\n",
      "2017-11-05T08:37:04.531092: step 668, loss 0.344674, acc 0.90625\n",
      "2017-11-05T08:37:08.629504: step 669, loss 0.556603, acc 0.84375\n",
      "2017-11-05T08:37:12.814477: step 670, loss 0.347735, acc 0.9375\n",
      "2017-11-05T08:37:16.933404: step 671, loss 0.41041, acc 0.9375\n",
      "2017-11-05T08:37:20.996791: step 672, loss 0.143376, acc 0.9375\n",
      "2017-11-05T08:37:25.084195: step 673, loss 0.301939, acc 0.9375\n",
      "2017-11-05T08:37:29.197618: step 674, loss 0.604806, acc 0.8125\n",
      "2017-11-05T08:37:33.318546: step 675, loss 0.366574, acc 0.875\n",
      "2017-11-05T08:37:37.398445: step 676, loss 0.139876, acc 0.96875\n",
      "2017-11-05T08:37:41.458830: step 677, loss 0.315726, acc 0.9375\n",
      "2017-11-05T08:37:45.663317: step 678, loss 0.0848749, acc 0.96875\n",
      "2017-11-05T08:37:49.802259: step 679, loss 0.31127, acc 0.90625\n",
      "2017-11-05T08:37:53.936696: step 680, loss 0.013072, acc 1\n",
      "2017-11-05T08:37:57.995580: step 681, loss 0.466588, acc 0.90625\n",
      "2017-11-05T08:38:02.074978: step 682, loss 0.208587, acc 0.90625\n",
      "2017-11-05T08:38:06.331503: step 683, loss 0.626897, acc 0.84375\n",
      "2017-11-05T08:38:09.077454: step 684, loss 0.165654, acc 0.95\n",
      "2017-11-05T08:38:13.222399: step 685, loss 0.137254, acc 0.9375\n",
      "2017-11-05T08:38:17.246759: step 686, loss 0.105485, acc 0.9375\n",
      "2017-11-05T08:38:21.317151: step 687, loss 0.278726, acc 0.90625\n",
      "2017-11-05T08:38:25.451689: step 688, loss 0.53189, acc 0.84375\n",
      "2017-11-05T08:38:29.470945: step 689, loss 0.279149, acc 0.90625\n",
      "2017-11-05T08:38:33.591379: step 690, loss 0.370818, acc 0.84375\n",
      "2017-11-05T08:38:37.711807: step 691, loss 0.0433138, acc 0.96875\n",
      "2017-11-05T08:38:41.801212: step 692, loss 0.540471, acc 0.90625\n",
      "2017-11-05T08:38:45.828074: step 693, loss 0.635705, acc 0.8125\n",
      "2017-11-05T08:38:49.986529: step 694, loss 0.33377, acc 0.875\n",
      "2017-11-05T08:38:54.043911: step 695, loss 0.349762, acc 0.90625\n",
      "2017-11-05T08:38:58.169353: step 696, loss 0.421912, acc 0.875\n",
      "2017-11-05T08:39:02.288779: step 697, loss 0.328277, acc 0.9375\n",
      "2017-11-05T08:39:06.433725: step 698, loss 0.311506, acc 0.90625\n",
      "2017-11-05T08:39:10.561658: step 699, loss 0.105416, acc 0.96875\n",
      "2017-11-05T08:39:14.659678: step 700, loss 0.127035, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:39:17.408631: step 700, loss 1.13624, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-700\n",
      "\n",
      "2017-11-05T08:39:22.981583: step 701, loss 0.123242, acc 0.9375\n",
      "2017-11-05T08:39:27.095108: step 702, loss 0.403685, acc 0.90625\n",
      "2017-11-05T08:39:31.295093: step 703, loss 0.485149, acc 0.90625\n",
      "2017-11-05T08:39:35.479065: step 704, loss 0.0393, acc 1\n",
      "2017-11-05T08:39:39.566955: step 705, loss 0.0922932, acc 0.96875\n",
      "2017-11-05T08:39:43.617333: step 706, loss 0.434753, acc 0.90625\n",
      "2017-11-05T08:39:47.691228: step 707, loss 0.361337, acc 0.90625\n",
      "2017-11-05T08:39:51.868696: step 708, loss 0.254864, acc 0.9375\n",
      "2017-11-05T08:39:56.153240: step 709, loss 0.0414023, acc 0.96875\n",
      "2017-11-05T08:40:00.389075: step 710, loss 0.0132013, acc 1\n",
      "2017-11-05T08:40:04.600066: step 711, loss 0.210967, acc 0.96875\n",
      "2017-11-05T08:40:08.747513: step 712, loss 0.775655, acc 0.84375\n",
      "2017-11-05T08:40:12.871944: step 713, loss 0.627666, acc 0.84375\n",
      "2017-11-05T08:40:17.038905: step 714, loss 0.0969569, acc 0.96875\n",
      "2017-11-05T08:40:21.147323: step 715, loss 0.250123, acc 0.9375\n",
      "2017-11-05T08:40:25.224220: step 716, loss 0.205807, acc 0.90625\n",
      "2017-11-05T08:40:29.329137: step 717, loss 0.685373, acc 0.875\n",
      "2017-11-05T08:40:33.540630: step 718, loss 0.354971, acc 0.90625\n",
      "2017-11-05T08:40:37.757126: step 719, loss 0.0871514, acc 0.96875\n",
      "2017-11-05T08:40:40.438031: step 720, loss 0.208315, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T08:40:44.630510: step 721, loss 0.436037, acc 0.84375\n",
      "2017-11-05T08:40:48.694398: step 722, loss 0.10029, acc 0.96875\n",
      "2017-11-05T08:40:52.699243: step 723, loss 0.220421, acc 0.9375\n",
      "2017-11-05T08:40:56.710093: step 724, loss 0.0883515, acc 0.96875\n",
      "2017-11-05T08:41:00.710936: step 725, loss 0.296503, acc 0.875\n",
      "2017-11-05T08:41:04.777325: step 726, loss 0.351976, acc 0.90625\n",
      "2017-11-05T08:41:08.788675: step 727, loss 0.233204, acc 0.875\n",
      "2017-11-05T08:41:13.878792: step 728, loss 0.0363384, acc 0.96875\n",
      "2017-11-05T08:41:18.203865: step 729, loss 0.133794, acc 0.96875\n",
      "2017-11-05T08:41:22.380332: step 730, loss 0.352268, acc 0.9375\n",
      "2017-11-05T08:41:26.506264: step 731, loss 0.484179, acc 0.90625\n",
      "2017-11-05T08:41:30.554140: step 732, loss 0.305992, acc 0.84375\n",
      "2017-11-05T08:41:34.567992: step 733, loss 0.561406, acc 0.875\n",
      "2017-11-05T08:41:38.645389: step 734, loss 0.19219, acc 0.9375\n",
      "2017-11-05T08:41:42.696267: step 735, loss 0.24527, acc 0.9375\n",
      "2017-11-05T08:41:46.769161: step 736, loss 0.575876, acc 0.875\n",
      "2017-11-05T08:41:50.921612: step 737, loss 0.553217, acc 0.84375\n",
      "2017-11-05T08:41:55.288215: step 738, loss 0.176069, acc 0.90625\n",
      "2017-11-05T08:41:59.458178: step 739, loss 0.362303, acc 0.9375\n",
      "2017-11-05T08:42:03.621636: step 740, loss 0.207268, acc 0.875\n",
      "2017-11-05T08:42:07.788097: step 741, loss 0.105158, acc 0.96875\n",
      "2017-11-05T08:42:11.926537: step 742, loss 0.707867, acc 0.84375\n",
      "2017-11-05T08:42:16.024449: step 743, loss 0.696107, acc 0.78125\n",
      "2017-11-05T08:42:20.104848: step 744, loss 0.339394, acc 0.9375\n",
      "2017-11-05T08:42:24.194754: step 745, loss 0.350306, acc 0.84375\n",
      "2017-11-05T08:42:28.315682: step 746, loss 0.585222, acc 0.84375\n",
      "2017-11-05T08:42:32.455178: step 747, loss 0.535203, acc 0.84375\n",
      "2017-11-05T08:42:36.603625: step 748, loss 0.452455, acc 0.875\n",
      "2017-11-05T08:42:40.651001: step 749, loss 0.286424, acc 0.875\n",
      "2017-11-05T08:42:44.805453: step 750, loss 0.149738, acc 0.96875\n",
      "2017-11-05T08:42:48.855331: step 751, loss 0.274083, acc 0.90625\n",
      "2017-11-05T08:42:52.944956: step 752, loss 0.49283, acc 0.90625\n",
      "2017-11-05T08:42:57.109915: step 753, loss 0.347268, acc 0.9375\n",
      "2017-11-05T08:43:01.164295: step 754, loss 0.216856, acc 0.90625\n",
      "2017-11-05T08:43:05.264229: step 755, loss 0.273015, acc 0.90625\n",
      "2017-11-05T08:43:07.880588: step 756, loss 0.574019, acc 0.75\n",
      "2017-11-05T08:43:11.896941: step 757, loss 0.235746, acc 0.90625\n",
      "2017-11-05T08:43:15.916298: step 758, loss 0.546153, acc 0.875\n",
      "2017-11-05T08:43:19.966675: step 759, loss 0.0280945, acc 1\n",
      "2017-11-05T08:43:24.024681: step 760, loss 0.216541, acc 0.9375\n",
      "2017-11-05T08:43:28.391782: step 761, loss 0.223096, acc 0.90625\n",
      "2017-11-05T08:43:32.498200: step 762, loss 0.326427, acc 0.9375\n",
      "2017-11-05T08:43:36.543074: step 763, loss 0.178961, acc 0.96875\n",
      "2017-11-05T08:43:40.694023: step 764, loss 0.357647, acc 0.875\n",
      "2017-11-05T08:43:44.766918: step 765, loss 0.15026, acc 0.96875\n",
      "2017-11-05T08:43:48.809790: step 766, loss 0.301755, acc 0.90625\n",
      "2017-11-05T08:43:52.816138: step 767, loss 0.362173, acc 0.9375\n",
      "2017-11-05T08:43:56.855007: step 768, loss 0.235553, acc 0.875\n",
      "2017-11-05T08:44:00.885370: step 769, loss 0.0402408, acc 0.96875\n",
      "2017-11-05T08:44:04.881216: step 770, loss 0.343462, acc 0.9375\n",
      "2017-11-05T08:44:08.906803: step 771, loss 0.132726, acc 0.90625\n",
      "2017-11-05T08:44:12.907646: step 772, loss 0.198506, acc 0.9375\n",
      "2017-11-05T08:44:17.024572: step 773, loss 0.0737745, acc 0.9375\n",
      "2017-11-05T08:44:21.147001: step 774, loss 0.34666, acc 0.90625\n",
      "2017-11-05T08:44:25.282439: step 775, loss 0.127758, acc 0.90625\n",
      "2017-11-05T08:44:29.280780: step 776, loss 0.113171, acc 0.96875\n",
      "2017-11-05T08:44:33.309643: step 777, loss 0.160226, acc 0.96875\n",
      "2017-11-05T08:44:37.396046: step 778, loss 0.371536, acc 0.875\n",
      "2017-11-05T08:44:41.463437: step 779, loss 0.0134157, acc 1\n",
      "2017-11-05T08:44:45.597388: step 780, loss 0.34848, acc 0.90625\n",
      "2017-11-05T08:44:49.743834: step 781, loss 0.415631, acc 0.84375\n",
      "2017-11-05T08:44:53.942818: step 782, loss 0.171789, acc 0.875\n",
      "2017-11-05T08:44:57.893126: step 783, loss 0.135015, acc 0.9375\n",
      "2017-11-05T08:45:02.000043: step 784, loss 0.060041, acc 0.96875\n",
      "2017-11-05T08:45:06.102591: step 785, loss 0.180301, acc 0.96875\n",
      "2017-11-05T08:45:10.205091: step 786, loss 0.132067, acc 0.96875\n",
      "2017-11-05T08:45:14.275483: step 787, loss 0.464644, acc 0.84375\n",
      "2017-11-05T08:45:18.283331: step 788, loss 0.235756, acc 0.90625\n",
      "2017-11-05T08:45:22.272164: step 789, loss 0.0355641, acc 0.96875\n",
      "2017-11-05T08:45:26.311535: step 790, loss 0.804868, acc 0.78125\n",
      "2017-11-05T08:45:30.436966: step 791, loss 0.0664154, acc 0.96875\n",
      "2017-11-05T08:45:33.050323: step 792, loss 0.473418, acc 0.9\n",
      "2017-11-05T08:45:37.033653: step 793, loss 0.53723, acc 0.8125\n",
      "2017-11-05T08:45:41.101544: step 794, loss 0.0763282, acc 0.9375\n",
      "2017-11-05T08:45:45.202958: step 795, loss 0.591071, acc 0.8125\n",
      "2017-11-05T08:45:49.335469: step 796, loss 0.0761534, acc 0.96875\n",
      "2017-11-05T08:45:53.454896: step 797, loss 0.251025, acc 0.90625\n",
      "2017-11-05T08:45:57.554809: step 798, loss 0.0473021, acc 0.96875\n",
      "2017-11-05T08:46:01.644215: step 799, loss 0.344106, acc 0.96875\n",
      "2017-11-05T08:46:05.810424: step 800, loss 0.355116, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:46:08.495453: step 800, loss 1.02036, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-800\n",
      "\n",
      "2017-11-05T08:46:14.074975: step 801, loss 0.420754, acc 0.875\n",
      "2017-11-05T08:46:18.129857: step 802, loss 0.27143, acc 0.90625\n",
      "2017-11-05T08:46:22.172729: step 803, loss 0.435686, acc 0.90625\n",
      "2017-11-05T08:46:26.209598: step 804, loss 0.173325, acc 0.9375\n",
      "2017-11-05T08:46:30.260476: step 805, loss 0.145725, acc 0.9375\n",
      "2017-11-05T08:46:34.505992: step 806, loss 0.035026, acc 0.96875\n",
      "2017-11-05T08:46:38.710480: step 807, loss 0.011713, acc 1\n",
      "2017-11-05T08:46:42.784638: step 808, loss 0.295527, acc 0.8125\n",
      "2017-11-05T08:46:46.859034: step 809, loss 0.399849, acc 0.90625\n",
      "2017-11-05T08:46:50.948636: step 810, loss 0.0733938, acc 0.96875\n",
      "2017-11-05T08:46:55.092712: step 811, loss 0.372337, acc 0.875\n",
      "2017-11-05T08:46:59.147278: step 812, loss 0.366387, acc 0.875\n",
      "2017-11-05T08:47:03.200658: step 813, loss 0.0467537, acc 1\n",
      "2017-11-05T08:47:07.310273: step 814, loss 0.591238, acc 0.90625\n",
      "2017-11-05T08:47:11.345640: step 815, loss 0.806, acc 0.8125\n",
      "2017-11-05T08:47:15.404023: step 816, loss 0.0699297, acc 0.96875\n",
      "2017-11-05T08:47:19.538462: step 817, loss 0.684756, acc 0.75\n",
      "2017-11-05T08:47:23.690413: step 818, loss 0.479425, acc 0.875\n",
      "2017-11-05T08:47:27.782319: step 819, loss 0.463879, acc 0.875\n",
      "2017-11-05T08:47:31.816185: step 820, loss 0.143604, acc 0.96875\n",
      "2017-11-05T08:47:35.857057: step 821, loss 0.151444, acc 0.9375\n",
      "2017-11-05T08:47:39.912938: step 822, loss 0.0888213, acc 0.9375\n",
      "2017-11-05T08:47:43.969821: step 823, loss 0.580628, acc 0.875\n",
      "2017-11-05T08:47:48.097754: step 824, loss 0.394664, acc 0.96875\n",
      "2017-11-05T08:47:52.220183: step 825, loss 0.255697, acc 0.90625\n",
      "2017-11-05T08:47:56.380139: step 826, loss 0.200655, acc 0.90625\n",
      "2017-11-05T08:48:00.484556: step 827, loss 0.561262, acc 0.75\n",
      "2017-11-05T08:48:03.135939: step 828, loss 0.256042, acc 0.85\n",
      "2017-11-05T08:48:07.166803: step 829, loss 0.334553, acc 0.875\n",
      "2017-11-05T08:48:11.351276: step 830, loss 0.0403849, acc 0.96875\n",
      "2017-11-05T08:48:15.442183: step 831, loss 0.311256, acc 0.9375\n",
      "2017-11-05T08:48:19.456536: step 832, loss 0.109204, acc 0.90625\n",
      "2017-11-05T08:48:23.668454: step 833, loss 0.343654, acc 0.84375\n",
      "2017-11-05T08:48:27.865936: step 834, loss 0.134025, acc 0.96875\n",
      "2017-11-05T08:48:31.932826: step 835, loss 0.245863, acc 0.90625\n",
      "2017-11-05T08:48:36.105301: step 836, loss 0.0569023, acc 0.96875\n",
      "2017-11-05T08:48:40.356322: step 837, loss 0.38666, acc 0.875\n",
      "2017-11-05T08:48:44.454233: step 838, loss 0.0844387, acc 0.96875\n",
      "2017-11-05T08:48:48.556648: step 839, loss 0.22378, acc 0.90625\n",
      "2017-11-05T08:48:52.711601: step 840, loss 0.175427, acc 0.9375\n",
      "2017-11-05T08:48:56.804008: step 841, loss 0.30101, acc 0.9375\n",
      "2017-11-05T08:49:00.929440: step 842, loss 0.604361, acc 0.875\n",
      "2017-11-05T08:49:04.981819: step 843, loss 0.0500719, acc 0.96875\n",
      "2017-11-05T08:49:09.123261: step 844, loss 0.558355, acc 0.875\n",
      "2017-11-05T08:49:13.277213: step 845, loss 0.406476, acc 0.84375\n",
      "2017-11-05T08:49:17.392637: step 846, loss 0.424628, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T08:49:21.601127: step 847, loss 0.278018, acc 0.875\n",
      "2017-11-05T08:49:25.723557: step 848, loss 0.70637, acc 0.78125\n",
      "2017-11-05T08:49:29.969574: step 849, loss 0.19974, acc 0.96875\n",
      "2017-11-05T08:49:34.130030: step 850, loss 0.127568, acc 0.9375\n",
      "2017-11-05T08:49:38.303996: step 851, loss 0.116275, acc 0.96875\n",
      "2017-11-05T08:49:42.426425: step 852, loss 0.268356, acc 0.84375\n",
      "2017-11-05T08:49:46.665937: step 853, loss 0.15555, acc 0.9375\n",
      "2017-11-05T08:49:50.888939: step 854, loss 0.323755, acc 0.90625\n",
      "2017-11-05T08:49:55.372145: step 855, loss 0.385234, acc 0.875\n",
      "2017-11-05T08:49:59.558119: step 856, loss 0.643276, acc 0.78125\n",
      "2017-11-05T08:50:03.885186: step 857, loss 0.123061, acc 0.96875\n",
      "2017-11-05T08:50:07.946572: step 858, loss 0.384408, acc 0.875\n",
      "2017-11-05T08:50:12.182582: step 859, loss 0.144859, acc 0.90625\n",
      "2017-11-05T08:50:16.319581: step 860, loss 0.258137, acc 0.9375\n",
      "2017-11-05T08:50:20.369104: step 861, loss 0.653717, acc 0.84375\n",
      "2017-11-05T08:50:24.447502: step 862, loss 0.127035, acc 0.9375\n",
      "2017-11-05T08:50:28.599453: step 863, loss 0.361955, acc 0.875\n",
      "2017-11-05T08:50:31.333395: step 864, loss 0.0996812, acc 0.95\n",
      "2017-11-05T08:50:35.540384: step 865, loss 0.13091, acc 0.9375\n",
      "2017-11-05T08:50:39.669318: step 866, loss 0.320273, acc 0.90625\n",
      "2017-11-05T08:50:43.788245: step 867, loss 0.0914503, acc 0.96875\n",
      "2017-11-05T08:50:47.870646: step 868, loss 0.199374, acc 0.875\n",
      "2017-11-05T08:50:51.954047: step 869, loss 0.316285, acc 0.90625\n",
      "2017-11-05T08:50:56.037449: step 870, loss 0.286837, acc 0.90625\n",
      "2017-11-05T08:51:00.173387: step 871, loss 0.258104, acc 0.90625\n",
      "2017-11-05T08:51:04.236774: step 872, loss 0.230087, acc 0.90625\n",
      "2017-11-05T08:51:08.372713: step 873, loss 0.374902, acc 0.90625\n",
      "2017-11-05T08:51:12.485328: step 874, loss 0.110793, acc 0.875\n",
      "2017-11-05T08:51:16.550717: step 875, loss 0.206409, acc 0.90625\n",
      "2017-11-05T08:51:20.638621: step 876, loss 0.356325, acc 0.84375\n",
      "2017-11-05T08:51:24.753844: step 877, loss 0.283284, acc 0.90625\n",
      "2017-11-05T08:51:28.825736: step 878, loss 0.158733, acc 0.90625\n",
      "2017-11-05T08:51:32.881118: step 879, loss 0.285923, acc 0.90625\n",
      "2017-11-05T08:51:36.933497: step 880, loss 0.388573, acc 0.875\n",
      "2017-11-05T08:51:41.019401: step 881, loss 0.531312, acc 0.875\n",
      "2017-11-05T08:51:45.052266: step 882, loss 0.272463, acc 0.90625\n",
      "2017-11-05T08:51:49.230736: step 883, loss 0.184131, acc 0.90625\n",
      "2017-11-05T08:51:53.313290: step 884, loss 0.0350542, acc 0.96875\n",
      "2017-11-05T08:51:57.471245: step 885, loss 0.157676, acc 0.9375\n",
      "2017-11-05T08:52:01.615704: step 886, loss 0.263822, acc 0.9375\n",
      "2017-11-05T08:52:05.724624: step 887, loss 0.360088, acc 0.84375\n",
      "2017-11-05T08:52:09.830671: step 888, loss 0.101246, acc 0.96875\n",
      "2017-11-05T08:52:13.967110: step 889, loss 0.1042, acc 0.90625\n",
      "2017-11-05T08:52:18.078531: step 890, loss 0.031591, acc 1\n",
      "2017-11-05T08:52:22.177477: step 891, loss 0.176915, acc 0.9375\n",
      "2017-11-05T08:52:26.272887: step 892, loss 0.302822, acc 0.90625\n",
      "2017-11-05T08:52:30.323765: step 893, loss 0.386188, acc 0.90625\n",
      "2017-11-05T08:52:34.468711: step 894, loss 0.369777, acc 0.90625\n",
      "2017-11-05T08:52:38.583134: step 895, loss 0.264757, acc 0.90625\n",
      "2017-11-05T08:52:42.706563: step 896, loss 0.291834, acc 0.90625\n",
      "2017-11-05T08:52:46.806477: step 897, loss 0.324457, acc 0.9375\n",
      "2017-11-05T08:52:50.879871: step 898, loss 0.210619, acc 0.875\n",
      "2017-11-05T08:52:55.001800: step 899, loss 0.18486, acc 0.9375\n",
      "2017-11-05T08:52:57.570125: step 900, loss 0.0226992, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:53:00.291058: step 900, loss 0.992767, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-900\n",
      "\n",
      "2017-11-05T08:53:06.041095: step 901, loss 0.179932, acc 0.90625\n",
      "2017-11-05T08:53:10.057697: step 902, loss 0.0742752, acc 0.96875\n",
      "2017-11-05T08:53:14.141097: step 903, loss 0.279829, acc 0.90625\n",
      "2017-11-05T08:53:18.186971: step 904, loss 0.0501419, acc 0.96875\n",
      "2017-11-05T08:53:22.316293: step 905, loss 0.0722973, acc 0.96875\n",
      "2017-11-05T08:53:26.486756: step 906, loss 0.567183, acc 0.84375\n",
      "2017-11-05T08:53:30.630701: step 907, loss 0.350356, acc 0.90625\n",
      "2017-11-05T08:53:34.715603: step 908, loss 0.27351, acc 0.90625\n",
      "2017-11-05T08:53:38.816517: step 909, loss 0.282812, acc 0.875\n",
      "2017-11-05T08:53:42.906923: step 910, loss 0.110084, acc 0.9375\n",
      "2017-11-05T08:53:46.963818: step 911, loss 0.320548, acc 0.90625\n",
      "2017-11-05T08:53:51.015698: step 912, loss 0.611245, acc 0.84375\n",
      "2017-11-05T08:53:55.059087: step 913, loss 0.444991, acc 0.90625\n",
      "2017-11-05T08:53:59.125260: step 914, loss 0.190003, acc 0.9375\n",
      "2017-11-05T08:54:03.267857: step 915, loss 0.217459, acc 0.9375\n",
      "2017-11-05T08:54:07.754045: step 916, loss 0.458572, acc 0.84375\n",
      "2017-11-05T08:54:12.556457: step 917, loss 0.395524, acc 0.875\n",
      "2017-11-05T08:54:16.860529: step 918, loss 0.219663, acc 0.875\n",
      "2017-11-05T08:54:20.984959: step 919, loss 0.0487987, acc 0.96875\n",
      "2017-11-05T08:54:25.045844: step 920, loss 0.0891757, acc 0.9375\n",
      "2017-11-05T08:54:29.127762: step 921, loss 0.481238, acc 0.875\n",
      "2017-11-05T08:54:33.235861: step 922, loss 0.156565, acc 0.9375\n",
      "2017-11-05T08:54:37.335775: step 923, loss 0.33381, acc 0.90625\n",
      "2017-11-05T08:54:41.416174: step 924, loss 0.0706459, acc 1\n",
      "2017-11-05T08:54:45.451041: step 925, loss 0.265476, acc 0.84375\n",
      "2017-11-05T08:54:49.486008: step 926, loss 0.502537, acc 0.75\n",
      "2017-11-05T08:54:53.508867: step 927, loss 0.210699, acc 0.90625\n",
      "2017-11-05T08:54:57.578759: step 928, loss 0.49777, acc 0.84375\n",
      "2017-11-05T08:55:01.645648: step 929, loss 0.107709, acc 0.9375\n",
      "2017-11-05T08:55:05.732553: step 930, loss 0.325046, acc 0.875\n",
      "2017-11-05T08:55:09.764274: step 931, loss 0.543039, acc 0.90625\n",
      "2017-11-05T08:55:13.837670: step 932, loss 0.262932, acc 0.90625\n",
      "2017-11-05T08:55:17.950881: step 933, loss 0.188793, acc 0.90625\n",
      "2017-11-05T08:55:22.023775: step 934, loss 0.641446, acc 0.875\n",
      "2017-11-05T08:55:26.141201: step 935, loss 0.350524, acc 0.9375\n",
      "2017-11-05T08:55:28.834114: step 936, loss 0.00947556, acc 1\n",
      "2017-11-05T08:55:32.988571: step 937, loss 0.228161, acc 0.96875\n",
      "2017-11-05T08:55:37.035632: step 938, loss 0.0787524, acc 0.9375\n",
      "2017-11-05T08:55:41.113681: step 939, loss 0.246638, acc 0.96875\n",
      "2017-11-05T08:55:45.226104: step 940, loss 0.169098, acc 0.90625\n",
      "2017-11-05T08:55:49.337525: step 941, loss 0.19091, acc 0.90625\n",
      "2017-11-05T08:55:53.387903: step 942, loss 0.432017, acc 0.84375\n",
      "2017-11-05T08:55:57.440788: step 943, loss 0.060839, acc 0.96875\n",
      "2017-11-05T08:56:01.521689: step 944, loss 0.258548, acc 0.90625\n",
      "2017-11-05T08:56:05.622060: step 945, loss 0.368834, acc 0.90625\n",
      "2017-11-05T08:56:09.748241: step 946, loss 0.622973, acc 0.78125\n",
      "2017-11-05T08:56:13.782609: step 947, loss 0.398442, acc 0.9375\n",
      "2017-11-05T08:56:17.790456: step 948, loss 0.314642, acc 0.875\n",
      "2017-11-05T08:56:21.933399: step 949, loss 0.0733242, acc 0.96875\n",
      "2017-11-05T08:56:26.046323: step 950, loss 0.50048, acc 0.875\n",
      "2017-11-05T08:56:30.176185: step 951, loss 0.0722386, acc 0.9375\n",
      "2017-11-05T08:56:34.442395: step 952, loss 0.152324, acc 0.90625\n",
      "2017-11-05T08:56:38.650469: step 953, loss 0.370097, acc 0.8125\n",
      "2017-11-05T08:56:42.775900: step 954, loss 0.304062, acc 0.90625\n",
      "2017-11-05T08:56:46.824277: step 955, loss 0.476497, acc 0.90625\n",
      "2017-11-05T08:56:50.896171: step 956, loss 0.073497, acc 0.9375\n",
      "2017-11-05T08:56:54.946048: step 957, loss 0.0468183, acc 0.96875\n",
      "2017-11-05T08:56:58.954395: step 958, loss 0.207819, acc 0.9375\n",
      "2017-11-05T08:57:03.079454: step 959, loss 0.0197355, acc 1\n",
      "2017-11-05T08:57:07.199881: step 960, loss 0.112946, acc 0.9375\n",
      "2017-11-05T08:57:11.319809: step 961, loss 0.731476, acc 0.84375\n",
      "2017-11-05T08:57:15.400208: step 962, loss 0.697715, acc 0.8125\n",
      "2017-11-05T08:57:19.576176: step 963, loss 0.143072, acc 0.96875\n",
      "2017-11-05T08:57:23.732629: step 964, loss 0.568904, acc 0.8125\n",
      "2017-11-05T08:57:27.834543: step 965, loss 0.416872, acc 0.875\n",
      "2017-11-05T08:57:32.021018: step 966, loss 0.313004, acc 0.90625\n",
      "2017-11-05T08:57:36.158458: step 967, loss 0.228466, acc 0.9375\n",
      "2017-11-05T08:57:40.316922: step 968, loss 0.600352, acc 0.75\n",
      "2017-11-05T08:57:44.395822: step 969, loss 0.347886, acc 0.875\n",
      "2017-11-05T08:57:48.524629: step 970, loss 0.348987, acc 0.875\n",
      "2017-11-05T08:57:52.623041: step 971, loss 0.160778, acc 0.9375\n",
      "2017-11-05T08:57:55.359485: step 972, loss 0.163551, acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T08:57:59.460426: step 973, loss 0.0365545, acc 1\n",
      "2017-11-05T08:58:03.531819: step 974, loss 0.275257, acc 0.875\n",
      "2017-11-05T08:58:07.648244: step 975, loss 0.228589, acc 0.90625\n",
      "2017-11-05T08:58:11.658208: step 976, loss 0.221417, acc 0.90625\n",
      "2017-11-05T08:58:15.701081: step 977, loss 0.302444, acc 0.9375\n",
      "2017-11-05T08:58:19.787367: step 978, loss 0.0755198, acc 0.9375\n",
      "2017-11-05T08:58:23.994083: step 979, loss 0.148487, acc 0.96875\n",
      "2017-11-05T08:58:28.340672: step 980, loss 0.0529018, acc 0.96875\n",
      "2017-11-05T08:58:32.407061: step 981, loss 0.0583511, acc 1\n",
      "2017-11-05T08:58:36.545516: step 982, loss 0.105557, acc 0.96875\n",
      "2017-11-05T08:58:40.663942: step 983, loss 0.184011, acc 0.9375\n",
      "2017-11-05T08:58:44.740095: step 984, loss 0.0542309, acc 0.96875\n",
      "2017-11-05T08:58:48.760963: step 985, loss 0.175846, acc 0.9375\n",
      "2017-11-05T08:58:52.885590: step 986, loss 0.0693414, acc 0.96875\n",
      "2017-11-05T08:58:56.986703: step 987, loss 0.352022, acc 0.875\n",
      "2017-11-05T08:59:01.082113: step 988, loss 0.148642, acc 0.96875\n",
      "2017-11-05T08:59:05.144499: step 989, loss 0.111574, acc 0.9375\n",
      "2017-11-05T08:59:09.271932: step 990, loss 0.237805, acc 0.875\n",
      "2017-11-05T08:59:13.391859: step 991, loss 0.210557, acc 0.90625\n",
      "2017-11-05T08:59:17.523295: step 992, loss 0.177372, acc 0.9375\n",
      "2017-11-05T08:59:21.628712: step 993, loss 0.333367, acc 0.875\n",
      "2017-11-05T08:59:25.688568: step 994, loss 0.241971, acc 0.9375\n",
      "2017-11-05T08:59:29.735098: step 995, loss 0.219327, acc 0.90625\n",
      "2017-11-05T08:59:33.796229: step 996, loss 0.351429, acc 0.875\n",
      "2017-11-05T08:59:37.953026: step 997, loss 0.280708, acc 0.90625\n",
      "2017-11-05T08:59:42.063947: step 998, loss 0.143385, acc 0.9375\n",
      "2017-11-05T08:59:46.102317: step 999, loss 0.448011, acc 0.8125\n",
      "2017-11-05T08:59:50.183216: step 1000, loss 0.119648, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T08:59:52.943378: step 1000, loss 0.835204, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-05T08:59:58.218722: step 1001, loss 0.48039, acc 0.875\n",
      "2017-11-05T09:00:02.515949: step 1002, loss 0.373221, acc 0.875\n",
      "2017-11-05T09:00:06.614007: step 1003, loss 0.130963, acc 0.9375\n",
      "2017-11-05T09:00:10.712920: step 1004, loss 0.310204, acc 0.875\n",
      "2017-11-05T09:00:14.772688: step 1005, loss 0.216629, acc 0.9375\n",
      "2017-11-05T09:00:18.833153: step 1006, loss 0.245449, acc 0.9375\n",
      "2017-11-05T09:00:22.901043: step 1007, loss 0.167035, acc 0.9375\n",
      "2017-11-05T09:00:25.593956: step 1008, loss 0.352136, acc 0.8\n",
      "2017-11-05T09:00:29.699260: step 1009, loss 0.692298, acc 0.8125\n",
      "2017-11-05T09:00:33.912212: step 1010, loss 0.211559, acc 0.9375\n",
      "2017-11-05T09:00:38.003007: step 1011, loss 0.178928, acc 0.96875\n",
      "2017-11-05T09:00:42.037718: step 1012, loss 0.300239, acc 0.875\n",
      "2017-11-05T09:00:46.055308: step 1013, loss 0.157547, acc 0.9375\n",
      "2017-11-05T09:00:50.105284: step 1014, loss 0.189768, acc 0.90625\n",
      "2017-11-05T09:00:54.189687: step 1015, loss 0.367355, acc 0.90625\n",
      "2017-11-05T09:00:58.191030: step 1016, loss 0.139629, acc 0.9375\n",
      "2017-11-05T09:01:02.240424: step 1017, loss 0.39788, acc 0.875\n",
      "2017-11-05T09:01:06.432403: step 1018, loss 0.238629, acc 0.84375\n",
      "2017-11-05T09:01:10.497487: step 1019, loss 0.262845, acc 0.84375\n",
      "2017-11-05T09:01:14.562876: step 1020, loss 0.29514, acc 0.90625\n",
      "2017-11-05T09:01:18.669293: step 1021, loss 0.00997839, acc 1\n",
      "2017-11-05T09:01:22.758791: step 1022, loss 0.012586, acc 1\n",
      "2017-11-05T09:01:26.928254: step 1023, loss 0.431321, acc 0.875\n",
      "2017-11-05T09:01:30.982134: step 1024, loss 0.114894, acc 0.96875\n",
      "2017-11-05T09:01:35.108970: step 1025, loss 0.187897, acc 0.90625\n",
      "2017-11-05T09:01:39.161850: step 1026, loss 0.0423639, acc 0.96875\n",
      "2017-11-05T09:01:43.267190: step 1027, loss 0.27862, acc 0.875\n",
      "2017-11-05T09:01:47.424144: step 1028, loss 0.0742575, acc 0.96875\n",
      "2017-11-05T09:01:51.527585: step 1029, loss 0.0991149, acc 0.9375\n",
      "2017-11-05T09:01:55.624496: step 1030, loss 0.400828, acc 0.8125\n",
      "2017-11-05T09:01:59.816975: step 1031, loss 0.11186, acc 0.9375\n",
      "2017-11-05T09:02:04.035973: step 1032, loss 0.36905, acc 0.90625\n",
      "2017-11-05T09:02:08.172412: step 1033, loss 0.101312, acc 0.9375\n",
      "2017-11-05T09:02:12.260317: step 1034, loss 0.0947921, acc 0.96875\n",
      "2017-11-05T09:02:16.507835: step 1035, loss 0.122435, acc 0.96875\n",
      "2017-11-05T09:02:20.540700: step 1036, loss 0.402993, acc 0.90625\n",
      "2017-11-05T09:02:24.679641: step 1037, loss 0.317551, acc 0.9375\n",
      "2017-11-05T09:02:28.745004: step 1038, loss 0.335188, acc 0.84375\n",
      "2017-11-05T09:02:32.842302: step 1039, loss 0.399266, acc 0.84375\n",
      "2017-11-05T09:02:36.975239: step 1040, loss 0.39859, acc 0.90625\n",
      "2017-11-05T09:02:41.061838: step 1041, loss 0.378147, acc 0.875\n",
      "2017-11-05T09:02:45.133741: step 1042, loss 0.34182, acc 0.875\n",
      "2017-11-05T09:02:49.262175: step 1043, loss 0.0731503, acc 0.9375\n",
      "2017-11-05T09:02:51.884926: step 1044, loss 0.0662785, acc 0.95\n",
      "2017-11-05T09:02:55.975027: step 1045, loss 0.236846, acc 0.9375\n",
      "2017-11-05T09:03:00.092951: step 1046, loss 0.225592, acc 0.90625\n",
      "2017-11-05T09:03:04.237897: step 1047, loss 0.272328, acc 0.84375\n",
      "2017-11-05T09:03:08.280278: step 1048, loss 0.374125, acc 0.875\n",
      "2017-11-05T09:03:12.337869: step 1049, loss 0.18426, acc 0.9375\n",
      "2017-11-05T09:03:16.428932: step 1050, loss 0.140048, acc 0.9375\n",
      "2017-11-05T09:03:20.517176: step 1051, loss 0.101584, acc 0.9375\n",
      "2017-11-05T09:03:24.802721: step 1052, loss 0.332061, acc 0.875\n",
      "2017-11-05T09:03:29.010211: step 1053, loss 0.127894, acc 0.90625\n",
      "2017-11-05T09:03:33.069095: step 1054, loss 0.271765, acc 0.90625\n",
      "2017-11-05T09:03:37.170394: step 1055, loss 0.285282, acc 0.9375\n",
      "2017-11-05T09:03:41.283369: step 1056, loss 0.158325, acc 0.9375\n",
      "2017-11-05T09:03:45.339252: step 1057, loss 0.101018, acc 0.96875\n",
      "2017-11-05T09:03:49.560251: step 1058, loss 0.114927, acc 0.96875\n",
      "2017-11-05T09:03:53.645095: step 1059, loss 0.309956, acc 0.90625\n",
      "2017-11-05T09:03:57.691467: step 1060, loss 0.314752, acc 0.9375\n",
      "2017-11-05T09:04:01.824403: step 1061, loss 0.45161, acc 0.875\n",
      "2017-11-05T09:04:05.864752: step 1062, loss 0.173791, acc 0.9375\n",
      "2017-11-05T09:04:09.982178: step 1063, loss 0.0823862, acc 0.96875\n",
      "2017-11-05T09:04:14.118617: step 1064, loss 0.159433, acc 0.9375\n",
      "2017-11-05T09:04:18.161490: step 1065, loss 0.137234, acc 0.96875\n",
      "2017-11-05T09:04:22.285920: step 1066, loss 0.112925, acc 0.9375\n",
      "2017-11-05T09:04:26.411852: step 1067, loss 0.309553, acc 0.9375\n",
      "2017-11-05T09:04:30.479242: step 1068, loss 0.602451, acc 0.8125\n",
      "2017-11-05T09:04:34.627692: step 1069, loss 0.253102, acc 0.90625\n",
      "2017-11-05T09:04:38.724531: step 1070, loss 0.253781, acc 0.90625\n",
      "2017-11-05T09:04:42.775409: step 1071, loss 0.131271, acc 0.9375\n",
      "2017-11-05T09:04:46.866816: step 1072, loss 0.120216, acc 0.96875\n",
      "2017-11-05T09:04:50.949217: step 1073, loss 0.241763, acc 0.9375\n",
      "2017-11-05T09:04:54.970708: step 1074, loss 0.0914249, acc 0.9375\n",
      "2017-11-05T09:04:59.102177: step 1075, loss 0.375937, acc 0.9375\n",
      "2017-11-05T09:05:03.203591: step 1076, loss 0.301384, acc 0.84375\n",
      "2017-11-05T09:05:07.327521: step 1077, loss 0.573181, acc 0.78125\n",
      "2017-11-05T09:05:11.453846: step 1078, loss 0.402857, acc 0.90625\n",
      "2017-11-05T09:05:15.500221: step 1079, loss 0.322498, acc 0.90625\n",
      "2017-11-05T09:05:18.125280: step 1080, loss 0.335556, acc 0.9\n",
      "2017-11-05T09:05:22.112613: step 1081, loss 0.321337, acc 0.875\n",
      "2017-11-05T09:05:26.224559: step 1082, loss 0.0570263, acc 0.96875\n",
      "2017-11-05T09:05:30.263929: step 1083, loss 0.146752, acc 0.90625\n",
      "2017-11-05T09:05:34.365844: step 1084, loss 0.501498, acc 0.84375\n",
      "2017-11-05T09:05:38.452248: step 1085, loss 0.0579682, acc 0.96875\n",
      "2017-11-05T09:05:42.572175: step 1086, loss 0.386781, acc 0.875\n",
      "2017-11-05T09:05:46.682596: step 1087, loss 0.339721, acc 0.9375\n",
      "2017-11-05T09:05:50.835046: step 1088, loss 0.646225, acc 0.84375\n",
      "2017-11-05T09:05:54.886135: step 1089, loss 0.381944, acc 0.90625\n",
      "2017-11-05T09:05:58.985048: step 1090, loss 0.43704, acc 0.875\n",
      "2017-11-05T09:06:03.070951: step 1091, loss 0.17462, acc 0.96875\n",
      "2017-11-05T09:06:07.191379: step 1092, loss 0.0934696, acc 0.9375\n",
      "2017-11-05T09:06:11.279784: step 1093, loss 0.169077, acc 0.9375\n",
      "2017-11-05T09:06:15.448246: step 1094, loss 0.243436, acc 0.90625\n",
      "2017-11-05T09:06:19.536651: step 1095, loss 0.194957, acc 0.875\n",
      "2017-11-05T09:06:23.688108: step 1096, loss 0.10823, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T09:06:27.822045: step 1097, loss 0.1813, acc 0.90625\n",
      "2017-11-05T09:06:31.845916: step 1098, loss 0.442747, acc 0.84375\n",
      "2017-11-05T09:06:36.108945: step 1099, loss 0.219892, acc 0.90625\n",
      "2017-11-05T09:06:40.218865: step 1100, loss 0.461198, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:06:42.879756: step 1100, loss 0.84454, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-05T09:06:48.330171: step 1101, loss 0.0357626, acc 1\n",
      "2017-11-05T09:06:52.457489: step 1102, loss 0.438158, acc 0.90625\n",
      "2017-11-05T09:06:56.514259: step 1103, loss 0.0919496, acc 0.9375\n",
      "2017-11-05T09:07:00.634687: step 1104, loss 0.163145, acc 0.9375\n",
      "2017-11-05T09:07:04.769624: step 1105, loss 0.109832, acc 0.9375\n",
      "2017-11-05T09:07:08.870084: step 1106, loss 0.233351, acc 0.875\n",
      "2017-11-05T09:07:12.962992: step 1107, loss 0.0565874, acc 0.9375\n",
      "2017-11-05T09:07:17.012870: step 1108, loss 0.200504, acc 0.875\n",
      "2017-11-05T09:07:21.089267: step 1109, loss 0.276976, acc 0.8125\n",
      "2017-11-05T09:07:25.249223: step 1110, loss 0.195376, acc 0.875\n",
      "2017-11-05T09:07:29.307969: step 1111, loss 0.213792, acc 0.90625\n",
      "2017-11-05T09:07:33.413386: step 1112, loss 0.11916, acc 0.96875\n",
      "2017-11-05T09:07:37.515308: step 1113, loss 0.313624, acc 0.9375\n",
      "2017-11-05T09:07:41.580043: step 1114, loss 0.131683, acc 0.96875\n",
      "2017-11-05T09:07:45.673058: step 1115, loss 0.603201, acc 0.8125\n",
      "2017-11-05T09:07:48.385485: step 1116, loss 0.126515, acc 0.9\n",
      "2017-11-05T09:07:52.561952: step 1117, loss 0.016441, acc 1\n",
      "2017-11-05T09:07:56.704090: step 1118, loss 0.159223, acc 0.90625\n",
      "2017-11-05T09:08:00.817024: step 1119, loss 0.0505957, acc 0.96875\n",
      "2017-11-05T09:08:04.942456: step 1120, loss 0.107838, acc 0.96875\n",
      "2017-11-05T09:08:08.958309: step 1121, loss 0.325279, acc 0.875\n",
      "2017-11-05T09:08:13.045245: step 1122, loss 0.147882, acc 0.9375\n",
      "2017-11-05T09:08:17.158668: step 1123, loss 0.0963579, acc 0.96875\n",
      "2017-11-05T09:08:21.207562: step 1124, loss 0.118617, acc 0.96875\n",
      "2017-11-05T09:08:25.401543: step 1125, loss 0.22216, acc 0.9375\n",
      "2017-11-05T09:08:29.654564: step 1126, loss 0.507091, acc 0.84375\n",
      "2017-11-05T09:08:33.765986: step 1127, loss 0.308461, acc 0.875\n",
      "2017-11-05T09:08:37.910430: step 1128, loss 0.358943, acc 0.84375\n",
      "2017-11-05T09:08:41.934301: step 1129, loss 0.211066, acc 0.9375\n",
      "2017-11-05T09:08:45.950655: step 1130, loss 0.163647, acc 0.90625\n",
      "2017-11-05T09:08:49.997030: step 1131, loss 0.19369, acc 0.96875\n",
      "2017-11-05T09:08:54.025892: step 1132, loss 0.210638, acc 0.90625\n",
      "2017-11-05T09:08:58.137314: step 1133, loss 0.311821, acc 0.875\n",
      "2017-11-05T09:09:02.211709: step 1134, loss 0.0914497, acc 0.96875\n",
      "2017-11-05T09:09:06.274096: step 1135, loss 0.293886, acc 0.9375\n",
      "2017-11-05T09:09:10.433051: step 1136, loss 0.151309, acc 0.90625\n",
      "2017-11-05T09:09:14.523457: step 1137, loss 0.396424, acc 0.8125\n",
      "2017-11-05T09:09:18.582841: step 1138, loss 0.129303, acc 0.9375\n",
      "2017-11-05T09:09:22.709774: step 1139, loss 0.0998422, acc 0.96875\n",
      "2017-11-05T09:09:26.796178: step 1140, loss 0.259253, acc 0.9375\n",
      "2017-11-05T09:09:30.938120: step 1141, loss 0.0228763, acc 1\n",
      "2017-11-05T09:09:34.993696: step 1142, loss 0.38353, acc 0.84375\n",
      "2017-11-05T09:09:39.084103: step 1143, loss 0.118142, acc 0.96875\n",
      "2017-11-05T09:09:43.157998: step 1144, loss 0.220926, acc 0.9375\n",
      "2017-11-05T09:09:47.280442: step 1145, loss 0.369948, acc 0.875\n",
      "2017-11-05T09:09:51.363844: step 1146, loss 0.108045, acc 0.9375\n",
      "2017-11-05T09:09:55.454750: step 1147, loss 0.12941, acc 0.90625\n",
      "2017-11-05T09:09:59.481611: step 1148, loss 0.299203, acc 0.875\n",
      "2017-11-05T09:10:03.839383: step 1149, loss 0.0276214, acc 1\n",
      "2017-11-05T09:10:07.921978: step 1150, loss 0.115702, acc 0.9375\n",
      "2017-11-05T09:10:12.008381: step 1151, loss 0.221014, acc 0.875\n",
      "2017-11-05T09:10:14.694290: step 1152, loss 0.111474, acc 0.95\n",
      "2017-11-05T09:10:18.832230: step 1153, loss 0.372491, acc 0.84375\n",
      "2017-11-05T09:10:22.901315: step 1154, loss 0.197649, acc 0.9375\n",
      "2017-11-05T09:10:26.974221: step 1155, loss 0.122983, acc 0.9375\n",
      "2017-11-05T09:10:31.087143: step 1156, loss 0.0878147, acc 0.9375\n",
      "2017-11-05T09:10:35.293632: step 1157, loss 0.154053, acc 0.9375\n",
      "2017-11-05T09:10:39.455480: step 1158, loss 0.0887743, acc 0.9375\n",
      "2017-11-05T09:10:43.515365: step 1159, loss 0.149719, acc 0.9375\n",
      "2017-11-05T09:10:47.584757: step 1160, loss 0.138331, acc 0.9375\n",
      "2017-11-05T09:10:51.631518: step 1161, loss 0.170688, acc 0.9375\n",
      "2017-11-05T09:10:55.703911: step 1162, loss 0.0895348, acc 0.96875\n",
      "2017-11-05T09:10:59.787198: step 1163, loss 0.1069, acc 0.9375\n",
      "2017-11-05T09:11:03.862789: step 1164, loss 0.320942, acc 0.90625\n",
      "2017-11-05T09:11:07.934184: step 1165, loss 0.23596, acc 0.9375\n",
      "2017-11-05T09:11:11.993066: step 1166, loss 0.135757, acc 0.9375\n",
      "2017-11-05T09:11:16.138012: step 1167, loss 0.223918, acc 0.90625\n",
      "2017-11-05T09:11:20.174880: step 1168, loss 0.188349, acc 0.875\n",
      "2017-11-05T09:11:24.292306: step 1169, loss 0.0314785, acc 1\n",
      "2017-11-05T09:11:28.391217: step 1170, loss 0.15537, acc 0.9375\n",
      "2017-11-05T09:11:32.491631: step 1171, loss 0.362437, acc 0.90625\n",
      "2017-11-05T09:11:36.559550: step 1172, loss 0.304546, acc 0.875\n",
      "2017-11-05T09:11:40.630442: step 1173, loss 0.194722, acc 0.9375\n",
      "2017-11-05T09:11:44.771385: step 1174, loss 0.197014, acc 0.96875\n",
      "2017-11-05T09:11:48.985379: step 1175, loss 0.163253, acc 0.9375\n",
      "2017-11-05T09:11:53.061775: step 1176, loss 0.118942, acc 0.96875\n",
      "2017-11-05T09:11:57.139673: step 1177, loss 0.302394, acc 0.875\n",
      "2017-11-05T09:12:01.368177: step 1178, loss 0.172883, acc 0.90625\n",
      "2017-11-05T09:12:05.470592: step 1179, loss 0.25407, acc 0.90625\n",
      "2017-11-05T09:12:09.607031: step 1180, loss 0.0984224, acc 0.9375\n",
      "2017-11-05T09:12:13.661913: step 1181, loss 0.205271, acc 0.875\n",
      "2017-11-05T09:12:17.784842: step 1182, loss 0.076585, acc 0.96875\n",
      "2017-11-05T09:12:21.954805: step 1183, loss 0.215512, acc 0.90625\n",
      "2017-11-05T09:12:26.137277: step 1184, loss 0.312385, acc 0.90625\n",
      "2017-11-05T09:12:30.303236: step 1185, loss 0.357274, acc 0.84375\n",
      "2017-11-05T09:12:34.440676: step 1186, loss 0.123321, acc 0.90625\n",
      "2017-11-05T09:12:38.608149: step 1187, loss 0.0192054, acc 1\n",
      "2017-11-05T09:12:41.197990: step 1188, loss 0.00406701, acc 1\n",
      "2017-11-05T09:12:45.334389: step 1189, loss 0.332521, acc 0.875\n",
      "2017-11-05T09:12:49.401215: step 1190, loss 0.456694, acc 0.84375\n",
      "2017-11-05T09:12:53.481113: step 1191, loss 0.137561, acc 0.90625\n",
      "2017-11-05T09:12:57.540498: step 1192, loss 0.346617, acc 0.875\n",
      "2017-11-05T09:13:01.593377: step 1193, loss 0.0741064, acc 0.96875\n",
      "2017-11-05T09:13:05.674903: step 1194, loss 0.288946, acc 0.875\n",
      "2017-11-05T09:13:09.804336: step 1195, loss 0.11521, acc 0.96875\n",
      "2017-11-05T09:13:13.911151: step 1196, loss 0.104561, acc 0.9375\n",
      "2017-11-05T09:13:17.978541: step 1197, loss 0.151333, acc 0.90625\n",
      "2017-11-05T09:13:22.247575: step 1198, loss 0.0222479, acc 1\n",
      "2017-11-05T09:13:26.383013: step 1199, loss 0.19571, acc 0.96875\n",
      "2017-11-05T09:13:30.474420: step 1200, loss 0.48905, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:13:33.164831: step 1200, loss 0.767528, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-05T09:13:38.814314: step 1201, loss 0.270983, acc 0.875\n",
      "2017-11-05T09:13:42.943748: step 1202, loss 0.267787, acc 0.84375\n",
      "2017-11-05T09:13:47.004133: step 1203, loss 0.0388207, acc 0.96875\n",
      "2017-11-05T09:13:51.095039: step 1204, loss 0.20907, acc 0.875\n",
      "2017-11-05T09:13:55.239985: step 1205, loss 0.288786, acc 0.875\n",
      "2017-11-05T09:13:59.352265: step 1206, loss 0.295636, acc 0.84375\n",
      "2017-11-05T09:14:03.441170: step 1207, loss 0.202184, acc 0.9375\n",
      "2017-11-05T09:14:07.477423: step 1208, loss 0.0693891, acc 0.96875\n",
      "2017-11-05T09:14:11.603354: step 1209, loss 0.475089, acc 0.875\n",
      "2017-11-05T09:14:15.678250: step 1210, loss 0.120412, acc 0.9375\n",
      "2017-11-05T09:14:19.732642: step 1211, loss 0.271507, acc 0.875\n",
      "2017-11-05T09:14:23.831054: step 1212, loss 0.104562, acc 0.9375\n",
      "2017-11-05T09:14:27.900446: step 1213, loss 0.270553, acc 0.875\n",
      "2017-11-05T09:14:31.997858: step 1214, loss 0.265499, acc 0.90625\n",
      "2017-11-05T09:14:36.163316: step 1215, loss 0.390525, acc 0.90625\n",
      "2017-11-05T09:14:40.361801: step 1216, loss 0.0789086, acc 0.96875\n",
      "2017-11-05T09:14:44.389161: step 1217, loss 0.285169, acc 0.90625\n",
      "2017-11-05T09:14:48.513092: step 1218, loss 0.233918, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T09:14:52.549460: step 1219, loss 0.284768, acc 0.875\n",
      "2017-11-05T09:14:56.684898: step 1220, loss 0.249389, acc 0.9375\n",
      "2017-11-05T09:15:00.708758: step 1221, loss 0.264226, acc 0.875\n",
      "2017-11-05T09:15:04.758135: step 1222, loss 0.186561, acc 0.9375\n",
      "2017-11-05T09:15:08.861551: step 1223, loss 0.224943, acc 0.875\n",
      "2017-11-05T09:15:11.456087: step 1224, loss 0.562124, acc 0.85\n",
      "2017-11-05T09:15:15.496368: step 1225, loss 0.0933174, acc 0.96875\n",
      "2017-11-05T09:15:19.551749: step 1226, loss 0.0799586, acc 0.96875\n",
      "2017-11-05T09:15:23.621641: step 1227, loss 0.0588351, acc 0.96875\n",
      "2017-11-05T09:15:27.673521: step 1228, loss 0.331073, acc 0.90625\n",
      "2017-11-05T09:15:31.719395: step 1229, loss 0.313592, acc 0.875\n",
      "2017-11-05T09:15:35.817430: step 1230, loss 0.144145, acc 0.96875\n",
      "2017-11-05T09:15:39.900831: step 1231, loss 0.194559, acc 0.9375\n",
      "2017-11-05T09:15:44.024261: step 1232, loss 0.639956, acc 0.75\n",
      "2017-11-05T09:15:48.025709: step 1233, loss 0.195833, acc 0.9375\n",
      "2017-11-05T09:15:52.065865: step 1234, loss 0.139178, acc 0.9375\n",
      "2017-11-05T09:15:56.184791: step 1235, loss 0.190633, acc 0.9375\n",
      "2017-11-05T09:16:00.233668: step 1236, loss 0.105991, acc 0.9375\n",
      "2017-11-05T09:16:04.274046: step 1237, loss 0.194662, acc 0.9375\n",
      "2017-11-05T09:16:08.334431: step 1238, loss 0.184896, acc 0.9375\n",
      "2017-11-05T09:16:12.475887: step 1239, loss 0.220494, acc 0.9375\n",
      "2017-11-05T09:16:16.549781: step 1240, loss 0.26695, acc 0.90625\n",
      "2017-11-05T09:16:20.573640: step 1241, loss 0.283551, acc 0.875\n",
      "2017-11-05T09:16:24.625861: step 1242, loss 0.0915912, acc 0.96875\n",
      "2017-11-05T09:16:28.685745: step 1243, loss 0.247664, acc 0.90625\n",
      "2017-11-05T09:16:32.761141: step 1244, loss 0.24229, acc 0.875\n",
      "2017-11-05T09:16:37.040682: step 1245, loss 0.481603, acc 0.875\n",
      "2017-11-05T09:16:41.254176: step 1246, loss 0.253971, acc 0.84375\n",
      "2017-11-05T09:16:45.287041: step 1247, loss 0.199854, acc 0.90625\n",
      "2017-11-05T09:16:49.414360: step 1248, loss 0.105574, acc 0.9375\n",
      "2017-11-05T09:16:53.571315: step 1249, loss 0.192601, acc 0.90625\n",
      "2017-11-05T09:16:57.587853: step 1250, loss 0.267014, acc 0.90625\n",
      "2017-11-05T09:17:01.708281: step 1251, loss 0.249459, acc 0.9375\n",
      "2017-11-05T09:17:05.715128: step 1252, loss 0.374707, acc 0.875\n",
      "2017-11-05T09:17:09.757000: step 1253, loss 0.111088, acc 0.9375\n",
      "2017-11-05T09:17:13.812882: step 1254, loss 0.459995, acc 0.84375\n",
      "2017-11-05T09:17:17.764210: step 1255, loss 0.111842, acc 0.96875\n",
      "2017-11-05T09:17:21.783565: step 1256, loss 0.0687288, acc 0.96875\n",
      "2017-11-05T09:17:25.878975: step 1257, loss 0.313969, acc 0.78125\n",
      "2017-11-05T09:17:29.875815: step 1258, loss 0.178646, acc 0.9375\n",
      "2017-11-05T09:17:33.890668: step 1259, loss 0.251554, acc 0.875\n",
      "2017-11-05T09:17:36.435364: step 1260, loss 0.110365, acc 0.95\n",
      "2017-11-05T09:17:40.458478: step 1261, loss 0.194817, acc 0.9375\n",
      "2017-11-05T09:17:44.405745: step 1262, loss 0.383655, acc 0.875\n",
      "2017-11-05T09:17:48.398405: step 1263, loss 0.26354, acc 0.9375\n",
      "2017-11-05T09:17:52.464643: step 1264, loss 0.185391, acc 0.9375\n",
      "2017-11-05T09:17:56.487001: step 1265, loss 0.147284, acc 0.96875\n",
      "2017-11-05T09:18:00.446834: step 1266, loss 0.25411, acc 0.90625\n",
      "2017-11-05T09:18:04.441183: step 1267, loss 0.19637, acc 0.875\n",
      "2017-11-05T09:18:08.384843: step 1268, loss 0.207699, acc 0.875\n",
      "2017-11-05T09:18:12.381703: step 1269, loss 0.189899, acc 0.9375\n",
      "2017-11-05T09:18:16.407563: step 1270, loss 0.17938, acc 0.9375\n",
      "2017-11-05T09:18:20.360881: step 1271, loss 0.22538, acc 0.90625\n",
      "2017-11-05T09:18:24.456135: step 1272, loss 0.246764, acc 0.90625\n",
      "2017-11-05T09:18:28.580635: step 1273, loss 0.145114, acc 0.875\n",
      "2017-11-05T09:18:32.564965: step 1274, loss 0.156381, acc 0.9375\n",
      "2017-11-05T09:18:36.650879: step 1275, loss 0.0462959, acc 1\n",
      "2017-11-05T09:18:40.793791: step 1276, loss 0.245342, acc 0.875\n",
      "2017-11-05T09:18:44.846670: step 1277, loss 0.217956, acc 0.90625\n",
      "2017-11-05T09:18:48.845014: step 1278, loss 0.161879, acc 0.9375\n",
      "2017-11-05T09:18:52.876876: step 1279, loss 0.208765, acc 0.9375\n",
      "2017-11-05T09:18:56.969285: step 1280, loss 0.170743, acc 0.96875\n",
      "2017-11-05T09:19:01.038676: step 1281, loss 0.143433, acc 0.96875\n",
      "2017-11-05T09:19:05.212642: step 1282, loss 0.190536, acc 0.9375\n",
      "2017-11-05T09:19:09.282533: step 1283, loss 0.43342, acc 0.875\n",
      "2017-11-05T09:19:13.346922: step 1284, loss 0.293912, acc 0.90625\n",
      "2017-11-05T09:19:17.500373: step 1285, loss 0.363011, acc 0.90625\n",
      "2017-11-05T09:19:21.545849: step 1286, loss 0.0191472, acc 1\n",
      "2017-11-05T09:19:25.636298: step 1287, loss 0.357746, acc 0.875\n",
      "2017-11-05T09:19:29.759728: step 1288, loss 0.212884, acc 0.875\n",
      "2017-11-05T09:19:33.821614: step 1289, loss 0.149651, acc 0.96875\n",
      "2017-11-05T09:19:37.913627: step 1290, loss 0.168987, acc 0.9375\n",
      "2017-11-05T09:19:42.095598: step 1291, loss 0.142632, acc 0.9375\n",
      "2017-11-05T09:19:46.296084: step 1292, loss 0.293228, acc 0.90625\n",
      "2017-11-05T09:19:50.409506: step 1293, loss 0.0957736, acc 0.96875\n",
      "2017-11-05T09:19:54.661527: step 1294, loss 0.522452, acc 0.71875\n",
      "2017-11-05T09:19:58.852505: step 1295, loss 0.0675207, acc 0.96875\n",
      "2017-11-05T09:20:01.790092: step 1296, loss 0.12299, acc 0.95\n",
      "2017-11-05T09:20:05.832465: step 1297, loss 0.227484, acc 0.875\n",
      "2017-11-05T09:20:09.915867: step 1298, loss 0.276853, acc 0.875\n",
      "2017-11-05T09:20:14.007273: step 1299, loss 0.110265, acc 0.90625\n",
      "2017-11-05T09:20:18.016130: step 1300, loss 0.204647, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:20:20.606471: step 1300, loss 0.973466, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-05T09:20:26.061045: step 1301, loss 0.283505, acc 0.90625\n",
      "2017-11-05T09:20:30.053883: step 1302, loss 0.27736, acc 0.9375\n",
      "2017-11-05T09:20:34.180703: step 1303, loss 0.242698, acc 0.9375\n",
      "2017-11-05T09:20:38.157029: step 1304, loss 0.431823, acc 0.84375\n",
      "2017-11-05T09:20:42.173883: step 1305, loss 0.352515, acc 0.90625\n",
      "2017-11-05T09:20:46.218257: step 1306, loss 0.00771632, acc 1\n",
      "2017-11-05T09:20:50.159056: step 1307, loss 0.206733, acc 0.9375\n",
      "2017-11-05T09:20:54.171602: step 1308, loss 0.230612, acc 0.875\n",
      "2017-11-05T09:20:58.194460: step 1309, loss 0.110427, acc 0.9375\n",
      "2017-11-05T09:21:02.223823: step 1310, loss 0.395383, acc 0.84375\n",
      "2017-11-05T09:21:06.294716: step 1311, loss 0.335453, acc 0.90625\n",
      "2017-11-05T09:21:10.281062: step 1312, loss 0.447225, acc 0.8125\n",
      "2017-11-05T09:21:14.271397: step 1313, loss 0.0371766, acc 1\n",
      "2017-11-05T09:21:18.249735: step 1314, loss 0.13989, acc 0.96875\n",
      "2017-11-05T09:21:22.251511: step 1315, loss 0.265778, acc 0.875\n",
      "2017-11-05T09:21:26.309417: step 1316, loss 0.0893691, acc 0.96875\n",
      "2017-11-05T09:21:30.321768: step 1317, loss 0.223219, acc 0.875\n",
      "2017-11-05T09:21:34.283083: step 1318, loss 0.185373, acc 0.90625\n",
      "2017-11-05T09:21:38.272417: step 1319, loss 0.142449, acc 0.96875\n",
      "2017-11-05T09:21:42.327432: step 1320, loss 0.244168, acc 0.875\n",
      "2017-11-05T09:21:46.326274: step 1321, loss 0.409034, acc 0.84375\n",
      "2017-11-05T09:21:50.361254: step 1322, loss 0.23875, acc 0.9375\n",
      "2017-11-05T09:21:54.508710: step 1323, loss 0.373883, acc 0.84375\n",
      "2017-11-05T09:21:58.535191: step 1324, loss 0.430965, acc 0.84375\n",
      "2017-11-05T09:22:02.615901: step 1325, loss 0.0610867, acc 1\n",
      "2017-11-05T09:22:06.589224: step 1326, loss 0.244372, acc 0.9375\n",
      "2017-11-05T09:22:10.657615: step 1327, loss 0.229487, acc 0.875\n",
      "2017-11-05T09:22:14.761531: step 1328, loss 0.16262, acc 0.9375\n",
      "2017-11-05T09:22:18.859442: step 1329, loss 0.359694, acc 0.84375\n",
      "2017-11-05T09:22:22.937841: step 1330, loss 0.207326, acc 0.84375\n",
      "2017-11-05T09:22:27.005730: step 1331, loss 0.0951624, acc 0.9375\n",
      "2017-11-05T09:22:29.569553: step 1332, loss 0.0796105, acc 0.95\n",
      "2017-11-05T09:22:33.663461: step 1333, loss 0.111381, acc 0.96875\n",
      "2017-11-05T09:22:37.729351: step 1334, loss 0.21922, acc 0.90625\n",
      "2017-11-05T09:22:41.851781: step 1335, loss 0.214961, acc 0.9375\n",
      "2017-11-05T09:22:45.850121: step 1336, loss 0.354791, acc 0.90625\n",
      "2017-11-05T09:22:49.875981: step 1337, loss 0.469834, acc 0.78125\n",
      "2017-11-05T09:22:53.974908: step 1338, loss 0.155515, acc 0.90625\n",
      "2017-11-05T09:22:58.212808: step 1339, loss 0.101164, acc 0.9375\n",
      "2017-11-05T09:23:02.306216: step 1340, loss 0.162562, acc 0.9375\n",
      "2017-11-05T09:23:06.289547: step 1341, loss 0.320279, acc 0.875\n",
      "2017-11-05T09:23:10.318420: step 1342, loss 0.225291, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T09:23:14.326268: step 1343, loss 0.264596, acc 0.9375\n",
      "2017-11-05T09:23:18.378115: step 1344, loss 0.052825, acc 1\n",
      "2017-11-05T09:23:22.431496: step 1345, loss 0.236824, acc 0.875\n",
      "2017-11-05T09:23:26.795595: step 1346, loss 0.270151, acc 0.90625\n",
      "2017-11-05T09:23:30.804946: step 1347, loss 0.249236, acc 0.875\n",
      "2017-11-05T09:23:34.889847: step 1348, loss 0.122146, acc 0.9375\n",
      "2017-11-05T09:23:38.903390: step 1349, loss 0.193033, acc 0.84375\n",
      "2017-11-05T09:23:42.938757: step 1350, loss 0.29365, acc 0.875\n",
      "2017-11-05T09:23:46.980629: step 1351, loss 0.143575, acc 0.90625\n",
      "2017-11-05T09:23:51.052912: step 1352, loss 0.171906, acc 0.9375\n",
      "2017-11-05T09:23:55.040499: step 1353, loss 0.247667, acc 0.875\n",
      "2017-11-05T09:23:59.088876: step 1354, loss 0.0502033, acc 0.96875\n",
      "2017-11-05T09:24:03.152997: step 1355, loss 0.139129, acc 0.9375\n",
      "2017-11-05T09:24:07.185025: step 1356, loss 0.25467, acc 0.90625\n",
      "2017-11-05T09:24:11.281947: step 1357, loss 0.227545, acc 0.90625\n",
      "2017-11-05T09:24:15.312085: step 1358, loss 0.208664, acc 0.90625\n",
      "2017-11-05T09:24:19.329964: step 1359, loss 0.147965, acc 0.9375\n",
      "2017-11-05T09:24:23.367503: step 1360, loss 0.119353, acc 0.96875\n",
      "2017-11-05T09:24:27.447401: step 1361, loss 0.181536, acc 0.875\n",
      "2017-11-05T09:24:31.476765: step 1362, loss 0.0141683, acc 1\n",
      "2017-11-05T09:24:35.651731: step 1363, loss 0.332821, acc 0.875\n",
      "2017-11-05T09:24:39.676178: step 1364, loss 0.349594, acc 0.90625\n",
      "2017-11-05T09:24:43.780095: step 1365, loss 0.330711, acc 0.8125\n",
      "2017-11-05T09:24:47.798950: step 1366, loss 0.381218, acc 0.84375\n",
      "2017-11-05T09:24:51.824310: step 1367, loss 0.182009, acc 0.90625\n",
      "2017-11-05T09:24:54.444672: step 1368, loss 0.0945124, acc 0.9\n",
      "2017-11-05T09:24:58.559037: step 1369, loss 0.18704, acc 0.90625\n",
      "2017-11-05T09:25:02.655948: step 1370, loss 0.193157, acc 0.90625\n",
      "2017-11-05T09:25:06.670300: step 1371, loss 0.181208, acc 0.9375\n",
      "2017-11-05T09:25:10.761707: step 1372, loss 0.347334, acc 0.84375\n",
      "2017-11-05T09:25:14.771556: step 1373, loss 0.485798, acc 0.8125\n",
      "2017-11-05T09:25:18.787410: step 1374, loss 0.0834564, acc 0.96875\n",
      "2017-11-05T09:25:22.787252: step 1375, loss 0.39846, acc 0.84375\n",
      "2017-11-05T09:25:26.824120: step 1376, loss 0.16311, acc 0.9375\n",
      "2017-11-05T09:25:30.925535: step 1377, loss 0.359267, acc 0.84375\n",
      "2017-11-05T09:25:34.946893: step 1378, loss 0.307132, acc 0.875\n",
      "2017-11-05T09:25:39.002273: step 1379, loss 0.179257, acc 0.9375\n",
      "2017-11-05T09:25:43.079671: step 1380, loss 0.114732, acc 0.9375\n",
      "2017-11-05T09:25:47.139055: step 1381, loss 0.0693194, acc 0.96875\n",
      "2017-11-05T09:25:51.163427: step 1382, loss 0.352433, acc 0.84375\n",
      "2017-11-05T09:25:55.187286: step 1383, loss 0.149084, acc 0.9375\n",
      "2017-11-05T09:25:59.234161: step 1384, loss 0.154919, acc 0.9375\n",
      "2017-11-05T09:26:03.345082: step 1385, loss 0.397093, acc 0.8125\n",
      "2017-11-05T09:26:07.415975: step 1386, loss 0.117532, acc 0.9375\n",
      "2017-11-05T09:26:11.494372: step 1387, loss 0.0470071, acc 0.96875\n",
      "2017-11-05T09:26:15.526738: step 1388, loss 0.184775, acc 0.90625\n",
      "2017-11-05T09:26:19.525078: step 1389, loss 0.198583, acc 0.90625\n",
      "2017-11-05T09:26:23.551939: step 1390, loss 0.274759, acc 0.90625\n",
      "2017-11-05T09:26:27.613179: step 1391, loss 0.0554801, acc 1\n",
      "2017-11-05T09:26:31.616536: step 1392, loss 0.139945, acc 0.9375\n",
      "2017-11-05T09:26:35.749974: step 1393, loss 0.284542, acc 0.875\n",
      "2017-11-05T09:26:39.856924: step 1394, loss 0.330984, acc 0.875\n",
      "2017-11-05T09:26:43.895792: step 1395, loss 0.13289, acc 0.9375\n",
      "2017-11-05T09:26:47.878816: step 1396, loss 0.0637611, acc 0.96875\n",
      "2017-11-05T09:26:51.917769: step 1397, loss 0.102873, acc 0.96875\n",
      "2017-11-05T09:26:56.010095: step 1398, loss 0.307613, acc 0.875\n",
      "2017-11-05T09:26:59.958789: step 1399, loss 0.179055, acc 0.9375\n",
      "2017-11-05T09:27:04.010168: step 1400, loss 0.118094, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:27:06.692132: step 1400, loss 0.768478, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-05T09:27:12.182237: step 1401, loss 0.342786, acc 0.875\n",
      "2017-11-05T09:27:16.166525: step 1402, loss 0.0288823, acc 1\n",
      "2017-11-05T09:27:20.254428: step 1403, loss 0.181479, acc 0.9375\n",
      "2017-11-05T09:27:22.881296: step 1404, loss 0.104036, acc 0.95\n",
      "2017-11-05T09:27:26.919360: step 1405, loss 0.182105, acc 0.9375\n",
      "2017-11-05T09:27:30.977244: step 1406, loss 0.165677, acc 0.96875\n",
      "2017-11-05T09:27:35.087664: step 1407, loss 0.375641, acc 0.875\n",
      "2017-11-05T09:27:39.178677: step 1408, loss 0.0611857, acc 0.96875\n",
      "2017-11-05T09:27:43.220230: step 1409, loss 0.049056, acc 0.96875\n",
      "2017-11-05T09:27:47.263121: step 1410, loss 0.15848, acc 0.875\n",
      "2017-11-05T09:27:51.362533: step 1411, loss 0.0654857, acc 1\n",
      "2017-11-05T09:27:55.390895: step 1412, loss 0.0367849, acc 1\n",
      "2017-11-05T09:27:59.446595: step 1413, loss 0.251146, acc 0.8125\n",
      "2017-11-05T09:28:03.561020: step 1414, loss 0.0845363, acc 0.9375\n",
      "2017-11-05T09:28:07.576872: step 1415, loss 0.102946, acc 0.96875\n",
      "2017-11-05T09:28:11.680788: step 1416, loss 0.253765, acc 0.84375\n",
      "2017-11-05T09:28:15.691638: step 1417, loss 0.503468, acc 0.78125\n",
      "2017-11-05T09:28:19.696489: step 1418, loss 0.283995, acc 0.84375\n",
      "2017-11-05T09:28:23.921992: step 1419, loss 0.0364282, acc 1\n",
      "2017-11-05T09:28:28.117973: step 1420, loss 0.226781, acc 0.90625\n",
      "2017-11-05T09:28:32.128823: step 1421, loss 0.207907, acc 0.90625\n",
      "2017-11-05T09:28:36.336312: step 1422, loss 0.348726, acc 0.90625\n",
      "2017-11-05T09:28:40.391396: step 1423, loss 0.117761, acc 0.96875\n",
      "2017-11-05T09:28:44.398619: step 1424, loss 0.0564581, acc 0.96875\n",
      "2017-11-05T09:28:48.524318: step 1425, loss 0.247343, acc 0.84375\n",
      "2017-11-05T09:28:52.577282: step 1426, loss 0.0315876, acc 1\n",
      "2017-11-05T09:28:56.651177: step 1427, loss 0.218976, acc 0.875\n",
      "2017-11-05T09:29:00.631520: step 1428, loss 0.0670984, acc 1\n",
      "2017-11-05T09:29:04.704413: step 1429, loss 0.221505, acc 0.875\n",
      "2017-11-05T09:29:08.795820: step 1430, loss 0.101234, acc 0.96875\n",
      "2017-11-05T09:29:12.897625: step 1431, loss 0.325508, acc 0.875\n",
      "2017-11-05T09:29:16.907473: step 1432, loss 0.255731, acc 0.90625\n",
      "2017-11-05T09:29:21.002984: step 1433, loss 0.222499, acc 0.90625\n",
      "2017-11-05T09:29:25.016836: step 1434, loss 0.256083, acc 0.84375\n",
      "2017-11-05T09:29:29.065713: step 1435, loss 0.398174, acc 0.8125\n",
      "2017-11-05T09:29:33.091086: step 1436, loss 0.312971, acc 0.875\n",
      "2017-11-05T09:29:37.118448: step 1437, loss 0.151257, acc 0.9375\n",
      "2017-11-05T09:29:41.253386: step 1438, loss 0.282266, acc 0.875\n",
      "2017-11-05T09:29:45.291755: step 1439, loss 0.256823, acc 0.90625\n",
      "2017-11-05T09:29:47.922129: step 1440, loss 0.360478, acc 0.8\n",
      "2017-11-05T09:29:52.073203: step 1441, loss 0.177601, acc 0.90625\n",
      "2017-11-05T09:29:56.150100: step 1442, loss 0.0777263, acc 0.96875\n",
      "2017-11-05T09:30:00.245510: step 1443, loss 0.0417482, acc 1\n",
      "2017-11-05T09:30:04.678160: step 1444, loss 0.271017, acc 0.84375\n",
      "2017-11-05T09:30:08.757058: step 1445, loss 0.281803, acc 0.84375\n",
      "2017-11-05T09:30:12.852968: step 1446, loss 0.1822, acc 0.90625\n",
      "2017-11-05T09:30:16.943875: step 1447, loss 0.117366, acc 0.90625\n",
      "2017-11-05T09:30:21.020272: step 1448, loss 0.172547, acc 0.90625\n",
      "2017-11-05T09:30:25.027619: step 1449, loss 0.145303, acc 0.90625\n",
      "2017-11-05T09:30:29.140041: step 1450, loss 0.359498, acc 0.875\n",
      "2017-11-05T09:30:33.211434: step 1451, loss 0.198462, acc 0.875\n",
      "2017-11-05T09:30:37.347373: step 1452, loss 0.297461, acc 0.84375\n",
      "2017-11-05T09:30:41.410260: step 1453, loss 0.246287, acc 0.875\n",
      "2017-11-05T09:30:45.417607: step 1454, loss 0.07066, acc 0.96875\n",
      "2017-11-05T09:30:49.534032: step 1455, loss 0.164627, acc 0.9375\n",
      "2017-11-05T09:30:53.639448: step 1456, loss 0.239803, acc 0.875\n",
      "2017-11-05T09:30:57.702335: step 1457, loss 0.0791253, acc 0.96875\n",
      "2017-11-05T09:31:01.740705: step 1458, loss 0.126349, acc 0.96875\n",
      "2017-11-05T09:31:05.884725: step 1459, loss 0.185653, acc 0.875\n",
      "2017-11-05T09:31:10.011657: step 1460, loss 0.384807, acc 0.84375\n",
      "2017-11-05T09:31:14.014501: step 1461, loss 0.144158, acc 0.9375\n",
      "2017-11-05T09:31:18.073385: step 1462, loss 0.322087, acc 0.84375\n",
      "2017-11-05T09:31:22.124263: step 1463, loss 0.0592476, acc 0.96875\n",
      "2017-11-05T09:31:26.189661: step 1464, loss 0.141677, acc 0.9375\n",
      "2017-11-05T09:31:30.213019: step 1465, loss 0.113007, acc 0.96875\n",
      "2017-11-05T09:31:34.298422: step 1466, loss 0.0384808, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T09:31:38.377325: step 1467, loss 0.154192, acc 0.9375\n",
      "2017-11-05T09:31:42.451220: step 1468, loss 0.15044, acc 0.9375\n",
      "2017-11-05T09:31:46.542628: step 1469, loss 0.357156, acc 0.90625\n",
      "2017-11-05T09:31:50.675564: step 1470, loss 0.182842, acc 0.96875\n",
      "2017-11-05T09:31:54.794329: step 1471, loss 0.163346, acc 0.9375\n",
      "2017-11-05T09:31:58.833548: step 1472, loss 0.107452, acc 0.90625\n",
      "2017-11-05T09:32:02.967800: step 1473, loss 0.171564, acc 0.90625\n",
      "2017-11-05T09:32:07.035190: step 1474, loss 0.329573, acc 0.875\n",
      "2017-11-05T09:32:11.117675: step 1475, loss 0.237534, acc 0.875\n",
      "2017-11-05T09:32:13.736464: step 1476, loss 0.0580857, acc 0.95\n",
      "2017-11-05T09:32:17.766328: step 1477, loss 0.203426, acc 0.90625\n",
      "2017-11-05T09:32:21.922790: step 1478, loss 0.14147, acc 0.90625\n",
      "2017-11-05T09:32:25.989180: step 1479, loss 0.127878, acc 0.9375\n",
      "2017-11-05T09:32:30.045562: step 1480, loss 0.169057, acc 0.90625\n",
      "2017-11-05T09:32:34.220529: step 1481, loss 0.178908, acc 0.9375\n",
      "2017-11-05T09:32:38.285417: step 1482, loss 0.0751335, acc 0.96875\n",
      "2017-11-05T09:32:42.373821: step 1483, loss 0.208686, acc 0.90625\n",
      "2017-11-05T09:32:46.422698: step 1484, loss 0.165744, acc 0.9375\n",
      "2017-11-05T09:32:50.457565: step 1485, loss 0.126788, acc 0.96875\n",
      "2017-11-05T09:32:54.592003: step 1486, loss 0.200968, acc 0.875\n",
      "2017-11-05T09:32:58.682910: step 1487, loss 0.112033, acc 0.9375\n",
      "2017-11-05T09:33:02.752301: step 1488, loss 0.186279, acc 0.875\n",
      "2017-11-05T09:33:06.841207: step 1489, loss 0.0661412, acc 0.96875\n",
      "2017-11-05T09:33:10.841049: step 1490, loss 0.0387718, acc 0.96875\n",
      "2017-11-05T09:33:14.836901: step 1491, loss 0.177959, acc 0.96875\n",
      "2017-11-05T09:33:18.858567: step 1492, loss 0.278362, acc 0.9375\n",
      "2017-11-05T09:33:22.982497: step 1493, loss 0.122923, acc 0.9375\n",
      "2017-11-05T09:33:27.167720: step 1494, loss 0.450366, acc 0.8125\n",
      "2017-11-05T09:33:31.263130: step 1495, loss 0.179006, acc 0.9375\n",
      "2017-11-05T09:33:35.350534: step 1496, loss 0.0773303, acc 0.96875\n",
      "2017-11-05T09:33:39.513492: step 1497, loss 0.298809, acc 0.875\n",
      "2017-11-05T09:33:43.514835: step 1498, loss 0.0453823, acc 1\n",
      "2017-11-05T09:33:47.596863: step 1499, loss 0.239488, acc 0.9375\n",
      "2017-11-05T09:33:51.684767: step 1500, loss 0.193086, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:33:54.278110: step 1500, loss 0.757068, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-05T09:33:59.836597: step 1501, loss 0.134116, acc 0.90625\n",
      "2017-11-05T09:34:03.823129: step 1502, loss 0.11526, acc 0.9375\n",
      "2017-11-05T09:34:07.879913: step 1503, loss 0.148898, acc 0.9375\n",
      "2017-11-05T09:34:12.018861: step 1504, loss 0.482016, acc 0.78125\n",
      "2017-11-05T09:34:16.018201: step 1505, loss 0.214129, acc 0.9375\n",
      "2017-11-05T09:34:19.994541: step 1506, loss 0.168928, acc 0.90625\n",
      "2017-11-05T09:34:23.991108: step 1507, loss 0.253043, acc 0.875\n",
      "2017-11-05T09:34:27.979942: step 1508, loss 0.176575, acc 0.9375\n",
      "2017-11-05T09:34:32.067347: step 1509, loss 0.339996, acc 0.875\n",
      "2017-11-05T09:34:36.199283: step 1510, loss 0.210323, acc 0.90625\n",
      "2017-11-05T09:34:40.336224: step 1511, loss 0.0646964, acc 0.9375\n",
      "2017-11-05T09:34:42.959474: step 1512, loss 0.412498, acc 0.85\n",
      "2017-11-05T09:34:47.011854: step 1513, loss 0.0910093, acc 0.9375\n",
      "2017-11-05T09:34:51.029709: step 1514, loss 0.14244, acc 0.96875\n",
      "2017-11-05T09:34:55.094598: step 1515, loss 0.331123, acc 0.84375\n",
      "2017-11-05T09:34:59.125463: step 1516, loss 0.21564, acc 0.84375\n",
      "2017-11-05T09:35:03.194238: step 1517, loss 0.303348, acc 0.90625\n",
      "2017-11-05T09:35:07.230606: step 1518, loss 0.18322, acc 0.9375\n",
      "2017-11-05T09:35:11.208434: step 1519, loss 0.217993, acc 0.875\n",
      "2017-11-05T09:35:15.260813: step 1520, loss 0.0659909, acc 1\n",
      "2017-11-05T09:35:19.255651: step 1521, loss 0.162509, acc 0.90625\n",
      "2017-11-05T09:35:23.358066: step 1522, loss 0.0599029, acc 0.96875\n",
      "2017-11-05T09:35:27.452475: step 1523, loss 0.218964, acc 0.90625\n",
      "2017-11-05T09:35:31.529371: step 1524, loss 0.259654, acc 0.90625\n",
      "2017-11-05T09:35:35.650300: step 1525, loss 0.197483, acc 0.90625\n",
      "2017-11-05T09:35:39.771228: step 1526, loss 0.263101, acc 0.84375\n",
      "2017-11-05T09:35:43.906666: step 1527, loss 0.262327, acc 0.875\n",
      "2017-11-05T09:35:47.877379: step 1528, loss 0.23719, acc 0.90625\n",
      "2017-11-05T09:35:51.967285: step 1529, loss 0.294876, acc 0.84375\n",
      "2017-11-05T09:35:56.019665: step 1530, loss 0.181975, acc 0.90625\n",
      "2017-11-05T09:36:00.103567: step 1531, loss 0.0301779, acc 1\n",
      "2017-11-05T09:36:04.138461: step 1532, loss 0.190735, acc 0.9375\n",
      "2017-11-05T09:36:08.207853: step 1533, loss 0.161094, acc 0.96875\n",
      "2017-11-05T09:36:12.228709: step 1534, loss 0.0545981, acc 0.96875\n",
      "2017-11-05T09:36:16.332125: step 1535, loss 0.284142, acc 0.875\n",
      "2017-11-05T09:36:20.422031: step 1536, loss 0.348206, acc 0.84375\n",
      "2017-11-05T09:36:24.489421: step 1537, loss 0.0475343, acc 1\n",
      "2017-11-05T09:36:28.501979: step 1538, loss 0.211359, acc 0.90625\n",
      "2017-11-05T09:36:32.593886: step 1539, loss 0.367741, acc 0.875\n",
      "2017-11-05T09:36:36.699826: step 1540, loss 0.304184, acc 0.90625\n",
      "2017-11-05T09:36:40.795237: step 1541, loss 0.182028, acc 0.9375\n",
      "2017-11-05T09:36:44.806086: step 1542, loss 0.187019, acc 0.90625\n",
      "2017-11-05T09:36:48.925013: step 1543, loss 0.204753, acc 0.90625\n",
      "2017-11-05T09:36:52.948882: step 1544, loss 0.168681, acc 0.9375\n",
      "2017-11-05T09:36:57.004286: step 1545, loss 0.0812016, acc 0.96875\n",
      "2017-11-05T09:37:01.104199: step 1546, loss 0.17266, acc 0.84375\n",
      "2017-11-05T09:37:05.146571: step 1547, loss 0.316772, acc 0.84375\n",
      "2017-11-05T09:37:07.814968: step 1548, loss 0.0147199, acc 1\n",
      "2017-11-05T09:37:11.868348: step 1549, loss 0.173534, acc 0.90625\n",
      "2017-11-05T09:37:16.017811: step 1550, loss 0.225721, acc 0.90625\n",
      "2017-11-05T09:37:20.036667: step 1551, loss 0.284029, acc 0.90625\n",
      "2017-11-05T09:37:24.044044: step 1552, loss 0.11545, acc 0.96875\n",
      "2017-11-05T09:37:28.079423: step 1553, loss 0.319532, acc 0.8125\n",
      "2017-11-05T09:37:32.150326: step 1554, loss 0.255033, acc 0.84375\n",
      "2017-11-05T09:37:36.261747: step 1555, loss 0.0411734, acc 0.96875\n",
      "2017-11-05T09:37:40.367164: step 1556, loss 0.189458, acc 0.90625\n",
      "2017-11-05T09:37:44.562145: step 1557, loss 0.133103, acc 0.9375\n",
      "2017-11-05T09:37:48.675763: step 1558, loss 0.259804, acc 0.875\n",
      "2017-11-05T09:37:52.734146: step 1559, loss 0.48192, acc 0.875\n",
      "2017-11-05T09:37:56.811544: step 1560, loss 0.0949506, acc 1\n",
      "2017-11-05T09:38:00.974001: step 1561, loss 0.13415, acc 0.9375\n",
      "2017-11-05T09:38:05.020376: step 1562, loss 0.216961, acc 0.875\n",
      "2017-11-05T09:38:09.156315: step 1563, loss 0.17615, acc 0.9375\n",
      "2017-11-05T09:38:13.194184: step 1564, loss 0.242427, acc 0.875\n",
      "2017-11-05T09:38:17.320616: step 1565, loss 0.0806314, acc 0.96875\n",
      "2017-11-05T09:38:21.408020: step 1566, loss 0.133192, acc 0.96875\n",
      "2017-11-05T09:38:25.558476: step 1567, loss 0.206196, acc 0.875\n",
      "2017-11-05T09:38:29.971111: step 1568, loss 0.35797, acc 0.90625\n",
      "2017-11-05T09:38:34.190609: step 1569, loss 0.20089, acc 0.90625\n",
      "2017-11-05T09:38:38.317541: step 1570, loss 0.0246824, acc 1\n",
      "2017-11-05T09:38:42.382430: step 1571, loss 0.139037, acc 0.9375\n",
      "2017-11-05T09:38:46.420298: step 1572, loss 0.165305, acc 0.9375\n",
      "2017-11-05T09:38:50.529719: step 1573, loss 0.187949, acc 0.875\n",
      "2017-11-05T09:38:54.576094: step 1574, loss 0.102094, acc 0.90625\n",
      "2017-11-05T09:38:58.666000: step 1575, loss 0.237983, acc 0.90625\n",
      "2017-11-05T09:39:02.748901: step 1576, loss 0.344245, acc 0.875\n",
      "2017-11-05T09:39:06.894846: step 1577, loss 0.120646, acc 0.9375\n",
      "2017-11-05T09:39:11.004266: step 1578, loss 0.083033, acc 0.96875\n",
      "2017-11-05T09:39:15.084666: step 1579, loss 0.162458, acc 0.90625\n",
      "2017-11-05T09:39:19.165565: step 1580, loss 0.246754, acc 0.90625\n",
      "2017-11-05T09:39:23.261477: step 1581, loss 0.385894, acc 0.78125\n",
      "2017-11-05T09:39:27.344377: step 1582, loss 0.284107, acc 0.9375\n",
      "2017-11-05T09:39:31.393754: step 1583, loss 0.156768, acc 0.96875\n",
      "2017-11-05T09:39:34.079663: step 1584, loss 0.0404263, acc 1\n",
      "2017-11-05T09:39:38.202214: step 1585, loss 0.526379, acc 0.84375\n",
      "2017-11-05T09:39:42.241084: step 1586, loss 0.362719, acc 0.875\n",
      "2017-11-05T09:39:46.430560: step 1587, loss 0.130947, acc 0.90625\n",
      "2017-11-05T09:39:50.544484: step 1588, loss 0.237473, acc 0.90625\n",
      "2017-11-05T09:39:54.574347: step 1589, loss 0.226191, acc 0.90625\n",
      "2017-11-05T09:39:58.686269: step 1590, loss 0.393901, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T09:40:03.044865: step 1591, loss 0.156045, acc 0.875\n",
      "2017-11-05T09:40:07.127766: step 1592, loss 0.212521, acc 0.90625\n",
      "2017-11-05T09:40:11.182648: step 1593, loss 0.0236903, acc 1\n",
      "2017-11-05T09:40:15.257559: step 1594, loss 0.1633, acc 0.9375\n",
      "2017-11-05T09:40:19.411009: step 1595, loss 0.402669, acc 0.875\n",
      "2017-11-05T09:40:23.496412: step 1596, loss 0.214894, acc 0.90625\n",
      "2017-11-05T09:40:27.565304: step 1597, loss 0.12015, acc 0.96875\n",
      "2017-11-05T09:40:31.650707: step 1598, loss 0.132444, acc 0.90625\n",
      "2017-11-05T09:40:35.899225: step 1599, loss 0.13268, acc 0.90625\n",
      "2017-11-05T09:40:39.955107: step 1600, loss 0.106897, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:40:42.710565: step 1600, loss 0.733699, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-05T09:40:47.923086: step 1601, loss 0.0764504, acc 0.96875\n",
      "2017-11-05T09:40:52.006498: step 1602, loss 0.152012, acc 0.9375\n",
      "2017-11-05T09:40:56.111340: step 1603, loss 0.0842766, acc 0.96875\n",
      "2017-11-05T09:41:00.130787: step 1604, loss 0.235779, acc 0.875\n",
      "2017-11-05T09:41:04.090008: step 1605, loss 0.209915, acc 0.90625\n",
      "2017-11-05T09:41:08.133560: step 1606, loss 0.380752, acc 0.84375\n",
      "2017-11-05T09:41:13.023176: step 1607, loss 0.292005, acc 0.90625\n",
      "2017-11-05T09:41:17.550893: step 1608, loss 0.075499, acc 0.96875\n",
      "2017-11-05T09:41:21.718854: step 1609, loss 0.12024, acc 0.9375\n",
      "2017-11-05T09:41:25.743682: step 1610, loss 0.0412582, acc 1\n",
      "2017-11-05T09:41:29.783568: step 1611, loss 0.224031, acc 0.875\n",
      "2017-11-05T09:41:33.842465: step 1612, loss 0.255341, acc 0.875\n",
      "2017-11-05T09:41:37.847811: step 1613, loss 0.248657, acc 0.9375\n",
      "2017-11-05T09:41:41.948746: step 1614, loss 0.146791, acc 0.90625\n",
      "2017-11-05T09:41:45.972597: step 1615, loss 0.240265, acc 0.875\n",
      "2017-11-05T09:41:50.090524: step 1616, loss 0.135063, acc 0.96875\n",
      "2017-11-05T09:41:54.201728: step 1617, loss 0.106171, acc 0.96875\n",
      "2017-11-05T09:41:58.242100: step 1618, loss 0.26547, acc 0.90625\n",
      "2017-11-05T09:42:02.408560: step 1619, loss 0.319668, acc 0.90625\n",
      "2017-11-05T09:42:05.052438: step 1620, loss 0.108312, acc 0.9\n",
      "2017-11-05T09:42:09.287948: step 1621, loss 0.126558, acc 0.90625\n",
      "2017-11-05T09:42:13.384358: step 1622, loss 0.183264, acc 0.9375\n",
      "2017-11-05T09:42:17.431234: step 1623, loss 0.300727, acc 0.875\n",
      "2017-11-05T09:42:21.465047: step 1624, loss 0.023123, acc 1\n",
      "2017-11-05T09:42:25.524432: step 1625, loss 0.202864, acc 0.90625\n",
      "2017-11-05T09:42:29.544789: step 1626, loss 0.0622836, acc 0.96875\n",
      "2017-11-05T09:42:33.691900: step 1627, loss 0.140054, acc 0.9375\n",
      "2017-11-05T09:42:37.824833: step 1628, loss 0.275156, acc 0.90625\n",
      "2017-11-05T09:42:41.898227: step 1629, loss 0.306331, acc 0.875\n",
      "2017-11-05T09:42:45.985038: step 1630, loss 0.24994, acc 0.875\n",
      "2017-11-05T09:42:50.042922: step 1631, loss 0.235957, acc 0.875\n",
      "2017-11-05T09:42:54.088796: step 1632, loss 0.0913756, acc 0.96875\n",
      "2017-11-05T09:42:58.166193: step 1633, loss 0.0954339, acc 0.96875\n",
      "2017-11-05T09:43:02.212078: step 1634, loss 0.316364, acc 0.8125\n",
      "2017-11-05T09:43:06.294980: step 1635, loss 0.0774168, acc 0.96875\n",
      "2017-11-05T09:43:10.333849: step 1636, loss 0.125967, acc 0.9375\n",
      "2017-11-05T09:43:14.385727: step 1637, loss 0.297715, acc 0.9375\n",
      "2017-11-05T09:43:18.403582: step 1638, loss 0.040741, acc 1\n",
      "2017-11-05T09:43:22.515197: step 1639, loss 0.0475541, acc 0.96875\n",
      "2017-11-05T09:43:26.681445: step 1640, loss 0.10802, acc 0.96875\n",
      "2017-11-05T09:43:30.720041: step 1641, loss 0.327159, acc 0.875\n",
      "2017-11-05T09:43:34.786931: step 1642, loss 0.137088, acc 0.9375\n",
      "2017-11-05T09:43:38.867331: step 1643, loss 0.16723, acc 0.90625\n",
      "2017-11-05T09:43:42.937223: step 1644, loss 0.119833, acc 0.90625\n",
      "2017-11-05T09:43:46.937065: step 1645, loss 0.17219, acc 0.9375\n",
      "2017-11-05T09:43:51.014962: step 1646, loss 0.020712, acc 1\n",
      "2017-11-05T09:43:55.202438: step 1647, loss 0.173059, acc 0.9375\n",
      "2017-11-05T09:43:59.231300: step 1648, loss 0.256107, acc 0.875\n",
      "2017-11-05T09:44:03.286727: step 1649, loss 0.20509, acc 0.90625\n",
      "2017-11-05T09:44:07.354815: step 1650, loss 0.207604, acc 0.84375\n",
      "2017-11-05T09:44:11.400783: step 1651, loss 0.0800543, acc 0.96875\n",
      "2017-11-05T09:44:15.504354: step 1652, loss 0.0832043, acc 0.96875\n",
      "2017-11-05T09:44:19.543723: step 1653, loss 0.418904, acc 0.84375\n",
      "2017-11-05T09:44:23.583602: step 1654, loss 0.277067, acc 0.875\n",
      "2017-11-05T09:44:27.671695: step 1655, loss 0.167176, acc 0.90625\n",
      "2017-11-05T09:44:30.292430: step 1656, loss 0.346789, acc 0.9\n",
      "2017-11-05T09:44:34.422375: step 1657, loss 0.107113, acc 0.96875\n",
      "2017-11-05T09:44:38.494443: step 1658, loss 0.285878, acc 0.90625\n",
      "2017-11-05T09:44:42.542319: step 1659, loss 0.118287, acc 0.96875\n",
      "2017-11-05T09:44:46.559598: step 1660, loss 0.0503341, acc 0.96875\n",
      "2017-11-05T09:44:50.618983: step 1661, loss 0.401636, acc 0.84375\n",
      "2017-11-05T09:44:54.638338: step 1662, loss 0.331003, acc 0.875\n",
      "2017-11-05T09:44:58.675598: step 1663, loss 0.0728988, acc 0.9375\n",
      "2017-11-05T09:45:02.805000: step 1664, loss 0.210093, acc 0.90625\n",
      "2017-11-05T09:45:06.859503: step 1665, loss 0.119389, acc 0.96875\n",
      "2017-11-05T09:45:10.919902: step 1666, loss 0.151214, acc 0.90625\n",
      "2017-11-05T09:45:14.997799: step 1667, loss 0.192263, acc 0.875\n",
      "2017-11-05T09:45:19.074696: step 1668, loss 0.0888768, acc 0.96875\n",
      "2017-11-05T09:45:23.172525: step 1669, loss 0.0998589, acc 0.90625\n",
      "2017-11-05T09:45:27.173869: step 1670, loss 0.20524, acc 0.9375\n",
      "2017-11-05T09:45:31.308306: step 1671, loss 0.17721, acc 0.9375\n",
      "2017-11-05T09:45:35.552707: step 1672, loss 0.185456, acc 0.90625\n",
      "2017-11-05T09:45:39.624491: step 1673, loss 0.1189, acc 0.9375\n",
      "2017-11-05T09:45:43.651064: step 1674, loss 0.387615, acc 0.8125\n",
      "2017-11-05T09:45:47.872564: step 1675, loss 0.0340143, acc 1\n",
      "2017-11-05T09:45:51.985486: step 1676, loss 0.247386, acc 0.9375\n",
      "2017-11-05T09:45:56.237088: step 1677, loss 0.228696, acc 0.875\n",
      "2017-11-05T09:46:00.285465: step 1678, loss 0.0702688, acc 0.96875\n",
      "2017-11-05T09:46:04.412803: step 1679, loss 0.285684, acc 0.875\n",
      "2017-11-05T09:46:08.481205: step 1680, loss 0.186762, acc 0.90625\n",
      "2017-11-05T09:46:12.570110: step 1681, loss 0.0434306, acc 1\n",
      "2017-11-05T09:46:16.577154: step 1682, loss 0.308877, acc 0.875\n",
      "2017-11-05T09:46:20.645545: step 1683, loss 0.135981, acc 0.90625\n",
      "2017-11-05T09:46:24.716938: step 1684, loss 0.0997952, acc 0.9375\n",
      "2017-11-05T09:46:28.845872: step 1685, loss 0.207504, acc 0.90625\n",
      "2017-11-05T09:46:33.014333: step 1686, loss 0.226455, acc 0.90625\n",
      "2017-11-05T09:46:37.498470: step 1687, loss 0.0717947, acc 0.9375\n",
      "2017-11-05T09:46:41.664971: step 1688, loss 0.161657, acc 0.9375\n",
      "2017-11-05T09:46:45.761284: step 1689, loss 0.295045, acc 0.84375\n",
      "2017-11-05T09:46:49.814664: step 1690, loss 0.0760181, acc 0.96875\n",
      "2017-11-05T09:46:53.950828: step 1691, loss 0.280422, acc 0.90625\n",
      "2017-11-05T09:46:56.517151: step 1692, loss 0.204325, acc 0.9\n",
      "2017-11-05T09:47:00.574033: step 1693, loss 0.103837, acc 1\n",
      "2017-11-05T09:47:04.619408: step 1694, loss 0.200063, acc 0.9375\n",
      "2017-11-05T09:47:08.701308: step 1695, loss 0.2116, acc 0.90625\n",
      "2017-11-05T09:47:12.760642: step 1696, loss 0.261632, acc 0.90625\n",
      "2017-11-05T09:47:16.836471: step 1697, loss 0.0228494, acc 1\n",
      "2017-11-05T09:47:20.839947: step 1698, loss 0.0262351, acc 1\n",
      "2017-11-05T09:47:24.830282: step 1699, loss 0.151279, acc 0.875\n",
      "2017-11-05T09:47:28.933587: step 1700, loss 0.217, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:47:31.607987: step 1700, loss 0.694004, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-05T09:47:36.897778: step 1701, loss 0.24282, acc 0.875\n",
      "2017-11-05T09:47:41.022933: step 1702, loss 0.261802, acc 0.84375\n",
      "2017-11-05T09:47:45.113840: step 1703, loss 0.190774, acc 0.9375\n",
      "2017-11-05T09:47:49.251398: step 1704, loss 0.116601, acc 0.9375\n",
      "2017-11-05T09:47:53.360316: step 1705, loss 0.201164, acc 0.90625\n",
      "2017-11-05T09:47:57.428227: step 1706, loss 0.367889, acc 0.78125\n",
      "2017-11-05T09:48:01.346398: step 1707, loss 0.258051, acc 0.875\n",
      "2017-11-05T09:48:05.296593: step 1708, loss 0.345958, acc 0.8125\n",
      "2017-11-05T09:48:09.242408: step 1709, loss 0.199082, acc 0.90625\n",
      "2017-11-05T09:48:13.210117: step 1710, loss 0.108621, acc 0.9375\n",
      "2017-11-05T09:48:17.210470: step 1711, loss 0.0595957, acc 0.96875\n",
      "2017-11-05T09:48:21.163675: step 1712, loss 0.0532804, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T09:48:25.250138: step 1713, loss 0.161523, acc 0.90625\n",
      "2017-11-05T09:48:29.466989: step 1714, loss 0.123016, acc 0.96875\n",
      "2017-11-05T09:48:33.518368: step 1715, loss 0.15773, acc 0.90625\n",
      "2017-11-05T09:48:37.552939: step 1716, loss 0.19546, acc 0.90625\n",
      "2017-11-05T09:48:41.669472: step 1717, loss 0.391084, acc 0.875\n",
      "2017-11-05T09:48:45.670314: step 1718, loss 0.15237, acc 0.90625\n",
      "2017-11-05T09:48:49.782736: step 1719, loss 0.165627, acc 0.9375\n",
      "2017-11-05T09:48:53.959704: step 1720, loss 0.281237, acc 0.875\n",
      "2017-11-05T09:48:58.017587: step 1721, loss 0.1754, acc 0.9375\n",
      "2017-11-05T09:49:02.203562: step 1722, loss 0.307384, acc 0.84375\n",
      "2017-11-05T09:49:06.222918: step 1723, loss 0.208481, acc 0.9375\n",
      "2017-11-05T09:49:10.275852: step 1724, loss 0.195357, acc 0.90625\n",
      "2017-11-05T09:49:14.554892: step 1725, loss 0.15224, acc 0.90625\n",
      "2017-11-05T09:49:18.561756: step 1726, loss 0.133423, acc 0.9375\n",
      "2017-11-05T09:49:22.714207: step 1727, loss 0.180795, acc 0.90625\n",
      "2017-11-05T09:49:25.343576: step 1728, loss 0.157115, acc 0.95\n",
      "2017-11-05T09:49:29.523793: step 1729, loss 0.180402, acc 0.875\n",
      "2017-11-05T09:49:33.699260: step 1730, loss 0.244443, acc 0.84375\n",
      "2017-11-05T09:49:37.900245: step 1731, loss 0.0561441, acc 0.96875\n",
      "2017-11-05T09:49:41.961130: step 1732, loss 0.207584, acc 0.9375\n",
      "2017-11-05T09:49:46.096068: step 1733, loss 0.0599868, acc 1\n",
      "2017-11-05T09:49:50.184473: step 1734, loss 0.0814462, acc 0.96875\n",
      "2017-11-05T09:49:54.350934: step 1735, loss 0.145255, acc 0.9375\n",
      "2017-11-05T09:49:58.392264: step 1736, loss 0.012714, acc 1\n",
      "2017-11-05T09:50:02.690342: step 1737, loss 0.132135, acc 0.90625\n",
      "2017-11-05T09:50:06.659294: step 1738, loss 0.338527, acc 0.90625\n",
      "2017-11-05T09:50:10.714447: step 1739, loss 0.103125, acc 0.96875\n",
      "2017-11-05T09:50:14.743547: step 1740, loss 0.109094, acc 0.9375\n",
      "2017-11-05T09:50:18.687237: step 1741, loss 0.383931, acc 0.8125\n",
      "2017-11-05T09:50:22.649553: step 1742, loss 0.191795, acc 0.9375\n",
      "2017-11-05T09:50:26.659919: step 1743, loss 0.0597946, acc 0.96875\n",
      "2017-11-05T09:50:30.643256: step 1744, loss 0.244723, acc 0.90625\n",
      "2017-11-05T09:50:34.740167: step 1745, loss 0.345526, acc 0.875\n",
      "2017-11-05T09:50:38.760524: step 1746, loss 0.295267, acc 0.875\n",
      "2017-11-05T09:50:42.776377: step 1747, loss 0.0670793, acc 0.96875\n",
      "2017-11-05T09:50:46.790229: step 1748, loss 0.163247, acc 0.9375\n",
      "2017-11-05T09:50:50.806083: step 1749, loss 0.250127, acc 0.9375\n",
      "2017-11-05T09:50:54.878478: step 1750, loss 0.173395, acc 0.90625\n",
      "2017-11-05T09:50:58.970384: step 1751, loss 0.144328, acc 0.96875\n",
      "2017-11-05T09:51:02.985737: step 1752, loss 0.295163, acc 0.84375\n",
      "2017-11-05T09:51:07.056629: step 1753, loss 0.182841, acc 0.875\n",
      "2017-11-05T09:51:11.161546: step 1754, loss 0.491062, acc 0.84375\n",
      "2017-11-05T09:51:15.128752: step 1755, loss 0.390969, acc 0.875\n",
      "2017-11-05T09:51:19.146134: step 1756, loss 0.115604, acc 0.9375\n",
      "2017-11-05T09:51:23.102155: step 1757, loss 0.28066, acc 0.90625\n",
      "2017-11-05T09:51:27.106010: step 1758, loss 0.179876, acc 0.875\n",
      "2017-11-05T09:51:31.107854: step 1759, loss 0.317941, acc 0.8125\n",
      "2017-11-05T09:51:35.109599: step 1760, loss 0.128792, acc 0.9375\n",
      "2017-11-05T09:51:39.149982: step 1761, loss 0.259772, acc 0.90625\n",
      "2017-11-05T09:51:43.168837: step 1762, loss 0.306397, acc 0.84375\n",
      "2017-11-05T09:51:47.100848: step 1763, loss 0.591993, acc 0.84375\n",
      "2017-11-05T09:51:49.764741: step 1764, loss 0.0236393, acc 1\n",
      "2017-11-05T09:51:53.755150: step 1765, loss 0.248344, acc 0.90625\n",
      "2017-11-05T09:51:57.716096: step 1766, loss 0.501572, acc 0.84375\n",
      "2017-11-05T09:52:01.732643: step 1767, loss 0.077107, acc 0.96875\n",
      "2017-11-05T09:52:05.828327: step 1768, loss 0.3285, acc 0.875\n",
      "2017-11-05T09:52:09.845194: step 1769, loss 0.14315, acc 0.9375\n",
      "2017-11-05T09:52:13.881937: step 1770, loss 0.285725, acc 0.90625\n",
      "2017-11-05T09:52:17.828891: step 1771, loss 0.416505, acc 0.875\n",
      "2017-11-05T09:52:21.820629: step 1772, loss 0.0545929, acc 1\n",
      "2017-11-05T09:52:25.798355: step 1773, loss 0.101707, acc 0.9375\n",
      "2017-11-05T09:52:29.793627: step 1774, loss 0.301284, acc 0.90625\n",
      "2017-11-05T09:52:33.890227: step 1775, loss 0.0505699, acc 0.96875\n",
      "2017-11-05T09:52:37.952768: step 1776, loss 0.0688397, acc 0.96875\n",
      "2017-11-05T09:52:41.963118: step 1777, loss 0.227886, acc 0.90625\n",
      "2017-11-05T09:52:45.969759: step 1778, loss 0.0648686, acc 0.96875\n",
      "2017-11-05T09:52:49.933768: step 1779, loss 0.241061, acc 0.90625\n",
      "2017-11-05T09:52:53.989551: step 1780, loss 0.128314, acc 0.9375\n",
      "2017-11-05T09:52:57.962874: step 1781, loss 0.232908, acc 0.90625\n",
      "2017-11-05T09:53:01.977226: step 1782, loss 0.301539, acc 0.84375\n",
      "2017-11-05T09:53:05.904787: step 1783, loss 0.356033, acc 0.8125\n",
      "2017-11-05T09:53:09.932761: step 1784, loss 0.184245, acc 0.90625\n",
      "2017-11-05T09:53:13.917481: step 1785, loss 0.2342, acc 0.90625\n",
      "2017-11-05T09:53:17.940660: step 1786, loss 0.145699, acc 0.9375\n",
      "2017-11-05T09:53:22.057585: step 1787, loss 0.209605, acc 0.90625\n",
      "2017-11-05T09:53:26.166506: step 1788, loss 0.279437, acc 0.875\n",
      "2017-11-05T09:53:30.150741: step 1789, loss 0.0875254, acc 0.96875\n",
      "2017-11-05T09:53:34.183755: step 1790, loss 0.0829131, acc 0.96875\n",
      "2017-11-05T09:53:38.271429: step 1791, loss 0.119138, acc 0.9375\n",
      "2017-11-05T09:53:42.225541: step 1792, loss 0.0542034, acc 0.96875\n",
      "2017-11-05T09:53:46.265939: step 1793, loss 0.0470581, acc 1\n",
      "2017-11-05T09:53:50.298291: step 1794, loss 0.132236, acc 0.9375\n",
      "2017-11-05T09:53:54.276242: step 1795, loss 0.314877, acc 0.90625\n",
      "2017-11-05T09:53:58.297257: step 1796, loss 0.133792, acc 0.9375\n",
      "2017-11-05T09:54:02.303146: step 1797, loss 0.0663915, acc 0.96875\n",
      "2017-11-05T09:54:06.284975: step 1798, loss 0.294978, acc 0.84375\n",
      "2017-11-05T09:54:10.372769: step 1799, loss 0.115901, acc 0.96875\n",
      "2017-11-05T09:54:12.872932: step 1800, loss 0.243497, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T09:54:15.440758: step 1800, loss 0.857431, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-05T09:54:20.822900: step 1801, loss 0.189419, acc 0.875\n",
      "2017-11-05T09:54:24.778559: step 1802, loss 0.380751, acc 0.84375\n",
      "2017-11-05T09:54:28.740379: step 1803, loss 0.154744, acc 0.90625\n",
      "2017-11-05T09:54:32.737722: step 1804, loss 0.23841, acc 0.90625\n",
      "2017-11-05T09:54:36.827213: step 1805, loss 0.023284, acc 1\n",
      "2017-11-05T09:54:40.827251: step 1806, loss 0.35669, acc 0.84375\n",
      "2017-11-05T09:54:44.816585: step 1807, loss 0.205383, acc 0.875\n",
      "2017-11-05T09:54:48.760888: step 1808, loss 0.0742073, acc 0.9375\n",
      "2017-11-05T09:54:52.813267: step 1809, loss 0.343208, acc 0.875\n",
      "2017-11-05T09:54:56.847634: step 1810, loss 0.0675385, acc 0.96875\n",
      "2017-11-05T09:55:00.849979: step 1811, loss 0.277446, acc 0.875\n",
      "2017-11-05T09:55:04.945901: step 1812, loss 0.285021, acc 0.875\n",
      "2017-11-05T09:55:08.969790: step 1813, loss 0.46991, acc 0.84375\n",
      "2017-11-05T09:55:12.971133: step 1814, loss 0.101887, acc 0.9375\n",
      "2017-11-05T09:55:16.973492: step 1815, loss 0.31888, acc 0.875\n",
      "2017-11-05T09:55:20.992348: step 1816, loss 0.0969456, acc 0.96875\n",
      "2017-11-05T09:55:24.982683: step 1817, loss 0.184062, acc 0.875\n",
      "2017-11-05T09:55:29.040550: step 1818, loss 0.111364, acc 0.9375\n",
      "2017-11-05T09:55:33.056961: step 1819, loss 0.174985, acc 0.90625\n",
      "2017-11-05T09:55:37.095861: step 1820, loss 0.187135, acc 0.875\n",
      "2017-11-05T09:55:41.189091: step 1821, loss 0.206127, acc 0.875\n",
      "2017-11-05T09:55:45.235719: step 1822, loss 0.210956, acc 0.9375\n",
      "2017-11-05T09:55:49.282383: step 1823, loss 0.157234, acc 0.9375\n",
      "2017-11-05T09:55:53.312441: step 1824, loss 0.100102, acc 0.96875\n",
      "2017-11-05T09:55:57.318787: step 1825, loss 0.0834472, acc 1\n",
      "2017-11-05T09:56:01.309159: step 1826, loss 0.232278, acc 0.875\n",
      "2017-11-05T09:56:05.333712: step 1827, loss 0.265506, acc 0.90625\n",
      "2017-11-05T09:56:09.388990: step 1828, loss 0.17221, acc 0.90625\n",
      "2017-11-05T09:56:13.465886: step 1829, loss 0.244198, acc 0.9375\n",
      "2017-11-05T09:56:17.436403: step 1830, loss 0.165352, acc 0.9375\n",
      "2017-11-05T09:56:21.438747: step 1831, loss 0.251646, acc 0.90625\n",
      "2017-11-05T09:56:25.466040: step 1832, loss 0.201902, acc 0.90625\n",
      "2017-11-05T09:56:29.446870: step 1833, loss 0.121822, acc 0.96875\n",
      "2017-11-05T09:56:33.490742: step 1834, loss 0.127259, acc 0.9375\n",
      "2017-11-05T09:56:37.543509: step 1835, loss 0.0735376, acc 0.9375\n",
      "2017-11-05T09:56:40.156254: step 1836, loss 0.0352366, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T09:56:44.180793: step 1837, loss 0.162504, acc 0.9375\n",
      "2017-11-05T09:56:48.203652: step 1838, loss 0.15444, acc 0.90625\n",
      "2017-11-05T09:56:52.224905: step 1839, loss 0.321559, acc 0.84375\n",
      "2017-11-05T09:56:56.280810: step 1840, loss 0.400983, acc 0.8125\n",
      "2017-11-05T09:57:00.290056: step 1841, loss 0.243413, acc 0.875\n",
      "2017-11-05T09:57:04.293176: step 1842, loss 0.212704, acc 0.9375\n",
      "2017-11-05T09:57:08.385663: step 1843, loss 0.17338, acc 0.9375\n",
      "2017-11-05T09:57:12.379000: step 1844, loss 0.244321, acc 0.84375\n",
      "2017-11-05T09:57:16.372319: step 1845, loss 0.153956, acc 0.9375\n",
      "2017-11-05T09:57:20.473733: step 1846, loss 0.0653466, acc 1\n",
      "2017-11-05T09:57:24.479579: step 1847, loss 0.130827, acc 0.90625\n",
      "2017-11-05T09:57:28.528501: step 1848, loss 0.227427, acc 0.90625\n",
      "2017-11-05T09:57:32.550358: step 1849, loss 0.274442, acc 0.90625\n",
      "2017-11-05T09:57:36.618256: step 1850, loss 0.251395, acc 0.90625\n",
      "2017-11-05T09:57:40.632897: step 1851, loss 0.353267, acc 0.875\n",
      "2017-11-05T09:57:44.730581: step 1852, loss 0.250314, acc 0.90625\n",
      "2017-11-05T09:57:48.820487: step 1853, loss 0.220817, acc 0.90625\n",
      "2017-11-05T09:57:52.937412: step 1854, loss 0.0693433, acc 0.96875\n",
      "2017-11-05T09:57:56.935252: step 1855, loss 0.0980058, acc 0.9375\n",
      "2017-11-05T09:58:00.989353: step 1856, loss 0.157681, acc 0.90625\n",
      "2017-11-05T09:58:05.065750: step 1857, loss 0.10179, acc 0.9375\n",
      "2017-11-05T09:58:09.120147: step 1858, loss 0.373434, acc 0.8125\n",
      "2017-11-05T09:58:13.116691: step 1859, loss 0.377554, acc 0.90625\n",
      "2017-11-05T09:58:17.212616: step 1860, loss 0.285076, acc 0.875\n",
      "2017-11-05T09:58:21.215961: step 1861, loss 0.142797, acc 0.90625\n",
      "2017-11-05T09:58:25.371098: step 1862, loss 0.284795, acc 0.84375\n",
      "2017-11-05T09:58:29.656160: step 1863, loss 0.173891, acc 0.875\n",
      "2017-11-05T09:58:33.821702: step 1864, loss 0.103557, acc 0.9375\n",
      "2017-11-05T09:58:37.941130: step 1865, loss 0.164719, acc 0.90625\n",
      "2017-11-05T09:58:42.000013: step 1866, loss 0.0866824, acc 0.96875\n",
      "2017-11-05T09:58:46.008361: step 1867, loss 0.0444391, acc 1\n",
      "2017-11-05T09:58:50.000307: step 1868, loss 0.213632, acc 0.90625\n",
      "2017-11-05T09:58:54.114616: step 1869, loss 0.150836, acc 0.9375\n",
      "2017-11-05T09:58:58.208025: step 1870, loss 0.413163, acc 0.84375\n",
      "2017-11-05T09:59:02.241891: step 1871, loss 0.128809, acc 0.9375\n",
      "2017-11-05T09:59:04.829229: step 1872, loss 0.143687, acc 0.95\n",
      "2017-11-05T09:59:08.879608: step 1873, loss 0.141624, acc 0.9375\n",
      "2017-11-05T09:59:13.040564: step 1874, loss 0.112076, acc 0.9375\n",
      "2017-11-05T09:59:17.078933: step 1875, loss 0.329512, acc 0.8125\n",
      "2017-11-05T09:59:21.171842: step 1876, loss 0.0952599, acc 0.9375\n",
      "2017-11-05T09:59:25.148167: step 1877, loss 0.362037, acc 0.78125\n",
      "2017-11-05T09:59:29.216574: step 1878, loss 0.192798, acc 0.90625\n",
      "2017-11-05T09:59:33.276620: step 1879, loss 0.130647, acc 0.96875\n",
      "2017-11-05T09:59:37.308486: step 1880, loss 0.384794, acc 0.84375\n",
      "2017-11-05T09:59:41.342874: step 1881, loss 0.0842121, acc 0.9375\n",
      "2017-11-05T09:59:45.483316: step 1882, loss 0.10813, acc 0.96875\n",
      "2017-11-05T09:59:49.521686: step 1883, loss 0.201901, acc 0.90625\n",
      "2017-11-05T09:59:53.572064: step 1884, loss 0.109811, acc 0.9375\n",
      "2017-11-05T09:59:57.640129: step 1885, loss 0.197138, acc 0.90625\n",
      "2017-11-05T10:00:02.144329: step 1886, loss 0.125052, acc 0.96875\n",
      "2017-11-05T10:00:06.244743: step 1887, loss 0.0265967, acc 1\n",
      "2017-11-05T10:00:10.283114: step 1888, loss 0.222423, acc 0.9375\n",
      "2017-11-05T10:00:14.358161: step 1889, loss 0.178399, acc 0.90625\n",
      "2017-11-05T10:00:18.397605: step 1890, loss 0.274919, acc 0.875\n",
      "2017-11-05T10:00:22.391978: step 1891, loss 0.187881, acc 0.9375\n",
      "2017-11-05T10:00:26.448420: step 1892, loss 0.140693, acc 0.9375\n",
      "2017-11-05T10:00:30.480814: step 1893, loss 0.151215, acc 0.875\n",
      "2017-11-05T10:00:34.570776: step 1894, loss 0.177737, acc 0.875\n",
      "2017-11-05T10:00:38.702074: step 1895, loss 0.174385, acc 0.90625\n",
      "2017-11-05T10:00:42.709921: step 1896, loss 0.154525, acc 0.90625\n",
      "2017-11-05T10:00:46.677127: step 1897, loss 0.212697, acc 0.9375\n",
      "2017-11-05T10:00:50.637554: step 1898, loss 0.142365, acc 0.96875\n",
      "2017-11-05T10:00:54.569400: step 1899, loss 0.210231, acc 0.875\n",
      "2017-11-05T10:00:58.517407: step 1900, loss 0.229583, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:01:01.142286: step 1900, loss 0.65916, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-05T10:01:06.590731: step 1901, loss 0.152676, acc 0.90625\n",
      "2017-11-05T10:01:10.644612: step 1902, loss 0.246925, acc 0.78125\n",
      "2017-11-05T10:01:14.658963: step 1903, loss 0.21498, acc 0.9375\n",
      "2017-11-05T10:01:18.625782: step 1904, loss 0.20971, acc 0.90625\n",
      "2017-11-05T10:01:22.588599: step 1905, loss 0.284494, acc 0.84375\n",
      "2017-11-05T10:01:26.578433: step 1906, loss 0.0897518, acc 0.96875\n",
      "2017-11-05T10:01:30.533243: step 1907, loss 0.133187, acc 0.96875\n",
      "2017-11-05T10:01:33.121582: step 1908, loss 0.00494527, acc 1\n",
      "2017-11-05T10:01:37.226499: step 1909, loss 0.326849, acc 0.875\n",
      "2017-11-05T10:01:41.221838: step 1910, loss 0.242327, acc 0.90625\n",
      "2017-11-05T10:01:45.291229: step 1911, loss 0.311993, acc 0.84375\n",
      "2017-11-05T10:01:49.280564: step 1912, loss 0.190004, acc 0.875\n",
      "2017-11-05T10:01:53.394988: step 1913, loss 0.153727, acc 0.9375\n",
      "2017-11-05T10:01:57.422349: step 1914, loss 0.0911472, acc 0.96875\n",
      "2017-11-05T10:02:01.475229: step 1915, loss 0.187716, acc 0.9375\n",
      "2017-11-05T10:02:05.554127: step 1916, loss 0.285131, acc 0.9375\n",
      "2017-11-05T10:02:09.568980: step 1917, loss 0.1091, acc 0.9375\n",
      "2017-11-05T10:02:13.667393: step 1918, loss 0.161005, acc 0.9375\n",
      "2017-11-05T10:02:17.690750: step 1919, loss 0.119487, acc 0.9375\n",
      "2017-11-05T10:02:21.735625: step 1920, loss 0.331141, acc 0.84375\n",
      "2017-11-05T10:02:25.778024: step 1921, loss 0.0637902, acc 0.96875\n",
      "2017-11-05T10:02:29.803888: step 1922, loss 0.0891999, acc 0.96875\n",
      "2017-11-05T10:02:33.895795: step 1923, loss 0.100668, acc 0.9375\n",
      "2017-11-05T10:02:37.987203: step 1924, loss 0.176144, acc 0.90625\n",
      "2017-11-05T10:02:42.023070: step 1925, loss 0.111042, acc 0.9375\n",
      "2017-11-05T10:02:46.042426: step 1926, loss 0.403844, acc 0.875\n",
      "2017-11-05T10:02:50.023755: step 1927, loss 0.161492, acc 0.9375\n",
      "2017-11-05T10:02:54.104155: step 1928, loss 0.14468, acc 0.90625\n",
      "2017-11-05T10:02:58.111502: step 1929, loss 0.198908, acc 0.90625\n",
      "2017-11-05T10:03:02.120851: step 1930, loss 0.158713, acc 0.9375\n",
      "2017-11-05T10:03:06.094675: step 1931, loss 0.305532, acc 0.84375\n",
      "2017-11-05T10:03:10.098019: step 1932, loss 0.245488, acc 0.875\n",
      "2017-11-05T10:03:14.235461: step 1933, loss 0.16487, acc 0.90625\n",
      "2017-11-05T10:03:19.175619: step 1934, loss 0.152173, acc 0.90625\n",
      "2017-11-05T10:03:23.629784: step 1935, loss 0.181841, acc 0.9375\n",
      "2017-11-05T10:03:27.799258: step 1936, loss 0.28668, acc 0.875\n",
      "2017-11-05T10:03:31.909613: step 1937, loss 0.130701, acc 0.90625\n",
      "2017-11-05T10:03:36.130793: step 1938, loss 0.241141, acc 0.8125\n",
      "2017-11-05T10:03:40.875215: step 1939, loss 0.103269, acc 0.9375\n",
      "2017-11-05T10:03:44.961155: step 1940, loss 0.164298, acc 0.90625\n",
      "2017-11-05T10:03:48.940983: step 1941, loss 0.05519, acc 0.96875\n",
      "2017-11-05T10:03:52.922824: step 1942, loss 0.156634, acc 0.9375\n",
      "2017-11-05T10:03:56.902212: step 1943, loss 0.11824, acc 0.96875\n",
      "2017-11-05T10:03:59.460964: step 1944, loss 0.110007, acc 0.95\n",
      "2017-11-05T10:04:03.435298: step 1945, loss 0.192828, acc 0.90625\n",
      "2017-11-05T10:04:07.427647: step 1946, loss 0.0122897, acc 1\n",
      "2017-11-05T10:04:11.384458: step 1947, loss 0.121426, acc 0.9375\n",
      "2017-11-05T10:04:15.321023: step 1948, loss 0.137218, acc 0.90625\n",
      "2017-11-05T10:04:19.243262: step 1949, loss 0.127114, acc 0.9375\n",
      "2017-11-05T10:04:23.236611: step 1950, loss 0.173058, acc 0.90625\n",
      "2017-11-05T10:04:27.201441: step 1951, loss 0.28649, acc 0.90625\n",
      "2017-11-05T10:04:31.115222: step 1952, loss 0.193664, acc 0.875\n",
      "2017-11-05T10:04:35.218177: step 1953, loss 0.128411, acc 0.9375\n",
      "2017-11-05T10:04:39.153913: step 1954, loss 0.128486, acc 0.9375\n",
      "2017-11-05T10:04:43.095617: step 1955, loss 0.251205, acc 0.9375\n",
      "2017-11-05T10:04:47.067440: step 1956, loss 0.252948, acc 0.90625\n",
      "2017-11-05T10:04:50.991253: step 1957, loss 0.148879, acc 0.96875\n",
      "2017-11-05T10:04:54.958070: step 1958, loss 0.111986, acc 0.9375\n",
      "2017-11-05T10:04:58.861854: step 1959, loss 0.255756, acc 0.875\n",
      "2017-11-05T10:05:02.840682: step 1960, loss 0.123865, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T10:05:06.825527: step 1961, loss 0.0065985, acc 1\n",
      "2017-11-05T10:05:10.789844: step 1962, loss 0.303593, acc 0.84375\n",
      "2017-11-05T10:05:14.720659: step 1963, loss 0.248799, acc 0.875\n",
      "2017-11-05T10:05:18.658002: step 1964, loss 0.513301, acc 0.84375\n",
      "2017-11-05T10:05:22.715536: step 1965, loss 0.0840431, acc 0.9375\n",
      "2017-11-05T10:05:26.715879: step 1966, loss 0.180332, acc 0.90625\n",
      "2017-11-05T10:05:30.718222: step 1967, loss 0.190089, acc 0.84375\n",
      "2017-11-05T10:05:34.640509: step 1968, loss 0.332538, acc 0.78125\n",
      "2017-11-05T10:05:38.637598: step 1969, loss 0.166348, acc 0.90625\n",
      "2017-11-05T10:05:42.620184: step 1970, loss 0.169772, acc 0.96875\n",
      "2017-11-05T10:05:46.677566: step 1971, loss 0.179231, acc 0.90625\n",
      "2017-11-05T10:05:50.629891: step 1972, loss 0.185739, acc 0.90625\n",
      "2017-11-05T10:05:54.619726: step 1973, loss 0.387561, acc 0.875\n",
      "2017-11-05T10:05:58.654593: step 1974, loss 0.213533, acc 0.90625\n",
      "2017-11-05T10:06:02.622913: step 1975, loss 0.125698, acc 0.90625\n",
      "2017-11-05T10:06:06.718850: step 1976, loss 0.328142, acc 0.90625\n",
      "2017-11-05T10:06:10.707684: step 1977, loss 0.147281, acc 0.9375\n",
      "2017-11-05T10:06:14.763066: step 1978, loss 0.019064, acc 1\n",
      "2017-11-05T10:06:18.829869: step 1979, loss 0.234955, acc 0.96875\n",
      "2017-11-05T10:06:21.360166: step 1980, loss 0.0833896, acc 0.95\n",
      "2017-11-05T10:06:25.438564: step 1981, loss 0.30055, acc 0.9375\n",
      "2017-11-05T10:06:29.537977: step 1982, loss 0.232431, acc 0.9375\n",
      "2017-11-05T10:06:33.717474: step 1983, loss 0.214698, acc 0.96875\n",
      "2017-11-05T10:06:37.806513: step 1984, loss 0.510436, acc 0.90625\n",
      "2017-11-05T10:06:41.888413: step 1985, loss 0.0565541, acc 0.96875\n",
      "2017-11-05T10:06:45.900652: step 1986, loss 0.377146, acc 0.84375\n",
      "2017-11-05T10:06:49.915504: step 1987, loss 0.212734, acc 0.90625\n",
      "2017-11-05T10:06:53.940364: step 1988, loss 0.152632, acc 0.90625\n",
      "2017-11-05T10:06:57.959721: step 1989, loss 0.310734, acc 0.90625\n",
      "2017-11-05T10:07:02.014102: step 1990, loss 0.335429, acc 0.9375\n",
      "2017-11-05T10:07:06.046967: step 1991, loss 0.475159, acc 0.78125\n",
      "2017-11-05T10:07:10.082334: step 1992, loss 0.14018, acc 0.96875\n",
      "2017-11-05T10:07:14.102190: step 1993, loss 0.252061, acc 0.875\n",
      "2017-11-05T10:07:18.069015: step 1994, loss 0.359323, acc 0.875\n",
      "2017-11-05T10:07:22.090205: step 1995, loss 0.165766, acc 0.90625\n",
      "2017-11-05T10:07:26.172107: step 1996, loss 0.404815, acc 0.875\n",
      "2017-11-05T10:07:30.183455: step 1997, loss 0.222251, acc 0.9375\n",
      "2017-11-05T10:07:34.282868: step 1998, loss 0.0358496, acc 0.96875\n",
      "2017-11-05T10:07:38.306227: step 1999, loss 0.0928073, acc 0.96875\n",
      "2017-11-05T10:07:42.346598: step 2000, loss 0.16826, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:07:44.981972: step 2000, loss 1.13479, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-05T10:07:50.905961: step 2001, loss 0.764432, acc 0.875\n",
      "2017-11-05T10:07:54.955338: step 2002, loss 0.0696107, acc 0.96875\n",
      "2017-11-05T10:07:58.967690: step 2003, loss 0.28951, acc 0.90625\n",
      "2017-11-05T10:08:02.997052: step 2004, loss 0.102712, acc 0.96875\n",
      "2017-11-05T10:08:07.003899: step 2005, loss 0.167361, acc 0.90625\n",
      "2017-11-05T10:08:11.051775: step 2006, loss 0.065603, acc 0.96875\n",
      "2017-11-05T10:08:15.127672: step 2007, loss 0.296062, acc 0.875\n",
      "2017-11-05T10:08:19.173546: step 2008, loss 0.23361, acc 0.84375\n",
      "2017-11-05T10:08:23.365024: step 2009, loss 0.206419, acc 0.9375\n",
      "2017-11-05T10:08:27.595531: step 2010, loss 0.0742507, acc 0.9375\n",
      "2017-11-05T10:08:31.589368: step 2011, loss 0.240009, acc 0.9375\n",
      "2017-11-05T10:08:35.767838: step 2012, loss 0.29954, acc 0.875\n",
      "2017-11-05T10:08:39.802204: step 2013, loss 0.0261929, acc 1\n",
      "2017-11-05T10:08:43.856585: step 2014, loss 0.132133, acc 0.9375\n",
      "2017-11-05T10:08:47.918971: step 2015, loss 0.310735, acc 0.8125\n",
      "2017-11-05T10:08:50.524823: step 2016, loss 0.166784, acc 0.85\n",
      "2017-11-05T10:08:54.606223: step 2017, loss 0.0389052, acc 1\n",
      "2017-11-05T10:08:58.629081: step 2018, loss 0.317446, acc 0.90625\n",
      "2017-11-05T10:09:02.708980: step 2019, loss 0.0134629, acc 1\n",
      "2017-11-05T10:09:06.700816: step 2020, loss 0.147469, acc 0.96875\n",
      "2017-11-05T10:09:10.742188: step 2021, loss 0.219041, acc 0.9375\n",
      "2017-11-05T10:09:14.767048: step 2022, loss 0.345502, acc 0.90625\n",
      "2017-11-05T10:09:18.835939: step 2023, loss 0.264629, acc 0.9375\n",
      "2017-11-05T10:09:22.895324: step 2024, loss 0.365068, acc 0.90625\n",
      "2017-11-05T10:09:26.957210: step 2025, loss 0.16502, acc 0.9375\n",
      "2017-11-05T10:09:31.016093: step 2026, loss 0.100484, acc 0.96875\n",
      "2017-11-05T10:09:35.092490: step 2027, loss 0.242393, acc 0.90625\n",
      "2017-11-05T10:09:39.126356: step 2028, loss 0.156685, acc 0.90625\n",
      "2017-11-05T10:09:43.179737: step 2029, loss 0.227731, acc 0.875\n",
      "2017-11-05T10:09:47.203596: step 2030, loss 0.201704, acc 0.9375\n",
      "2017-11-05T10:09:51.281993: step 2031, loss 0.176345, acc 0.90625\n",
      "2017-11-05T10:09:55.382907: step 2032, loss 0.0756012, acc 0.96875\n",
      "2017-11-05T10:09:59.453299: step 2033, loss 0.0315055, acc 1\n",
      "2017-11-05T10:10:03.877943: step 2034, loss 0.150103, acc 0.90625\n",
      "2017-11-05T10:10:07.967864: step 2035, loss 0.321346, acc 0.84375\n",
      "2017-11-05T10:10:12.117312: step 2036, loss 0.208738, acc 0.875\n",
      "2017-11-05T10:10:16.177698: step 2037, loss 0.28144, acc 0.875\n",
      "2017-11-05T10:10:20.316638: step 2038, loss 0.273603, acc 0.90625\n",
      "2017-11-05T10:10:24.419053: step 2039, loss 0.472802, acc 0.84375\n",
      "2017-11-05T10:10:28.488953: step 2040, loss 0.135358, acc 0.90625\n",
      "2017-11-05T10:10:32.524321: step 2041, loss 0.196342, acc 0.9375\n",
      "2017-11-05T10:10:36.646749: step 2042, loss 0.112305, acc 0.9375\n",
      "2017-11-05T10:10:40.749165: step 2043, loss 0.374237, acc 0.78125\n",
      "2017-11-05T10:10:44.769521: step 2044, loss 0.165617, acc 0.9375\n",
      "2017-11-05T10:10:48.823902: step 2045, loss 0.0845657, acc 0.96875\n",
      "2017-11-05T10:10:52.894295: step 2046, loss 0.0638351, acc 1\n",
      "2017-11-05T10:10:56.992206: step 2047, loss 0.0924659, acc 0.96875\n",
      "2017-11-05T10:11:01.109632: step 2048, loss 0.0995948, acc 0.9375\n",
      "2017-11-05T10:11:05.158508: step 2049, loss 0.128904, acc 0.9375\n",
      "2017-11-05T10:11:09.209387: step 2050, loss 0.138721, acc 0.9375\n",
      "2017-11-05T10:11:13.261766: step 2051, loss 0.293266, acc 0.90625\n",
      "2017-11-05T10:11:15.891134: step 2052, loss 0.3481, acc 0.9\n",
      "2017-11-05T10:11:19.956023: step 2053, loss 0.0905278, acc 0.9375\n",
      "2017-11-05T10:11:24.063942: step 2054, loss 0.14524, acc 0.9375\n",
      "2017-11-05T10:11:28.055277: step 2055, loss 0.106352, acc 0.96875\n",
      "2017-11-05T10:11:32.125670: step 2056, loss 0.392248, acc 0.84375\n",
      "2017-11-05T10:11:36.174547: step 2057, loss 0.107127, acc 0.90625\n",
      "2017-11-05T10:11:40.272458: step 2058, loss 0.144756, acc 0.96875\n",
      "2017-11-05T10:11:44.273301: step 2059, loss 0.300618, acc 0.875\n",
      "2017-11-05T10:11:48.370712: step 2060, loss 0.164596, acc 0.90625\n",
      "2017-11-05T10:11:52.484635: step 2061, loss 0.0807442, acc 0.96875\n",
      "2017-11-05T10:11:56.660603: step 2062, loss 0.077829, acc 0.9375\n",
      "2017-11-05T10:12:00.780530: step 2063, loss 0.150456, acc 0.90625\n",
      "2017-11-05T10:12:04.812895: step 2064, loss 0.100233, acc 0.96875\n",
      "2017-11-05T10:12:08.955839: step 2065, loss 0.211206, acc 0.90625\n",
      "2017-11-05T10:12:13.073765: step 2066, loss 0.117918, acc 0.9375\n",
      "2017-11-05T10:12:17.059597: step 2067, loss 0.224547, acc 0.84375\n",
      "2017-11-05T10:12:21.161512: step 2068, loss 0.223986, acc 0.90625\n",
      "2017-11-05T10:12:25.244413: step 2069, loss 0.176424, acc 0.9375\n",
      "2017-11-05T10:12:29.295291: step 2070, loss 0.190474, acc 0.9375\n",
      "2017-11-05T10:12:33.399212: step 2071, loss 0.111967, acc 0.9375\n",
      "2017-11-05T10:12:37.487116: step 2072, loss 0.105766, acc 0.96875\n",
      "2017-11-05T10:12:41.506973: step 2073, loss 0.28578, acc 0.875\n",
      "2017-11-05T10:12:45.591375: step 2074, loss 0.163123, acc 0.90625\n",
      "2017-11-05T10:12:49.597721: step 2075, loss 0.270879, acc 0.90625\n",
      "2017-11-05T10:12:53.733660: step 2076, loss 0.12047, acc 0.9375\n",
      "2017-11-05T10:12:57.682466: step 2077, loss 0.0219696, acc 1\n",
      "2017-11-05T10:13:01.765867: step 2078, loss 0.171283, acc 0.90625\n",
      "2017-11-05T10:13:05.844766: step 2079, loss 0.112571, acc 0.96875\n",
      "2017-11-05T10:13:09.948682: step 2080, loss 0.590333, acc 0.78125\n",
      "2017-11-05T10:13:13.988552: step 2081, loss 0.0447313, acc 0.96875\n",
      "2017-11-05T10:13:18.052439: step 2082, loss 0.0747772, acc 0.96875\n",
      "2017-11-05T10:13:22.081803: step 2083, loss 0.0958472, acc 0.96875\n",
      "2017-11-05T10:13:26.345332: step 2084, loss 0.206557, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T10:13:30.450249: step 2085, loss 0.253409, acc 0.875\n",
      "2017-11-05T10:13:34.530648: step 2086, loss 0.293925, acc 0.875\n",
      "2017-11-05T10:13:38.598038: step 2087, loss 0.157767, acc 0.9375\n",
      "2017-11-05T10:13:41.202889: step 2088, loss 0.321781, acc 0.95\n",
      "2017-11-05T10:13:45.249264: step 2089, loss 0.255407, acc 0.84375\n",
      "2017-11-05T10:13:49.273124: step 2090, loss 0.0976189, acc 0.96875\n",
      "2017-11-05T10:13:53.303487: step 2091, loss 0.116992, acc 0.9375\n",
      "2017-11-05T10:13:57.288819: step 2092, loss 0.285585, acc 0.875\n",
      "2017-11-05T10:14:01.317681: step 2093, loss 0.175232, acc 0.9375\n",
      "2017-11-05T10:14:05.401584: step 2094, loss 0.136462, acc 0.90625\n",
      "2017-11-05T10:14:09.530017: step 2095, loss 0.110699, acc 0.90625\n",
      "2017-11-05T10:14:13.519351: step 2096, loss 0.0843784, acc 0.9375\n",
      "2017-11-05T10:14:17.596749: step 2097, loss 0.114553, acc 0.9375\n",
      "2017-11-05T10:14:21.649128: step 2098, loss 0.0421199, acc 1\n",
      "2017-11-05T10:14:25.667983: step 2099, loss 0.116714, acc 0.9375\n",
      "2017-11-05T10:14:29.781922: step 2100, loss 0.326833, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:14:32.481340: step 2100, loss 0.868989, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-05T10:14:38.238057: step 2101, loss 0.134029, acc 0.9375\n",
      "2017-11-05T10:14:42.274425: step 2102, loss 0.560299, acc 0.78125\n",
      "2017-11-05T10:14:46.286776: step 2103, loss 0.193777, acc 0.90625\n",
      "2017-11-05T10:14:50.353165: step 2104, loss 0.0375376, acc 1\n",
      "2017-11-05T10:14:54.393536: step 2105, loss 0.24366, acc 0.875\n",
      "2017-11-05T10:14:58.501955: step 2106, loss 0.093776, acc 0.96875\n",
      "2017-11-05T10:15:02.537823: step 2107, loss 0.272811, acc 0.84375\n",
      "2017-11-05T10:15:06.627228: step 2108, loss 0.20774, acc 0.875\n",
      "2017-11-05T10:15:10.681109: step 2109, loss 0.115281, acc 0.9375\n",
      "2017-11-05T10:15:14.690458: step 2110, loss 0.0727231, acc 0.96875\n",
      "2017-11-05T10:15:18.716819: step 2111, loss 0.237318, acc 0.90625\n",
      "2017-11-05T10:15:22.701650: step 2112, loss 0.154818, acc 0.9375\n",
      "2017-11-05T10:15:26.774044: step 2113, loss 0.265897, acc 0.875\n",
      "2017-11-05T10:15:30.784402: step 2114, loss 0.165706, acc 0.9375\n",
      "2017-11-05T10:15:34.857296: step 2115, loss 0.14767, acc 0.9375\n",
      "2017-11-05T10:15:38.886659: step 2116, loss 0.361986, acc 0.84375\n",
      "2017-11-05T10:15:42.879496: step 2117, loss 0.134372, acc 0.9375\n",
      "2017-11-05T10:15:46.942383: step 2118, loss 0.273811, acc 0.875\n",
      "2017-11-05T10:15:51.036292: step 2119, loss 0.091546, acc 0.96875\n",
      "2017-11-05T10:15:55.049143: step 2120, loss 0.261219, acc 0.90625\n",
      "2017-11-05T10:15:59.097519: step 2121, loss 0.148262, acc 0.9375\n",
      "2017-11-05T10:16:03.115875: step 2122, loss 0.196276, acc 0.9375\n",
      "2017-11-05T10:16:07.194773: step 2123, loss 0.295769, acc 0.84375\n",
      "2017-11-05T10:16:09.800123: step 2124, loss 0.238767, acc 0.9\n",
      "2017-11-05T10:16:13.892532: step 2125, loss 0.225253, acc 0.875\n",
      "2017-11-05T10:16:17.951916: step 2126, loss 0.164794, acc 0.90625\n",
      "2017-11-05T10:16:21.999292: step 2127, loss 0.139659, acc 0.90625\n",
      "2017-11-05T10:16:26.050171: step 2128, loss 0.219142, acc 0.9375\n",
      "2017-11-05T10:16:30.089541: step 2129, loss 0.0714132, acc 0.96875\n",
      "2017-11-05T10:16:34.270011: step 2130, loss 0.140503, acc 0.90625\n",
      "2017-11-05T10:16:38.478001: step 2131, loss 0.307109, acc 0.875\n",
      "2017-11-05T10:16:42.653468: step 2132, loss 0.135112, acc 0.9375\n",
      "2017-11-05T10:16:46.696841: step 2133, loss 0.0841841, acc 0.9375\n",
      "2017-11-05T10:16:50.666663: step 2134, loss 0.15484, acc 0.9375\n",
      "2017-11-05T10:16:54.727546: step 2135, loss 0.216505, acc 0.84375\n",
      "2017-11-05T10:16:58.759411: step 2136, loss 0.290936, acc 0.875\n",
      "2017-11-05T10:17:02.798282: step 2137, loss 0.147586, acc 0.96875\n",
      "2017-11-05T10:17:06.883684: step 2138, loss 0.0387244, acc 1\n",
      "2017-11-05T10:17:10.925557: step 2139, loss 0.0631236, acc 1\n",
      "2017-11-05T10:17:14.997950: step 2140, loss 0.165722, acc 0.9375\n",
      "2017-11-05T10:17:19.006798: step 2141, loss 0.221992, acc 0.875\n",
      "2017-11-05T10:17:23.024788: step 2142, loss 0.277575, acc 0.875\n",
      "2017-11-05T10:17:27.045645: step 2143, loss 0.247815, acc 0.90625\n",
      "2017-11-05T10:17:31.156566: step 2144, loss 0.233625, acc 0.875\n",
      "2017-11-05T10:17:35.157410: step 2145, loss 0.120662, acc 0.96875\n",
      "2017-11-05T10:17:39.223879: step 2146, loss 0.135955, acc 0.96875\n",
      "2017-11-05T10:17:43.240233: step 2147, loss 0.0708334, acc 1\n",
      "2017-11-05T10:17:47.291112: step 2148, loss 0.15293, acc 0.90625\n",
      "2017-11-05T10:17:51.275442: step 2149, loss 0.183862, acc 0.90625\n",
      "2017-11-05T10:17:55.377857: step 2150, loss 0.135742, acc 0.90625\n",
      "2017-11-05T10:17:59.464761: step 2151, loss 0.107389, acc 0.9375\n",
      "2017-11-05T10:18:03.564174: step 2152, loss 0.196404, acc 0.90625\n",
      "2017-11-05T10:18:07.620056: step 2153, loss 0.148985, acc 0.96875\n",
      "2017-11-05T10:18:11.731477: step 2154, loss 0.335048, acc 0.875\n",
      "2017-11-05T10:18:15.834392: step 2155, loss 0.124448, acc 0.9375\n",
      "2017-11-05T10:18:19.862755: step 2156, loss 0.122137, acc 0.9375\n",
      "2017-11-05T10:18:24.129407: step 2157, loss 0.307799, acc 0.9375\n",
      "2017-11-05T10:18:28.481999: step 2158, loss 0.194276, acc 0.9375\n",
      "2017-11-05T10:18:32.597924: step 2159, loss 0.179684, acc 0.9375\n",
      "2017-11-05T10:18:35.249308: step 2160, loss 0.399315, acc 0.8\n",
      "2017-11-05T10:18:39.426276: step 2161, loss 0.124094, acc 0.96875\n",
      "2017-11-05T10:18:43.637268: step 2162, loss 0.212961, acc 0.875\n",
      "2017-11-05T10:18:47.798225: step 2163, loss 0.373337, acc 0.78125\n",
      "2017-11-05T10:18:51.912147: step 2164, loss 0.0657493, acc 0.96875\n",
      "2017-11-05T10:18:56.026571: step 2165, loss 0.18107, acc 0.90625\n",
      "2017-11-05T10:19:00.185026: step 2166, loss 0.268133, acc 0.9375\n",
      "2017-11-05T10:19:04.286941: step 2167, loss 0.229471, acc 0.9375\n",
      "2017-11-05T10:19:08.406367: step 2168, loss 0.186519, acc 0.90625\n",
      "2017-11-05T10:19:12.490770: step 2169, loss 0.29473, acc 0.875\n",
      "2017-11-05T10:19:16.675743: step 2170, loss 0.28173, acc 0.90625\n",
      "2017-11-05T10:19:20.765649: step 2171, loss 0.113423, acc 0.9375\n",
      "2017-11-05T10:19:24.896084: step 2172, loss 0.144832, acc 0.90625\n",
      "2017-11-05T10:19:29.032023: step 2173, loss 0.310398, acc 0.875\n",
      "2017-11-05T10:19:33.094909: step 2174, loss 0.10743, acc 0.96875\n",
      "2017-11-05T10:19:37.125273: step 2175, loss 0.105368, acc 0.9375\n",
      "2017-11-05T10:19:41.268217: step 2176, loss 0.277633, acc 0.875\n",
      "2017-11-05T10:19:45.367130: step 2177, loss 0.25472, acc 0.84375\n",
      "2017-11-05T10:19:49.496563: step 2178, loss 0.148337, acc 0.90625\n",
      "2017-11-05T10:19:53.729071: step 2179, loss 0.442995, acc 0.78125\n",
      "2017-11-05T10:19:57.816476: step 2180, loss 0.165098, acc 0.90625\n",
      "2017-11-05T10:20:02.432756: step 2181, loss 0.265748, acc 0.875\n",
      "2017-11-05T10:20:06.737815: step 2182, loss 0.184439, acc 0.90625\n",
      "2017-11-05T10:20:10.886763: step 2183, loss 0.161279, acc 0.90625\n",
      "2017-11-05T10:20:14.954653: step 2184, loss 0.176391, acc 0.875\n",
      "2017-11-05T10:20:19.022043: step 2185, loss 0.323555, acc 0.90625\n",
      "2017-11-05T10:20:23.115075: step 2186, loss 0.253669, acc 0.90625\n",
      "2017-11-05T10:20:27.545900: step 2187, loss 0.302327, acc 0.9375\n",
      "2017-11-05T10:20:31.584269: step 2188, loss 0.0379, acc 1\n",
      "2017-11-05T10:20:35.678179: step 2189, loss 0.138608, acc 0.9375\n",
      "2017-11-05T10:20:39.740502: step 2190, loss 0.035319, acc 0.96875\n",
      "2017-11-05T10:20:43.788911: step 2191, loss 0.153743, acc 0.90625\n",
      "2017-11-05T10:20:47.782600: step 2192, loss 0.160183, acc 0.90625\n",
      "2017-11-05T10:20:51.773276: step 2193, loss 0.228455, acc 0.9375\n",
      "2017-11-05T10:20:55.778122: step 2194, loss 0.191125, acc 0.90625\n",
      "2017-11-05T10:20:59.777990: step 2195, loss 0.236232, acc 0.875\n",
      "2017-11-05T10:21:02.316680: step 2196, loss 0.296912, acc 0.9\n",
      "2017-11-05T10:21:06.341195: step 2197, loss 0.358798, acc 0.8125\n",
      "2017-11-05T10:21:10.287509: step 2198, loss 0.235881, acc 0.84375\n",
      "2017-11-05T10:21:14.289854: step 2199, loss 0.274346, acc 0.9375\n",
      "2017-11-05T10:21:18.325721: step 2200, loss 0.126078, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:21:20.958091: step 2200, loss 0.673308, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-05T10:21:26.313557: step 2201, loss 0.139412, acc 0.90625\n",
      "2017-11-05T10:21:30.321953: step 2202, loss 0.159297, acc 0.9375\n",
      "2017-11-05T10:21:34.443882: step 2203, loss 0.212393, acc 0.96875\n",
      "2017-11-05T10:21:38.489757: step 2204, loss 0.110694, acc 0.9375\n",
      "2017-11-05T10:21:42.510001: step 2205, loss 0.152349, acc 0.875\n",
      "2017-11-05T10:21:46.567397: step 2206, loss 0.325831, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T10:21:50.727051: step 2207, loss 0.295368, acc 0.875\n",
      "2017-11-05T10:21:54.775427: step 2208, loss 0.150316, acc 0.90625\n",
      "2017-11-05T10:21:58.791145: step 2209, loss 0.149452, acc 0.9375\n",
      "2017-11-05T10:22:02.816484: step 2210, loss 0.200572, acc 0.90625\n",
      "2017-11-05T10:22:06.932315: step 2211, loss 0.0784874, acc 0.96875\n",
      "2017-11-05T10:22:11.074758: step 2212, loss 0.213879, acc 0.9375\n",
      "2017-11-05T10:22:15.105122: step 2213, loss 0.164954, acc 0.90625\n",
      "2017-11-05T10:22:19.307108: step 2214, loss 0.255347, acc 0.875\n",
      "2017-11-05T10:22:23.421031: step 2215, loss 0.146299, acc 0.9375\n",
      "2017-11-05T10:22:27.506935: step 2216, loss 0.203582, acc 0.90625\n",
      "2017-11-05T10:22:31.600843: step 2217, loss 0.3339, acc 0.875\n",
      "2017-11-05T10:22:35.727776: step 2218, loss 0.282132, acc 0.84375\n",
      "2017-11-05T10:22:39.805674: step 2219, loss 0.182682, acc 0.9375\n",
      "2017-11-05T10:22:43.858553: step 2220, loss 0.222891, acc 0.9375\n",
      "2017-11-05T10:22:47.895421: step 2221, loss 0.220766, acc 0.90625\n",
      "2017-11-05T10:22:51.915778: step 2222, loss 0.118374, acc 0.90625\n",
      "2017-11-05T10:22:55.962653: step 2223, loss 0.178815, acc 0.875\n",
      "2017-11-05T10:23:00.068070: step 2224, loss 0.154889, acc 0.9375\n",
      "2017-11-05T10:23:04.302093: step 2225, loss 0.252469, acc 0.90625\n",
      "2017-11-05T10:23:08.334458: step 2226, loss 0.207573, acc 0.84375\n",
      "2017-11-05T10:23:12.378173: step 2227, loss 0.22546, acc 0.9375\n",
      "2017-11-05T10:23:16.353206: step 2228, loss 0.34961, acc 0.875\n",
      "2017-11-05T10:23:20.347207: step 2229, loss 0.284608, acc 0.875\n",
      "2017-11-05T10:23:24.496715: step 2230, loss 0.0195689, acc 1\n",
      "2017-11-05T10:23:28.554021: step 2231, loss 0.0969275, acc 0.96875\n",
      "2017-11-05T10:23:31.037927: step 2232, loss 0.152728, acc 0.95\n",
      "2017-11-05T10:23:35.089873: step 2233, loss 0.106179, acc 0.9375\n",
      "2017-11-05T10:23:39.080098: step 2234, loss 0.0494664, acc 0.96875\n",
      "2017-11-05T10:23:43.021397: step 2235, loss 0.0874406, acc 0.96875\n",
      "2017-11-05T10:23:47.037251: step 2236, loss 0.049748, acc 0.96875\n",
      "2017-11-05T10:23:50.995953: step 2237, loss 0.0164339, acc 1\n",
      "2017-11-05T10:23:54.986789: step 2238, loss 0.450789, acc 0.8125\n",
      "2017-11-05T10:23:59.012488: step 2239, loss 0.177343, acc 0.90625\n",
      "2017-11-05T10:24:02.950286: step 2240, loss 0.12949, acc 0.9375\n",
      "2017-11-05T10:24:06.944124: step 2241, loss 0.0630892, acc 0.9375\n",
      "2017-11-05T10:24:10.943465: step 2242, loss 0.256155, acc 0.875\n",
      "2017-11-05T10:24:14.874866: step 2243, loss 0.238422, acc 0.875\n",
      "2017-11-05T10:24:18.890781: step 2244, loss 0.22104, acc 0.90625\n",
      "2017-11-05T10:24:22.860616: step 2245, loss 0.209381, acc 0.875\n",
      "2017-11-05T10:24:26.789389: step 2246, loss 0.060275, acc 0.96875\n",
      "2017-11-05T10:24:30.896944: step 2247, loss 0.169019, acc 0.90625\n",
      "2017-11-05T10:24:34.946710: step 2248, loss 0.141608, acc 0.90625\n",
      "2017-11-05T10:24:38.935437: step 2249, loss 0.327373, acc 0.8125\n",
      "2017-11-05T10:24:42.918541: step 2250, loss 0.131586, acc 0.9375\n",
      "2017-11-05T10:24:46.927319: step 2251, loss 0.166778, acc 0.90625\n",
      "2017-11-05T10:24:50.898029: step 2252, loss 0.295523, acc 0.90625\n",
      "2017-11-05T10:24:54.857842: step 2253, loss 0.114508, acc 0.90625\n",
      "2017-11-05T10:24:58.816656: step 2254, loss 0.235017, acc 0.84375\n",
      "2017-11-05T10:25:02.792480: step 2255, loss 0.119289, acc 0.9375\n",
      "2017-11-05T10:25:06.824405: step 2256, loss 0.191691, acc 0.90625\n",
      "2017-11-05T10:25:10.839272: step 2257, loss 0.380633, acc 0.84375\n",
      "2017-11-05T10:25:14.800106: step 2258, loss 0.207588, acc 0.9375\n",
      "2017-11-05T10:25:18.773930: step 2259, loss 0.132221, acc 0.9375\n",
      "2017-11-05T10:25:22.804672: step 2260, loss 0.279598, acc 0.90625\n",
      "2017-11-05T10:25:26.752878: step 2261, loss 0.19194, acc 0.90625\n",
      "2017-11-05T10:25:30.723699: step 2262, loss 0.352814, acc 0.84375\n",
      "2017-11-05T10:25:34.702028: step 2263, loss 0.0879936, acc 0.96875\n",
      "2017-11-05T10:25:38.666843: step 2264, loss 0.203622, acc 0.90625\n",
      "2017-11-05T10:25:42.701750: step 2265, loss 0.108256, acc 0.9375\n",
      "2017-11-05T10:25:46.643325: step 2266, loss 0.1396, acc 0.9375\n",
      "2017-11-05T10:25:50.609643: step 2267, loss 0.267618, acc 0.90625\n",
      "2017-11-05T10:25:53.162457: step 2268, loss 0.37884, acc 0.75\n",
      "2017-11-05T10:25:57.212835: step 2269, loss 0.0986041, acc 0.9375\n",
      "2017-11-05T10:26:01.303742: step 2270, loss 0.0693484, acc 0.96875\n",
      "2017-11-05T10:26:05.383140: step 2271, loss 0.246379, acc 0.875\n",
      "2017-11-05T10:26:09.403497: step 2272, loss 0.638315, acc 0.78125\n",
      "2017-11-05T10:26:13.492402: step 2273, loss 0.0801186, acc 0.9375\n",
      "2017-11-05T10:26:17.538277: step 2274, loss 0.0876611, acc 0.9375\n",
      "2017-11-05T10:26:21.479578: step 2275, loss 0.150913, acc 0.90625\n",
      "2017-11-05T10:26:25.470296: step 2276, loss 0.06035, acc 0.96875\n",
      "2017-11-05T10:26:29.414486: step 2277, loss 0.0797239, acc 0.9375\n",
      "2017-11-05T10:26:33.420832: step 2278, loss 0.16306, acc 0.90625\n",
      "2017-11-05T10:26:37.431182: step 2279, loss 0.130569, acc 0.9375\n",
      "2017-11-05T10:26:41.430023: step 2280, loss 0.0647646, acc 0.96875\n",
      "2017-11-05T10:26:45.380831: step 2281, loss 0.136698, acc 0.96875\n",
      "2017-11-05T10:26:49.402705: step 2282, loss 0.297447, acc 0.875\n",
      "2017-11-05T10:26:53.378119: step 2283, loss 0.152858, acc 0.9375\n",
      "2017-11-05T10:26:57.348929: step 2284, loss 0.05376, acc 0.96875\n",
      "2017-11-05T10:27:01.386317: step 2285, loss 0.20426, acc 0.9375\n",
      "2017-11-05T10:27:05.359851: step 2286, loss 0.266252, acc 0.875\n",
      "2017-11-05T10:27:09.416233: step 2287, loss 0.188641, acc 0.875\n",
      "2017-11-05T10:27:13.368285: step 2288, loss 0.160237, acc 0.9375\n",
      "2017-11-05T10:27:17.319411: step 2289, loss 0.199937, acc 0.90625\n",
      "2017-11-05T10:27:21.303143: step 2290, loss 0.33728, acc 0.875\n",
      "2017-11-05T10:27:25.245371: step 2291, loss 0.402683, acc 0.875\n",
      "2017-11-05T10:27:29.162670: step 2292, loss 0.224491, acc 0.90625\n",
      "2017-11-05T10:27:33.093971: step 2293, loss 0.37885, acc 0.78125\n",
      "2017-11-05T10:27:37.040776: step 2294, loss 0.260014, acc 0.875\n",
      "2017-11-05T10:27:41.001644: step 2295, loss 0.356965, acc 0.84375\n",
      "2017-11-05T10:27:44.932947: step 2296, loss 0.0216125, acc 1\n",
      "2017-11-05T10:27:48.946360: step 2297, loss 0.175283, acc 0.90625\n",
      "2017-11-05T10:27:52.908083: step 2298, loss 0.128095, acc 0.96875\n",
      "2017-11-05T10:27:56.899765: step 2299, loss 0.0562492, acc 1\n",
      "2017-11-05T10:28:00.869435: step 2300, loss 0.238152, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:28:03.576131: step 2300, loss 0.874717, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-05T10:28:09.076450: step 2301, loss 0.0183681, acc 1\n",
      "2017-11-05T10:28:13.128329: step 2302, loss 0.254902, acc 0.9375\n",
      "2017-11-05T10:28:17.210729: step 2303, loss 0.28586, acc 0.90625\n",
      "2017-11-05T10:28:19.828589: step 2304, loss 0.53547, acc 0.75\n",
      "2017-11-05T10:28:23.839939: step 2305, loss 0.027506, acc 1\n",
      "2017-11-05T10:28:27.885314: step 2306, loss 0.141779, acc 0.90625\n",
      "2017-11-05T10:28:31.899166: step 2307, loss 0.160926, acc 0.90625\n",
      "2017-11-05T10:28:36.054438: step 2308, loss 0.186046, acc 0.9375\n",
      "2017-11-05T10:28:40.113323: step 2309, loss 0.220591, acc 0.90625\n",
      "2017-11-05T10:28:44.149690: step 2310, loss 0.187215, acc 0.90625\n",
      "2017-11-05T10:28:48.150532: step 2311, loss 0.240728, acc 0.875\n",
      "2017-11-05T10:28:52.229431: step 2312, loss 0.0590949, acc 0.96875\n",
      "2017-11-05T10:28:56.248982: step 2313, loss 0.182675, acc 0.9375\n",
      "2017-11-05T10:29:00.287670: step 2314, loss 0.318877, acc 0.8125\n",
      "2017-11-05T10:29:04.246743: step 2315, loss 0.0934061, acc 0.96875\n",
      "2017-11-05T10:29:08.259607: step 2316, loss 0.112095, acc 0.96875\n",
      "2017-11-05T10:29:12.267957: step 2317, loss 0.0298723, acc 0.96875\n",
      "2017-11-05T10:29:16.220263: step 2318, loss 0.197269, acc 0.90625\n",
      "2017-11-05T10:29:20.298662: step 2319, loss 0.163705, acc 0.90625\n",
      "2017-11-05T10:29:24.438603: step 2320, loss 0.0852364, acc 0.9375\n",
      "2017-11-05T10:29:28.599734: step 2321, loss 0.207551, acc 0.90625\n",
      "2017-11-05T10:29:32.588973: step 2322, loss 0.407496, acc 0.875\n",
      "2017-11-05T10:29:36.612832: step 2323, loss 0.29226, acc 0.9375\n",
      "2017-11-05T10:29:40.638400: step 2324, loss 0.400456, acc 0.78125\n",
      "2017-11-05T10:29:44.713481: step 2325, loss 0.127721, acc 0.96875\n",
      "2017-11-05T10:29:48.744345: step 2326, loss 0.258029, acc 0.84375\n",
      "2017-11-05T10:29:52.801310: step 2327, loss 0.116265, acc 0.96875\n",
      "2017-11-05T10:29:56.841661: step 2328, loss 0.707148, acc 0.8125\n",
      "2017-11-05T10:30:01.008121: step 2329, loss 0.487283, acc 0.875\n",
      "2017-11-05T10:30:05.158326: step 2330, loss 0.216401, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T10:30:09.184185: step 2331, loss 0.132312, acc 0.9375\n",
      "2017-11-05T10:30:13.172408: step 2332, loss 0.0977435, acc 0.96875\n",
      "2017-11-05T10:30:17.160254: step 2333, loss 0.354783, acc 0.875\n",
      "2017-11-05T10:30:21.210132: step 2334, loss 0.333333, acc 0.875\n",
      "2017-11-05T10:30:25.171458: step 2335, loss 0.425936, acc 0.78125\n",
      "2017-11-05T10:30:29.187313: step 2336, loss 0.436532, acc 0.84375\n",
      "2017-11-05T10:30:33.217175: step 2337, loss 0.442626, acc 0.8125\n",
      "2017-11-05T10:30:37.298473: step 2338, loss 0.160156, acc 0.9375\n",
      "2017-11-05T10:30:41.296442: step 2339, loss 0.393411, acc 0.875\n",
      "2017-11-05T10:30:43.878776: step 2340, loss 0.323989, acc 0.85\n",
      "2017-11-05T10:30:47.950170: step 2341, loss 0.125643, acc 0.9375\n",
      "2017-11-05T10:30:51.942897: step 2342, loss 0.163097, acc 0.9375\n",
      "2017-11-05T10:30:55.962646: step 2343, loss 0.324722, acc 0.875\n",
      "2017-11-05T10:30:59.983503: step 2344, loss 0.198754, acc 0.90625\n",
      "2017-11-05T10:31:04.048892: step 2345, loss 0.183728, acc 0.90625\n",
      "2017-11-05T10:31:08.115781: step 2346, loss 0.219248, acc 0.9375\n",
      "2017-11-05T10:31:12.169662: step 2347, loss 0.0580497, acc 0.96875\n",
      "2017-11-05T10:31:16.185595: step 2348, loss 0.190045, acc 0.90625\n",
      "2017-11-05T10:31:20.158995: step 2349, loss 0.137027, acc 0.9375\n",
      "2017-11-05T10:31:24.234891: step 2350, loss 0.200353, acc 0.90625\n",
      "2017-11-05T10:31:28.285769: step 2351, loss 0.0699493, acc 0.96875\n",
      "2017-11-05T10:31:32.295118: step 2352, loss 0.145418, acc 0.9375\n",
      "2017-11-05T10:31:36.388027: step 2353, loss 0.1758, acc 0.9375\n",
      "2017-11-05T10:31:40.470927: step 2354, loss 0.188251, acc 0.90625\n",
      "2017-11-05T10:31:44.547324: step 2355, loss 0.248784, acc 0.875\n",
      "2017-11-05T10:31:48.541662: step 2356, loss 0.215003, acc 0.875\n",
      "2017-11-05T10:31:52.702118: step 2357, loss 0.0894115, acc 0.9375\n",
      "2017-11-05T10:31:56.806535: step 2358, loss 0.112047, acc 0.9375\n",
      "2017-11-05T10:32:00.791155: step 2359, loss 0.119842, acc 0.9375\n",
      "2017-11-05T10:32:04.853311: step 2360, loss 0.230948, acc 0.875\n",
      "2017-11-05T10:32:08.949884: step 2361, loss 0.210335, acc 0.875\n",
      "2017-11-05T10:32:13.056302: step 2362, loss 0.299666, acc 0.90625\n",
      "2017-11-05T10:32:17.022404: step 2363, loss 0.106894, acc 0.9375\n",
      "2017-11-05T10:32:21.093814: step 2364, loss 0.534648, acc 0.78125\n",
      "2017-11-05T10:32:25.101354: step 2365, loss 0.254857, acc 0.875\n",
      "2017-11-05T10:32:29.123049: step 2366, loss 0.147525, acc 0.90625\n",
      "2017-11-05T10:32:33.203949: step 2367, loss 0.0739179, acc 0.96875\n",
      "2017-11-05T10:32:37.270848: step 2368, loss 0.0513809, acc 0.96875\n",
      "2017-11-05T10:32:41.322858: step 2369, loss 0.293508, acc 0.84375\n",
      "2017-11-05T10:32:45.421095: step 2370, loss 0.148666, acc 0.90625\n",
      "2017-11-05T10:32:49.395419: step 2371, loss 0.260904, acc 0.875\n",
      "2017-11-05T10:32:53.437679: step 2372, loss 0.155334, acc 0.9375\n",
      "2017-11-05T10:32:57.576511: step 2373, loss 0.222997, acc 0.90625\n",
      "2017-11-05T10:33:01.638397: step 2374, loss 0.164017, acc 0.9375\n",
      "2017-11-05T10:33:05.690776: step 2375, loss 0.173628, acc 0.90625\n",
      "2017-11-05T10:33:08.292852: step 2376, loss 0.421432, acc 0.85\n",
      "2017-11-05T10:33:12.316618: step 2377, loss 0.313366, acc 0.84375\n",
      "2017-11-05T10:33:16.415169: step 2378, loss 0.16334, acc 0.9375\n",
      "2017-11-05T10:33:20.381693: step 2379, loss 0.185848, acc 0.9375\n",
      "2017-11-05T10:33:24.561164: step 2380, loss 0.0668202, acc 0.96875\n",
      "2017-11-05T10:33:28.594822: step 2381, loss 0.243328, acc 0.90625\n",
      "2017-11-05T10:33:32.620418: step 2382, loss 0.353656, acc 0.78125\n",
      "2017-11-05T10:33:36.617758: step 2383, loss 0.0664664, acc 0.96875\n",
      "2017-11-05T10:33:40.721174: step 2384, loss 0.241115, acc 0.875\n",
      "2017-11-05T10:33:44.714512: step 2385, loss 0.0895057, acc 0.96875\n",
      "2017-11-05T10:33:48.713353: step 2386, loss 0.283353, acc 0.90625\n",
      "2017-11-05T10:33:52.749721: step 2387, loss 0.229033, acc 0.875\n",
      "2017-11-05T10:33:56.779990: step 2388, loss 0.06703, acc 0.96875\n",
      "2017-11-05T10:34:00.906423: step 2389, loss 0.0799614, acc 0.96875\n",
      "2017-11-05T10:34:05.065378: step 2390, loss 0.248966, acc 0.84375\n",
      "2017-11-05T10:34:09.160287: step 2391, loss 0.246393, acc 0.90625\n",
      "2017-11-05T10:34:13.200158: step 2392, loss 0.082532, acc 0.9375\n",
      "2017-11-05T10:34:17.263935: step 2393, loss 0.0680611, acc 0.96875\n",
      "2017-11-05T10:34:21.371353: step 2394, loss 0.119455, acc 0.9375\n",
      "2017-11-05T10:34:25.473268: step 2395, loss 0.149119, acc 0.90625\n",
      "2017-11-05T10:34:29.521644: step 2396, loss 0.178507, acc 0.90625\n",
      "2017-11-05T10:34:33.920270: step 2397, loss 0.225872, acc 0.9375\n",
      "2017-11-05T10:34:38.124257: step 2398, loss 0.403442, acc 0.8125\n",
      "2017-11-05T10:34:42.133605: step 2399, loss 0.160395, acc 0.9375\n",
      "2017-11-05T10:34:46.169974: step 2400, loss 0.201463, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:34:48.783831: step 2400, loss 0.657358, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-05T10:34:54.199981: step 2401, loss 0.227065, acc 0.875\n",
      "2017-11-05T10:34:58.320407: step 2402, loss 0.194981, acc 0.9375\n",
      "2017-11-05T10:35:02.376790: step 2403, loss 0.188123, acc 0.875\n",
      "2017-11-05T10:35:06.432671: step 2404, loss 0.10622, acc 0.96875\n",
      "2017-11-05T10:35:10.492056: step 2405, loss 0.211757, acc 0.84375\n",
      "2017-11-05T10:35:14.510410: step 2406, loss 0.0513308, acc 0.96875\n",
      "2017-11-05T10:35:18.580303: step 2407, loss 0.108756, acc 0.9375\n",
      "2017-11-05T10:35:22.592153: step 2408, loss 0.289667, acc 0.90625\n",
      "2017-11-05T10:35:26.627020: step 2409, loss 0.308898, acc 0.78125\n",
      "2017-11-05T10:35:30.621359: step 2410, loss 0.328697, acc 0.84375\n",
      "2017-11-05T10:35:34.661229: step 2411, loss 0.188849, acc 0.9375\n",
      "2017-11-05T10:35:37.241562: step 2412, loss 0.150514, acc 0.9\n",
      "2017-11-05T10:35:41.252412: step 2413, loss 0.191672, acc 0.875\n",
      "2017-11-05T10:35:45.323805: step 2414, loss 0.111085, acc 0.9375\n",
      "2017-11-05T10:35:49.365177: step 2415, loss 0.163732, acc 0.90625\n",
      "2017-11-05T10:35:53.427063: step 2416, loss 0.0895702, acc 0.90625\n",
      "2017-11-05T10:35:57.469936: step 2417, loss 0.122087, acc 0.9375\n",
      "2017-11-05T10:36:01.561345: step 2418, loss 0.105708, acc 0.9375\n",
      "2017-11-05T10:36:05.602214: step 2419, loss 0.37405, acc 0.8125\n",
      "2017-11-05T10:36:09.605059: step 2420, loss 0.100467, acc 0.9375\n",
      "2017-11-05T10:36:13.653936: step 2421, loss 0.128822, acc 0.9375\n",
      "2017-11-05T10:36:17.660782: step 2422, loss 0.19709, acc 0.875\n",
      "2017-11-05T10:36:21.697651: step 2423, loss 0.213431, acc 0.9375\n",
      "2017-11-05T10:36:25.743025: step 2424, loss 0.101447, acc 0.9375\n",
      "2017-11-05T10:36:29.759879: step 2425, loss 0.12674, acc 0.9375\n",
      "2017-11-05T10:36:33.928341: step 2426, loss 0.16012, acc 0.90625\n",
      "2017-11-05T10:36:38.011241: step 2427, loss 0.161066, acc 0.90625\n",
      "2017-11-05T10:36:42.124526: step 2428, loss 0.245184, acc 0.84375\n",
      "2017-11-05T10:36:46.236690: step 2429, loss 0.0344054, acc 1\n",
      "2017-11-05T10:36:50.347110: step 2430, loss 0.0993323, acc 0.9375\n",
      "2017-11-05T10:36:54.436516: step 2431, loss 0.112734, acc 0.96875\n",
      "2017-11-05T10:36:58.516415: step 2432, loss 0.177141, acc 0.90625\n",
      "2017-11-05T10:37:02.632340: step 2433, loss 0.0639327, acc 0.96875\n",
      "2017-11-05T10:37:06.669709: step 2434, loss 0.0745327, acc 0.9375\n",
      "2017-11-05T10:37:10.808649: step 2435, loss 0.0148857, acc 1\n",
      "2017-11-05T10:37:14.874538: step 2436, loss 0.0962242, acc 0.90625\n",
      "2017-11-05T10:37:18.930420: step 2437, loss 0.29773, acc 0.90625\n",
      "2017-11-05T10:37:22.991806: step 2438, loss 0.166591, acc 0.90625\n",
      "2017-11-05T10:37:27.043184: step 2439, loss 0.181263, acc 0.9375\n",
      "2017-11-05T10:37:31.121583: step 2440, loss 0.171421, acc 0.90625\n",
      "2017-11-05T10:37:35.296548: step 2441, loss 0.15446, acc 0.90625\n",
      "2017-11-05T10:37:39.405969: step 2442, loss 0.48788, acc 0.84375\n",
      "2017-11-05T10:37:43.473859: step 2443, loss 0.155204, acc 0.90625\n",
      "2017-11-05T10:37:47.599008: step 2444, loss 0.157716, acc 0.875\n",
      "2017-11-05T10:37:51.675405: step 2445, loss 0.35644, acc 0.875\n",
      "2017-11-05T10:37:55.784824: step 2446, loss 0.129263, acc 0.9375\n",
      "2017-11-05T10:37:59.916760: step 2447, loss 0.25289, acc 0.84375\n",
      "2017-11-05T10:38:02.654205: step 2448, loss 0.103525, acc 0.95\n",
      "2017-11-05T10:38:06.766127: step 2449, loss 0.120464, acc 0.96875\n",
      "2017-11-05T10:38:10.878049: step 2450, loss 0.132435, acc 0.96875\n",
      "2017-11-05T10:38:15.056517: step 2451, loss 0.228951, acc 0.9375\n",
      "2017-11-05T10:38:19.319547: step 2452, loss 0.273096, acc 0.90625\n",
      "2017-11-05T10:38:23.444477: step 2453, loss 0.039311, acc 1\n",
      "2017-11-05T10:38:27.725519: step 2454, loss 0.0893628, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T10:38:31.750879: step 2455, loss 0.174575, acc 0.90625\n",
      "2017-11-05T10:38:36.037426: step 2456, loss 0.10547, acc 0.9375\n",
      "2017-11-05T10:38:40.165859: step 2457, loss 0.0222318, acc 1\n",
      "2017-11-05T10:38:44.203227: step 2458, loss 0.338627, acc 0.84375\n",
      "2017-11-05T10:38:48.380813: step 2459, loss 0.143074, acc 0.90625\n",
      "2017-11-05T10:38:52.526759: step 2460, loss 0.110106, acc 0.96875\n",
      "2017-11-05T10:38:56.655192: step 2461, loss 0.31227, acc 0.875\n",
      "2017-11-05T10:39:00.634810: step 2462, loss 0.2737, acc 0.875\n",
      "2017-11-05T10:39:04.640129: step 2463, loss 0.060211, acc 0.96875\n",
      "2017-11-05T10:39:08.614954: step 2464, loss 0.225161, acc 0.875\n",
      "2017-11-05T10:39:12.615296: step 2465, loss 0.131045, acc 0.9375\n",
      "2017-11-05T10:39:16.580614: step 2466, loss 0.194678, acc 0.9375\n",
      "2017-11-05T10:39:20.559440: step 2467, loss 0.0797767, acc 0.96875\n",
      "2017-11-05T10:39:24.545161: step 2468, loss 0.194078, acc 0.9375\n",
      "2017-11-05T10:39:28.502974: step 2469, loss 0.114815, acc 0.9375\n",
      "2017-11-05T10:39:32.422759: step 2470, loss 0.25081, acc 0.90625\n",
      "2017-11-05T10:39:36.304552: step 2471, loss 0.256365, acc 0.84375\n",
      "2017-11-05T10:39:40.211718: step 2472, loss 0.249681, acc 0.875\n",
      "2017-11-05T10:39:44.140932: step 2473, loss 0.0406005, acc 1\n",
      "2017-11-05T10:39:48.049688: step 2474, loss 0.285176, acc 0.875\n",
      "2017-11-05T10:39:52.059423: step 2475, loss 0.0839331, acc 0.96875\n",
      "2017-11-05T10:39:56.016962: step 2476, loss 0.0985897, acc 0.96875\n",
      "2017-11-05T10:39:59.990986: step 2477, loss 0.112361, acc 0.9375\n",
      "2017-11-05T10:40:04.226376: step 2478, loss 0.352212, acc 0.875\n",
      "2017-11-05T10:40:08.221339: step 2479, loss 0.2164, acc 0.9375\n",
      "2017-11-05T10:40:12.215094: step 2480, loss 0.224816, acc 0.875\n",
      "2017-11-05T10:40:16.108162: step 2481, loss 0.298223, acc 0.84375\n",
      "2017-11-05T10:40:20.038472: step 2482, loss 0.374117, acc 0.875\n",
      "2017-11-05T10:40:23.971105: step 2483, loss 0.237567, acc 0.875\n",
      "2017-11-05T10:40:26.475823: step 2484, loss 0.1552, acc 0.95\n",
      "2017-11-05T10:40:30.398461: step 2485, loss 0.11419, acc 0.9375\n",
      "2017-11-05T10:40:34.439654: step 2486, loss 0.162269, acc 0.9375\n",
      "2017-11-05T10:40:38.410607: step 2487, loss 0.276483, acc 0.90625\n",
      "2017-11-05T10:40:42.375109: step 2488, loss 0.0629834, acc 1\n",
      "2017-11-05T10:40:46.300624: step 2489, loss 0.232438, acc 0.9375\n",
      "2017-11-05T10:40:50.314014: step 2490, loss 0.178676, acc 0.90625\n",
      "2017-11-05T10:40:54.275921: step 2491, loss 0.311133, acc 0.875\n",
      "2017-11-05T10:40:58.197617: step 2492, loss 0.256511, acc 0.84375\n",
      "2017-11-05T10:41:02.190446: step 2493, loss 0.100265, acc 0.9375\n",
      "2017-11-05T10:41:06.146069: step 2494, loss 0.22573, acc 0.875\n",
      "2017-11-05T10:41:10.522869: step 2495, loss 0.140204, acc 0.90625\n",
      "2017-11-05T10:41:15.544963: step 2496, loss 0.162422, acc 0.9375\n",
      "2017-11-05T10:41:19.493385: step 2497, loss 0.225714, acc 0.9375\n",
      "2017-11-05T10:41:23.566563: step 2498, loss 0.265349, acc 0.90625\n",
      "2017-11-05T10:41:27.509359: step 2499, loss 0.2141, acc 0.90625\n",
      "2017-11-05T10:41:31.477767: step 2500, loss 0.204629, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:41:34.073092: step 2500, loss 0.792652, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-05T10:41:39.395498: step 2501, loss 0.0400465, acc 0.96875\n",
      "2017-11-05T10:41:43.359815: step 2502, loss 0.121401, acc 0.9375\n",
      "2017-11-05T10:41:47.337356: step 2503, loss 0.118384, acc 0.96875\n",
      "2017-11-05T10:41:51.400743: step 2504, loss 0.155514, acc 0.90625\n",
      "2017-11-05T10:41:55.518669: step 2505, loss 0.191004, acc 0.90625\n",
      "2017-11-05T10:41:59.473479: step 2506, loss 0.323905, acc 0.84375\n",
      "2017-11-05T10:42:03.502341: step 2507, loss 0.177557, acc 0.875\n",
      "2017-11-05T10:42:07.556722: step 2508, loss 0.336233, acc 0.84375\n",
      "2017-11-05T10:42:11.749201: step 2509, loss 0.07439, acc 0.96875\n",
      "2017-11-05T10:42:15.781066: step 2510, loss 0.162221, acc 0.9375\n",
      "2017-11-05T10:42:19.692345: step 2511, loss 0.162072, acc 0.9375\n",
      "2017-11-05T10:42:23.702441: step 2512, loss 0.198472, acc 0.90625\n",
      "2017-11-05T10:42:27.749361: step 2513, loss 0.236122, acc 0.84375\n",
      "2017-11-05T10:42:31.670529: step 2514, loss 0.296378, acc 0.90625\n",
      "2017-11-05T10:42:35.779510: step 2515, loss 0.0986477, acc 0.96875\n",
      "2017-11-05T10:42:39.800752: step 2516, loss 0.217204, acc 0.875\n",
      "2017-11-05T10:42:43.764777: step 2517, loss 0.116183, acc 0.9375\n",
      "2017-11-05T10:42:47.735622: step 2518, loss 0.0962426, acc 0.9375\n",
      "2017-11-05T10:42:51.739375: step 2519, loss 0.206445, acc 0.90625\n",
      "2017-11-05T10:42:54.231535: step 2520, loss 0.0288537, acc 1\n",
      "2017-11-05T10:42:58.178840: step 2521, loss 0.156284, acc 0.9375\n",
      "2017-11-05T10:43:02.113481: step 2522, loss 0.2858, acc 0.84375\n",
      "2017-11-05T10:43:06.101197: step 2523, loss 0.195034, acc 0.90625\n",
      "2017-11-05T10:43:10.032242: step 2524, loss 0.154522, acc 0.90625\n",
      "2017-11-05T10:43:14.045093: step 2525, loss 0.207519, acc 0.90625\n",
      "2017-11-05T10:43:18.071868: step 2526, loss 0.0582642, acc 0.96875\n",
      "2017-11-05T10:43:22.028891: step 2527, loss 0.0904229, acc 0.96875\n",
      "2017-11-05T10:43:26.027383: step 2528, loss 0.160071, acc 0.90625\n",
      "2017-11-05T10:43:29.998009: step 2529, loss 0.0602473, acc 0.96875\n",
      "2017-11-05T10:43:33.934562: step 2530, loss 0.225491, acc 0.90625\n",
      "2017-11-05T10:43:37.926826: step 2531, loss 0.310309, acc 0.84375\n",
      "2017-11-05T10:43:41.916001: step 2532, loss 0.112036, acc 0.96875\n",
      "2017-11-05T10:43:45.904402: step 2533, loss 0.0655991, acc 0.96875\n",
      "2017-11-05T10:43:49.921531: step 2534, loss 0.0983847, acc 0.96875\n",
      "2017-11-05T10:43:53.876868: step 2535, loss 0.15141, acc 0.875\n",
      "2017-11-05T10:43:57.910760: step 2536, loss 0.217018, acc 0.9375\n",
      "2017-11-05T10:44:01.871781: step 2537, loss 0.205867, acc 0.9375\n",
      "2017-11-05T10:44:05.910573: step 2538, loss 0.133965, acc 0.96875\n",
      "2017-11-05T10:44:09.897916: step 2539, loss 0.22027, acc 0.875\n",
      "2017-11-05T10:44:13.941366: step 2540, loss 0.083711, acc 0.96875\n",
      "2017-11-05T10:44:17.973195: step 2541, loss 0.12816, acc 0.9375\n",
      "2017-11-05T10:44:21.959415: step 2542, loss 0.577293, acc 0.8125\n",
      "2017-11-05T10:44:26.330062: step 2543, loss 0.0953266, acc 0.9375\n",
      "2017-11-05T10:44:30.470151: step 2544, loss 0.0702526, acc 0.96875\n",
      "2017-11-05T10:44:34.592654: step 2545, loss 0.201562, acc 0.90625\n",
      "2017-11-05T10:44:38.664500: step 2546, loss 0.165901, acc 0.9375\n",
      "2017-11-05T10:44:42.648720: step 2547, loss 0.217992, acc 0.9375\n",
      "2017-11-05T10:44:46.665119: step 2548, loss 0.243627, acc 0.9375\n",
      "2017-11-05T10:44:50.663466: step 2549, loss 0.364626, acc 0.84375\n",
      "2017-11-05T10:44:54.619163: step 2550, loss 0.306159, acc 0.90625\n",
      "2017-11-05T10:44:58.588715: step 2551, loss 0.201985, acc 0.875\n",
      "2017-11-05T10:45:02.581602: step 2552, loss 0.100337, acc 0.96875\n",
      "2017-11-05T10:45:06.575327: step 2553, loss 0.224692, acc 0.90625\n",
      "2017-11-05T10:45:10.586592: step 2554, loss 0.0541627, acc 1\n",
      "2017-11-05T10:45:14.615927: step 2555, loss 0.278446, acc 0.84375\n",
      "2017-11-05T10:45:17.239562: step 2556, loss 0.211985, acc 0.95\n",
      "2017-11-05T10:45:21.223921: step 2557, loss 0.131013, acc 0.9375\n",
      "2017-11-05T10:45:25.190396: step 2558, loss 0.225492, acc 0.90625\n",
      "2017-11-05T10:45:29.184746: step 2559, loss 0.099499, acc 0.96875\n",
      "2017-11-05T10:45:33.209929: step 2560, loss 0.270318, acc 0.84375\n",
      "2017-11-05T10:45:37.206293: step 2561, loss 0.210328, acc 0.875\n",
      "2017-11-05T10:45:41.218173: step 2562, loss 0.223112, acc 0.90625\n",
      "2017-11-05T10:45:45.153995: step 2563, loss 0.279286, acc 0.90625\n",
      "2017-11-05T10:45:49.196225: step 2564, loss 0.105192, acc 0.96875\n",
      "2017-11-05T10:45:53.170166: step 2565, loss 0.113074, acc 0.96875\n",
      "2017-11-05T10:45:57.206557: step 2566, loss 0.400145, acc 0.8125\n",
      "2017-11-05T10:46:01.154374: step 2567, loss 0.254322, acc 0.875\n",
      "2017-11-05T10:46:05.245073: step 2568, loss 0.177433, acc 0.90625\n",
      "2017-11-05T10:46:09.266777: step 2569, loss 0.219573, acc 0.9375\n",
      "2017-11-05T10:46:13.322664: step 2570, loss 0.130082, acc 0.875\n",
      "2017-11-05T10:46:17.463972: step 2571, loss 0.209532, acc 0.90625\n",
      "2017-11-05T10:46:21.557381: step 2572, loss 0.177264, acc 0.90625\n",
      "2017-11-05T10:46:25.563727: step 2573, loss 0.222192, acc 0.875\n",
      "2017-11-05T10:46:29.568073: step 2574, loss 0.139834, acc 0.9375\n",
      "2017-11-05T10:46:33.659980: step 2575, loss 0.0493341, acc 1\n",
      "2017-11-05T10:46:37.885483: step 2576, loss 0.197626, acc 0.9375\n",
      "2017-11-05T10:46:42.079964: step 2577, loss 0.24172, acc 0.90625\n",
      "2017-11-05T10:46:46.155871: step 2578, loss 0.278196, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T10:46:50.162157: step 2579, loss 0.0841393, acc 0.96875\n",
      "2017-11-05T10:46:54.168391: step 2580, loss 0.023249, acc 1\n",
      "2017-11-05T10:46:58.295825: step 2581, loss 0.0447324, acc 0.96875\n",
      "2017-11-05T10:47:02.386230: step 2582, loss 0.115311, acc 0.9375\n",
      "2017-11-05T10:47:06.442113: step 2583, loss 0.0346917, acc 0.96875\n",
      "2017-11-05T10:47:10.439645: step 2584, loss 0.217493, acc 0.90625\n",
      "2017-11-05T10:47:14.404468: step 2585, loss 0.119597, acc 0.9375\n",
      "2017-11-05T10:47:18.530916: step 2586, loss 0.213443, acc 0.9375\n",
      "2017-11-05T10:47:22.549008: step 2587, loss 0.0672828, acc 0.96875\n",
      "2017-11-05T10:47:26.537882: step 2588, loss 0.0960737, acc 0.9375\n",
      "2017-11-05T10:47:30.589480: step 2589, loss 0.180512, acc 0.9375\n",
      "2017-11-05T10:47:34.594326: step 2590, loss 0.0906351, acc 0.9375\n",
      "2017-11-05T10:47:38.541131: step 2591, loss 0.00635266, acc 1\n",
      "2017-11-05T10:47:41.124992: step 2592, loss 0.39746, acc 0.85\n",
      "2017-11-05T10:47:45.133555: step 2593, loss 0.0945701, acc 0.9375\n",
      "2017-11-05T10:47:49.159398: step 2594, loss 0.137571, acc 0.875\n",
      "2017-11-05T10:47:53.093692: step 2595, loss 0.13553, acc 0.90625\n",
      "2017-11-05T10:47:57.135048: step 2596, loss 0.283942, acc 0.84375\n",
      "2017-11-05T10:48:01.134035: step 2597, loss 0.107252, acc 0.9375\n",
      "2017-11-05T10:48:05.162310: step 2598, loss 0.176226, acc 0.90625\n",
      "2017-11-05T10:48:09.152644: step 2599, loss 0.130507, acc 0.9375\n",
      "2017-11-05T10:48:13.185554: step 2600, loss 0.043476, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:48:15.861957: step 2600, loss 0.840492, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-05T10:48:21.134624: step 2601, loss 0.352252, acc 0.84375\n",
      "2017-11-05T10:48:25.273065: step 2602, loss 0.260893, acc 0.875\n",
      "2017-11-05T10:48:29.321943: step 2603, loss 0.079988, acc 0.96875\n",
      "2017-11-05T10:48:33.415579: step 2604, loss 0.258417, acc 0.875\n",
      "2017-11-05T10:48:37.436086: step 2605, loss 0.189386, acc 0.90625\n",
      "2017-11-05T10:48:41.694887: step 2606, loss 0.286176, acc 0.90625\n",
      "2017-11-05T10:48:45.784306: step 2607, loss 0.224217, acc 0.9375\n",
      "2017-11-05T10:48:49.919244: step 2608, loss 0.263268, acc 0.90625\n",
      "2017-11-05T10:48:54.016086: step 2609, loss 0.140218, acc 0.9375\n",
      "2017-11-05T10:48:58.264119: step 2610, loss 0.233585, acc 0.84375\n",
      "2017-11-05T10:49:02.372923: step 2611, loss 0.133904, acc 0.875\n",
      "2017-11-05T10:49:06.511865: step 2612, loss 0.017871, acc 1\n",
      "2017-11-05T10:49:10.678325: step 2613, loss 0.0531657, acc 0.96875\n",
      "2017-11-05T10:49:14.746215: step 2614, loss 0.308113, acc 0.90625\n",
      "2017-11-05T10:49:18.854634: step 2615, loss 0.186659, acc 0.84375\n",
      "2017-11-05T10:49:22.943193: step 2616, loss 0.213728, acc 0.90625\n",
      "2017-11-05T10:49:27.020591: step 2617, loss 0.263024, acc 0.875\n",
      "2017-11-05T10:49:31.140018: step 2618, loss 0.0662246, acc 0.96875\n",
      "2017-11-05T10:49:35.186393: step 2619, loss 0.187805, acc 0.875\n",
      "2017-11-05T10:49:39.421902: step 2620, loss 0.287191, acc 0.84375\n",
      "2017-11-05T10:49:43.561843: step 2621, loss 0.325241, acc 0.875\n",
      "2017-11-05T10:49:47.784344: step 2622, loss 0.262829, acc 0.84375\n",
      "2017-11-05T10:49:51.982338: step 2623, loss 0.0895964, acc 0.96875\n",
      "2017-11-05T10:49:56.014410: step 2624, loss 0.0747643, acc 0.96875\n",
      "2017-11-05T10:49:59.994359: step 2625, loss 0.386838, acc 0.78125\n",
      "2017-11-05T10:50:04.317599: step 2626, loss 0.151591, acc 0.90625\n",
      "2017-11-05T10:50:08.355274: step 2627, loss 0.040558, acc 0.96875\n",
      "2017-11-05T10:50:10.956123: step 2628, loss 0.21338, acc 0.95\n",
      "2017-11-05T10:50:15.020510: step 2629, loss 0.132891, acc 0.9375\n",
      "2017-11-05T10:50:19.073584: step 2630, loss 0.380929, acc 0.8125\n",
      "2017-11-05T10:50:23.109868: step 2631, loss 0.0930166, acc 0.96875\n",
      "2017-11-05T10:50:27.103290: step 2632, loss 0.178101, acc 0.90625\n",
      "2017-11-05T10:50:31.149402: step 2633, loss 0.173291, acc 0.90625\n",
      "2017-11-05T10:50:35.289326: step 2634, loss 0.246184, acc 0.875\n",
      "2017-11-05T10:50:39.305225: step 2635, loss 0.13767, acc 0.9375\n",
      "2017-11-05T10:50:43.284626: step 2636, loss 0.21258, acc 0.9375\n",
      "2017-11-05T10:50:47.328999: step 2637, loss 0.28416, acc 0.875\n",
      "2017-11-05T10:50:51.336231: step 2638, loss 0.0852441, acc 0.9375\n",
      "2017-11-05T10:50:55.387756: step 2639, loss 0.0641916, acc 0.96875\n",
      "2017-11-05T10:50:59.362624: step 2640, loss 0.0627004, acc 0.96875\n",
      "2017-11-05T10:51:03.402994: step 2641, loss 0.194483, acc 0.90625\n",
      "2017-11-05T10:51:07.461388: step 2642, loss 0.0227464, acc 1\n",
      "2017-11-05T10:51:11.542798: step 2643, loss 0.20261, acc 0.9375\n",
      "2017-11-05T10:51:15.559432: step 2644, loss 0.0964364, acc 0.96875\n",
      "2017-11-05T10:51:19.552441: step 2645, loss 0.483678, acc 0.78125\n",
      "2017-11-05T10:51:23.588309: step 2646, loss 0.080952, acc 0.96875\n",
      "2017-11-05T10:51:27.575529: step 2647, loss 0.282584, acc 0.84375\n",
      "2017-11-05T10:51:31.575880: step 2648, loss 0.237797, acc 0.875\n",
      "2017-11-05T10:51:35.638760: step 2649, loss 0.162758, acc 0.9375\n",
      "2017-11-05T10:51:39.651111: step 2650, loss 0.199651, acc 0.9375\n",
      "2017-11-05T10:51:43.726896: step 2651, loss 0.252632, acc 0.875\n",
      "2017-11-05T10:51:47.720763: step 2652, loss 0.37422, acc 0.8125\n",
      "2017-11-05T10:51:51.820080: step 2653, loss 0.0679348, acc 1\n",
      "2017-11-05T10:51:55.878744: step 2654, loss 0.166427, acc 0.9375\n",
      "2017-11-05T10:51:59.904251: step 2655, loss 0.195545, acc 0.90625\n",
      "2017-11-05T10:52:03.922007: step 2656, loss 0.0718635, acc 0.96875\n",
      "2017-11-05T10:52:07.978406: step 2657, loss 0.400951, acc 0.875\n",
      "2017-11-05T10:52:12.188399: step 2658, loss 0.135793, acc 0.9375\n",
      "2017-11-05T10:52:16.224765: step 2659, loss 0.217596, acc 0.90625\n",
      "2017-11-05T10:52:20.313170: step 2660, loss 0.161636, acc 0.96875\n",
      "2017-11-05T10:52:24.399073: step 2661, loss 0.158428, acc 0.9375\n",
      "2017-11-05T10:52:28.486477: step 2662, loss 0.0577589, acc 1\n",
      "2017-11-05T10:52:32.594897: step 2663, loss 0.242244, acc 0.90625\n",
      "2017-11-05T10:52:35.275802: step 2664, loss 0.0152811, acc 1\n",
      "2017-11-05T10:52:39.335187: step 2665, loss 0.227776, acc 0.90625\n",
      "2017-11-05T10:52:43.408581: step 2666, loss 0.167725, acc 0.90625\n",
      "2017-11-05T10:52:47.454455: step 2667, loss 0.020112, acc 1\n",
      "2017-11-05T10:52:51.478703: step 2668, loss 0.205461, acc 0.90625\n",
      "2017-11-05T10:52:55.504103: step 2669, loss 0.0282055, acc 1\n",
      "2017-11-05T10:52:59.559483: step 2670, loss 0.156239, acc 0.9375\n",
      "2017-11-05T10:53:03.678410: step 2671, loss 0.0358859, acc 1\n",
      "2017-11-05T10:53:07.697774: step 2672, loss 0.159662, acc 0.9375\n",
      "2017-11-05T10:53:11.748739: step 2673, loss 0.0349116, acc 1\n",
      "2017-11-05T10:53:15.777007: step 2674, loss 0.206604, acc 0.90625\n",
      "2017-11-05T10:53:19.819379: step 2675, loss 0.111487, acc 0.9375\n",
      "2017-11-05T10:53:23.979067: step 2676, loss 0.0118901, acc 1\n",
      "2017-11-05T10:53:28.304180: step 2677, loss 0.189352, acc 0.90625\n",
      "2017-11-05T10:53:32.303101: step 2678, loss 0.0810673, acc 0.96875\n",
      "2017-11-05T10:53:36.374799: step 2679, loss 0.154269, acc 0.90625\n",
      "2017-11-05T10:53:40.376340: step 2680, loss 0.120029, acc 0.9375\n",
      "2017-11-05T10:53:44.329674: step 2681, loss 0.101093, acc 0.9375\n",
      "2017-11-05T10:53:48.349537: step 2682, loss 0.241745, acc 0.90625\n",
      "2017-11-05T10:53:52.397544: step 2683, loss 0.26354, acc 0.90625\n",
      "2017-11-05T10:53:56.420912: step 2684, loss 0.169785, acc 0.90625\n",
      "2017-11-05T10:54:00.485812: step 2685, loss 0.239725, acc 0.90625\n",
      "2017-11-05T10:54:04.536704: step 2686, loss 0.173871, acc 0.90625\n",
      "2017-11-05T10:54:08.570265: step 2687, loss 0.104242, acc 0.96875\n",
      "2017-11-05T10:54:12.601747: step 2688, loss 0.0788538, acc 0.96875\n",
      "2017-11-05T10:54:16.635882: step 2689, loss 0.133966, acc 0.9375\n",
      "2017-11-05T10:54:20.686873: step 2690, loss 0.277273, acc 0.875\n",
      "2017-11-05T10:54:24.708325: step 2691, loss 0.527109, acc 0.84375\n",
      "2017-11-05T10:54:28.798832: step 2692, loss 0.332652, acc 0.84375\n",
      "2017-11-05T10:54:32.854715: step 2693, loss 0.108941, acc 0.9375\n",
      "2017-11-05T10:54:36.922588: step 2694, loss 0.140025, acc 0.9375\n",
      "2017-11-05T10:54:40.933647: step 2695, loss 0.255202, acc 0.90625\n",
      "2017-11-05T10:54:44.969420: step 2696, loss 0.241179, acc 0.875\n",
      "2017-11-05T10:54:48.993639: step 2697, loss 0.209092, acc 0.875\n",
      "2017-11-05T10:54:52.999269: step 2698, loss 0.200062, acc 0.9375\n",
      "2017-11-05T10:54:57.025180: step 2699, loss 0.299161, acc 0.9375\n",
      "2017-11-05T10:54:59.553867: step 2700, loss 0.311663, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T10:55:02.173616: step 2700, loss 0.67891, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2700\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T10:55:07.430452: step 2701, loss 0.185959, acc 0.90625\n",
      "2017-11-05T10:55:11.438284: step 2702, loss 0.304126, acc 0.875\n",
      "2017-11-05T10:55:15.437023: step 2703, loss 0.162919, acc 0.90625\n",
      "2017-11-05T10:55:19.465789: step 2704, loss 0.27534, acc 0.875\n",
      "2017-11-05T10:55:23.420285: step 2705, loss 0.0195046, acc 1\n",
      "2017-11-05T10:55:27.441726: step 2706, loss 0.0860124, acc 0.96875\n",
      "2017-11-05T10:55:31.411436: step 2707, loss 0.216511, acc 0.9375\n",
      "2017-11-05T10:55:35.467036: step 2708, loss 0.386321, acc 0.90625\n",
      "2017-11-05T10:55:39.480388: step 2709, loss 0.0919612, acc 0.96875\n",
      "2017-11-05T10:55:43.516270: step 2710, loss 0.0852336, acc 0.96875\n",
      "2017-11-05T10:55:47.470911: step 2711, loss 0.0988241, acc 0.96875\n",
      "2017-11-05T10:55:51.510781: step 2712, loss 0.545478, acc 0.84375\n",
      "2017-11-05T10:55:55.518831: step 2713, loss 0.607234, acc 0.78125\n",
      "2017-11-05T10:55:59.534053: step 2714, loss 0.0661865, acc 0.96875\n",
      "2017-11-05T10:56:03.528662: step 2715, loss 0.0857829, acc 0.90625\n",
      "2017-11-05T10:56:07.528504: step 2716, loss 0.211715, acc 0.90625\n",
      "2017-11-05T10:56:11.551878: step 2717, loss 0.333162, acc 0.84375\n",
      "2017-11-05T10:56:15.535045: step 2718, loss 0.299281, acc 0.9375\n",
      "2017-11-05T10:56:19.559558: step 2719, loss 0.319033, acc 0.90625\n",
      "2017-11-05T10:56:23.536773: step 2720, loss 0.0879748, acc 0.96875\n",
      "2017-11-05T10:56:27.565024: step 2721, loss 0.181538, acc 0.90625\n",
      "2017-11-05T10:56:31.630914: step 2722, loss 0.178755, acc 0.90625\n",
      "2017-11-05T10:56:35.783864: step 2723, loss 0.203041, acc 0.9375\n",
      "2017-11-05T10:56:39.858259: step 2724, loss 0.107265, acc 0.96875\n",
      "2017-11-05T10:56:43.858922: step 2725, loss 0.164812, acc 0.9375\n",
      "2017-11-05T10:56:47.869271: step 2726, loss 0.235182, acc 0.9375\n",
      "2017-11-05T10:56:51.916892: step 2727, loss 0.317309, acc 0.875\n",
      "2017-11-05T10:56:55.986679: step 2728, loss 0.123825, acc 0.90625\n",
      "2017-11-05T10:56:59.941877: step 2729, loss 0.433653, acc 0.84375\n",
      "2017-11-05T10:57:03.934727: step 2730, loss 0.0800537, acc 0.9375\n",
      "2017-11-05T10:57:08.013647: step 2731, loss 0.185194, acc 0.90625\n",
      "2017-11-05T10:57:12.026091: step 2732, loss 0.48312, acc 0.78125\n",
      "2017-11-05T10:57:16.036440: step 2733, loss 0.133499, acc 0.9375\n",
      "2017-11-05T10:57:20.007921: step 2734, loss 0.0826762, acc 0.96875\n",
      "2017-11-05T10:57:24.084818: step 2735, loss 0.267535, acc 0.875\n",
      "2017-11-05T10:57:26.655646: step 2736, loss 0.271761, acc 0.85\n",
      "2017-11-05T10:57:30.682519: step 2737, loss 0.262948, acc 0.84375\n",
      "2017-11-05T10:57:34.730236: step 2738, loss 0.165518, acc 0.90625\n",
      "2017-11-05T10:57:38.775611: step 2739, loss 0.114977, acc 0.9375\n",
      "2017-11-05T10:57:42.739315: step 2740, loss 0.111563, acc 0.9375\n",
      "2017-11-05T10:57:46.780687: step 2741, loss 0.218764, acc 0.90625\n",
      "2017-11-05T10:57:50.796054: step 2742, loss 0.194997, acc 0.84375\n",
      "2017-11-05T10:57:54.946002: step 2743, loss 0.252251, acc 0.90625\n",
      "2017-11-05T10:57:58.893646: step 2744, loss 0.357631, acc 0.84375\n",
      "2017-11-05T10:58:02.956033: step 2745, loss 0.260134, acc 0.90625\n",
      "2017-11-05T10:58:06.927364: step 2746, loss 0.172853, acc 0.96875\n",
      "2017-11-05T10:58:10.986151: step 2747, loss 0.352515, acc 0.875\n",
      "2017-11-05T10:58:14.981404: step 2748, loss 0.429016, acc 0.875\n",
      "2017-11-05T10:58:18.972741: step 2749, loss 0.24073, acc 0.9375\n",
      "2017-11-05T10:58:23.125191: step 2750, loss 0.0794976, acc 0.96875\n",
      "2017-11-05T10:58:27.545786: step 2751, loss 0.149804, acc 0.9375\n",
      "2017-11-05T10:58:31.572659: step 2752, loss 0.298407, acc 0.875\n",
      "2017-11-05T10:58:35.744527: step 2753, loss 0.04495, acc 0.96875\n",
      "2017-11-05T10:58:39.745371: step 2754, loss 0.165059, acc 0.90625\n",
      "2017-11-05T10:58:43.784996: step 2755, loss 0.0996314, acc 0.9375\n",
      "2017-11-05T10:58:47.771829: step 2756, loss 0.0964594, acc 0.9375\n",
      "2017-11-05T10:58:51.817203: step 2757, loss 0.525554, acc 0.78125\n",
      "2017-11-05T10:58:55.848067: step 2758, loss 0.258232, acc 0.90625\n",
      "2017-11-05T10:58:59.878716: step 2759, loss 0.11804, acc 0.96875\n",
      "2017-11-05T10:59:03.950376: step 2760, loss 0.0980576, acc 0.9375\n",
      "2017-11-05T10:59:07.929218: step 2761, loss 0.171542, acc 0.96875\n",
      "2017-11-05T10:59:11.988101: step 2762, loss 0.193563, acc 0.9375\n",
      "2017-11-05T10:59:15.965630: step 2763, loss 0.122616, acc 0.90625\n",
      "2017-11-05T10:59:20.013042: step 2764, loss 0.427393, acc 0.78125\n",
      "2017-11-05T10:59:24.018678: step 2765, loss 0.289555, acc 0.90625\n",
      "2017-11-05T10:59:28.016216: step 2766, loss 0.0710022, acc 0.9375\n",
      "2017-11-05T10:59:32.156158: step 2767, loss 0.390791, acc 0.875\n",
      "2017-11-05T10:59:36.204535: step 2768, loss 0.191975, acc 0.90625\n",
      "2017-11-05T10:59:40.231396: step 2769, loss 0.210371, acc 0.9375\n",
      "2017-11-05T10:59:44.233057: step 2770, loss 0.21726, acc 0.9375\n",
      "2017-11-05T10:59:48.253651: step 2771, loss 0.299216, acc 0.875\n",
      "2017-11-05T10:59:50.870671: step 2772, loss 0.0232549, acc 1\n",
      "2017-11-05T10:59:54.890184: step 2773, loss 0.0335546, acc 1\n",
      "2017-11-05T10:59:58.901692: step 2774, loss 0.289595, acc 0.875\n",
      "2017-11-05T11:00:03.234383: step 2775, loss 0.21001, acc 0.875\n",
      "2017-11-05T11:00:07.250857: step 2776, loss 0.0958823, acc 0.9375\n",
      "2017-11-05T11:00:11.307281: step 2777, loss 0.0719386, acc 0.96875\n",
      "2017-11-05T11:00:15.412374: step 2778, loss 0.186447, acc 0.9375\n",
      "2017-11-05T11:00:19.364601: step 2779, loss 0.0720794, acc 0.96875\n",
      "2017-11-05T11:00:23.417636: step 2780, loss 0.231358, acc 0.90625\n",
      "2017-11-05T11:00:27.500234: step 2781, loss 0.0836623, acc 0.9375\n",
      "2017-11-05T11:00:31.490569: step 2782, loss 0.29923, acc 0.84375\n",
      "2017-11-05T11:00:35.758603: step 2783, loss 0.0598338, acc 0.96875\n",
      "2017-11-05T11:00:39.850206: step 2784, loss 0.0586028, acc 0.9375\n",
      "2017-11-05T11:00:43.895465: step 2785, loss 0.0854179, acc 0.9375\n",
      "2017-11-05T11:00:47.923064: step 2786, loss 0.0282022, acc 1\n",
      "2017-11-05T11:00:51.914915: step 2787, loss 0.267152, acc 0.84375\n",
      "2017-11-05T11:00:55.927335: step 2788, loss 0.189529, acc 0.9375\n",
      "2017-11-05T11:00:59.981092: step 2789, loss 0.160363, acc 0.9375\n",
      "2017-11-05T11:01:04.054512: step 2790, loss 0.0666804, acc 0.96875\n",
      "2017-11-05T11:01:08.084249: step 2791, loss 0.242169, acc 0.90625\n",
      "2017-11-05T11:01:12.174043: step 2792, loss 0.113722, acc 0.90625\n",
      "2017-11-05T11:01:16.224831: step 2793, loss 0.125003, acc 0.9375\n",
      "2017-11-05T11:01:20.216873: step 2794, loss 0.198741, acc 0.9375\n",
      "2017-11-05T11:01:24.227730: step 2795, loss 0.0628548, acc 0.96875\n",
      "2017-11-05T11:01:28.331892: step 2796, loss 0.422275, acc 0.8125\n",
      "2017-11-05T11:01:32.332142: step 2797, loss 0.106026, acc 0.96875\n",
      "2017-11-05T11:01:36.418059: step 2798, loss 0.176444, acc 0.875\n",
      "2017-11-05T11:01:40.466015: step 2799, loss 0.176656, acc 0.9375\n",
      "2017-11-05T11:01:44.499288: step 2800, loss 0.145312, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:01:47.124167: step 2800, loss 0.712043, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-05T11:01:52.405819: step 2801, loss 0.286378, acc 0.8125\n",
      "2017-11-05T11:01:56.410314: step 2802, loss 0.303518, acc 0.875\n",
      "2017-11-05T11:02:00.398148: step 2803, loss 0.311273, acc 0.84375\n",
      "2017-11-05T11:02:04.469049: step 2804, loss 0.33774, acc 0.90625\n",
      "2017-11-05T11:02:08.555476: step 2805, loss 0.24513, acc 0.90625\n",
      "2017-11-05T11:02:12.677805: step 2806, loss 0.230086, acc 0.90625\n",
      "2017-11-05T11:02:16.697661: step 2807, loss 0.168106, acc 0.90625\n",
      "2017-11-05T11:02:19.204830: step 2808, loss 0.223714, acc 0.9\n",
      "2017-11-05T11:02:23.161142: step 2809, loss 0.228453, acc 0.875\n",
      "2017-11-05T11:02:27.159620: step 2810, loss 0.252515, acc 0.90625\n",
      "2017-11-05T11:02:31.192754: step 2811, loss 0.137354, acc 0.9375\n",
      "2017-11-05T11:02:35.407748: step 2812, loss 0.0993348, acc 0.96875\n",
      "2017-11-05T11:02:39.464631: step 2813, loss 0.239459, acc 0.90625\n",
      "2017-11-05T11:02:43.509505: step 2814, loss 0.0428555, acc 0.96875\n",
      "2017-11-05T11:02:47.606417: step 2815, loss 0.324696, acc 0.84375\n",
      "2017-11-05T11:02:51.630775: step 2816, loss 0.177686, acc 0.90625\n",
      "2017-11-05T11:02:55.651633: step 2817, loss 0.326774, acc 0.84375\n",
      "2017-11-05T11:02:59.797579: step 2818, loss 0.0559997, acc 0.96875\n",
      "2017-11-05T11:03:03.819937: step 2819, loss 0.105126, acc 0.9375\n",
      "2017-11-05T11:03:07.816276: step 2820, loss 0.227308, acc 0.9375\n",
      "2017-11-05T11:03:11.886169: step 2821, loss 0.112297, acc 0.9375\n",
      "2017-11-05T11:03:15.918271: step 2822, loss 0.151302, acc 0.9375\n",
      "2017-11-05T11:03:19.925098: step 2823, loss 0.0541406, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:03:24.028695: step 2824, loss 0.314777, acc 0.875\n",
      "2017-11-05T11:03:28.116599: step 2825, loss 0.0555123, acc 0.96875\n",
      "2017-11-05T11:03:32.091046: step 2826, loss 0.537222, acc 0.8125\n",
      "2017-11-05T11:03:36.154524: step 2827, loss 0.20426, acc 0.90625\n",
      "2017-11-05T11:03:40.196285: step 2828, loss 0.563262, acc 0.78125\n",
      "2017-11-05T11:03:44.224413: step 2829, loss 0.159302, acc 0.9375\n",
      "2017-11-05T11:03:48.254270: step 2830, loss 0.30806, acc 0.84375\n",
      "2017-11-05T11:03:52.231477: step 2831, loss 0.51785, acc 0.78125\n",
      "2017-11-05T11:03:56.242943: step 2832, loss 0.217194, acc 0.96875\n",
      "2017-11-05T11:04:00.270910: step 2833, loss 0.120979, acc 0.9375\n",
      "2017-11-05T11:04:04.322789: step 2834, loss 0.107959, acc 0.9375\n",
      "2017-11-05T11:04:08.306982: step 2835, loss 0.101344, acc 0.96875\n",
      "2017-11-05T11:04:12.333343: step 2836, loss 0.0544255, acc 0.96875\n",
      "2017-11-05T11:04:16.339806: step 2837, loss 0.206768, acc 0.9375\n",
      "2017-11-05T11:04:20.367167: step 2838, loss 0.184009, acc 0.875\n",
      "2017-11-05T11:04:24.405037: step 2839, loss 0.161828, acc 0.90625\n",
      "2017-11-05T11:04:28.468424: step 2840, loss 0.410582, acc 0.875\n",
      "2017-11-05T11:04:32.417539: step 2841, loss 0.459411, acc 0.875\n",
      "2017-11-05T11:04:36.444962: step 2842, loss 0.0397165, acc 0.96875\n",
      "2017-11-05T11:04:40.386244: step 2843, loss 0.142642, acc 0.96875\n",
      "2017-11-05T11:04:42.880485: step 2844, loss 0.237357, acc 0.9\n",
      "2017-11-05T11:04:46.821555: step 2845, loss 0.193927, acc 0.90625\n",
      "2017-11-05T11:04:50.716374: step 2846, loss 0.0891903, acc 0.9375\n",
      "2017-11-05T11:04:54.659174: step 2847, loss 0.174663, acc 0.90625\n",
      "2017-11-05T11:04:58.573470: step 2848, loss 0.203504, acc 0.9375\n",
      "2017-11-05T11:05:02.464253: step 2849, loss 0.0758763, acc 1\n",
      "2017-11-05T11:05:06.354429: step 2850, loss 0.180683, acc 0.90625\n",
      "2017-11-05T11:05:10.259224: step 2851, loss 0.238094, acc 0.90625\n",
      "2017-11-05T11:05:14.203054: step 2852, loss 0.0959445, acc 0.96875\n",
      "2017-11-05T11:05:18.143546: step 2853, loss 0.126421, acc 0.90625\n",
      "2017-11-05T11:05:22.065297: step 2854, loss 0.161655, acc 0.96875\n",
      "2017-11-05T11:05:26.028414: step 2855, loss 0.310659, acc 0.8125\n",
      "2017-11-05T11:05:29.930498: step 2856, loss 0.297092, acc 0.84375\n",
      "2017-11-05T11:05:33.859764: step 2857, loss 0.244192, acc 0.90625\n",
      "2017-11-05T11:05:37.778465: step 2858, loss 0.198578, acc 0.90625\n",
      "2017-11-05T11:05:41.736192: step 2859, loss 0.359316, acc 0.90625\n",
      "2017-11-05T11:05:45.651510: step 2860, loss 0.479885, acc 0.875\n",
      "2017-11-05T11:05:49.563131: step 2861, loss 0.227671, acc 0.90625\n",
      "2017-11-05T11:05:53.503429: step 2862, loss 0.10441, acc 0.96875\n",
      "2017-11-05T11:05:57.394651: step 2863, loss 0.261178, acc 0.84375\n",
      "2017-11-05T11:06:01.297326: step 2864, loss 0.146305, acc 0.9375\n",
      "2017-11-05T11:06:05.240062: step 2865, loss 0.0928204, acc 0.96875\n",
      "2017-11-05T11:06:09.155522: step 2866, loss 0.257885, acc 0.84375\n",
      "2017-11-05T11:06:13.094239: step 2867, loss 0.0771247, acc 0.96875\n",
      "2017-11-05T11:06:17.013795: step 2868, loss 0.156358, acc 0.90625\n",
      "2017-11-05T11:06:20.910582: step 2869, loss 0.13526, acc 0.90625\n",
      "2017-11-05T11:06:24.817858: step 2870, loss 0.0653927, acc 0.96875\n",
      "2017-11-05T11:06:28.762217: step 2871, loss 0.162358, acc 0.9375\n",
      "2017-11-05T11:06:32.742652: step 2872, loss 0.219913, acc 0.90625\n",
      "2017-11-05T11:06:36.768068: step 2873, loss 0.222363, acc 0.8125\n",
      "2017-11-05T11:06:40.767045: step 2874, loss 0.223233, acc 0.90625\n",
      "2017-11-05T11:06:44.764678: step 2875, loss 0.158762, acc 0.9375\n",
      "2017-11-05T11:06:48.691367: step 2876, loss 0.0915724, acc 0.9375\n",
      "2017-11-05T11:06:52.619902: step 2877, loss 0.0587174, acc 0.96875\n",
      "2017-11-05T11:06:56.545207: step 2878, loss 0.260123, acc 0.9375\n",
      "2017-11-05T11:07:00.491873: step 2879, loss 0.290119, acc 0.9375\n",
      "2017-11-05T11:07:03.027064: step 2880, loss 0.118251, acc 0.9\n",
      "2017-11-05T11:07:06.977616: step 2881, loss 0.118964, acc 0.96875\n",
      "2017-11-05T11:07:10.913070: step 2882, loss 0.186755, acc 0.9375\n",
      "2017-11-05T11:07:14.862891: step 2883, loss 0.207815, acc 0.90625\n",
      "2017-11-05T11:07:18.797335: step 2884, loss 0.201856, acc 0.90625\n",
      "2017-11-05T11:07:22.747484: step 2885, loss 0.116937, acc 0.96875\n",
      "2017-11-05T11:07:26.656585: step 2886, loss 0.024966, acc 1\n",
      "2017-11-05T11:07:30.582390: step 2887, loss 0.221671, acc 0.875\n",
      "2017-11-05T11:07:34.497577: step 2888, loss 0.147434, acc 0.90625\n",
      "2017-11-05T11:07:38.402555: step 2889, loss 0.174393, acc 0.90625\n",
      "2017-11-05T11:07:42.368773: step 2890, loss 0.13741, acc 0.96875\n",
      "2017-11-05T11:07:46.300081: step 2891, loss 0.224709, acc 0.875\n",
      "2017-11-05T11:07:50.275405: step 2892, loss 0.0965289, acc 0.9375\n",
      "2017-11-05T11:07:54.170076: step 2893, loss 0.375868, acc 0.875\n",
      "2017-11-05T11:07:58.092870: step 2894, loss 0.279352, acc 0.90625\n",
      "2017-11-05T11:08:02.018715: step 2895, loss 0.0688974, acc 1\n",
      "2017-11-05T11:08:05.959066: step 2896, loss 0.0297294, acc 1\n",
      "2017-11-05T11:08:09.944398: step 2897, loss 0.0467928, acc 0.96875\n",
      "2017-11-05T11:08:13.923225: step 2898, loss 0.12519, acc 0.96875\n",
      "2017-11-05T11:08:17.885540: step 2899, loss 0.312783, acc 0.875\n",
      "2017-11-05T11:08:21.890386: step 2900, loss 0.144361, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:08:24.721898: step 2900, loss 0.882835, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-05T11:08:30.128221: step 2901, loss 0.138989, acc 0.96875\n",
      "2017-11-05T11:08:34.227052: step 2902, loss 0.263604, acc 0.875\n",
      "2017-11-05T11:08:38.215388: step 2903, loss 0.280937, acc 0.84375\n",
      "2017-11-05T11:08:42.170900: step 2904, loss 0.132388, acc 0.9375\n",
      "2017-11-05T11:08:46.133835: step 2905, loss 0.0523369, acc 0.96875\n",
      "2017-11-05T11:08:50.099168: step 2906, loss 0.0978993, acc 0.96875\n",
      "2017-11-05T11:08:54.067032: step 2907, loss 0.206468, acc 0.9375\n",
      "2017-11-05T11:08:57.996869: step 2908, loss 0.182611, acc 0.9375\n",
      "2017-11-05T11:09:02.060529: step 2909, loss 0.0965627, acc 0.9375\n",
      "2017-11-05T11:09:06.023376: step 2910, loss 0.1745, acc 0.90625\n",
      "2017-11-05T11:09:09.965209: step 2911, loss 0.345816, acc 0.875\n",
      "2017-11-05T11:09:13.909360: step 2912, loss 0.109372, acc 0.9375\n",
      "2017-11-05T11:09:17.856051: step 2913, loss 0.063114, acc 0.96875\n",
      "2017-11-05T11:09:21.784760: step 2914, loss 0.146485, acc 0.9375\n",
      "2017-11-05T11:09:25.757375: step 2915, loss 0.177957, acc 0.90625\n",
      "2017-11-05T11:09:28.290578: step 2916, loss 0.182995, acc 0.9\n",
      "2017-11-05T11:09:32.228793: step 2917, loss 0.254305, acc 0.875\n",
      "2017-11-05T11:09:36.154681: step 2918, loss 0.152304, acc 0.9375\n",
      "2017-11-05T11:09:40.119313: step 2919, loss 0.125906, acc 0.90625\n",
      "2017-11-05T11:09:44.029194: step 2920, loss 0.0680598, acc 0.96875\n",
      "2017-11-05T11:09:47.982742: step 2921, loss 0.108093, acc 0.96875\n",
      "2017-11-05T11:09:51.988089: step 2922, loss 0.0870846, acc 0.9375\n",
      "2017-11-05T11:09:55.923885: step 2923, loss 0.0608483, acc 0.96875\n",
      "2017-11-05T11:09:59.847087: step 2924, loss 0.112375, acc 0.9375\n",
      "2017-11-05T11:10:04.085599: step 2925, loss 0.230415, acc 0.9375\n",
      "2017-11-05T11:10:08.116357: step 2926, loss 0.0943854, acc 0.9375\n",
      "2017-11-05T11:10:12.123269: step 2927, loss 0.107159, acc 0.9375\n",
      "2017-11-05T11:10:16.047147: step 2928, loss 0.467107, acc 0.84375\n",
      "2017-11-05T11:10:19.955786: step 2929, loss 0.34399, acc 0.78125\n",
      "2017-11-05T11:10:23.918816: step 2930, loss 0.0269885, acc 1\n",
      "2017-11-05T11:10:27.856128: step 2931, loss 0.152977, acc 0.90625\n",
      "2017-11-05T11:10:31.796235: step 2932, loss 0.058893, acc 0.96875\n",
      "2017-11-05T11:10:35.857240: step 2933, loss 0.101517, acc 0.96875\n",
      "2017-11-05T11:10:39.814592: step 2934, loss 0.129352, acc 0.96875\n",
      "2017-11-05T11:10:43.779204: step 2935, loss 0.226626, acc 0.875\n",
      "2017-11-05T11:10:47.738901: step 2936, loss 0.352742, acc 0.84375\n",
      "2017-11-05T11:10:51.714317: step 2937, loss 0.119655, acc 0.96875\n",
      "2017-11-05T11:10:55.689150: step 2938, loss 0.0441886, acc 0.96875\n",
      "2017-11-05T11:10:59.585120: step 2939, loss 0.324153, acc 0.84375\n",
      "2017-11-05T11:11:03.514203: step 2940, loss 0.178972, acc 0.9375\n",
      "2017-11-05T11:11:07.486101: step 2941, loss 0.184083, acc 0.9375\n",
      "2017-11-05T11:11:11.430965: step 2942, loss 0.479813, acc 0.8125\n",
      "2017-11-05T11:11:15.378828: step 2943, loss 0.45359, acc 0.8125\n",
      "2017-11-05T11:11:19.293504: step 2944, loss 0.169269, acc 0.90625\n",
      "2017-11-05T11:11:23.206147: step 2945, loss 0.082393, acc 0.96875\n",
      "2017-11-05T11:11:27.137829: step 2946, loss 0.272179, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:11:31.110546: step 2947, loss 0.105746, acc 0.90625\n",
      "2017-11-05T11:11:35.121395: step 2948, loss 0.28991, acc 0.875\n",
      "2017-11-05T11:11:39.064572: step 2949, loss 0.326293, acc 0.875\n",
      "2017-11-05T11:11:43.023886: step 2950, loss 0.300026, acc 0.9375\n",
      "2017-11-05T11:11:47.009717: step 2951, loss 0.239201, acc 0.9375\n",
      "2017-11-05T11:11:49.727549: step 2952, loss 0.118544, acc 1\n",
      "2017-11-05T11:11:53.784432: step 2953, loss 0.298773, acc 0.875\n",
      "2017-11-05T11:11:57.707223: step 2954, loss 0.215487, acc 0.875\n",
      "2017-11-05T11:12:01.638635: step 2955, loss 0.0808722, acc 0.9375\n",
      "2017-11-05T11:12:05.622946: step 2956, loss 0.216535, acc 0.90625\n",
      "2017-11-05T11:12:09.518896: step 2957, loss 0.132166, acc 0.9375\n",
      "2017-11-05T11:12:13.518236: step 2958, loss 0.369146, acc 0.90625\n",
      "2017-11-05T11:12:17.578862: step 2959, loss 0.213317, acc 0.90625\n",
      "2017-11-05T11:12:21.540087: step 2960, loss 0.0554589, acc 1\n",
      "2017-11-05T11:12:25.469394: step 2961, loss 0.0576562, acc 0.96875\n",
      "2017-11-05T11:12:29.428493: step 2962, loss 0.376269, acc 0.875\n",
      "2017-11-05T11:12:33.436341: step 2963, loss 0.115638, acc 0.96875\n",
      "2017-11-05T11:12:37.497727: step 2964, loss 0.304762, acc 0.90625\n",
      "2017-11-05T11:12:41.494581: step 2965, loss 0.121412, acc 0.90625\n",
      "2017-11-05T11:12:45.427375: step 2966, loss 0.0367424, acc 0.96875\n",
      "2017-11-05T11:12:49.479141: step 2967, loss 0.0628488, acc 0.96875\n",
      "2017-11-05T11:12:53.518011: step 2968, loss 0.249572, acc 0.90625\n",
      "2017-11-05T11:12:57.481328: step 2969, loss 0.254459, acc 0.90625\n",
      "2017-11-05T11:13:01.436138: step 2970, loss 0.207295, acc 0.90625\n",
      "2017-11-05T11:13:05.432432: step 2971, loss 0.205324, acc 0.90625\n",
      "2017-11-05T11:13:09.442267: step 2972, loss 0.0175123, acc 1\n",
      "2017-11-05T11:13:13.421936: step 2973, loss 0.113241, acc 0.96875\n",
      "2017-11-05T11:13:17.387525: step 2974, loss 0.122832, acc 0.9375\n",
      "2017-11-05T11:13:21.373372: step 2975, loss 0.23995, acc 0.90625\n",
      "2017-11-05T11:13:25.510704: step 2976, loss 0.182003, acc 0.90625\n",
      "2017-11-05T11:13:29.617376: step 2977, loss 0.162598, acc 0.9375\n",
      "2017-11-05T11:13:33.581380: step 2978, loss 0.070637, acc 0.96875\n",
      "2017-11-05T11:13:37.579230: step 2979, loss 0.163885, acc 0.9375\n",
      "2017-11-05T11:13:41.546435: step 2980, loss 0.279709, acc 0.90625\n",
      "2017-11-05T11:13:45.539788: step 2981, loss 0.289854, acc 0.90625\n",
      "2017-11-05T11:13:49.510027: step 2982, loss 0.354457, acc 0.8125\n",
      "2017-11-05T11:13:53.471599: step 2983, loss 0.144501, acc 0.9375\n",
      "2017-11-05T11:13:57.447436: step 2984, loss 0.202743, acc 0.875\n",
      "2017-11-05T11:14:01.375626: step 2985, loss 0.187329, acc 0.9375\n",
      "2017-11-05T11:14:05.374525: step 2986, loss 0.366925, acc 0.8125\n",
      "2017-11-05T11:14:09.356872: step 2987, loss 0.107645, acc 0.96875\n",
      "2017-11-05T11:14:11.862152: step 2988, loss 0.129415, acc 0.95\n",
      "2017-11-05T11:14:15.796542: step 2989, loss 0.0657791, acc 1\n",
      "2017-11-05T11:14:19.778127: step 2990, loss 0.220793, acc 0.84375\n",
      "2017-11-05T11:14:23.829506: step 2991, loss 0.230508, acc 0.84375\n",
      "2017-11-05T11:14:27.754831: step 2992, loss 0.258387, acc 0.875\n",
      "2017-11-05T11:14:31.715186: step 2993, loss 0.0629623, acc 0.96875\n",
      "2017-11-05T11:14:35.754891: step 2994, loss 0.140428, acc 0.9375\n",
      "2017-11-05T11:14:39.682692: step 2995, loss 0.1203, acc 0.9375\n",
      "2017-11-05T11:14:43.666023: step 2996, loss 0.307443, acc 0.84375\n",
      "2017-11-05T11:14:47.628913: step 2997, loss 0.0486738, acc 0.96875\n",
      "2017-11-05T11:14:51.646783: step 2998, loss 0.0752627, acc 0.96875\n",
      "2017-11-05T11:14:55.596603: step 2999, loss 0.303308, acc 0.875\n",
      "2017-11-05T11:14:59.624982: step 3000, loss 0.154638, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:15:02.178766: step 3000, loss 0.872838, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-05T11:15:07.629277: step 3001, loss 0.112453, acc 0.96875\n",
      "2017-11-05T11:15:11.644630: step 3002, loss 0.248009, acc 0.90625\n",
      "2017-11-05T11:15:15.593144: step 3003, loss 0.311641, acc 0.84375\n",
      "2017-11-05T11:15:19.529453: step 3004, loss 0.232268, acc 0.84375\n",
      "2017-11-05T11:15:23.509560: step 3005, loss 0.108874, acc 0.9375\n",
      "2017-11-05T11:15:27.482273: step 3006, loss 0.234762, acc 0.875\n",
      "2017-11-05T11:15:31.458486: step 3007, loss 0.438185, acc 0.875\n",
      "2017-11-05T11:15:35.450491: step 3008, loss 0.0995283, acc 0.9375\n",
      "2017-11-05T11:15:39.421266: step 3009, loss 0.200077, acc 0.9375\n",
      "2017-11-05T11:15:43.412626: step 3010, loss 0.119997, acc 0.96875\n",
      "2017-11-05T11:15:47.370184: step 3011, loss 0.130929, acc 0.96875\n",
      "2017-11-05T11:15:51.367614: step 3012, loss 0.172382, acc 0.84375\n",
      "2017-11-05T11:15:55.432888: step 3013, loss 0.22198, acc 0.875\n",
      "2017-11-05T11:15:59.372461: step 3014, loss 0.488941, acc 0.75\n",
      "2017-11-05T11:16:03.390124: step 3015, loss 0.276216, acc 0.875\n",
      "2017-11-05T11:16:07.477029: step 3016, loss 0.217679, acc 0.875\n",
      "2017-11-05T11:16:11.494770: step 3017, loss 0.182031, acc 0.84375\n",
      "2017-11-05T11:16:15.480491: step 3018, loss 0.05806, acc 1\n",
      "2017-11-05T11:16:19.482350: step 3019, loss 0.192416, acc 0.90625\n",
      "2017-11-05T11:16:23.481705: step 3020, loss 0.205668, acc 0.875\n",
      "2017-11-05T11:16:27.470950: step 3021, loss 0.280882, acc 0.875\n",
      "2017-11-05T11:16:31.490307: step 3022, loss 0.039258, acc 0.96875\n",
      "2017-11-05T11:16:35.803871: step 3023, loss 0.17393, acc 0.9375\n",
      "2017-11-05T11:16:38.399716: step 3024, loss 0.0881586, acc 0.95\n",
      "2017-11-05T11:16:42.470062: step 3025, loss 0.0674458, acc 0.96875\n",
      "2017-11-05T11:16:46.428403: step 3026, loss 0.0457058, acc 0.96875\n",
      "2017-11-05T11:16:50.396629: step 3027, loss 0.128042, acc 0.90625\n",
      "2017-11-05T11:16:54.366449: step 3028, loss 0.130032, acc 0.9375\n",
      "2017-11-05T11:16:58.388322: step 3029, loss 0.492431, acc 0.8125\n",
      "2017-11-05T11:17:02.420734: step 3030, loss 0.212053, acc 0.90625\n",
      "2017-11-05T11:17:06.351801: step 3031, loss 0.217717, acc 0.90625\n",
      "2017-11-05T11:17:10.301623: step 3032, loss 0.203122, acc 0.90625\n",
      "2017-11-05T11:17:14.326495: step 3033, loss 0.264956, acc 0.875\n",
      "2017-11-05T11:17:18.344702: step 3034, loss 0.0382164, acc 1\n",
      "2017-11-05T11:17:22.360488: step 3035, loss 0.178213, acc 0.90625\n",
      "2017-11-05T11:17:26.346854: step 3036, loss 0.175919, acc 0.90625\n",
      "2017-11-05T11:17:30.352422: step 3037, loss 0.358711, acc 0.8125\n",
      "2017-11-05T11:17:34.323742: step 3038, loss 0.150137, acc 0.9375\n",
      "2017-11-05T11:17:38.311634: step 3039, loss 0.151404, acc 0.9375\n",
      "2017-11-05T11:17:42.273164: step 3040, loss 0.0792678, acc 0.9375\n",
      "2017-11-05T11:17:46.291064: step 3041, loss 0.269484, acc 0.875\n",
      "2017-11-05T11:17:50.268277: step 3042, loss 0.0893775, acc 0.96875\n",
      "2017-11-05T11:17:54.258625: step 3043, loss 0.150086, acc 0.9375\n",
      "2017-11-05T11:17:58.312632: step 3044, loss 0.182012, acc 0.90625\n",
      "2017-11-05T11:18:02.342599: step 3045, loss 0.0494604, acc 0.96875\n",
      "2017-11-05T11:18:06.308919: step 3046, loss 0.200861, acc 0.90625\n",
      "2017-11-05T11:18:10.310157: step 3047, loss 0.191984, acc 0.90625\n",
      "2017-11-05T11:18:14.291484: step 3048, loss 0.0447583, acc 1\n",
      "2017-11-05T11:18:18.248806: step 3049, loss 0.134347, acc 0.9375\n",
      "2017-11-05T11:18:22.407151: step 3050, loss 0.250382, acc 0.90625\n",
      "2017-11-05T11:18:26.516070: step 3051, loss 0.320214, acc 0.84375\n",
      "2017-11-05T11:18:30.490911: step 3052, loss 0.27598, acc 0.875\n",
      "2017-11-05T11:18:34.644702: step 3053, loss 0.0557742, acc 1\n",
      "2017-11-05T11:18:38.764168: step 3054, loss 0.395319, acc 0.8125\n",
      "2017-11-05T11:18:42.881093: step 3055, loss 0.169949, acc 0.90625\n",
      "2017-11-05T11:18:46.881936: step 3056, loss 0.0724904, acc 0.9375\n",
      "2017-11-05T11:18:51.034887: step 3057, loss 0.347353, acc 0.875\n",
      "2017-11-05T11:18:55.092271: step 3058, loss 0.143669, acc 0.9375\n",
      "2017-11-05T11:18:59.284249: step 3059, loss 0.46123, acc 0.84375\n",
      "2017-11-05T11:19:01.945661: step 3060, loss 0.228763, acc 0.9\n",
      "2017-11-05T11:19:06.025060: step 3061, loss 0.0928026, acc 0.96875\n",
      "2017-11-05T11:19:10.277581: step 3062, loss 0.0853675, acc 0.96875\n",
      "2017-11-05T11:19:14.330961: step 3063, loss 0.25831, acc 0.90625\n",
      "2017-11-05T11:19:18.547457: step 3064, loss 0.0272181, acc 1\n",
      "2017-11-05T11:19:22.675319: step 3065, loss 0.127224, acc 0.875\n",
      "2017-11-05T11:19:26.796749: step 3066, loss 0.134794, acc 0.9375\n",
      "2017-11-05T11:19:30.879651: step 3067, loss 0.218318, acc 0.875\n",
      "2017-11-05T11:19:35.202220: step 3068, loss 0.0300449, acc 1\n",
      "2017-11-05T11:19:39.160032: step 3069, loss 0.256367, acc 0.84375\n",
      "2017-11-05T11:19:43.489109: step 3070, loss 0.0799618, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:19:47.640058: step 3071, loss 0.161036, acc 0.9375\n",
      "2017-11-05T11:19:51.593911: step 3072, loss 0.173969, acc 0.90625\n",
      "2017-11-05T11:19:55.629780: step 3073, loss 0.0610655, acc 0.96875\n",
      "2017-11-05T11:19:59.663550: step 3074, loss 0.117721, acc 0.96875\n",
      "2017-11-05T11:20:03.939749: step 3075, loss 0.189016, acc 0.875\n",
      "2017-11-05T11:20:07.970465: step 3076, loss 0.409581, acc 0.8125\n",
      "2017-11-05T11:20:12.052843: step 3077, loss 0.16816, acc 0.90625\n",
      "2017-11-05T11:20:16.009895: step 3078, loss 0.0730165, acc 0.96875\n",
      "2017-11-05T11:20:19.987238: step 3079, loss 0.211931, acc 0.9375\n",
      "2017-11-05T11:20:23.961890: step 3080, loss 0.186426, acc 0.875\n",
      "2017-11-05T11:20:27.971750: step 3081, loss 0.600625, acc 0.84375\n",
      "2017-11-05T11:20:31.921509: step 3082, loss 0.22816, acc 0.9375\n",
      "2017-11-05T11:20:36.060476: step 3083, loss 0.17035, acc 0.90625\n",
      "2017-11-05T11:20:40.051819: step 3084, loss 0.00389265, acc 1\n",
      "2017-11-05T11:20:44.072085: step 3085, loss 0.114854, acc 0.9375\n",
      "2017-11-05T11:20:48.124464: step 3086, loss 0.195213, acc 0.84375\n",
      "2017-11-05T11:20:52.118103: step 3087, loss 0.18658, acc 0.875\n",
      "2017-11-05T11:20:56.080819: step 3088, loss 0.0714424, acc 0.96875\n",
      "2017-11-05T11:21:00.102425: step 3089, loss 0.370994, acc 0.84375\n",
      "2017-11-05T11:21:04.148310: step 3090, loss 0.0802637, acc 0.96875\n",
      "2017-11-05T11:21:08.121263: step 3091, loss 0.210515, acc 0.875\n",
      "2017-11-05T11:21:12.111123: step 3092, loss 0.0930643, acc 0.96875\n",
      "2017-11-05T11:21:16.095952: step 3093, loss 0.155963, acc 0.9375\n",
      "2017-11-05T11:21:20.130890: step 3094, loss 0.325168, acc 0.8125\n",
      "2017-11-05T11:21:24.064214: step 3095, loss 0.222721, acc 0.875\n",
      "2017-11-05T11:21:26.671692: step 3096, loss 0.102968, acc 0.95\n",
      "2017-11-05T11:21:30.653546: step 3097, loss 0.205451, acc 0.875\n",
      "2017-11-05T11:21:34.712934: step 3098, loss 0.0999506, acc 0.9375\n",
      "2017-11-05T11:21:38.661710: step 3099, loss 0.0915365, acc 0.9375\n",
      "2017-11-05T11:21:42.630522: step 3100, loss 0.133014, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:21:45.242767: step 3100, loss 0.919088, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-05T11:21:50.536688: step 3101, loss 0.25941, acc 0.875\n",
      "2017-11-05T11:21:54.599573: step 3102, loss 0.330522, acc 0.90625\n",
      "2017-11-05T11:21:58.567837: step 3103, loss 0.28289, acc 0.875\n",
      "2017-11-05T11:22:02.601092: step 3104, loss 0.0607226, acc 0.96875\n",
      "2017-11-05T11:22:06.591759: step 3105, loss 0.0747094, acc 0.96875\n",
      "2017-11-05T11:22:10.618372: step 3106, loss 0.208759, acc 0.90625\n",
      "2017-11-05T11:22:14.678696: step 3107, loss 0.140106, acc 0.9375\n",
      "2017-11-05T11:22:18.716318: step 3108, loss 0.137794, acc 0.9375\n",
      "2017-11-05T11:22:22.654945: step 3109, loss 0.0952968, acc 0.9375\n",
      "2017-11-05T11:22:26.655112: step 3110, loss 0.315693, acc 0.84375\n",
      "2017-11-05T11:22:30.699987: step 3111, loss 0.248988, acc 0.84375\n",
      "2017-11-05T11:22:34.854212: step 3112, loss 0.166289, acc 0.9375\n",
      "2017-11-05T11:22:38.852561: step 3113, loss 0.0809396, acc 0.96875\n",
      "2017-11-05T11:22:42.872304: step 3114, loss 0.178062, acc 0.84375\n",
      "2017-11-05T11:22:46.850661: step 3115, loss 0.196065, acc 0.84375\n",
      "2017-11-05T11:22:50.869815: step 3116, loss 0.0848023, acc 0.96875\n",
      "2017-11-05T11:22:54.837422: step 3117, loss 0.0483053, acc 1\n",
      "2017-11-05T11:22:58.899318: step 3118, loss 0.25011, acc 0.90625\n",
      "2017-11-05T11:23:02.941690: step 3119, loss 0.079868, acc 0.9375\n",
      "2017-11-05T11:23:06.990067: step 3120, loss 0.230439, acc 0.875\n",
      "2017-11-05T11:23:11.025935: step 3121, loss 0.224844, acc 0.875\n",
      "2017-11-05T11:23:15.097327: step 3122, loss 0.247515, acc 0.90625\n",
      "2017-11-05T11:23:19.174725: step 3123, loss 0.0356542, acc 1\n",
      "2017-11-05T11:23:23.374208: step 3124, loss 0.0980607, acc 0.9375\n",
      "2017-11-05T11:23:27.611629: step 3125, loss 0.190418, acc 0.9375\n",
      "2017-11-05T11:23:31.609969: step 3126, loss 0.282474, acc 0.84375\n",
      "2017-11-05T11:23:35.579185: step 3127, loss 0.181404, acc 0.90625\n",
      "2017-11-05T11:23:39.720204: step 3128, loss 0.243337, acc 0.875\n",
      "2017-11-05T11:23:43.704537: step 3129, loss 0.0478978, acc 1\n",
      "2017-11-05T11:23:47.749961: step 3130, loss 0.134013, acc 0.9375\n",
      "2017-11-05T11:23:51.749302: step 3131, loss 0.246672, acc 0.875\n",
      "2017-11-05T11:23:54.347663: step 3132, loss 0.192187, acc 0.95\n",
      "2017-11-05T11:23:58.388407: step 3133, loss 0.258613, acc 0.9375\n",
      "2017-11-05T11:24:02.373239: step 3134, loss 0.176862, acc 0.875\n",
      "2017-11-05T11:24:06.447029: step 3135, loss 0.267219, acc 0.84375\n",
      "2017-11-05T11:24:10.467779: step 3136, loss 0.192915, acc 0.875\n",
      "2017-11-05T11:24:14.478983: step 3137, loss 0.164697, acc 0.90625\n",
      "2017-11-05T11:24:18.461813: step 3138, loss 0.0157809, acc 1\n",
      "2017-11-05T11:24:22.466826: step 3139, loss 0.109132, acc 0.9375\n",
      "2017-11-05T11:24:26.457442: step 3140, loss 0.148058, acc 0.90625\n",
      "2017-11-05T11:24:30.491242: step 3141, loss 0.141443, acc 0.9375\n",
      "2017-11-05T11:24:34.581158: step 3142, loss 0.16747, acc 0.96875\n",
      "2017-11-05T11:24:38.654052: step 3143, loss 0.0469896, acc 0.96875\n",
      "2017-11-05T11:24:42.655412: step 3144, loss 0.0738647, acc 0.96875\n",
      "2017-11-05T11:24:46.644637: step 3145, loss 0.253442, acc 0.9375\n",
      "2017-11-05T11:24:50.663509: step 3146, loss 0.194697, acc 0.9375\n",
      "2017-11-05T11:24:54.687655: step 3147, loss 0.138757, acc 0.9375\n",
      "2017-11-05T11:24:58.713538: step 3148, loss 0.344264, acc 0.90625\n",
      "2017-11-05T11:25:02.810514: step 3149, loss 0.142144, acc 0.9375\n",
      "2017-11-05T11:25:06.872900: step 3150, loss 0.0811753, acc 0.9375\n",
      "2017-11-05T11:25:10.901264: step 3151, loss 0.0970323, acc 0.9375\n",
      "2017-11-05T11:25:14.971155: step 3152, loss 0.101351, acc 0.96875\n",
      "2017-11-05T11:25:18.991513: step 3153, loss 0.0814003, acc 0.9375\n",
      "2017-11-05T11:25:22.971340: step 3154, loss 0.177589, acc 0.90625\n",
      "2017-11-05T11:25:26.926650: step 3155, loss 0.332269, acc 0.84375\n",
      "2017-11-05T11:25:30.943197: step 3156, loss 0.374527, acc 0.90625\n",
      "2017-11-05T11:25:34.987716: step 3157, loss 0.0703666, acc 0.96875\n",
      "2017-11-05T11:25:38.983088: step 3158, loss 0.122391, acc 0.9375\n",
      "2017-11-05T11:25:42.994705: step 3159, loss 0.0564963, acc 0.96875\n",
      "2017-11-05T11:25:47.022261: step 3160, loss 0.252579, acc 0.84375\n",
      "2017-11-05T11:25:51.045929: step 3161, loss 0.201185, acc 0.875\n",
      "2017-11-05T11:25:55.042292: step 3162, loss 0.252701, acc 0.9375\n",
      "2017-11-05T11:25:59.087667: step 3163, loss 0.16715, acc 0.875\n",
      "2017-11-05T11:26:03.144530: step 3164, loss 0.28825, acc 0.8125\n",
      "2017-11-05T11:26:07.193593: step 3165, loss 0.134789, acc 0.9375\n",
      "2017-11-05T11:26:11.257120: step 3166, loss 0.307531, acc 0.8125\n",
      "2017-11-05T11:26:15.234947: step 3167, loss 0.409487, acc 0.84375\n",
      "2017-11-05T11:26:17.828985: step 3168, loss 0.294398, acc 0.9\n",
      "2017-11-05T11:26:21.861013: step 3169, loss 0.168033, acc 0.9375\n",
      "2017-11-05T11:26:25.883871: step 3170, loss 0.132917, acc 0.9375\n",
      "2017-11-05T11:26:29.864459: step 3171, loss 0.114531, acc 0.96875\n",
      "2017-11-05T11:26:33.936579: step 3172, loss 0.108498, acc 0.96875\n",
      "2017-11-05T11:26:38.005998: step 3173, loss 0.408323, acc 0.875\n",
      "2017-11-05T11:26:41.992483: step 3174, loss 0.178448, acc 0.9375\n",
      "2017-11-05T11:26:46.018761: step 3175, loss 0.408135, acc 0.875\n",
      "2017-11-05T11:26:50.085658: step 3176, loss 0.397228, acc 0.84375\n",
      "2017-11-05T11:26:54.102721: step 3177, loss 0.314039, acc 0.875\n",
      "2017-11-05T11:26:58.062448: step 3178, loss 0.29385, acc 0.84375\n",
      "2017-11-05T11:27:02.121219: step 3179, loss 0.264605, acc 0.875\n",
      "2017-11-05T11:27:06.142479: step 3180, loss 0.037512, acc 1\n",
      "2017-11-05T11:27:10.170841: step 3181, loss 0.310414, acc 0.9375\n",
      "2017-11-05T11:27:14.165202: step 3182, loss 0.160967, acc 0.9375\n",
      "2017-11-05T11:27:18.176343: step 3183, loss 0.258854, acc 0.9375\n",
      "2017-11-05T11:27:22.216508: step 3184, loss 0.735826, acc 0.8125\n",
      "2017-11-05T11:27:26.269217: step 3185, loss 0.199333, acc 0.9375\n",
      "2017-11-05T11:27:30.265193: step 3186, loss 0.13085, acc 0.9375\n",
      "2017-11-05T11:27:34.200488: step 3187, loss 0.0952421, acc 0.9375\n",
      "2017-11-05T11:27:38.189060: step 3188, loss 0.0469931, acc 1\n",
      "2017-11-05T11:27:42.178183: step 3189, loss 0.12809, acc 0.9375\n",
      "2017-11-05T11:27:46.215272: step 3190, loss 0.115226, acc 0.9375\n",
      "2017-11-05T11:27:50.232298: step 3191, loss 0.485567, acc 0.90625\n",
      "2017-11-05T11:27:54.292132: step 3192, loss 0.302687, acc 0.9375\n",
      "2017-11-05T11:27:58.309297: step 3193, loss 0.323298, acc 0.90625\n",
      "2017-11-05T11:28:02.357564: step 3194, loss 0.318071, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:28:06.326384: step 3195, loss 0.556397, acc 0.84375\n",
      "2017-11-05T11:28:10.401379: step 3196, loss 0.44115, acc 0.875\n",
      "2017-11-05T11:28:14.439678: step 3197, loss 0.117012, acc 0.96875\n",
      "2017-11-05T11:28:18.366012: step 3198, loss 0.159855, acc 0.96875\n",
      "2017-11-05T11:28:22.462178: step 3199, loss 0.15198, acc 0.90625\n",
      "2017-11-05T11:28:26.663163: step 3200, loss 0.0512689, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:28:29.250508: step 3200, loss 0.776328, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-05T11:28:34.572881: step 3201, loss 0.251365, acc 0.875\n",
      "2017-11-05T11:28:38.513784: step 3202, loss 0.0731353, acc 1\n",
      "2017-11-05T11:28:42.515649: step 3203, loss 0.330436, acc 0.8125\n",
      "2017-11-05T11:28:45.058980: step 3204, loss 0.121175, acc 0.95\n",
      "2017-11-05T11:28:49.072342: step 3205, loss 0.0430557, acc 1\n",
      "2017-11-05T11:28:53.062094: step 3206, loss 0.0906209, acc 0.96875\n",
      "2017-11-05T11:28:57.086550: step 3207, loss 0.218847, acc 0.90625\n",
      "2017-11-05T11:29:01.124808: step 3208, loss 0.144888, acc 0.96875\n",
      "2017-11-05T11:29:05.106651: step 3209, loss 0.0832386, acc 0.9375\n",
      "2017-11-05T11:29:09.115522: step 3210, loss 0.3042, acc 0.84375\n",
      "2017-11-05T11:29:13.165659: step 3211, loss 0.180192, acc 0.9375\n",
      "2017-11-05T11:29:17.234050: step 3212, loss 0.306931, acc 0.875\n",
      "2017-11-05T11:29:21.232576: step 3213, loss 0.312066, acc 0.90625\n",
      "2017-11-05T11:29:25.204286: step 3214, loss 0.174591, acc 0.9375\n",
      "2017-11-05T11:29:29.226547: step 3215, loss 0.157074, acc 0.90625\n",
      "2017-11-05T11:29:33.231425: step 3216, loss 0.0975337, acc 0.9375\n",
      "2017-11-05T11:29:37.231261: step 3217, loss 0.0554333, acc 0.96875\n",
      "2017-11-05T11:29:41.239879: step 3218, loss 0.128243, acc 0.9375\n",
      "2017-11-05T11:29:45.259152: step 3219, loss 0.124942, acc 0.9375\n",
      "2017-11-05T11:29:49.316676: step 3220, loss 0.259526, acc 0.84375\n",
      "2017-11-05T11:29:53.317936: step 3221, loss 0.176449, acc 0.90625\n",
      "2017-11-05T11:29:57.274261: step 3222, loss 0.110196, acc 0.90625\n",
      "2017-11-05T11:30:01.582321: step 3223, loss 0.256943, acc 0.875\n",
      "2017-11-05T11:30:05.646723: step 3224, loss 0.0266609, acc 1\n",
      "2017-11-05T11:30:09.633555: step 3225, loss 0.10317, acc 0.96875\n",
      "2017-11-05T11:30:13.666338: step 3226, loss 0.198481, acc 0.90625\n",
      "2017-11-05T11:30:17.697132: step 3227, loss 0.0381172, acc 1\n",
      "2017-11-05T11:30:21.643406: step 3228, loss 0.323317, acc 0.8125\n",
      "2017-11-05T11:30:25.596560: step 3229, loss 0.160523, acc 0.875\n",
      "2017-11-05T11:30:29.586911: step 3230, loss 0.0724521, acc 0.96875\n",
      "2017-11-05T11:30:33.710257: step 3231, loss 0.126964, acc 0.96875\n",
      "2017-11-05T11:30:37.739861: step 3232, loss 0.336708, acc 0.8125\n",
      "2017-11-05T11:30:41.751712: step 3233, loss 0.147181, acc 0.875\n",
      "2017-11-05T11:30:45.749554: step 3234, loss 0.26215, acc 0.875\n",
      "2017-11-05T11:30:49.744279: step 3235, loss 0.163798, acc 0.9375\n",
      "2017-11-05T11:30:53.747123: step 3236, loss 0.207513, acc 0.875\n",
      "2017-11-05T11:30:57.789081: step 3237, loss 0.187431, acc 0.90625\n",
      "2017-11-05T11:31:01.774798: step 3238, loss 0.191179, acc 0.90625\n",
      "2017-11-05T11:31:05.756141: step 3239, loss 0.301473, acc 0.875\n",
      "2017-11-05T11:31:08.357992: step 3240, loss 0.341364, acc 0.85\n",
      "2017-11-05T11:31:12.409794: step 3241, loss 0.304563, acc 0.8125\n",
      "2017-11-05T11:31:16.474706: step 3242, loss 0.181086, acc 0.9375\n",
      "2017-11-05T11:31:20.474548: step 3243, loss 0.129163, acc 0.9375\n",
      "2017-11-05T11:31:24.543644: step 3244, loss 0.155225, acc 0.875\n",
      "2017-11-05T11:31:28.556496: step 3245, loss 0.165561, acc 0.90625\n",
      "2017-11-05T11:31:32.539478: step 3246, loss 0.0863605, acc 0.96875\n",
      "2017-11-05T11:31:36.613899: step 3247, loss 0.191007, acc 0.96875\n",
      "2017-11-05T11:31:40.610128: step 3248, loss 0.117382, acc 0.9375\n",
      "2017-11-05T11:31:44.624360: step 3249, loss 0.182077, acc 0.9375\n",
      "2017-11-05T11:31:48.683763: step 3250, loss 0.141034, acc 0.9375\n",
      "2017-11-05T11:31:52.690080: step 3251, loss 0.207598, acc 0.9375\n",
      "2017-11-05T11:31:56.869168: step 3252, loss 0.463655, acc 0.84375\n",
      "2017-11-05T11:32:00.831516: step 3253, loss 0.0104842, acc 1\n",
      "2017-11-05T11:32:04.824883: step 3254, loss 0.16522, acc 0.9375\n",
      "2017-11-05T11:32:08.828625: step 3255, loss 0.425566, acc 0.84375\n",
      "2017-11-05T11:32:12.826466: step 3256, loss 0.254358, acc 0.875\n",
      "2017-11-05T11:32:16.963405: step 3257, loss 0.22495, acc 0.90625\n",
      "2017-11-05T11:32:20.976023: step 3258, loss 0.156608, acc 0.90625\n",
      "2017-11-05T11:32:24.998269: step 3259, loss 0.0770365, acc 1\n",
      "2017-11-05T11:32:29.031751: step 3260, loss 0.0924391, acc 0.96875\n",
      "2017-11-05T11:32:33.085785: step 3261, loss 0.096924, acc 0.90625\n",
      "2017-11-05T11:32:37.231731: step 3262, loss 0.270118, acc 0.875\n",
      "2017-11-05T11:32:41.178108: step 3263, loss 0.132257, acc 0.9375\n",
      "2017-11-05T11:32:45.207986: step 3264, loss 0.316132, acc 0.8125\n",
      "2017-11-05T11:32:49.264368: step 3265, loss 0.142629, acc 0.9375\n",
      "2017-11-05T11:32:53.344655: step 3266, loss 0.195372, acc 0.90625\n",
      "2017-11-05T11:32:57.356005: step 3267, loss 0.173515, acc 0.9375\n",
      "2017-11-05T11:33:01.543017: step 3268, loss 0.223943, acc 0.875\n",
      "2017-11-05T11:33:05.561950: step 3269, loss 0.332749, acc 0.875\n",
      "2017-11-05T11:33:09.621095: step 3270, loss 0.250612, acc 0.875\n",
      "2017-11-05T11:33:13.672205: step 3271, loss 0.167324, acc 0.90625\n",
      "2017-11-05T11:33:17.743216: step 3272, loss 0.0990636, acc 0.96875\n",
      "2017-11-05T11:33:21.879154: step 3273, loss 0.217982, acc 0.875\n",
      "2017-11-05T11:33:26.248366: step 3274, loss 0.308497, acc 0.90625\n",
      "2017-11-05T11:33:30.237896: step 3275, loss 0.101836, acc 0.96875\n",
      "2017-11-05T11:33:32.839745: step 3276, loss 0.119752, acc 0.95\n",
      "2017-11-05T11:33:36.878614: step 3277, loss 0.0549652, acc 0.96875\n",
      "2017-11-05T11:33:40.909479: step 3278, loss 0.192055, acc 0.90625\n",
      "2017-11-05T11:33:44.920828: step 3279, loss 0.0289118, acc 1\n",
      "2017-11-05T11:33:48.933567: step 3280, loss 0.287029, acc 0.875\n",
      "2017-11-05T11:33:52.972993: step 3281, loss 0.158802, acc 0.9375\n",
      "2017-11-05T11:33:57.033440: step 3282, loss 0.322733, acc 0.84375\n",
      "2017-11-05T11:34:01.023276: step 3283, loss 0.309804, acc 0.875\n",
      "2017-11-05T11:34:05.003103: step 3284, loss 0.232522, acc 0.875\n",
      "2017-11-05T11:34:08.996845: step 3285, loss 0.444449, acc 0.84375\n",
      "2017-11-05T11:34:13.018130: step 3286, loss 0.266346, acc 0.90625\n",
      "2017-11-05T11:34:17.041486: step 3287, loss 0.144696, acc 0.9375\n",
      "2017-11-05T11:34:21.066742: step 3288, loss 0.225351, acc 0.90625\n",
      "2017-11-05T11:34:25.021052: step 3289, loss 0.148982, acc 0.875\n",
      "2017-11-05T11:34:29.075231: step 3290, loss 0.265235, acc 0.875\n",
      "2017-11-05T11:34:33.117818: step 3291, loss 0.0950876, acc 0.96875\n",
      "2017-11-05T11:34:37.210242: step 3292, loss 0.0858321, acc 0.96875\n",
      "2017-11-05T11:34:41.225998: step 3293, loss 0.201027, acc 0.90625\n",
      "2017-11-05T11:34:45.230344: step 3294, loss 0.404772, acc 0.84375\n",
      "2017-11-05T11:34:49.209895: step 3295, loss 0.241377, acc 0.90625\n",
      "2017-11-05T11:34:53.213741: step 3296, loss 0.14602, acc 0.875\n",
      "2017-11-05T11:34:57.253610: step 3297, loss 0.432945, acc 0.78125\n",
      "2017-11-05T11:35:01.315145: step 3298, loss 0.150905, acc 0.9375\n",
      "2017-11-05T11:35:05.288894: step 3299, loss 0.0828371, acc 0.9375\n",
      "2017-11-05T11:35:09.305262: step 3300, loss 0.230682, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:35:12.024536: step 3300, loss 0.681115, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-05T11:35:17.113277: step 3301, loss 0.428039, acc 0.8125\n",
      "2017-11-05T11:35:21.202334: step 3302, loss 0.247579, acc 0.875\n",
      "2017-11-05T11:35:25.197699: step 3303, loss 0.22459, acc 0.90625\n",
      "2017-11-05T11:35:29.201543: step 3304, loss 0.142368, acc 0.9375\n",
      "2017-11-05T11:35:33.175367: step 3305, loss 0.183843, acc 0.875\n",
      "2017-11-05T11:35:37.153073: step 3306, loss 0.360831, acc 0.84375\n",
      "2017-11-05T11:35:41.201316: step 3307, loss 0.421104, acc 0.78125\n",
      "2017-11-05T11:35:45.207647: step 3308, loss 0.189311, acc 0.875\n",
      "2017-11-05T11:35:49.300587: step 3309, loss 0.147467, acc 0.9375\n",
      "2017-11-05T11:35:53.266793: step 3310, loss 0.251924, acc 0.9375\n",
      "2017-11-05T11:35:57.288718: step 3311, loss 0.135487, acc 0.96875\n",
      "2017-11-05T11:35:59.932523: step 3312, loss 0.240804, acc 0.9\n",
      "2017-11-05T11:36:03.931898: step 3313, loss 0.119124, acc 0.9375\n",
      "2017-11-05T11:36:07.929379: step 3314, loss 0.0901153, acc 0.96875\n",
      "2017-11-05T11:36:11.972208: step 3315, loss 0.12097, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:36:16.030014: step 3316, loss 0.0528547, acc 0.96875\n",
      "2017-11-05T11:36:20.037761: step 3317, loss 0.0579731, acc 0.96875\n",
      "2017-11-05T11:36:24.071836: step 3318, loss 0.0907774, acc 0.96875\n",
      "2017-11-05T11:36:28.068490: step 3319, loss 0.0657907, acc 0.96875\n",
      "2017-11-05T11:36:32.072849: step 3320, loss 0.203717, acc 0.875\n",
      "2017-11-05T11:36:36.220807: step 3321, loss 0.183463, acc 0.90625\n",
      "2017-11-05T11:36:40.242665: step 3322, loss 0.288176, acc 0.90625\n",
      "2017-11-05T11:36:44.255906: step 3323, loss 0.0535661, acc 0.96875\n",
      "2017-11-05T11:36:48.282509: step 3324, loss 0.258645, acc 0.84375\n",
      "2017-11-05T11:36:52.281865: step 3325, loss 0.164994, acc 0.9375\n",
      "2017-11-05T11:36:56.404480: step 3326, loss 0.0899322, acc 0.9375\n",
      "2017-11-05T11:37:00.402270: step 3327, loss 0.174553, acc 0.90625\n",
      "2017-11-05T11:37:04.419077: step 3328, loss 0.185083, acc 0.9375\n",
      "2017-11-05T11:37:08.510495: step 3329, loss 0.10799, acc 0.90625\n",
      "2017-11-05T11:37:12.493827: step 3330, loss 0.0626259, acc 0.96875\n",
      "2017-11-05T11:37:16.456668: step 3331, loss 0.204622, acc 0.90625\n",
      "2017-11-05T11:37:20.476541: step 3332, loss 0.205603, acc 0.9375\n",
      "2017-11-05T11:37:24.554438: step 3333, loss 0.215095, acc 0.90625\n",
      "2017-11-05T11:37:28.585494: step 3334, loss 0.0537297, acc 0.96875\n",
      "2017-11-05T11:37:32.543807: step 3335, loss 0.192014, acc 0.90625\n",
      "2017-11-05T11:37:36.549707: step 3336, loss 0.106217, acc 0.9375\n",
      "2017-11-05T11:37:40.581264: step 3337, loss 0.286153, acc 0.84375\n",
      "2017-11-05T11:37:44.567780: step 3338, loss 0.125717, acc 0.9375\n",
      "2017-11-05T11:37:48.651193: step 3339, loss 0.171995, acc 0.90625\n",
      "2017-11-05T11:37:52.620043: step 3340, loss 0.167226, acc 0.96875\n",
      "2017-11-05T11:37:56.690628: step 3341, loss 0.264156, acc 0.90625\n",
      "2017-11-05T11:38:00.703674: step 3342, loss 0.381931, acc 0.8125\n",
      "2017-11-05T11:38:04.754439: step 3343, loss 0.213421, acc 0.90625\n",
      "2017-11-05T11:38:08.845847: step 3344, loss 0.087067, acc 0.96875\n",
      "2017-11-05T11:38:12.872708: step 3345, loss 0.10107, acc 0.90625\n",
      "2017-11-05T11:38:16.847043: step 3346, loss 0.475514, acc 0.78125\n",
      "2017-11-05T11:38:20.861098: step 3347, loss 0.115851, acc 0.96875\n",
      "2017-11-05T11:38:23.690495: step 3348, loss 0.180636, acc 0.95\n",
      "2017-11-05T11:38:27.935012: step 3349, loss 0.0566493, acc 1\n",
      "2017-11-05T11:38:31.952367: step 3350, loss 0.0247256, acc 1\n",
      "2017-11-05T11:38:36.090807: step 3351, loss 0.13094, acc 0.90625\n",
      "2017-11-05T11:38:40.161699: step 3352, loss 0.066287, acc 0.96875\n",
      "2017-11-05T11:38:44.232592: step 3353, loss 0.267855, acc 0.84375\n",
      "2017-11-05T11:38:48.296980: step 3354, loss 0.219253, acc 0.90625\n",
      "2017-11-05T11:38:52.371374: step 3355, loss 0.169416, acc 0.90625\n",
      "2017-11-05T11:38:56.307671: step 3356, loss 0.211111, acc 0.90625\n",
      "2017-11-05T11:39:00.474632: step 3357, loss 0.168516, acc 0.9375\n",
      "2017-11-05T11:39:04.481979: step 3358, loss 0.212729, acc 0.90625\n",
      "2017-11-05T11:39:08.521850: step 3359, loss 0.0975023, acc 0.9375\n",
      "2017-11-05T11:39:12.448140: step 3360, loss 0.0875151, acc 0.96875\n",
      "2017-11-05T11:39:16.426466: step 3361, loss 0.162856, acc 0.9375\n",
      "2017-11-05T11:39:20.375537: step 3362, loss 0.378939, acc 0.8125\n",
      "2017-11-05T11:39:24.405901: step 3363, loss 0.0329033, acc 1\n",
      "2017-11-05T11:39:28.360238: step 3364, loss 0.0306685, acc 1\n",
      "2017-11-05T11:39:32.275441: step 3365, loss 0.110106, acc 0.90625\n",
      "2017-11-05T11:39:36.194788: step 3366, loss 0.124082, acc 0.9375\n",
      "2017-11-05T11:39:40.099746: step 3367, loss 0.155944, acc 0.9375\n",
      "2017-11-05T11:39:43.984564: step 3368, loss 0.0599316, acc 0.96875\n",
      "2017-11-05T11:39:47.928036: step 3369, loss 0.261017, acc 0.84375\n",
      "2017-11-05T11:39:51.844901: step 3370, loss 0.320194, acc 0.875\n",
      "2017-11-05T11:39:55.768057: step 3371, loss 0.207277, acc 0.875\n",
      "2017-11-05T11:39:59.784316: step 3372, loss 0.152177, acc 0.875\n",
      "2017-11-05T11:40:03.942962: step 3373, loss 0.110991, acc 0.9375\n",
      "2017-11-05T11:40:07.869150: step 3374, loss 0.0762621, acc 0.96875\n",
      "2017-11-05T11:40:11.826999: step 3375, loss 0.163805, acc 0.90625\n",
      "2017-11-05T11:40:15.723655: step 3376, loss 0.181795, acc 0.875\n",
      "2017-11-05T11:40:19.609967: step 3377, loss 0.128815, acc 0.9375\n",
      "2017-11-05T11:40:23.513032: step 3378, loss 0.513697, acc 0.8125\n",
      "2017-11-05T11:40:27.497363: step 3379, loss 0.241358, acc 0.84375\n",
      "2017-11-05T11:40:31.390180: step 3380, loss 0.0571277, acc 0.96875\n",
      "2017-11-05T11:40:35.445667: step 3381, loss 0.20698, acc 0.9375\n",
      "2017-11-05T11:40:39.357372: step 3382, loss 0.215909, acc 0.90625\n",
      "2017-11-05T11:40:43.255635: step 3383, loss 0.202007, acc 0.90625\n",
      "2017-11-05T11:40:45.760414: step 3384, loss 0.264724, acc 0.85\n",
      "2017-11-05T11:40:49.678660: step 3385, loss 0.137926, acc 0.90625\n",
      "2017-11-05T11:40:53.594976: step 3386, loss 0.0529986, acc 0.9375\n",
      "2017-11-05T11:40:57.524278: step 3387, loss 0.436601, acc 0.78125\n",
      "2017-11-05T11:41:01.453275: step 3388, loss 0.123238, acc 0.90625\n",
      "2017-11-05T11:41:05.363505: step 3389, loss 0.298824, acc 0.84375\n",
      "2017-11-05T11:41:09.425792: step 3390, loss 0.0737379, acc 0.96875\n",
      "2017-11-05T11:41:14.343436: step 3391, loss 0.375404, acc 0.8125\n",
      "2017-11-05T11:41:18.485426: step 3392, loss 0.243176, acc 0.84375\n",
      "2017-11-05T11:41:22.558832: step 3393, loss 0.0304859, acc 1\n",
      "2017-11-05T11:41:26.465499: step 3394, loss 0.17969, acc 0.90625\n",
      "2017-11-05T11:41:30.415617: step 3395, loss 0.100002, acc 0.96875\n",
      "2017-11-05T11:41:34.363592: step 3396, loss 0.0204867, acc 1\n",
      "2017-11-05T11:41:38.327688: step 3397, loss 0.208119, acc 0.875\n",
      "2017-11-05T11:41:42.263134: step 3398, loss 0.336356, acc 0.90625\n",
      "2017-11-05T11:41:46.204504: step 3399, loss 0.0954142, acc 0.9375\n",
      "2017-11-05T11:41:50.269797: step 3400, loss 0.186498, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:41:52.838526: step 3400, loss 0.928557, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-05T11:41:58.233897: step 3401, loss 0.438802, acc 0.78125\n",
      "2017-11-05T11:42:02.240243: step 3402, loss 0.090021, acc 0.96875\n",
      "2017-11-05T11:42:06.279113: step 3403, loss 0.323154, acc 0.8125\n",
      "2017-11-05T11:42:10.218458: step 3404, loss 0.0873765, acc 0.9375\n",
      "2017-11-05T11:42:14.159396: step 3405, loss 0.298771, acc 0.78125\n",
      "2017-11-05T11:42:18.219281: step 3406, loss 0.452658, acc 0.8125\n",
      "2017-11-05T11:42:22.172590: step 3407, loss 0.151791, acc 0.90625\n",
      "2017-11-05T11:42:26.129748: step 3408, loss 0.0831359, acc 0.96875\n",
      "2017-11-05T11:42:30.082802: step 3409, loss 0.112478, acc 0.9375\n",
      "2017-11-05T11:42:34.187866: step 3410, loss 0.191792, acc 0.875\n",
      "2017-11-05T11:42:38.144995: step 3411, loss 0.100979, acc 0.9375\n",
      "2017-11-05T11:42:42.094957: step 3412, loss 0.289298, acc 0.84375\n",
      "2017-11-05T11:42:46.047489: step 3413, loss 0.103233, acc 0.9375\n",
      "2017-11-05T11:42:50.018077: step 3414, loss 0.185595, acc 0.9375\n",
      "2017-11-05T11:42:53.969787: step 3415, loss 0.248267, acc 0.90625\n",
      "2017-11-05T11:42:57.897671: step 3416, loss 0.481224, acc 0.8125\n",
      "2017-11-05T11:43:01.832297: step 3417, loss 0.0820809, acc 0.9375\n",
      "2017-11-05T11:43:05.816138: step 3418, loss 0.0999654, acc 0.9375\n",
      "2017-11-05T11:43:09.760954: step 3419, loss 0.061853, acc 0.9375\n",
      "2017-11-05T11:43:12.288750: step 3420, loss 0.210856, acc 0.8\n",
      "2017-11-05T11:43:16.242915: step 3421, loss 0.216374, acc 0.84375\n",
      "2017-11-05T11:43:20.198382: step 3422, loss 0.0597886, acc 0.96875\n",
      "2017-11-05T11:43:24.327088: step 3423, loss 0.156927, acc 0.90625\n",
      "2017-11-05T11:43:28.479752: step 3424, loss 0.114341, acc 0.9375\n",
      "2017-11-05T11:43:32.485984: step 3425, loss 0.0444463, acc 0.96875\n",
      "2017-11-05T11:43:36.448708: step 3426, loss 0.167859, acc 0.90625\n",
      "2017-11-05T11:43:40.437042: step 3427, loss 0.21386, acc 0.90625\n",
      "2017-11-05T11:43:44.385849: step 3428, loss 0.178299, acc 0.90625\n",
      "2017-11-05T11:43:48.363674: step 3429, loss 0.339018, acc 0.8125\n",
      "2017-11-05T11:43:52.375054: step 3430, loss 0.132978, acc 0.96875\n",
      "2017-11-05T11:43:56.293033: step 3431, loss 0.11659, acc 0.9375\n",
      "2017-11-05T11:44:00.267184: step 3432, loss 0.14988, acc 0.90625\n",
      "2017-11-05T11:44:04.315580: step 3433, loss 0.100071, acc 0.96875\n",
      "2017-11-05T11:44:08.247025: step 3434, loss 0.0509152, acc 0.96875\n",
      "2017-11-05T11:44:12.173702: step 3435, loss 0.19427, acc 0.84375\n",
      "2017-11-05T11:44:16.132015: step 3436, loss 0.0555117, acc 1\n",
      "2017-11-05T11:44:20.065605: step 3437, loss 0.0821086, acc 0.9375\n",
      "2017-11-05T11:44:24.074915: step 3438, loss 0.1451, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:44:28.036912: step 3439, loss 0.263534, acc 0.90625\n",
      "2017-11-05T11:44:31.950193: step 3440, loss 0.230124, acc 0.90625\n",
      "2017-11-05T11:44:36.062462: step 3441, loss 0.205057, acc 0.90625\n",
      "2017-11-05T11:44:39.998270: step 3442, loss 0.0897871, acc 0.9375\n",
      "2017-11-05T11:44:43.948600: step 3443, loss 0.527989, acc 0.8125\n",
      "2017-11-05T11:44:47.871888: step 3444, loss 0.0534219, acc 0.96875\n",
      "2017-11-05T11:44:51.865268: step 3445, loss 0.0882241, acc 0.96875\n",
      "2017-11-05T11:44:55.777564: step 3446, loss 0.12058, acc 0.9375\n",
      "2017-11-05T11:44:59.798922: step 3447, loss 0.189063, acc 0.90625\n",
      "2017-11-05T11:45:03.753746: step 3448, loss 0.260064, acc 0.8125\n",
      "2017-11-05T11:45:07.702551: step 3449, loss 0.313054, acc 0.875\n",
      "2017-11-05T11:45:11.659776: step 3450, loss 0.210582, acc 0.90625\n",
      "2017-11-05T11:45:15.625594: step 3451, loss 0.243, acc 0.9375\n",
      "2017-11-05T11:45:19.619860: step 3452, loss 0.365605, acc 0.84375\n",
      "2017-11-05T11:45:23.572169: step 3453, loss 0.245887, acc 0.84375\n",
      "2017-11-05T11:45:27.533999: step 3454, loss 0.0721979, acc 0.96875\n",
      "2017-11-05T11:45:31.495839: step 3455, loss 0.282182, acc 0.8125\n",
      "2017-11-05T11:45:33.999071: step 3456, loss 0.121989, acc 0.9\n",
      "2017-11-05T11:45:37.978927: step 3457, loss 0.0341721, acc 0.96875\n",
      "2017-11-05T11:45:41.978321: step 3458, loss 0.252892, acc 0.90625\n",
      "2017-11-05T11:45:45.975716: step 3459, loss 0.18834, acc 0.90625\n",
      "2017-11-05T11:45:49.895860: step 3460, loss 0.17189, acc 0.90625\n",
      "2017-11-05T11:45:53.819228: step 3461, loss 0.2067, acc 0.875\n",
      "2017-11-05T11:45:57.772073: step 3462, loss 0.186787, acc 0.90625\n",
      "2017-11-05T11:46:01.690061: step 3463, loss 0.160371, acc 0.9375\n",
      "2017-11-05T11:46:05.844711: step 3464, loss 0.11833, acc 0.9375\n",
      "2017-11-05T11:46:09.797648: step 3465, loss 0.302057, acc 0.90625\n",
      "2017-11-05T11:46:13.748646: step 3466, loss 0.180582, acc 0.90625\n",
      "2017-11-05T11:46:17.717231: step 3467, loss 0.203958, acc 0.96875\n",
      "2017-11-05T11:46:21.712761: step 3468, loss 0.383751, acc 0.8125\n",
      "2017-11-05T11:46:25.643829: step 3469, loss 0.11054, acc 0.9375\n",
      "2017-11-05T11:46:29.613192: step 3470, loss 0.185873, acc 0.875\n",
      "2017-11-05T11:46:33.641952: step 3471, loss 0.243505, acc 0.90625\n",
      "2017-11-05T11:46:37.917489: step 3472, loss 0.185768, acc 0.875\n",
      "2017-11-05T11:46:42.008896: step 3473, loss 0.303642, acc 0.8125\n",
      "2017-11-05T11:46:46.019796: step 3474, loss 0.126464, acc 0.9375\n",
      "2017-11-05T11:46:49.904445: step 3475, loss 0.215572, acc 0.90625\n",
      "2017-11-05T11:46:53.891775: step 3476, loss 0.0429803, acc 0.96875\n",
      "2017-11-05T11:46:57.836459: step 3477, loss 0.0314491, acc 0.96875\n",
      "2017-11-05T11:47:01.795677: step 3478, loss 0.219625, acc 0.84375\n",
      "2017-11-05T11:47:05.761508: step 3479, loss 0.110996, acc 0.90625\n",
      "2017-11-05T11:47:09.811952: step 3480, loss 0.145581, acc 0.90625\n",
      "2017-11-05T11:47:13.833950: step 3481, loss 0.233814, acc 0.90625\n",
      "2017-11-05T11:47:17.745762: step 3482, loss 0.1424, acc 0.9375\n",
      "2017-11-05T11:47:21.677555: step 3483, loss 0.0809114, acc 0.9375\n",
      "2017-11-05T11:47:25.824833: step 3484, loss 0.412114, acc 0.78125\n",
      "2017-11-05T11:47:29.912430: step 3485, loss 0.0740104, acc 0.96875\n",
      "2017-11-05T11:47:34.055761: step 3486, loss 0.0487187, acc 0.96875\n",
      "2017-11-05T11:47:38.216855: step 3487, loss 0.0303945, acc 1\n",
      "2017-11-05T11:47:42.330778: step 3488, loss 0.207214, acc 0.875\n",
      "2017-11-05T11:47:46.389662: step 3489, loss 0.172512, acc 0.90625\n",
      "2017-11-05T11:47:50.346576: step 3490, loss 0.385128, acc 0.84375\n",
      "2017-11-05T11:47:54.332138: step 3491, loss 0.0470058, acc 1\n",
      "2017-11-05T11:47:56.873328: step 3492, loss 0.215743, acc 0.9\n",
      "2017-11-05T11:48:00.982248: step 3493, loss 0.13752, acc 0.90625\n",
      "2017-11-05T11:48:05.034126: step 3494, loss 0.295075, acc 0.8125\n",
      "2017-11-05T11:48:09.008040: step 3495, loss 0.145236, acc 0.90625\n",
      "2017-11-05T11:48:12.973358: step 3496, loss 0.156889, acc 0.9375\n",
      "2017-11-05T11:48:16.959190: step 3497, loss 0.198556, acc 0.875\n",
      "2017-11-05T11:48:20.964052: step 3498, loss 0.191406, acc 0.90625\n",
      "2017-11-05T11:48:25.052456: step 3499, loss 0.055265, acc 1\n",
      "2017-11-05T11:48:29.436071: step 3500, loss 0.0287837, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:48:32.049428: step 3500, loss 0.745547, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-05T11:48:37.913572: step 3501, loss 0.328865, acc 0.84375\n",
      "2017-11-05T11:48:42.136573: step 3502, loss 0.210769, acc 0.84375\n",
      "2017-11-05T11:48:46.139918: step 3503, loss 0.125979, acc 0.90625\n",
      "2017-11-05T11:48:50.266651: step 3504, loss 0.173298, acc 0.90625\n",
      "2017-11-05T11:48:54.285667: step 3505, loss 0.27575, acc 0.84375\n",
      "2017-11-05T11:48:58.306524: step 3506, loss 0.115207, acc 0.9375\n",
      "2017-11-05T11:49:02.389926: step 3507, loss 0.0709503, acc 0.96875\n",
      "2017-11-05T11:49:06.453313: step 3508, loss 0.118139, acc 0.90625\n",
      "2017-11-05T11:49:10.595267: step 3509, loss 0.319889, acc 0.875\n",
      "2017-11-05T11:49:14.811324: step 3510, loss 0.188102, acc 0.875\n",
      "2017-11-05T11:49:18.875212: step 3511, loss 0.245027, acc 0.875\n",
      "2017-11-05T11:49:22.960614: step 3512, loss 0.0288934, acc 1\n",
      "2017-11-05T11:49:27.034009: step 3513, loss 0.159841, acc 0.90625\n",
      "2017-11-05T11:49:31.147932: step 3514, loss 0.0398366, acc 1\n",
      "2017-11-05T11:49:35.136266: step 3515, loss 0.176035, acc 0.90625\n",
      "2017-11-05T11:49:39.349759: step 3516, loss 0.177682, acc 0.9375\n",
      "2017-11-05T11:49:43.496707: step 3517, loss 0.160518, acc 0.9375\n",
      "2017-11-05T11:49:47.708699: step 3518, loss 0.331345, acc 0.84375\n",
      "2017-11-05T11:49:51.887168: step 3519, loss 0.106384, acc 0.9375\n",
      "2017-11-05T11:49:56.149697: step 3520, loss 0.197198, acc 0.90625\n",
      "2017-11-05T11:50:00.489280: step 3521, loss 0.14494, acc 0.90625\n",
      "2017-11-05T11:50:04.779830: step 3522, loss 0.0692819, acc 0.9375\n",
      "2017-11-05T11:50:08.840214: step 3523, loss 0.37084, acc 0.78125\n",
      "2017-11-05T11:50:12.928119: step 3524, loss 0.13264, acc 0.9375\n",
      "2017-11-05T11:50:17.010019: step 3525, loss 0.244056, acc 0.9375\n",
      "2017-11-05T11:50:21.079911: step 3526, loss 0.186465, acc 0.90625\n",
      "2017-11-05T11:50:25.130289: step 3527, loss 0.139387, acc 0.9375\n",
      "2017-11-05T11:50:27.617555: step 3528, loss 0.259482, acc 0.9\n",
      "2017-11-05T11:50:31.679953: step 3529, loss 0.126831, acc 0.9375\n",
      "2017-11-05T11:50:35.861718: step 3530, loss 0.0262953, acc 1\n",
      "2017-11-05T11:50:39.906092: step 3531, loss 0.110889, acc 0.9375\n",
      "2017-11-05T11:50:43.880415: step 3532, loss 0.0947593, acc 0.9375\n",
      "2017-11-05T11:50:47.907276: step 3533, loss 0.290456, acc 0.875\n",
      "2017-11-05T11:50:51.840071: step 3534, loss 0.0689638, acc 0.96875\n",
      "2017-11-05T11:50:55.866432: step 3535, loss 0.258618, acc 0.90625\n",
      "2017-11-05T11:50:59.827746: step 3536, loss 0.12455, acc 0.9375\n",
      "2017-11-05T11:51:03.847005: step 3537, loss 0.263256, acc 0.875\n",
      "2017-11-05T11:51:07.905905: step 3538, loss 0.247969, acc 0.90625\n",
      "2017-11-05T11:51:11.853710: step 3539, loss 0.335987, acc 0.8125\n",
      "2017-11-05T11:51:15.919599: step 3540, loss 0.079296, acc 0.96875\n",
      "2017-11-05T11:51:19.887919: step 3541, loss 0.335242, acc 0.8125\n",
      "2017-11-05T11:51:23.850735: step 3542, loss 0.238254, acc 0.90625\n",
      "2017-11-05T11:51:27.844573: step 3543, loss 0.157763, acc 0.9375\n",
      "2017-11-05T11:51:31.804386: step 3544, loss 0.0732974, acc 1\n",
      "2017-11-05T11:51:35.807230: step 3545, loss 0.106809, acc 0.9375\n",
      "2017-11-05T11:51:39.834092: step 3546, loss 0.366036, acc 0.90625\n",
      "2017-11-05T11:51:43.899480: step 3547, loss 0.206774, acc 0.9375\n",
      "2017-11-05T11:51:47.896821: step 3548, loss 0.283991, acc 0.875\n",
      "2017-11-05T11:51:51.898664: step 3549, loss 0.144762, acc 0.96875\n",
      "2017-11-05T11:51:55.991572: step 3550, loss 0.140567, acc 0.9375\n",
      "2017-11-05T11:51:59.964395: step 3551, loss 0.120088, acc 0.96875\n",
      "2017-11-05T11:52:04.000763: step 3552, loss 0.265158, acc 0.90625\n",
      "2017-11-05T11:52:08.043651: step 3553, loss 0.173956, acc 0.90625\n",
      "2017-11-05T11:52:12.126552: step 3554, loss 0.146668, acc 0.9375\n",
      "2017-11-05T11:52:16.176143: step 3555, loss 0.0525763, acc 0.96875\n",
      "2017-11-05T11:52:20.344105: step 3556, loss 0.148618, acc 0.90625\n",
      "2017-11-05T11:52:24.411995: step 3557, loss 0.25904, acc 0.875\n",
      "2017-11-05T11:52:28.482887: step 3558, loss 0.357504, acc 0.84375\n",
      "2017-11-05T11:52:32.604816: step 3559, loss 0.0407528, acc 1\n",
      "2017-11-05T11:52:36.769776: step 3560, loss 0.101043, acc 0.9375\n",
      "2017-11-05T11:52:40.926730: step 3561, loss 0.196129, acc 0.875\n",
      "2017-11-05T11:52:44.942583: step 3562, loss 0.348614, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:52:48.971446: step 3563, loss 0.294696, acc 0.84375\n",
      "2017-11-05T11:52:51.557282: step 3564, loss 0.0540902, acc 1\n",
      "2017-11-05T11:52:55.675209: step 3565, loss 0.0685765, acc 0.96875\n",
      "2017-11-05T11:52:59.681668: step 3566, loss 0.09349, acc 0.9375\n",
      "2017-11-05T11:53:03.759065: step 3567, loss 0.201382, acc 0.90625\n",
      "2017-11-05T11:53:07.890000: step 3568, loss 0.0508095, acc 0.96875\n",
      "2017-11-05T11:53:11.899349: step 3569, loss 0.0885524, acc 0.9375\n",
      "2017-11-05T11:53:16.008268: step 3570, loss 0.186679, acc 0.9375\n",
      "2017-11-05T11:53:20.068883: step 3571, loss 0.33121, acc 0.875\n",
      "2017-11-05T11:53:24.154865: step 3572, loss 0.229693, acc 0.90625\n",
      "2017-11-05T11:53:28.291804: step 3573, loss 0.227943, acc 0.84375\n",
      "2017-11-05T11:53:33.078328: step 3574, loss 0.238323, acc 0.9375\n",
      "2017-11-05T11:53:37.939782: step 3575, loss 0.0594214, acc 0.96875\n",
      "2017-11-05T11:53:42.229830: step 3576, loss 0.362962, acc 0.8125\n",
      "2017-11-05T11:53:46.227672: step 3577, loss 0.191779, acc 0.90625\n",
      "2017-11-05T11:53:50.251031: step 3578, loss 0.180016, acc 0.90625\n",
      "2017-11-05T11:53:54.297905: step 3579, loss 0.270698, acc 0.90625\n",
      "2017-11-05T11:53:58.407326: step 3580, loss 0.253334, acc 0.875\n",
      "2017-11-05T11:54:02.494428: step 3581, loss 0.210209, acc 0.875\n",
      "2017-11-05T11:54:06.513284: step 3582, loss 0.312696, acc 0.84375\n",
      "2017-11-05T11:54:10.497615: step 3583, loss 0.0766846, acc 0.9375\n",
      "2017-11-05T11:54:14.591221: step 3584, loss 0.315093, acc 0.875\n",
      "2017-11-05T11:54:18.572911: step 3585, loss 0.170196, acc 0.9375\n",
      "2017-11-05T11:54:22.653811: step 3586, loss 0.122544, acc 0.96875\n",
      "2017-11-05T11:54:26.587605: step 3587, loss 0.105257, acc 0.9375\n",
      "2017-11-05T11:54:30.633981: step 3588, loss 0.176285, acc 0.9375\n",
      "2017-11-05T11:54:34.741757: step 3589, loss 0.269475, acc 0.90625\n",
      "2017-11-05T11:54:38.834089: step 3590, loss 0.162111, acc 0.90625\n",
      "2017-11-05T11:54:42.810418: step 3591, loss 0.365993, acc 0.8125\n",
      "2017-11-05T11:54:46.811261: step 3592, loss 0.370181, acc 0.78125\n",
      "2017-11-05T11:54:50.802596: step 3593, loss 0.062026, acc 0.96875\n",
      "2017-11-05T11:54:54.812446: step 3594, loss 0.114038, acc 0.9375\n",
      "2017-11-05T11:54:58.811787: step 3595, loss 0.256658, acc 0.875\n",
      "2017-11-05T11:55:02.823137: step 3596, loss 0.21165, acc 0.875\n",
      "2017-11-05T11:55:06.916546: step 3597, loss 0.0793975, acc 0.96875\n",
      "2017-11-05T11:55:10.969926: step 3598, loss 0.207152, acc 0.875\n",
      "2017-11-05T11:55:14.965265: step 3599, loss 0.219641, acc 0.90625\n",
      "2017-11-05T11:55:17.544098: step 3600, loss 0.224569, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T11:55:20.186975: step 3600, loss 0.687829, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\0\\1509842765\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C935E198>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\n",
      "\n",
      "2017-11-05T11:55:31.015118: step 1, loss 1.7392, acc 0.6875\n",
      "2017-11-05T11:55:35.023466: step 2, loss 1.10168, acc 0.75\n",
      "2017-11-05T11:55:39.163122: step 3, loss 0.706643, acc 0.90625\n",
      "2017-11-05T11:55:43.188363: step 4, loss 1.14391, acc 0.84375\n",
      "2017-11-05T11:55:47.236239: step 5, loss 0.0195904, acc 1\n",
      "2017-11-05T11:55:51.271106: step 6, loss 0.397229, acc 0.9375\n",
      "2017-11-05T11:55:55.278637: step 7, loss 1.11692, acc 0.90625\n",
      "2017-11-05T11:55:59.313003: step 8, loss 1.06736, acc 0.875\n",
      "2017-11-05T11:56:03.423653: step 9, loss 1.71834, acc 0.90625\n",
      "2017-11-05T11:56:07.444511: step 10, loss 1.99302, acc 0.8125\n",
      "2017-11-05T11:56:11.463939: step 11, loss 0.623607, acc 0.96875\n",
      "2017-11-05T11:56:15.512315: step 12, loss 3.02055, acc 0.8125\n",
      "2017-11-05T11:56:19.563696: step 13, loss 1.07299, acc 0.9375\n",
      "2017-11-05T11:56:23.642983: step 14, loss 2.23096, acc 0.78125\n",
      "2017-11-05T11:56:27.663379: step 15, loss 1.40916, acc 0.84375\n",
      "2017-11-05T11:56:31.645209: step 16, loss 2.19683, acc 0.8125\n",
      "2017-11-05T11:56:35.908205: step 17, loss 1.27503, acc 0.78125\n",
      "2017-11-05T11:56:40.035944: step 18, loss 1.4007, acc 0.78125\n",
      "2017-11-05T11:56:44.054711: step 19, loss 0.681758, acc 0.8125\n",
      "2017-11-05T11:56:48.115203: step 20, loss 1.89634, acc 0.71875\n",
      "2017-11-05T11:56:52.258802: step 21, loss 1.34253, acc 0.6875\n",
      "2017-11-05T11:56:56.271152: step 22, loss 0.365971, acc 0.84375\n",
      "2017-11-05T11:57:00.376069: step 23, loss 1.82149, acc 0.75\n",
      "2017-11-05T11:57:04.438999: step 24, loss 1.87456, acc 0.71875\n",
      "2017-11-05T11:57:08.462858: step 25, loss 1.13228, acc 0.71875\n",
      "2017-11-05T11:57:12.532031: step 26, loss 2.14011, acc 0.65625\n",
      "2017-11-05T11:57:16.536376: step 27, loss 0.950051, acc 0.84375\n",
      "2017-11-05T11:57:20.513861: step 28, loss 0.995448, acc 0.84375\n",
      "2017-11-05T11:57:24.563739: step 29, loss 1.38878, acc 0.84375\n",
      "2017-11-05T11:57:28.596604: step 30, loss 1.20245, acc 0.84375\n",
      "2017-11-05T11:57:32.708147: step 31, loss 2.16758, acc 0.8125\n",
      "2017-11-05T11:57:36.807242: step 32, loss 0.573574, acc 0.90625\n",
      "2017-11-05T11:57:40.840497: step 33, loss 0.785932, acc 0.96875\n",
      "2017-11-05T11:57:44.869568: step 34, loss 0.895452, acc 0.90625\n",
      "2017-11-05T11:57:49.002896: step 35, loss 2.76878, acc 0.8125\n",
      "2017-11-05T11:57:51.682067: step 36, loss 1.87327, acc 0.8\n",
      "2017-11-05T11:57:55.892027: step 37, loss 1.13891, acc 0.84375\n",
      "2017-11-05T11:58:00.037210: step 38, loss 0.319813, acc 0.90625\n",
      "2017-11-05T11:58:04.129117: step 39, loss 0.985438, acc 0.84375\n",
      "2017-11-05T11:58:08.300582: step 40, loss 0.451959, acc 0.84375\n",
      "2017-11-05T11:58:12.407597: step 41, loss 1.33425, acc 0.84375\n",
      "2017-11-05T11:58:16.479990: step 42, loss 0.708888, acc 0.84375\n",
      "2017-11-05T11:58:20.588910: step 43, loss 1.17728, acc 0.78125\n",
      "2017-11-05T11:58:24.861727: step 44, loss 0.899347, acc 0.8125\n",
      "2017-11-05T11:58:29.035617: step 45, loss 0.596963, acc 0.90625\n",
      "2017-11-05T11:58:33.153979: step 46, loss 0.897659, acc 0.78125\n",
      "2017-11-05T11:58:37.409159: step 47, loss 0.927062, acc 0.875\n",
      "2017-11-05T11:58:41.496680: step 48, loss 0.520974, acc 0.84375\n",
      "2017-11-05T11:58:45.592861: step 49, loss 1.3944, acc 0.84375\n",
      "2017-11-05T11:58:49.748939: step 50, loss 1.84083, acc 0.8125\n",
      "2017-11-05T11:58:53.922905: step 51, loss 0.313536, acc 0.875\n",
      "2017-11-05T11:58:58.007740: step 52, loss 0.61912, acc 0.9375\n",
      "2017-11-05T11:59:02.141177: step 53, loss 0.46007, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T11:59:06.354170: step 54, loss 1.43545, acc 0.84375\n",
      "2017-11-05T11:59:10.492611: step 55, loss 1.54035, acc 0.8125\n",
      "2017-11-05T11:59:14.488950: step 56, loss 1.76158, acc 0.90625\n",
      "2017-11-05T11:59:18.547835: step 57, loss 0.417451, acc 0.875\n",
      "2017-11-05T11:59:22.569110: step 58, loss 0.0694487, acc 0.96875\n",
      "2017-11-05T11:59:26.658017: step 59, loss 0.0782834, acc 0.96875\n",
      "2017-11-05T11:59:30.701888: step 60, loss 0.951443, acc 0.75\n",
      "2017-11-05T11:59:34.820481: step 61, loss 1.0765, acc 0.8125\n",
      "2017-11-05T11:59:38.880287: step 62, loss 1.27361, acc 0.75\n",
      "2017-11-05T11:59:42.938170: step 63, loss 0.735491, acc 0.84375\n",
      "2017-11-05T11:59:47.214706: step 64, loss 0.657249, acc 0.875\n",
      "2017-11-05T11:59:51.302611: step 65, loss 0.862615, acc 0.84375\n",
      "2017-11-05T11:59:55.463506: step 66, loss 0.980923, acc 0.875\n",
      "2017-11-05T11:59:59.554286: step 67, loss 1.94201, acc 0.78125\n",
      "2017-11-05T12:00:03.886364: step 68, loss 0.747229, acc 0.875\n",
      "2017-11-05T12:00:08.001789: step 69, loss 1.37034, acc 0.8125\n",
      "2017-11-05T12:00:12.136727: step 70, loss 0.664034, acc 0.875\n",
      "2017-11-05T12:00:16.133626: step 71, loss 0.0490743, acc 0.96875\n",
      "2017-11-05T12:00:18.657116: step 72, loss 1.45678, acc 0.8\n",
      "2017-11-05T12:00:23.652727: step 73, loss 0.557442, acc 0.875\n",
      "2017-11-05T12:00:27.813683: step 74, loss 0.697192, acc 0.84375\n",
      "2017-11-05T12:00:32.000158: step 75, loss 1.40639, acc 0.84375\n",
      "2017-11-05T12:00:36.172721: step 76, loss 0.883145, acc 0.875\n",
      "2017-11-05T12:00:40.368143: step 77, loss 1.90773, acc 0.75\n",
      "2017-11-05T12:00:44.446534: step 78, loss 0.714937, acc 0.84375\n",
      "2017-11-05T12:00:48.561698: step 79, loss 0.981673, acc 0.8125\n",
      "2017-11-05T12:00:52.660887: step 80, loss 0.716817, acc 0.8125\n",
      "2017-11-05T12:00:56.882053: step 81, loss 0.869019, acc 0.8125\n",
      "2017-11-05T12:01:01.215632: step 82, loss 0.770697, acc 0.8125\n",
      "2017-11-05T12:01:05.308474: step 83, loss 0.339394, acc 0.84375\n",
      "2017-11-05T12:01:09.632546: step 84, loss 0.600779, acc 0.90625\n",
      "2017-11-05T12:01:13.839960: step 85, loss 0.971731, acc 0.78125\n",
      "2017-11-05T12:01:17.871517: step 86, loss 0.811298, acc 0.84375\n",
      "2017-11-05T12:01:21.959666: step 87, loss 1.68197, acc 0.71875\n",
      "2017-11-05T12:01:26.038772: step 88, loss 0.540361, acc 0.78125\n",
      "2017-11-05T12:01:30.122673: step 89, loss 0.953567, acc 0.75\n",
      "2017-11-05T12:01:34.695391: step 90, loss 1.0279, acc 0.875\n",
      "2017-11-05T12:01:38.973604: step 91, loss 0.723396, acc 0.875\n",
      "2017-11-05T12:01:43.102038: step 92, loss 0.645768, acc 0.9375\n",
      "2017-11-05T12:01:47.203953: step 93, loss 1.8242, acc 0.78125\n",
      "2017-11-05T12:01:51.240827: step 94, loss 1.09755, acc 0.90625\n",
      "2017-11-05T12:01:55.469894: step 95, loss 0.485689, acc 0.90625\n",
      "2017-11-05T12:01:59.736121: step 96, loss 0.214839, acc 0.90625\n",
      "2017-11-05T12:02:03.841938: step 97, loss 0.867556, acc 0.875\n",
      "2017-11-05T12:02:07.916332: step 98, loss 1.89306, acc 0.8125\n",
      "2017-11-05T12:02:12.028942: step 99, loss 1.04514, acc 0.9375\n",
      "2017-11-05T12:02:16.124501: step 100, loss 1.06216, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:02:18.705222: step 100, loss 1.78454, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-100\n",
      "\n",
      "2017-11-05T12:02:25.883987: step 101, loss 0.138469, acc 0.9375\n",
      "2017-11-05T12:02:29.868825: step 102, loss 0.714057, acc 0.875\n",
      "2017-11-05T12:02:34.028203: step 103, loss 0.577312, acc 0.9375\n",
      "2017-11-05T12:02:38.187225: step 104, loss 0.67122, acc 0.875\n",
      "2017-11-05T12:02:42.203079: step 105, loss 0.268465, acc 0.9375\n",
      "2017-11-05T12:02:46.261962: step 106, loss 1.03973, acc 0.875\n",
      "2017-11-05T12:02:50.269445: step 107, loss 0.260737, acc 0.9375\n",
      "2017-11-05T12:02:52.907319: step 108, loss 1.18159, acc 0.75\n",
      "2017-11-05T12:02:56.973208: step 109, loss 0.163452, acc 0.9375\n",
      "2017-11-05T12:03:01.153178: step 110, loss 1.27295, acc 0.75\n",
      "2017-11-05T12:03:05.260596: step 111, loss 0.374123, acc 0.875\n",
      "2017-11-05T12:03:09.397536: step 112, loss 0.910611, acc 0.75\n",
      "2017-11-05T12:03:13.476970: step 113, loss 0.540644, acc 0.84375\n",
      "2017-11-05T12:03:17.563373: step 114, loss 1.40254, acc 0.6875\n",
      "2017-11-05T12:03:21.633266: step 115, loss 0.792815, acc 0.875\n",
      "2017-11-05T12:03:25.840754: step 116, loss 1.63216, acc 0.6875\n",
      "2017-11-05T12:03:29.893134: step 117, loss 0.509326, acc 0.875\n",
      "2017-11-05T12:03:34.083111: step 118, loss 0.622836, acc 0.875\n",
      "2017-11-05T12:03:38.223554: step 119, loss 1.03728, acc 0.75\n",
      "2017-11-05T12:03:42.349612: step 120, loss 0.672045, acc 0.78125\n",
      "2017-11-05T12:03:46.506409: step 121, loss 0.301611, acc 0.84375\n",
      "2017-11-05T12:03:50.507252: step 122, loss 0.535345, acc 0.90625\n",
      "2017-11-05T12:03:54.661204: step 123, loss 0.0074417, acc 1\n",
      "2017-11-05T12:03:58.727147: step 124, loss 0.67162, acc 0.90625\n",
      "2017-11-05T12:04:02.812550: step 125, loss 1.29427, acc 0.8125\n",
      "2017-11-05T12:04:06.850919: step 126, loss 0.302659, acc 0.90625\n",
      "2017-11-05T12:04:10.858482: step 127, loss 0.45918, acc 0.90625\n",
      "2017-11-05T12:04:14.946387: step 128, loss 0.728571, acc 0.84375\n",
      "2017-11-05T12:04:19.068703: step 129, loss 0.491938, acc 0.875\n",
      "2017-11-05T12:04:23.135593: step 130, loss 0.180875, acc 0.90625\n",
      "2017-11-05T12:04:27.183469: step 131, loss 0.544089, acc 0.9375\n",
      "2017-11-05T12:04:31.158294: step 132, loss 0.731327, acc 0.90625\n",
      "2017-11-05T12:04:35.314246: step 133, loss 0.0471523, acc 0.96875\n",
      "2017-11-05T12:04:39.336604: step 134, loss 0.833124, acc 0.875\n",
      "2017-11-05T12:04:43.346955: step 135, loss 0.467546, acc 0.8125\n",
      "2017-11-05T12:04:47.349839: step 136, loss 1.88906, acc 0.78125\n",
      "2017-11-05T12:04:51.355656: step 137, loss 0.408911, acc 0.9375\n",
      "2017-11-05T12:04:55.422502: step 138, loss 0.35904, acc 0.875\n",
      "2017-11-05T12:04:59.417341: step 139, loss 0.538332, acc 0.84375\n",
      "2017-11-05T12:05:03.455141: step 140, loss 0.532001, acc 0.875\n",
      "2017-11-05T12:05:07.450831: step 141, loss 1.09925, acc 0.875\n",
      "2017-11-05T12:05:11.498758: step 142, loss 0.483677, acc 0.9375\n",
      "2017-11-05T12:05:15.548228: step 143, loss 1.04578, acc 0.78125\n",
      "2017-11-05T12:05:18.103314: step 144, loss 0.876842, acc 0.85\n",
      "2017-11-05T12:05:22.114164: step 145, loss 1.25475, acc 0.8125\n",
      "2017-11-05T12:05:26.126514: step 146, loss 0.434126, acc 0.84375\n",
      "2017-11-05T12:05:30.254480: step 147, loss 0.226369, acc 0.90625\n",
      "2017-11-05T12:05:34.221495: step 148, loss 0.337848, acc 0.90625\n",
      "2017-11-05T12:05:38.232306: step 149, loss 1.04666, acc 0.78125\n",
      "2017-11-05T12:05:42.279805: step 150, loss 1.04025, acc 0.78125\n",
      "2017-11-05T12:05:46.256659: step 151, loss 0.275184, acc 0.875\n",
      "2017-11-05T12:05:50.325550: step 152, loss 0.465801, acc 0.90625\n",
      "2017-11-05T12:05:54.403966: step 153, loss 0.88725, acc 0.75\n",
      "2017-11-05T12:05:58.523893: step 154, loss 0.126161, acc 0.96875\n",
      "2017-11-05T12:06:02.579775: step 155, loss 0.371869, acc 0.9375\n",
      "2017-11-05T12:06:06.630144: step 156, loss 0.305442, acc 0.9375\n",
      "2017-11-05T12:06:10.684024: step 157, loss 0.210348, acc 0.9375\n",
      "2017-11-05T12:06:14.720393: step 158, loss 0.733878, acc 0.875\n",
      "2017-11-05T12:06:18.791293: step 159, loss 0.120286, acc 0.96875\n",
      "2017-11-05T12:06:22.755609: step 160, loss 0.377635, acc 0.90625\n",
      "2017-11-05T12:06:26.676311: step 161, loss 1.97187, acc 0.78125\n",
      "2017-11-05T12:06:30.729191: step 162, loss 0.347181, acc 0.875\n",
      "2017-11-05T12:06:34.822599: step 163, loss 0.260307, acc 0.9375\n",
      "2017-11-05T12:06:38.812947: step 164, loss 0.773997, acc 0.90625\n",
      "2017-11-05T12:06:42.816246: step 165, loss 0.0208892, acc 1\n",
      "2017-11-05T12:06:46.851635: step 166, loss 0.120456, acc 0.9375\n",
      "2017-11-05T12:06:50.854979: step 167, loss 0.993368, acc 0.875\n",
      "2017-11-05T12:06:54.915096: step 168, loss 1.07136, acc 0.8125\n",
      "2017-11-05T12:06:58.914897: step 169, loss 0.889288, acc 0.84375\n",
      "2017-11-05T12:07:02.883256: step 170, loss 0.483144, acc 0.875\n",
      "2017-11-05T12:07:06.856195: step 171, loss 0.483649, acc 0.875\n",
      "2017-11-05T12:07:10.874508: step 172, loss 0.770901, acc 0.84375\n",
      "2017-11-05T12:07:14.898532: step 173, loss 0.690212, acc 0.84375\n",
      "2017-11-05T12:07:18.999224: step 174, loss 1.24502, acc 0.71875\n",
      "2017-11-05T12:07:23.097487: step 175, loss 0.974353, acc 0.78125\n",
      "2017-11-05T12:07:27.172291: step 176, loss 0.0612376, acc 0.96875\n",
      "2017-11-05T12:07:31.277676: step 177, loss 1.0017, acc 0.8125\n",
      "2017-11-05T12:07:35.309052: step 178, loss 0.466474, acc 0.90625\n",
      "2017-11-05T12:07:39.324499: step 179, loss 0.754316, acc 0.8125\n",
      "2017-11-05T12:07:41.980547: step 180, loss 0.69036, acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T12:07:46.041626: step 181, loss 0.446473, acc 0.9375\n",
      "2017-11-05T12:07:50.091004: step 182, loss 0.678365, acc 0.84375\n",
      "2017-11-05T12:07:54.052555: step 183, loss 0.638423, acc 0.90625\n",
      "2017-11-05T12:07:58.045604: step 184, loss 0.195261, acc 0.96875\n",
      "2017-11-05T12:08:02.127514: step 185, loss 0.493999, acc 0.9375\n",
      "2017-11-05T12:08:06.189400: step 186, loss 0.39365, acc 0.9375\n",
      "2017-11-05T12:08:10.245507: step 187, loss 0.99982, acc 0.8125\n",
      "2017-11-05T12:08:14.263362: step 188, loss 0.825857, acc 0.8125\n",
      "2017-11-05T12:08:18.235183: step 189, loss 0.863483, acc 0.84375\n",
      "2017-11-05T12:08:22.452680: step 190, loss 0.672145, acc 0.78125\n",
      "2017-11-05T12:08:26.672227: step 191, loss 0.506525, acc 0.84375\n",
      "2017-11-05T12:08:30.705145: step 192, loss 0.320116, acc 0.84375\n",
      "2017-11-05T12:08:34.871620: step 193, loss 0.148731, acc 0.9375\n",
      "2017-11-05T12:08:38.970714: step 194, loss 0.535268, acc 0.90625\n",
      "2017-11-05T12:08:43.124664: step 195, loss 0.142134, acc 0.9375\n",
      "2017-11-05T12:08:47.130511: step 196, loss 0.048919, acc 0.96875\n",
      "2017-11-05T12:08:51.247093: step 197, loss 0.992126, acc 0.71875\n",
      "2017-11-05T12:08:55.266334: step 198, loss 0.750072, acc 0.8125\n",
      "2017-11-05T12:08:59.365247: step 199, loss 0.245406, acc 0.90625\n",
      "2017-11-05T12:09:03.518198: step 200, loss 0.476338, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:09:06.102922: step 200, loss 1.58754, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-200\n",
      "\n",
      "2017-11-05T12:09:11.822783: step 201, loss 0.383183, acc 0.90625\n",
      "2017-11-05T12:09:15.856149: step 202, loss 0.280712, acc 0.9375\n",
      "2017-11-05T12:09:20.343837: step 203, loss 1.1893, acc 0.78125\n",
      "2017-11-05T12:09:24.365194: step 204, loss 1.48034, acc 0.75\n",
      "2017-11-05T12:09:28.385051: step 205, loss 0.575746, acc 0.90625\n",
      "2017-11-05T12:09:32.453942: step 206, loss 0.330071, acc 0.90625\n",
      "2017-11-05T12:09:36.589381: step 207, loss 0.295528, acc 0.9375\n",
      "2017-11-05T12:09:40.665277: step 208, loss 0.176683, acc 0.90625\n",
      "2017-11-05T12:09:44.776110: step 209, loss 0.686192, acc 0.875\n",
      "2017-11-05T12:09:48.893037: step 210, loss 0.241139, acc 0.9375\n",
      "2017-11-05T12:09:52.968432: step 211, loss 0.404242, acc 0.9375\n",
      "2017-11-05T12:09:57.029316: step 212, loss 0.114187, acc 0.96875\n",
      "2017-11-05T12:10:01.329142: step 213, loss 0.171195, acc 0.9375\n",
      "2017-11-05T12:10:05.535019: step 214, loss 0.664756, acc 0.78125\n",
      "2017-11-05T12:10:09.561150: step 215, loss 0.99265, acc 0.78125\n",
      "2017-11-05T12:10:12.115736: step 216, loss 0.359189, acc 0.9\n",
      "2017-11-05T12:10:16.194354: step 217, loss 0.441765, acc 0.9375\n",
      "2017-11-05T12:10:20.289546: step 218, loss 0.281697, acc 0.9375\n",
      "2017-11-05T12:10:24.277379: step 219, loss 0.161748, acc 0.9375\n",
      "2017-11-05T12:10:28.373790: step 220, loss 0.151613, acc 0.9375\n",
      "2017-11-05T12:10:32.484210: step 221, loss 0.477555, acc 0.875\n",
      "2017-11-05T12:10:36.678190: step 222, loss 0.173281, acc 0.96875\n",
      "2017-11-05T12:10:40.793114: step 223, loss 0.212882, acc 0.90625\n",
      "2017-11-05T12:10:44.944076: step 224, loss 1.02367, acc 0.78125\n",
      "2017-11-05T12:10:49.006463: step 225, loss 0.841114, acc 0.78125\n",
      "2017-11-05T12:10:53.158914: step 226, loss 0.738603, acc 0.9375\n",
      "2017-11-05T12:10:57.165527: step 227, loss 0.500386, acc 0.875\n",
      "2017-11-05T12:11:01.274446: step 228, loss 0.861365, acc 0.8125\n",
      "2017-11-05T12:11:05.395375: step 229, loss 1.23465, acc 0.78125\n",
      "2017-11-05T12:11:09.542938: step 230, loss 0.531881, acc 0.90625\n",
      "2017-11-05T12:11:13.613468: step 231, loss 0.548717, acc 0.875\n",
      "2017-11-05T12:11:17.653840: step 232, loss 0.390301, acc 0.84375\n",
      "2017-11-05T12:11:21.730245: step 233, loss 0.28141, acc 0.84375\n",
      "2017-11-05T12:11:26.002281: step 234, loss 0.641392, acc 0.875\n",
      "2017-11-05T12:11:30.045155: step 235, loss 0.415756, acc 0.90625\n",
      "2017-11-05T12:11:34.181459: step 236, loss 0.0736442, acc 0.9375\n",
      "2017-11-05T12:11:38.297681: step 237, loss 0.0678018, acc 0.96875\n",
      "2017-11-05T12:11:42.360680: step 238, loss 0.804476, acc 0.90625\n",
      "2017-11-05T12:11:46.694760: step 239, loss 0.666221, acc 0.8125\n",
      "2017-11-05T12:11:50.813209: step 240, loss 1.04367, acc 0.8125\n",
      "2017-11-05T12:11:54.925403: step 241, loss 0.666399, acc 0.875\n",
      "2017-11-05T12:11:59.032321: step 242, loss 0.373601, acc 0.875\n",
      "2017-11-05T12:12:03.079137: step 243, loss 0.837206, acc 0.8125\n",
      "2017-11-05T12:12:07.066356: step 244, loss 0.507953, acc 0.9375\n",
      "2017-11-05T12:12:11.139240: step 245, loss 0.495404, acc 0.9375\n",
      "2017-11-05T12:12:15.122983: step 246, loss 0.360976, acc 0.90625\n",
      "2017-11-05T12:12:19.178410: step 247, loss 0.241666, acc 0.90625\n",
      "2017-11-05T12:12:23.334324: step 248, loss 1.03023, acc 0.84375\n",
      "2017-11-05T12:12:27.320043: step 249, loss 0.108892, acc 0.9375\n",
      "2017-11-05T12:12:31.496117: step 250, loss 0.789725, acc 0.9375\n",
      "2017-11-05T12:12:35.652069: step 251, loss 0.458353, acc 0.90625\n",
      "2017-11-05T12:12:38.310958: step 252, loss 0.126128, acc 0.95\n",
      "2017-11-05T12:12:42.415375: step 253, loss 0.189972, acc 0.90625\n",
      "2017-11-05T12:12:47.010139: step 254, loss 0.419268, acc 0.9375\n",
      "2017-11-05T12:12:51.156085: step 255, loss 1.23598, acc 0.84375\n",
      "2017-11-05T12:12:55.238487: step 256, loss 0.326223, acc 0.90625\n",
      "2017-11-05T12:12:59.276855: step 257, loss 0.235708, acc 0.875\n",
      "2017-11-05T12:13:03.365761: step 258, loss 0.312551, acc 0.875\n",
      "2017-11-05T12:13:07.519211: step 259, loss 0.423363, acc 0.90625\n",
      "2017-11-05T12:13:11.663727: step 260, loss 0.521214, acc 0.875\n",
      "2017-11-05T12:13:15.706506: step 261, loss 0.20677, acc 0.9375\n",
      "2017-11-05T12:13:19.816927: step 262, loss 1.57944, acc 0.625\n",
      "2017-11-05T12:13:24.026918: step 263, loss 0.54079, acc 0.78125\n",
      "2017-11-05T12:13:28.237420: step 264, loss 0.699001, acc 0.875\n",
      "2017-11-05T12:13:32.355346: step 265, loss 0.307004, acc 0.90625\n",
      "2017-11-05T12:13:36.415750: step 266, loss 0.361318, acc 0.84375\n",
      "2017-11-05T12:13:40.440123: step 267, loss 0.1528, acc 0.96875\n",
      "2017-11-05T12:13:44.478589: step 268, loss 0.425602, acc 0.90625\n",
      "2017-11-05T12:13:48.660949: step 269, loss 0.37247, acc 0.9375\n",
      "2017-11-05T12:13:52.699318: step 270, loss 0.330499, acc 0.84375\n",
      "2017-11-05T12:13:56.785722: step 271, loss 0.340546, acc 0.96875\n",
      "2017-11-05T12:14:00.834990: step 272, loss 0.493955, acc 0.90625\n",
      "2017-11-05T12:14:04.948913: step 273, loss 0.268845, acc 0.9375\n",
      "2017-11-05T12:14:09.053081: step 274, loss 0.704543, acc 0.875\n",
      "2017-11-05T12:14:13.107110: step 275, loss 0.140102, acc 0.96875\n",
      "2017-11-05T12:14:17.129355: step 276, loss 0.348082, acc 0.9375\n",
      "2017-11-05T12:14:21.204250: step 277, loss 0.333841, acc 0.96875\n",
      "2017-11-05T12:14:25.286151: step 278, loss 0.816801, acc 0.90625\n",
      "2017-11-05T12:14:29.407773: step 279, loss 0.884841, acc 0.8125\n",
      "2017-11-05T12:14:33.582739: step 280, loss 0.311325, acc 0.84375\n",
      "2017-11-05T12:14:37.686155: step 281, loss 0.83663, acc 0.84375\n",
      "2017-11-05T12:14:41.730529: step 282, loss 0.155218, acc 0.9375\n",
      "2017-11-05T12:14:45.787411: step 283, loss 0.417287, acc 0.78125\n",
      "2017-11-05T12:14:49.818107: step 284, loss 0.248049, acc 0.90625\n",
      "2017-11-05T12:14:53.837354: step 285, loss 0.610478, acc 0.8125\n",
      "2017-11-05T12:14:57.845606: step 286, loss 0.395248, acc 0.9375\n",
      "2017-11-05T12:15:01.890989: step 287, loss 0.691547, acc 0.875\n",
      "2017-11-05T12:15:04.491211: step 288, loss 0.697176, acc 0.8\n",
      "2017-11-05T12:15:08.522814: step 289, loss 0.41233, acc 0.96875\n",
      "2017-11-05T12:15:12.604714: step 290, loss 0.680272, acc 0.875\n",
      "2017-11-05T12:15:16.634577: step 291, loss 0.0173359, acc 1\n",
      "2017-11-05T12:15:20.706972: step 292, loss 1.03285, acc 0.8125\n",
      "2017-11-05T12:15:24.803382: step 293, loss 0.455996, acc 0.90625\n",
      "2017-11-05T12:15:28.826240: step 294, loss 1.24221, acc 0.875\n",
      "2017-11-05T12:15:32.842083: step 295, loss 0.0350881, acc 1\n",
      "2017-11-05T12:15:36.880462: step 296, loss 0.411929, acc 0.90625\n",
      "2017-11-05T12:15:40.974069: step 297, loss 0.459724, acc 0.875\n",
      "2017-11-05T12:15:45.043961: step 298, loss 0.0872329, acc 0.96875\n",
      "2017-11-05T12:15:49.157716: step 299, loss 0.797157, acc 0.875\n",
      "2017-11-05T12:15:53.477672: step 300, loss 0.693627, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:15:56.331956: step 300, loss 1.58584, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-300\n",
      "\n",
      "2017-11-05T12:16:02.103153: step 301, loss 0.0576839, acc 0.96875\n",
      "2017-11-05T12:16:06.254102: step 302, loss 0.281268, acc 0.90625\n",
      "2017-11-05T12:16:10.406971: step 303, loss 0.075857, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T12:16:14.490820: step 304, loss 0.408855, acc 0.875\n",
      "2017-11-05T12:16:18.522685: step 305, loss 0.250704, acc 0.90625\n",
      "2017-11-05T12:16:22.538539: step 306, loss 0.366063, acc 0.90625\n",
      "2017-11-05T12:16:26.608431: step 307, loss 0.654524, acc 0.90625\n",
      "2017-11-05T12:16:30.659264: step 308, loss 0.239906, acc 0.9375\n",
      "2017-11-05T12:16:34.976331: step 309, loss 0.765329, acc 0.84375\n",
      "2017-11-05T12:16:39.217845: step 310, loss 0.520176, acc 0.875\n",
      "2017-11-05T12:16:43.316685: step 311, loss 0.357368, acc 0.90625\n",
      "2017-11-05T12:16:47.398084: step 312, loss 1.2819, acc 0.75\n",
      "2017-11-05T12:16:51.423045: step 313, loss 0.526933, acc 0.875\n",
      "2017-11-05T12:16:55.401812: step 314, loss 0.476201, acc 0.875\n",
      "2017-11-05T12:16:59.386975: step 315, loss 0.470648, acc 0.84375\n",
      "2017-11-05T12:17:03.429238: step 316, loss 0.610956, acc 0.8125\n",
      "2017-11-05T12:17:07.478730: step 317, loss 0.408881, acc 0.9375\n",
      "2017-11-05T12:17:11.530730: step 318, loss 0.469726, acc 0.90625\n",
      "2017-11-05T12:17:15.487438: step 319, loss 0.280059, acc 0.875\n",
      "2017-11-05T12:17:19.545322: step 320, loss 0.415898, acc 0.875\n",
      "2017-11-05T12:17:23.597201: step 321, loss 0.968593, acc 0.84375\n",
      "2017-11-05T12:17:27.603548: step 322, loss 0.213211, acc 0.9375\n",
      "2017-11-05T12:17:31.592381: step 323, loss 0.628952, acc 0.84375\n",
      "2017-11-05T12:17:34.193675: step 324, loss 0.108891, acc 0.95\n",
      "2017-11-05T12:17:38.373144: step 325, loss 0.683665, acc 0.875\n",
      "2017-11-05T12:17:42.463797: step 326, loss 0.0729052, acc 0.96875\n",
      "2017-11-05T12:17:46.455903: step 327, loss 0.109444, acc 0.875\n",
      "2017-11-05T12:17:50.605210: step 328, loss 0.0676593, acc 0.96875\n",
      "2017-11-05T12:17:54.652171: step 329, loss 0.292205, acc 0.90625\n",
      "2017-11-05T12:17:58.789611: step 330, loss 0.390713, acc 0.90625\n",
      "2017-11-05T12:18:02.924048: step 331, loss 0.595602, acc 0.9375\n",
      "2017-11-05T12:18:07.078001: step 332, loss 0.743762, acc 0.84375\n",
      "2017-11-05T12:18:11.119872: step 333, loss 0.0922248, acc 0.96875\n",
      "2017-11-05T12:18:15.175276: step 334, loss 0.179031, acc 0.9375\n",
      "2017-11-05T12:18:19.196132: step 335, loss 0.320967, acc 0.9375\n",
      "2017-11-05T12:18:23.399620: step 336, loss 0.205201, acc 0.96875\n",
      "2017-11-05T12:18:27.456001: step 337, loss 0.069846, acc 0.96875\n",
      "2017-11-05T12:18:31.555914: step 338, loss 0.00673399, acc 1\n",
      "2017-11-05T12:18:35.734136: step 339, loss 0.0704296, acc 0.9375\n",
      "2017-11-05T12:18:39.930239: step 340, loss 0.361337, acc 0.9375\n",
      "2017-11-05T12:18:44.095428: step 341, loss 0.343985, acc 0.84375\n",
      "2017-11-05T12:18:48.258887: step 342, loss 0.203048, acc 0.9375\n",
      "2017-11-05T12:18:52.368807: step 343, loss 0.849264, acc 0.875\n",
      "2017-11-05T12:18:56.537769: step 344, loss 0.0308613, acc 1\n",
      "2017-11-05T12:19:00.722743: step 345, loss 0.129713, acc 0.96875\n",
      "2017-11-05T12:19:04.922227: step 346, loss 1.37949, acc 0.71875\n",
      "2017-11-05T12:19:09.102697: step 347, loss 0.8986, acc 0.8125\n",
      "2017-11-05T12:19:13.241638: step 348, loss 0.191851, acc 0.875\n",
      "2017-11-05T12:19:17.411100: step 349, loss 0.26651, acc 0.90625\n",
      "2017-11-05T12:19:21.620591: step 350, loss 0.540105, acc 0.84375\n",
      "2017-11-05T12:19:25.722006: step 351, loss 0.794826, acc 0.75\n",
      "2017-11-05T12:19:29.935000: step 352, loss 1.02488, acc 0.78125\n",
      "2017-11-05T12:19:34.087950: step 353, loss 0.528659, acc 0.84375\n",
      "2017-11-05T12:19:38.289936: step 354, loss 0.597711, acc 0.875\n",
      "2017-11-05T12:19:42.513937: step 355, loss 0.337419, acc 0.84375\n",
      "2017-11-05T12:19:46.779968: step 356, loss 0.376462, acc 0.90625\n",
      "2017-11-05T12:19:51.101539: step 357, loss 0.579276, acc 0.84375\n",
      "2017-11-05T12:19:55.219465: step 358, loss 0.404664, acc 0.90625\n",
      "2017-11-05T12:19:59.360908: step 359, loss 0.782468, acc 0.90625\n",
      "2017-11-05T12:20:02.313006: step 360, loss 0.346236, acc 0.9\n",
      "2017-11-05T12:20:06.516492: step 361, loss 0.372035, acc 0.96875\n",
      "2017-11-05T12:20:10.599393: step 362, loss 0.205708, acc 0.90625\n",
      "2017-11-05T12:20:14.655901: step 363, loss 0.693029, acc 0.875\n",
      "2017-11-05T12:20:18.732006: step 364, loss 0.162334, acc 0.9375\n",
      "2017-11-05T12:20:22.738365: step 365, loss 0.10214, acc 0.9375\n",
      "2017-11-05T12:20:26.759845: step 366, loss 0.642541, acc 0.90625\n",
      "2017-11-05T12:20:31.162110: step 367, loss 0.710914, acc 0.875\n",
      "2017-11-05T12:20:35.483295: step 368, loss 0.251541, acc 0.9375\n",
      "2017-11-05T12:20:39.503947: step 369, loss 0.4241, acc 0.90625\n",
      "2017-11-05T12:20:43.522303: step 370, loss 0.324539, acc 0.875\n",
      "2017-11-05T12:20:47.494846: step 371, loss 0.267851, acc 0.90625\n",
      "2017-11-05T12:20:51.504736: step 372, loss 0.687666, acc 0.84375\n",
      "2017-11-05T12:20:55.522110: step 373, loss 0.644427, acc 0.8125\n",
      "2017-11-05T12:20:59.532460: step 374, loss 0.545182, acc 0.84375\n",
      "2017-11-05T12:21:03.487770: step 375, loss 0.691649, acc 0.8125\n",
      "2017-11-05T12:21:07.518660: step 376, loss 0.475125, acc 0.84375\n",
      "2017-11-05T12:21:11.573144: step 377, loss 0.484562, acc 0.78125\n",
      "2017-11-05T12:21:15.600661: step 378, loss 0.446512, acc 0.875\n",
      "2017-11-05T12:21:19.600657: step 379, loss 0.247171, acc 0.90625\n",
      "2017-11-05T12:21:23.630019: step 380, loss 0.138478, acc 0.90625\n",
      "2017-11-05T12:21:27.637401: step 381, loss 0.482772, acc 0.84375\n",
      "2017-11-05T12:21:31.624627: step 382, loss 0.564584, acc 0.9375\n",
      "2017-11-05T12:21:35.767570: step 383, loss 0.387274, acc 0.875\n",
      "2017-11-05T12:21:39.744896: step 384, loss 0.594866, acc 0.84375\n",
      "2017-11-05T12:21:43.648994: step 385, loss 0.512527, acc 0.90625\n",
      "2017-11-05T12:21:47.664580: step 386, loss 0.332976, acc 0.9375\n",
      "2017-11-05T12:21:51.699332: step 387, loss 0.33203, acc 0.96875\n",
      "2017-11-05T12:21:55.685051: step 388, loss 0.235178, acc 0.90625\n",
      "2017-11-05T12:22:00.209266: step 389, loss 0.405625, acc 0.90625\n",
      "2017-11-05T12:22:04.305676: step 390, loss 0.265891, acc 0.90625\n",
      "2017-11-05T12:22:08.434020: step 391, loss 0.934399, acc 0.8125\n",
      "2017-11-05T12:22:12.548945: step 392, loss 0.470565, acc 0.9375\n",
      "2017-11-05T12:22:16.651860: step 393, loss 0.195823, acc 0.9375\n",
      "2017-11-05T12:22:20.698235: step 394, loss 0.527077, acc 0.8125\n",
      "2017-11-05T12:22:24.800664: step 395, loss 0.221879, acc 0.9375\n",
      "2017-11-05T12:22:27.483571: step 396, loss 0.443518, acc 0.95\n",
      "2017-11-05T12:22:31.483416: step 397, loss 0.292342, acc 0.96875\n",
      "2017-11-05T12:22:35.602844: step 398, loss 0.00994493, acc 1\n",
      "2017-11-05T12:22:39.680642: step 399, loss 0.298317, acc 0.9375\n",
      "2017-11-05T12:22:43.719364: step 400, loss 0.194305, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:22:46.415780: step 400, loss 1.52281, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-400\n",
      "\n",
      "2017-11-05T12:22:52.121478: step 401, loss 0.591574, acc 0.84375\n",
      "2017-11-05T12:22:56.109543: step 402, loss 0.432909, acc 0.90625\n",
      "2017-11-05T12:23:00.138763: step 403, loss 0.533681, acc 0.84375\n",
      "2017-11-05T12:23:04.194308: step 404, loss 0.416702, acc 0.90625\n",
      "2017-11-05T12:23:08.257626: step 405, loss 0.0999528, acc 0.9375\n",
      "2017-11-05T12:23:12.328778: step 406, loss 0.184061, acc 0.9375\n",
      "2017-11-05T12:23:16.298732: step 407, loss 0.567832, acc 0.8125\n",
      "2017-11-05T12:23:20.260391: step 408, loss 0.647873, acc 0.84375\n",
      "2017-11-05T12:23:24.512379: step 409, loss 0.213311, acc 0.9375\n",
      "2017-11-05T12:23:28.493497: step 410, loss 0.437675, acc 0.90625\n",
      "2017-11-05T12:23:32.457778: step 411, loss 0.455165, acc 0.90625\n",
      "2017-11-05T12:23:36.576858: step 412, loss 0.336583, acc 0.84375\n",
      "2017-11-05T12:23:40.595714: step 413, loss 0.264936, acc 0.90625\n",
      "2017-11-05T12:23:44.692821: step 414, loss 0.43963, acc 0.9375\n",
      "2017-11-05T12:23:48.699668: step 415, loss 0.640438, acc 0.78125\n",
      "2017-11-05T12:23:52.771743: step 416, loss 0.400825, acc 0.875\n",
      "2017-11-05T12:23:56.832848: step 417, loss 0.364557, acc 0.90625\n",
      "2017-11-05T12:24:00.947772: step 418, loss 0.280636, acc 0.96875\n",
      "2017-11-05T12:24:05.134747: step 419, loss 0.587027, acc 0.875\n",
      "2017-11-05T12:24:09.217648: step 420, loss 0.582291, acc 0.8125\n",
      "2017-11-05T12:24:13.350084: step 421, loss 0.274844, acc 0.9375\n",
      "2017-11-05T12:24:17.368439: step 422, loss 0.782893, acc 0.875\n",
      "2017-11-05T12:24:21.374515: step 423, loss 0.23369, acc 0.90625\n",
      "2017-11-05T12:24:25.493595: step 424, loss 1.29777, acc 0.71875\n",
      "2017-11-05T12:24:29.586504: step 425, loss 0.284666, acc 0.90625\n",
      "2017-11-05T12:24:33.719439: step 426, loss 0.593083, acc 0.84375\n",
      "2017-11-05T12:24:37.867129: step 427, loss 0.354274, acc 0.90625\n",
      "2017-11-05T12:24:41.882482: step 428, loss 0.703187, acc 0.8125\n",
      "2017-11-05T12:24:45.885826: step 429, loss 0.38792, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T12:24:49.984738: step 430, loss 0.335686, acc 0.875\n",
      "2017-11-05T12:24:54.024108: step 431, loss 0.228779, acc 0.84375\n",
      "2017-11-05T12:24:56.608945: step 432, loss 0.79468, acc 0.85\n",
      "2017-11-05T12:25:00.688344: step 433, loss 0.551107, acc 0.875\n",
      "2017-11-05T12:25:04.735719: step 434, loss 0.350025, acc 0.90625\n",
      "2017-11-05T12:25:08.797605: step 435, loss 0.281447, acc 0.90625\n",
      "2017-11-05T12:25:12.987973: step 436, loss 0.111499, acc 0.96875\n",
      "2017-11-05T12:25:16.977807: step 437, loss 0.587151, acc 0.875\n",
      "2017-11-05T12:25:21.018149: step 438, loss 0.288627, acc 0.9375\n",
      "2017-11-05T12:25:25.020383: step 439, loss 0.0494376, acc 0.96875\n",
      "2017-11-05T12:25:28.998708: step 440, loss 0.351818, acc 0.9375\n",
      "2017-11-05T12:25:33.010067: step 441, loss 0.230606, acc 0.875\n",
      "2017-11-05T12:25:37.343146: step 442, loss 0.181825, acc 0.9375\n",
      "2017-11-05T12:25:41.479277: step 443, loss 0.293663, acc 0.90625\n",
      "2017-11-05T12:25:45.477619: step 444, loss 1.16207, acc 0.875\n",
      "2017-11-05T12:25:49.543924: step 445, loss 0.0190328, acc 1\n",
      "2017-11-05T12:25:53.591747: step 446, loss 0.484981, acc 0.875\n",
      "2017-11-05T12:25:57.557106: step 447, loss 0.101139, acc 0.9375\n",
      "2017-11-05T12:26:01.719063: step 448, loss 0.338658, acc 0.875\n",
      "2017-11-05T12:26:05.729412: step 449, loss 0.381012, acc 0.96875\n",
      "2017-11-05T12:26:09.895873: step 450, loss 0.23536, acc 0.9375\n",
      "2017-11-05T12:26:13.917821: step 451, loss 0.00199685, acc 1\n",
      "2017-11-05T12:26:17.926671: step 452, loss 0.267254, acc 0.90625\n",
      "2017-11-05T12:26:21.917960: step 453, loss 0.589166, acc 0.84375\n",
      "2017-11-05T12:26:25.916194: step 454, loss 0.413631, acc 0.875\n",
      "2017-11-05T12:26:29.949910: step 455, loss 0.306581, acc 0.84375\n",
      "2017-11-05T12:26:34.020966: step 456, loss 0.160803, acc 0.90625\n",
      "2017-11-05T12:26:38.062398: step 457, loss 0.236449, acc 0.90625\n",
      "2017-11-05T12:26:42.119227: step 458, loss 0.0731192, acc 1\n",
      "2017-11-05T12:26:46.151709: step 459, loss 0.789569, acc 0.84375\n",
      "2017-11-05T12:26:50.161447: step 460, loss 0.169812, acc 0.9375\n",
      "2017-11-05T12:26:54.208822: step 461, loss 0.311873, acc 0.9375\n",
      "2017-11-05T12:26:58.248003: step 462, loss 0.256298, acc 0.90625\n",
      "2017-11-05T12:27:02.257109: step 463, loss 0.130986, acc 0.9375\n",
      "2017-11-05T12:27:06.351671: step 464, loss 0.379961, acc 0.90625\n",
      "2017-11-05T12:27:10.359668: step 465, loss 0.53749, acc 0.75\n",
      "2017-11-05T12:27:14.388417: step 466, loss 0.676516, acc 0.8125\n",
      "2017-11-05T12:27:18.380641: step 467, loss 0.28285, acc 0.9375\n",
      "2017-11-05T12:27:20.952469: step 468, loss 0.136612, acc 0.95\n",
      "2017-11-05T12:27:24.980838: step 469, loss 0.112091, acc 0.9375\n",
      "2017-11-05T12:27:28.999192: step 470, loss 0.457531, acc 0.90625\n",
      "2017-11-05T12:27:33.001815: step 471, loss 0.423181, acc 0.875\n",
      "2017-11-05T12:27:37.010318: step 472, loss 0.516716, acc 0.84375\n",
      "2017-11-05T12:27:41.040182: step 473, loss 0.0284879, acc 1\n",
      "2017-11-05T12:27:45.030517: step 474, loss 0.901417, acc 0.84375\n",
      "2017-11-05T12:27:49.097098: step 475, loss 0.991455, acc 0.84375\n",
      "2017-11-05T12:27:53.041789: step 476, loss 0.488145, acc 0.90625\n",
      "2017-11-05T12:27:57.096570: step 477, loss 0.453933, acc 0.9375\n",
      "2017-11-05T12:28:01.125640: step 478, loss 0.329803, acc 0.84375\n",
      "2017-11-05T12:28:05.093285: step 479, loss 0.468159, acc 0.875\n",
      "2017-11-05T12:28:09.132656: step 480, loss 0.43605, acc 0.78125\n",
      "2017-11-05T12:28:13.169727: step 481, loss 0.394727, acc 0.9375\n",
      "2017-11-05T12:28:17.290849: step 482, loss 0.31987, acc 0.90625\n",
      "2017-11-05T12:28:21.389773: step 483, loss 0.23534, acc 0.9375\n",
      "2017-11-05T12:28:25.555122: step 484, loss 0.129403, acc 0.9375\n",
      "2017-11-05T12:28:29.798061: step 485, loss 0.500899, acc 0.84375\n",
      "2017-11-05T12:28:34.000047: step 486, loss 0.264762, acc 0.90625\n",
      "2017-11-05T12:28:38.127479: step 487, loss 0.622275, acc 0.84375\n",
      "2017-11-05T12:28:42.204376: step 488, loss 0.206699, acc 0.90625\n",
      "2017-11-05T12:28:46.188207: step 489, loss 0.199838, acc 0.875\n",
      "2017-11-05T12:28:50.179043: step 490, loss 0.410033, acc 0.875\n",
      "2017-11-05T12:28:54.176395: step 491, loss 0.270553, acc 0.90625\n",
      "2017-11-05T12:28:58.485957: step 492, loss 0.355077, acc 0.90625\n",
      "2017-11-05T12:29:02.608887: step 493, loss 0.16237, acc 0.9375\n",
      "2017-11-05T12:29:06.614898: step 494, loss 0.805069, acc 0.84375\n",
      "2017-11-05T12:29:10.628259: step 495, loss 0.147949, acc 0.9375\n",
      "2017-11-05T12:29:14.753702: step 496, loss 0.357254, acc 0.9375\n",
      "2017-11-05T12:29:18.697041: step 497, loss 0.439485, acc 0.90625\n",
      "2017-11-05T12:29:22.685956: step 498, loss 0.23219, acc 0.875\n",
      "2017-11-05T12:29:26.694962: step 499, loss 0.322958, acc 0.875\n",
      "2017-11-05T12:29:30.715819: step 500, loss 0.20379, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:29:33.393722: step 500, loss 1.40688, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-500\n",
      "\n",
      "2017-11-05T12:29:39.487253: step 501, loss 0.316524, acc 0.84375\n",
      "2017-11-05T12:29:43.459575: step 502, loss 0.592717, acc 0.875\n",
      "2017-11-05T12:29:47.497944: step 503, loss 0.173262, acc 0.9375\n",
      "2017-11-05T12:29:50.092756: step 504, loss 0.600184, acc 0.8\n",
      "2017-11-05T12:29:54.149436: step 505, loss 0.214604, acc 0.90625\n",
      "2017-11-05T12:29:58.218102: step 506, loss 0.0396059, acc 1\n",
      "2017-11-05T12:30:02.614151: step 507, loss 0.0397952, acc 1\n",
      "2017-11-05T12:30:06.699554: step 508, loss 0.354148, acc 0.9375\n",
      "2017-11-05T12:30:10.677730: step 509, loss 0.569062, acc 0.84375\n",
      "2017-11-05T12:30:14.660060: step 510, loss 0.369787, acc 0.875\n",
      "2017-11-05T12:30:18.805005: step 511, loss 0.467275, acc 0.90625\n",
      "2017-11-05T12:30:22.847878: step 512, loss 0.158797, acc 0.96875\n",
      "2017-11-05T12:30:27.006221: step 513, loss 0.0959722, acc 0.96875\n",
      "2017-11-05T12:30:31.069116: step 514, loss 0.350798, acc 0.90625\n",
      "2017-11-05T12:30:35.262095: step 515, loss 0.211587, acc 0.90625\n",
      "2017-11-05T12:30:39.376519: step 516, loss 0.366313, acc 0.9375\n",
      "2017-11-05T12:30:43.423895: step 517, loss 0.218561, acc 0.9375\n",
      "2017-11-05T12:30:47.510297: step 518, loss 0.261056, acc 0.75\n",
      "2017-11-05T12:30:51.559174: step 519, loss 0.310806, acc 0.90625\n",
      "2017-11-05T12:30:55.637573: step 520, loss 0.600221, acc 0.84375\n",
      "2017-11-05T12:30:59.698959: step 521, loss 0.269904, acc 0.9375\n",
      "2017-11-05T12:31:03.817721: step 522, loss 0.54644, acc 0.875\n",
      "2017-11-05T12:31:07.936067: step 523, loss 0.188169, acc 0.9375\n",
      "2017-11-05T12:31:11.997822: step 524, loss 0.786998, acc 0.84375\n",
      "2017-11-05T12:31:16.076228: step 525, loss 0.141837, acc 0.9375\n",
      "2017-11-05T12:31:20.156629: step 526, loss 0.179159, acc 0.9375\n",
      "2017-11-05T12:31:24.263060: step 527, loss 0.24489, acc 0.9375\n",
      "2017-11-05T12:31:28.378985: step 528, loss 0.388536, acc 0.96875\n",
      "2017-11-05T12:31:32.521505: step 529, loss 0.436644, acc 0.8125\n",
      "2017-11-05T12:31:36.669628: step 530, loss 0.615268, acc 0.90625\n",
      "2017-11-05T12:31:40.739020: step 531, loss 0.492475, acc 0.8125\n",
      "2017-11-05T12:31:44.825442: step 532, loss 0.715779, acc 0.8125\n",
      "2017-11-05T12:31:49.072960: step 533, loss 0.688231, acc 0.90625\n",
      "2017-11-05T12:31:53.125385: step 534, loss 0.384331, acc 0.84375\n",
      "2017-11-05T12:31:57.373786: step 535, loss 0.557529, acc 0.84375\n",
      "2017-11-05T12:32:01.525374: step 536, loss 0.353585, acc 0.84375\n",
      "2017-11-05T12:32:05.605785: step 537, loss 0.344898, acc 0.84375\n",
      "2017-11-05T12:32:09.734279: step 538, loss 0.0611343, acc 1\n",
      "2017-11-05T12:32:13.890122: step 539, loss 0.40504, acc 0.90625\n",
      "2017-11-05T12:32:16.603551: step 540, loss 0.355807, acc 0.9\n",
      "2017-11-05T12:32:20.663942: step 541, loss 0.34155, acc 0.84375\n",
      "2017-11-05T12:32:24.719364: step 542, loss 0.658963, acc 0.78125\n",
      "2017-11-05T12:32:29.010414: step 543, loss 0.399203, acc 0.8125\n",
      "2017-11-05T12:32:33.155504: step 544, loss 0.250784, acc 0.875\n",
      "2017-11-05T12:32:37.407525: step 545, loss 0.414177, acc 0.90625\n",
      "2017-11-05T12:32:41.574486: step 546, loss 0.134552, acc 0.96875\n",
      "2017-11-05T12:32:45.616860: step 547, loss 0.260736, acc 0.9375\n",
      "2017-11-05T12:32:49.766307: step 548, loss 0.050118, acc 0.96875\n",
      "2017-11-05T12:32:53.845205: step 549, loss 0.499424, acc 0.875\n",
      "2017-11-05T12:32:57.918601: step 550, loss 0.219543, acc 0.96875\n",
      "2017-11-05T12:33:01.979485: step 551, loss 0.338403, acc 0.90625\n",
      "2017-11-05T12:33:06.109572: step 552, loss 0.139037, acc 0.9375\n",
      "2017-11-05T12:33:10.147441: step 553, loss 0.431061, acc 0.875\n",
      "2017-11-05T12:33:14.145781: step 554, loss 0.333841, acc 0.9375\n",
      "2017-11-05T12:33:18.115482: step 555, loss 0.239754, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T12:33:22.135348: step 556, loss 0.405801, acc 0.90625\n",
      "2017-11-05T12:33:26.285223: step 557, loss 0.288413, acc 0.84375\n",
      "2017-11-05T12:33:30.273212: step 558, loss 0.443639, acc 0.875\n",
      "2017-11-05T12:33:34.327602: step 559, loss 0.589959, acc 0.84375\n",
      "2017-11-05T12:33:38.358967: step 560, loss 0.162933, acc 0.96875\n",
      "2017-11-05T12:33:42.390846: step 561, loss 0.201489, acc 0.9375\n",
      "2017-11-05T12:33:46.402196: step 562, loss 0.265072, acc 0.84375\n",
      "2017-11-05T12:33:50.404431: step 563, loss 0.174517, acc 0.875\n",
      "2017-11-05T12:33:54.470821: step 564, loss 0.506007, acc 0.875\n",
      "2017-11-05T12:33:58.450709: step 565, loss 0.263227, acc 0.90625\n",
      "2017-11-05T12:34:02.537423: step 566, loss 0.501309, acc 0.84375\n",
      "2017-11-05T12:34:06.927054: step 567, loss 0.489086, acc 0.90625\n",
      "2017-11-05T12:34:11.008478: step 568, loss 0.560172, acc 0.875\n",
      "2017-11-05T12:34:15.021830: step 569, loss 0.451965, acc 0.875\n",
      "2017-11-05T12:34:19.004656: step 570, loss 0.2934, acc 0.90625\n",
      "2017-11-05T12:34:23.015431: step 571, loss 0.773316, acc 0.84375\n",
      "2017-11-05T12:34:27.109579: step 572, loss 0.267147, acc 0.90625\n",
      "2017-11-05T12:34:31.126430: step 573, loss 0.263082, acc 0.90625\n",
      "2017-11-05T12:34:35.288550: step 574, loss 0.453464, acc 0.90625\n",
      "2017-11-05T12:34:39.474916: step 575, loss 0.214616, acc 0.90625\n",
      "2017-11-05T12:34:42.095778: step 576, loss 0.68929, acc 0.85\n",
      "2017-11-05T12:34:46.172077: step 577, loss 0.31309, acc 0.90625\n",
      "2017-11-05T12:34:50.190990: step 578, loss 0.187589, acc 0.9375\n",
      "2017-11-05T12:34:54.223855: step 579, loss 0.192918, acc 0.9375\n",
      "2017-11-05T12:34:58.308885: step 580, loss 0.277406, acc 0.90625\n",
      "2017-11-05T12:35:02.338748: step 581, loss 0.272178, acc 0.875\n",
      "2017-11-05T12:35:06.510325: step 582, loss 0.102429, acc 0.96875\n",
      "2017-11-05T12:35:10.630753: step 583, loss 0.459294, acc 0.90625\n",
      "2017-11-05T12:35:14.719296: step 584, loss 0.313277, acc 0.875\n",
      "2017-11-05T12:35:18.770555: step 585, loss 0.149187, acc 0.96875\n",
      "2017-11-05T12:35:22.837394: step 586, loss 0.155647, acc 0.9375\n",
      "2017-11-05T12:35:26.936812: step 587, loss 0.329594, acc 0.875\n",
      "2017-11-05T12:35:31.017225: step 588, loss 0.153716, acc 0.96875\n",
      "2017-11-05T12:35:35.137101: step 589, loss 0.0789823, acc 0.9375\n",
      "2017-11-05T12:35:39.236013: step 590, loss 0.470264, acc 0.875\n",
      "2017-11-05T12:35:43.214340: step 591, loss 0.53562, acc 0.90625\n",
      "2017-11-05T12:35:47.208294: step 592, loss 0.409432, acc 0.875\n",
      "2017-11-05T12:35:51.211503: step 593, loss 0.277757, acc 0.875\n",
      "2017-11-05T12:35:55.223124: step 594, loss 0.156492, acc 0.9375\n",
      "2017-11-05T12:35:59.198296: step 595, loss 0.145664, acc 0.96875\n",
      "2017-11-05T12:36:03.263990: step 596, loss 0.258745, acc 0.90625\n",
      "2017-11-05T12:36:07.281463: step 597, loss 0.222679, acc 0.96875\n",
      "2017-11-05T12:36:11.315561: step 598, loss 0.496107, acc 0.875\n",
      "2017-11-05T12:36:15.350811: step 599, loss 0.475512, acc 0.90625\n",
      "2017-11-05T12:36:19.339145: step 600, loss 0.306071, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:36:21.923481: step 600, loss 1.35502, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-600\n",
      "\n",
      "2017-11-05T12:36:27.241084: step 601, loss 0.756845, acc 0.875\n",
      "2017-11-05T12:36:31.279669: step 602, loss 0.0699016, acc 0.96875\n",
      "2017-11-05T12:36:35.433120: step 603, loss 0.388581, acc 0.90625\n",
      "2017-11-05T12:36:39.454452: step 604, loss 0.380585, acc 0.90625\n",
      "2017-11-05T12:36:43.472633: step 605, loss 0.484836, acc 0.875\n",
      "2017-11-05T12:36:47.498724: step 606, loss 0.0613774, acc 0.96875\n",
      "2017-11-05T12:36:51.544643: step 607, loss 0.387331, acc 0.90625\n",
      "2017-11-05T12:36:55.602267: step 608, loss 0.683318, acc 0.90625\n",
      "2017-11-05T12:36:59.681165: step 609, loss 0.217753, acc 0.90625\n",
      "2017-11-05T12:37:03.763566: step 610, loss 0.52678, acc 0.8125\n",
      "2017-11-05T12:37:07.873486: step 611, loss 0.275542, acc 0.9375\n",
      "2017-11-05T12:37:10.543981: step 612, loss 0.31083, acc 0.95\n",
      "2017-11-05T12:37:14.554139: step 613, loss 0.398541, acc 0.9375\n",
      "2017-11-05T12:37:18.601960: step 614, loss 0.0635328, acc 0.96875\n",
      "2017-11-05T12:37:22.578786: step 615, loss 0.353548, acc 0.90625\n",
      "2017-11-05T12:37:26.580391: step 616, loss 0.406809, acc 0.90625\n",
      "2017-11-05T12:37:30.480789: step 617, loss 0.360858, acc 0.84375\n",
      "2017-11-05T12:37:34.481180: step 618, loss 0.412465, acc 0.9375\n",
      "2017-11-05T12:37:38.452944: step 619, loss 0.177835, acc 0.90625\n",
      "2017-11-05T12:37:42.431270: step 620, loss 0.378983, acc 0.84375\n",
      "2017-11-05T12:37:46.431113: step 621, loss 0.499915, acc 0.78125\n",
      "2017-11-05T12:37:50.439961: step 622, loss 0.270862, acc 0.9375\n",
      "2017-11-05T12:37:54.529759: step 623, loss 0.320965, acc 0.90625\n",
      "2017-11-05T12:37:58.531111: step 624, loss 0.123309, acc 0.9375\n",
      "2017-11-05T12:38:02.564145: step 625, loss 0.104422, acc 0.9375\n",
      "2017-11-05T12:38:06.597983: step 626, loss 0.172552, acc 0.96875\n",
      "2017-11-05T12:38:10.658477: step 627, loss 0.634535, acc 0.78125\n",
      "2017-11-05T12:38:14.745575: step 628, loss 0.259591, acc 0.9375\n",
      "2017-11-05T12:38:18.749769: step 629, loss 0.294112, acc 0.9375\n",
      "2017-11-05T12:38:22.856687: step 630, loss 0.350177, acc 0.90625\n",
      "2017-11-05T12:38:27.078687: step 631, loss 0.157988, acc 0.96875\n",
      "2017-11-05T12:38:31.093039: step 632, loss 0.216179, acc 0.90625\n",
      "2017-11-05T12:38:35.273009: step 633, loss 0.607056, acc 0.84375\n",
      "2017-11-05T12:38:39.321773: step 634, loss 0.22076, acc 0.9375\n",
      "2017-11-05T12:38:43.360143: step 635, loss 0.244901, acc 0.90625\n",
      "2017-11-05T12:38:47.412497: step 636, loss 0.533935, acc 0.90625\n",
      "2017-11-05T12:38:51.383568: step 637, loss 0.369261, acc 0.90625\n",
      "2017-11-05T12:38:55.499208: step 638, loss 0.0991947, acc 0.9375\n",
      "2017-11-05T12:38:59.574076: step 639, loss 0.335685, acc 0.875\n",
      "2017-11-05T12:39:03.596324: step 640, loss 0.271134, acc 0.9375\n",
      "2017-11-05T12:39:07.751425: step 641, loss 0.640575, acc 0.90625\n",
      "2017-11-05T12:39:11.799100: step 642, loss 0.419228, acc 0.875\n",
      "2017-11-05T12:39:15.792938: step 643, loss 0.22348, acc 0.90625\n",
      "2017-11-05T12:39:19.790665: step 644, loss 0.188833, acc 0.90625\n",
      "2017-11-05T12:39:23.957756: step 645, loss 0.0676638, acc 0.96875\n",
      "2017-11-05T12:39:27.957793: step 646, loss 0.269709, acc 0.90625\n",
      "2017-11-05T12:39:31.966140: step 647, loss 0.0929518, acc 0.9375\n",
      "2017-11-05T12:39:34.622528: step 648, loss 0.0180917, acc 1\n",
      "2017-11-05T12:39:39.004642: step 649, loss 0.168999, acc 0.90625\n",
      "2017-11-05T12:39:43.101595: step 650, loss 0.760927, acc 0.84375\n",
      "2017-11-05T12:39:47.131231: step 651, loss 0.589781, acc 0.71875\n",
      "2017-11-05T12:39:51.145582: step 652, loss 0.711835, acc 0.8125\n",
      "2017-11-05T12:39:55.155939: step 653, loss 0.46974, acc 0.875\n",
      "2017-11-05T12:39:59.219914: step 654, loss 0.275133, acc 0.90625\n",
      "2017-11-05T12:40:03.492450: step 655, loss 0.288502, acc 0.875\n",
      "2017-11-05T12:40:07.567380: step 656, loss 0.166483, acc 0.9375\n",
      "2017-11-05T12:40:11.589511: step 657, loss 0.37932, acc 0.8125\n",
      "2017-11-05T12:40:15.648395: step 658, loss 0.139249, acc 0.96875\n",
      "2017-11-05T12:40:19.670253: step 659, loss 0.15874, acc 0.9375\n",
      "2017-11-05T12:40:23.736690: step 660, loss 0.213381, acc 0.9375\n",
      "2017-11-05T12:40:27.747022: step 661, loss 0.615094, acc 0.78125\n",
      "2017-11-05T12:40:31.743567: step 662, loss 0.200715, acc 0.9375\n",
      "2017-11-05T12:40:35.940552: step 663, loss 0.144472, acc 0.90625\n",
      "2017-11-05T12:40:40.005949: step 664, loss 0.255687, acc 0.90625\n",
      "2017-11-05T12:40:44.052125: step 665, loss 0.376064, acc 0.90625\n",
      "2017-11-05T12:40:48.123518: step 666, loss 0.608237, acc 0.875\n",
      "2017-11-05T12:40:52.215926: step 667, loss 0.246065, acc 0.90625\n",
      "2017-11-05T12:40:56.220772: step 668, loss 0.149374, acc 0.90625\n",
      "2017-11-05T12:41:00.154959: step 669, loss 0.595195, acc 0.8125\n",
      "2017-11-05T12:41:04.190327: step 670, loss 0.157807, acc 0.9375\n",
      "2017-11-05T12:41:08.144637: step 671, loss 0.0189485, acc 1\n",
      "2017-11-05T12:41:13.062521: step 672, loss 0.330819, acc 0.875\n",
      "2017-11-05T12:41:17.610251: step 673, loss 0.160761, acc 0.90625\n",
      "2017-11-05T12:41:21.664148: step 674, loss 0.403024, acc 0.875\n",
      "2017-11-05T12:41:25.679142: step 675, loss 0.380605, acc 0.875\n",
      "2017-11-05T12:41:29.656486: step 676, loss 0.31266, acc 0.875\n",
      "2017-11-05T12:41:33.600243: step 677, loss 0.0934735, acc 0.96875\n",
      "2017-11-05T12:41:37.531306: step 678, loss 0.152258, acc 0.9375\n",
      "2017-11-05T12:41:41.482997: step 679, loss 0.673587, acc 0.8125\n",
      "2017-11-05T12:41:45.528027: step 680, loss 0.152683, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T12:41:49.596919: step 681, loss 0.171555, acc 0.9375\n",
      "2017-11-05T12:41:53.596761: step 682, loss 0.285067, acc 0.96875\n",
      "2017-11-05T12:41:57.617118: step 683, loss 0.564993, acc 0.875\n",
      "2017-11-05T12:42:00.190946: step 684, loss 0.077326, acc 0.95\n",
      "2017-11-05T12:42:04.254834: step 685, loss 0.26582, acc 0.875\n",
      "2017-11-05T12:42:08.268618: step 686, loss 0.167903, acc 0.90625\n",
      "2017-11-05T12:42:12.223857: step 687, loss 0.246901, acc 0.90625\n",
      "2017-11-05T12:42:16.232676: step 688, loss 0.32807, acc 0.9375\n",
      "2017-11-05T12:42:20.270544: step 689, loss 0.0300625, acc 1\n",
      "2017-11-05T12:42:24.282107: step 690, loss 0.642986, acc 0.8125\n",
      "2017-11-05T12:42:28.323958: step 691, loss 0.211735, acc 0.90625\n",
      "2017-11-05T12:42:32.410862: step 692, loss 0.27999, acc 0.90625\n",
      "2017-11-05T12:42:36.541776: step 693, loss 0.658613, acc 0.78125\n",
      "2017-11-05T12:42:40.545769: step 694, loss 0.475107, acc 0.8125\n",
      "2017-11-05T12:42:44.474400: step 695, loss 0.158424, acc 0.90625\n",
      "2017-11-05T12:42:48.441315: step 696, loss 0.414152, acc 0.875\n",
      "2017-11-05T12:42:52.480916: step 697, loss 0.402418, acc 0.84375\n",
      "2017-11-05T12:42:56.495951: step 698, loss 0.187828, acc 0.90625\n",
      "2017-11-05T12:43:00.500797: step 699, loss 0.228123, acc 0.90625\n",
      "2017-11-05T12:43:04.475622: step 700, loss 0.225295, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:43:07.169422: step 700, loss 1.25691, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-700\n",
      "\n",
      "2017-11-05T12:43:12.681783: step 701, loss 0.180878, acc 0.90625\n",
      "2017-11-05T12:43:16.671046: step 702, loss 0.301098, acc 0.875\n",
      "2017-11-05T12:43:20.738119: step 703, loss 0.315775, acc 0.875\n",
      "2017-11-05T12:43:25.017169: step 704, loss 0.17756, acc 0.96875\n",
      "2017-11-05T12:43:29.153107: step 705, loss 0.651161, acc 0.84375\n",
      "2017-11-05T12:43:33.156397: step 706, loss 0.342656, acc 0.875\n",
      "2017-11-05T12:43:37.138608: step 707, loss 0.332981, acc 0.90625\n",
      "2017-11-05T12:43:41.147464: step 708, loss 0.397391, acc 0.90625\n",
      "2017-11-05T12:43:45.148317: step 709, loss 0.0355239, acc 0.96875\n",
      "2017-11-05T12:43:49.226009: step 710, loss 0.307657, acc 0.9375\n",
      "2017-11-05T12:43:53.279460: step 711, loss 0.273105, acc 0.9375\n",
      "2017-11-05T12:43:57.838251: step 712, loss 0.331658, acc 0.90625\n",
      "2017-11-05T12:44:01.947238: step 713, loss 0.110562, acc 0.9375\n",
      "2017-11-05T12:44:05.993779: step 714, loss 0.25915, acc 0.84375\n",
      "2017-11-05T12:44:10.030105: step 715, loss 0.964653, acc 0.71875\n",
      "2017-11-05T12:44:14.244600: step 716, loss 0.0505305, acc 0.96875\n",
      "2017-11-05T12:44:18.461596: step 717, loss 0.0827469, acc 0.96875\n",
      "2017-11-05T12:44:22.660463: step 718, loss 0.162341, acc 0.9375\n",
      "2017-11-05T12:44:26.726396: step 719, loss 0.0350908, acc 1\n",
      "2017-11-05T12:44:29.430338: step 720, loss 0.516472, acc 0.9\n",
      "2017-11-05T12:44:33.567278: step 721, loss 0.590285, acc 0.90625\n",
      "2017-11-05T12:44:37.842018: step 722, loss 0.439682, acc 0.90625\n",
      "2017-11-05T12:44:42.057231: step 723, loss 0.277108, acc 0.9375\n",
      "2017-11-05T12:44:46.150578: step 724, loss 0.135438, acc 0.96875\n",
      "2017-11-05T12:44:50.193951: step 725, loss 0.243586, acc 0.875\n",
      "2017-11-05T12:44:54.321285: step 726, loss 0.201701, acc 0.875\n",
      "2017-11-05T12:44:58.382171: step 727, loss 0.0467102, acc 0.96875\n",
      "2017-11-05T12:45:02.444557: step 728, loss 0.539427, acc 0.8125\n",
      "2017-11-05T12:45:06.541097: step 729, loss 0.221307, acc 0.90625\n",
      "2017-11-05T12:45:10.647454: step 730, loss 0.0809273, acc 0.9375\n",
      "2017-11-05T12:45:14.767882: step 731, loss 0.149779, acc 0.9375\n",
      "2017-11-05T12:45:18.910826: step 732, loss 0.355257, acc 0.875\n",
      "2017-11-05T12:45:23.097140: step 733, loss 0.153138, acc 0.9375\n",
      "2017-11-05T12:45:27.255625: step 734, loss 0.500657, acc 0.90625\n",
      "2017-11-05T12:45:31.428591: step 735, loss 0.182265, acc 0.96875\n",
      "2017-11-05T12:45:35.584543: step 736, loss 0.446305, acc 0.84375\n",
      "2017-11-05T12:45:39.691461: step 737, loss 0.117603, acc 0.96875\n",
      "2017-11-05T12:45:43.792375: step 738, loss 0.110707, acc 0.9375\n",
      "2017-11-05T12:45:47.944325: step 739, loss 0.17048, acc 0.96875\n",
      "2017-11-05T12:45:52.015217: step 740, loss 0.402514, acc 0.90625\n",
      "2017-11-05T12:45:56.184180: step 741, loss 0.14052, acc 0.9375\n",
      "2017-11-05T12:46:00.243564: step 742, loss 0.180988, acc 0.9375\n",
      "2017-11-05T12:46:04.354073: step 743, loss 0.593238, acc 0.84375\n",
      "2017-11-05T12:46:08.484617: step 744, loss 0.309844, acc 0.875\n",
      "2017-11-05T12:46:12.562708: step 745, loss 0.20463, acc 0.9375\n",
      "2017-11-05T12:46:16.691833: step 746, loss 0.0839786, acc 0.96875\n",
      "2017-11-05T12:46:20.751498: step 747, loss 0.455431, acc 0.78125\n",
      "2017-11-05T12:46:24.850021: step 748, loss 0.65086, acc 0.8125\n",
      "2017-11-05T12:46:28.914263: step 749, loss 0.318806, acc 0.90625\n",
      "2017-11-05T12:46:33.077721: step 750, loss 0.217553, acc 0.9375\n",
      "2017-11-05T12:46:37.436318: step 751, loss 0.532806, acc 0.84375\n",
      "2017-11-05T12:46:41.562425: step 752, loss 0.115509, acc 0.9375\n",
      "2017-11-05T12:46:45.687856: step 753, loss 0.334542, acc 0.875\n",
      "2017-11-05T12:46:49.821293: step 754, loss 0.749769, acc 0.71875\n",
      "2017-11-05T12:46:53.915702: step 755, loss 0.403033, acc 0.84375\n",
      "2017-11-05T12:46:56.588101: step 756, loss 0.402333, acc 0.8\n",
      "2017-11-05T12:47:00.657493: step 757, loss 0.322851, acc 0.90625\n",
      "2017-11-05T12:47:04.751902: step 758, loss 0.2333, acc 0.90625\n",
      "2017-11-05T12:47:08.855318: step 759, loss 0.293399, acc 0.9375\n",
      "2017-11-05T12:47:12.985253: step 760, loss 0.59959, acc 0.875\n",
      "2017-11-05T12:47:17.058146: step 761, loss 0.166769, acc 0.96875\n",
      "2017-11-05T12:47:21.140046: step 762, loss 0.0958023, acc 0.9375\n",
      "2017-11-05T12:47:25.283490: step 763, loss 0.235212, acc 0.9375\n",
      "2017-11-05T12:47:29.331492: step 764, loss 0.25956, acc 0.875\n",
      "2017-11-05T12:47:33.417896: step 765, loss 0.257184, acc 0.90625\n",
      "2017-11-05T12:47:37.542827: step 766, loss 0.0942072, acc 0.9375\n",
      "2017-11-05T12:47:41.678265: step 767, loss 0.274992, acc 0.9375\n",
      "2017-11-05T12:47:45.764168: step 768, loss 0.404954, acc 0.84375\n",
      "2017-11-05T12:47:49.952645: step 769, loss 0.159915, acc 0.9375\n",
      "2017-11-05T12:47:54.025038: step 770, loss 0.265412, acc 0.84375\n",
      "2017-11-05T12:47:58.111502: step 771, loss 0.278612, acc 0.875\n",
      "2017-11-05T12:48:02.220421: step 772, loss 0.714373, acc 0.78125\n",
      "2017-11-05T12:48:06.347854: step 773, loss 0.231378, acc 0.90625\n",
      "2017-11-05T12:48:10.412243: step 774, loss 0.083971, acc 0.96875\n",
      "2017-11-05T12:48:14.484221: step 775, loss 0.074164, acc 0.96875\n",
      "2017-11-05T12:48:18.566690: step 776, loss 0.0478549, acc 0.96875\n",
      "2017-11-05T12:48:22.764172: step 777, loss 0.29375, acc 0.9375\n",
      "2017-11-05T12:48:26.921626: step 778, loss 0.184101, acc 0.9375\n",
      "2017-11-05T12:48:31.029433: step 779, loss 0.226061, acc 0.875\n",
      "2017-11-05T12:48:35.182885: step 780, loss 0.435241, acc 0.9375\n",
      "2017-11-05T12:48:39.365125: step 781, loss 0.684092, acc 0.875\n",
      "2017-11-05T12:48:43.562608: step 782, loss 0.198415, acc 0.96875\n",
      "2017-11-05T12:48:47.786304: step 783, loss 0.0267872, acc 1\n",
      "2017-11-05T12:48:51.819670: step 784, loss 0.257387, acc 0.9375\n",
      "2017-11-05T12:48:56.046173: step 785, loss 0.122452, acc 0.96875\n",
      "2017-11-05T12:49:00.232163: step 786, loss 0.417638, acc 0.90625\n",
      "2017-11-05T12:49:04.493781: step 787, loss 0.556814, acc 0.78125\n",
      "2017-11-05T12:49:08.861885: step 788, loss 0.267484, acc 0.875\n",
      "2017-11-05T12:49:12.971805: step 789, loss 0.0534048, acc 0.96875\n",
      "2017-11-05T12:49:17.163784: step 790, loss 0.693291, acc 0.78125\n",
      "2017-11-05T12:49:21.333748: step 791, loss 0.0700107, acc 0.96875\n",
      "2017-11-05T12:49:24.058683: step 792, loss 0.170829, acc 0.95\n",
      "2017-11-05T12:49:28.218639: step 793, loss 0.618326, acc 0.84375\n",
      "2017-11-05T12:49:32.370089: step 794, loss 0.228984, acc 0.9375\n",
      "2017-11-05T12:49:36.734689: step 795, loss 0.117977, acc 0.96875\n",
      "2017-11-05T12:49:40.909156: step 796, loss 0.127177, acc 0.96875\n",
      "2017-11-05T12:49:45.249740: step 797, loss 0.374564, acc 0.84375\n",
      "2017-11-05T12:49:49.563400: step 798, loss 0.367626, acc 0.84375\n",
      "2017-11-05T12:49:53.721855: step 799, loss 0.353367, acc 0.84375\n",
      "2017-11-05T12:49:57.788327: step 800, loss 0.348442, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:50:00.458614: step 800, loss 1.19263, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-800\n",
      "\n",
      "2017-11-05T12:50:06.171445: step 801, loss 0.538135, acc 0.875\n",
      "2017-11-05T12:50:10.154775: step 802, loss 0.357487, acc 0.84375\n",
      "2017-11-05T12:50:14.171140: step 803, loss 0.050585, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T12:50:18.184939: step 804, loss 0.33826, acc 0.9375\n",
      "2017-11-05T12:50:22.175284: step 805, loss 0.422445, acc 0.8125\n",
      "2017-11-05T12:50:26.152110: step 806, loss 0.0346961, acc 1\n",
      "2017-11-05T12:50:30.172968: step 807, loss 0.320023, acc 0.84375\n",
      "2017-11-05T12:50:34.352436: step 808, loss 0.294705, acc 0.875\n",
      "2017-11-05T12:50:38.404202: step 809, loss 0.448048, acc 0.8125\n",
      "2017-11-05T12:50:42.459634: step 810, loss 0.249452, acc 0.9375\n",
      "2017-11-05T12:50:46.455885: step 811, loss 0.150734, acc 0.9375\n",
      "2017-11-05T12:50:50.529484: step 812, loss 0.0956317, acc 0.96875\n",
      "2017-11-05T12:50:54.508812: step 813, loss 0.142338, acc 0.96875\n",
      "2017-11-05T12:50:58.541208: step 814, loss 0.339345, acc 0.8125\n",
      "2017-11-05T12:51:02.590986: step 815, loss 0.0414423, acc 1\n",
      "2017-11-05T12:51:06.585341: step 816, loss 0.184114, acc 0.9375\n",
      "2017-11-05T12:51:10.687756: step 817, loss 0.418121, acc 0.90625\n",
      "2017-11-05T12:51:15.268010: step 818, loss 0.390143, acc 0.90625\n",
      "2017-11-05T12:51:19.280770: step 819, loss 0.131341, acc 0.9375\n",
      "2017-11-05T12:51:23.257497: step 820, loss 0.00835314, acc 1\n",
      "2017-11-05T12:51:27.331414: step 821, loss 0.0110106, acc 1\n",
      "2017-11-05T12:51:31.288328: step 822, loss 0.333643, acc 0.875\n",
      "2017-11-05T12:51:35.319693: step 823, loss 0.0771175, acc 0.96875\n",
      "2017-11-05T12:51:39.275019: step 824, loss 0.0339949, acc 1\n",
      "2017-11-05T12:51:43.293374: step 825, loss 0.10815, acc 0.96875\n",
      "2017-11-05T12:51:47.368770: step 826, loss 0.338143, acc 0.875\n",
      "2017-11-05T12:51:51.410641: step 827, loss 0.137485, acc 0.9375\n",
      "2017-11-05T12:51:53.921425: step 828, loss 0.278773, acc 0.9\n",
      "2017-11-05T12:51:57.940302: step 829, loss 0.144225, acc 0.9375\n",
      "2017-11-05T12:52:01.975048: step 830, loss 0.0788515, acc 0.96875\n",
      "2017-11-05T12:52:05.971888: step 831, loss 0.233989, acc 0.9375\n",
      "2017-11-05T12:52:09.970228: step 832, loss 0.0830981, acc 0.96875\n",
      "2017-11-05T12:52:13.971586: step 833, loss 0.528572, acc 0.84375\n",
      "2017-11-05T12:52:17.990443: step 834, loss 0.131359, acc 0.96875\n",
      "2017-11-05T12:52:22.031217: step 835, loss 0.0369291, acc 1\n",
      "2017-11-05T12:52:25.962395: step 836, loss 0.200116, acc 0.96875\n",
      "2017-11-05T12:52:30.072707: step 837, loss 0.249672, acc 0.90625\n",
      "2017-11-05T12:52:34.214847: step 838, loss 0.154804, acc 0.96875\n",
      "2017-11-05T12:52:38.315762: step 839, loss 0.296663, acc 0.90625\n",
      "2017-11-05T12:52:42.326828: step 840, loss 0.353568, acc 0.8125\n",
      "2017-11-05T12:52:46.310040: step 841, loss 0.297679, acc 0.875\n",
      "2017-11-05T12:52:50.282966: step 842, loss 0.255477, acc 0.90625\n",
      "2017-11-05T12:52:54.386269: step 843, loss 0.250285, acc 0.875\n",
      "2017-11-05T12:52:58.393139: step 844, loss 0.365248, acc 0.875\n",
      "2017-11-05T12:53:02.392480: step 845, loss 0.414533, acc 0.90625\n",
      "2017-11-05T12:53:06.488390: step 846, loss 0.222202, acc 0.90625\n",
      "2017-11-05T12:53:10.515253: step 847, loss 0.406388, acc 0.875\n",
      "2017-11-05T12:53:14.572903: step 848, loss 0.348328, acc 0.8125\n",
      "2017-11-05T12:53:18.685325: step 849, loss 0.437129, acc 0.78125\n",
      "2017-11-05T12:53:22.775232: step 850, loss 0.126653, acc 0.96875\n",
      "2017-11-05T12:53:26.994745: step 851, loss 0.0387482, acc 0.96875\n",
      "2017-11-05T12:53:30.991877: step 852, loss 0.263043, acc 0.90625\n",
      "2017-11-05T12:53:35.037347: step 853, loss 0.159639, acc 0.9375\n",
      "2017-11-05T12:53:39.101767: step 854, loss 0.344406, acc 0.875\n",
      "2017-11-05T12:53:43.173117: step 855, loss 0.465265, acc 0.84375\n",
      "2017-11-05T12:53:47.214310: step 856, loss 0.232411, acc 0.90625\n",
      "2017-11-05T12:53:51.259510: step 857, loss 0.226588, acc 0.9375\n",
      "2017-11-05T12:53:55.366225: step 858, loss 0.263536, acc 0.90625\n",
      "2017-11-05T12:53:59.446779: step 859, loss 0.375809, acc 0.84375\n",
      "2017-11-05T12:54:03.466135: step 860, loss 0.370202, acc 0.875\n",
      "2017-11-05T12:54:07.455469: step 861, loss 0.396477, acc 0.90625\n",
      "2017-11-05T12:54:11.522888: step 862, loss 0.0516955, acc 0.96875\n",
      "2017-11-05T12:54:15.548449: step 863, loss 0.22486, acc 0.90625\n",
      "2017-11-05T12:54:18.128257: step 864, loss 0.41581, acc 0.95\n",
      "2017-11-05T12:54:22.266963: step 865, loss 0.159308, acc 0.9375\n",
      "2017-11-05T12:54:26.366876: step 866, loss 0.282086, acc 0.9375\n",
      "2017-11-05T12:54:30.477297: step 867, loss 0.190874, acc 0.875\n",
      "2017-11-05T12:54:34.558698: step 868, loss 0.354558, acc 0.875\n",
      "2017-11-05T12:54:38.673120: step 869, loss 0.27602, acc 0.84375\n",
      "2017-11-05T12:54:42.625929: step 870, loss 0.372803, acc 0.90625\n",
      "2017-11-05T12:54:46.720338: step 871, loss 0.438136, acc 0.84375\n",
      "2017-11-05T12:54:50.709673: step 872, loss 0.37562, acc 0.875\n",
      "2017-11-05T12:54:54.784568: step 873, loss 0.118289, acc 0.9375\n",
      "2017-11-05T12:54:58.835947: step 874, loss 0.474414, acc 0.90625\n",
      "2017-11-05T12:55:02.844295: step 875, loss 0.205177, acc 0.90625\n",
      "2017-11-05T12:55:07.021263: step 876, loss 0.421246, acc 0.875\n",
      "2017-11-05T12:55:11.049984: step 877, loss 0.287171, acc 0.9375\n",
      "2017-11-05T12:55:15.102144: step 878, loss 0.157279, acc 0.96875\n",
      "2017-11-05T12:55:19.176039: step 879, loss 0.373701, acc 0.84375\n",
      "2017-11-05T12:55:23.127346: step 880, loss 0.157539, acc 0.96875\n",
      "2017-11-05T12:55:27.181728: step 881, loss 0.034461, acc 1\n",
      "2017-11-05T12:55:31.185072: step 882, loss 0.218344, acc 0.9375\n",
      "2017-11-05T12:55:35.227945: step 883, loss 0.174479, acc 0.90625\n",
      "2017-11-05T12:55:39.240295: step 884, loss 0.259337, acc 0.875\n",
      "2017-11-05T12:55:43.327199: step 885, loss 0.370259, acc 0.875\n",
      "2017-11-05T12:55:47.427664: step 886, loss 0.385054, acc 0.84375\n",
      "2017-11-05T12:55:51.512066: step 887, loss 0.19266, acc 0.9375\n",
      "2017-11-05T12:55:55.592144: step 888, loss 0.273745, acc 0.96875\n",
      "2017-11-05T12:55:59.622008: step 889, loss 0.023778, acc 1\n",
      "2017-11-05T12:56:03.635360: step 890, loss 0.167442, acc 0.9375\n",
      "2017-11-05T12:56:07.726266: step 891, loss 0.0988005, acc 0.96875\n",
      "2017-11-05T12:56:11.822676: step 892, loss 0.125195, acc 0.9375\n",
      "2017-11-05T12:56:15.930596: step 893, loss 0.433004, acc 0.9375\n",
      "2017-11-05T12:56:19.937443: step 894, loss 0.615748, acc 0.8125\n",
      "2017-11-05T12:56:24.000778: step 895, loss 0.104009, acc 0.96875\n",
      "2017-11-05T12:56:28.042960: step 896, loss 0.241894, acc 0.875\n",
      "2017-11-05T12:56:32.079327: step 897, loss 0.205546, acc 0.9375\n",
      "2017-11-05T12:56:36.210764: step 898, loss 0.199685, acc 0.90625\n",
      "2017-11-05T12:56:40.209605: step 899, loss 0.569671, acc 0.84375\n",
      "2017-11-05T12:56:42.815957: step 900, loss 0.0247475, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T12:56:45.508870: step 900, loss 0.901624, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-900\n",
      "\n",
      "2017-11-05T12:56:50.905686: step 901, loss 0.133245, acc 0.90625\n",
      "2017-11-05T12:56:54.972270: step 902, loss 0.136598, acc 0.96875\n",
      "2017-11-05T12:56:59.023735: step 903, loss 0.155069, acc 0.90625\n",
      "2017-11-05T12:57:03.064106: step 904, loss 0.369067, acc 0.875\n",
      "2017-11-05T12:57:07.200045: step 905, loss 0.0476382, acc 1\n",
      "2017-11-05T12:57:11.303961: step 906, loss 0.113839, acc 0.9375\n",
      "2017-11-05T12:57:15.387362: step 907, loss 0.15195, acc 0.96875\n",
      "2017-11-05T12:57:19.488459: step 908, loss 0.343526, acc 0.90625\n",
      "2017-11-05T12:57:23.554364: step 909, loss 0.41859, acc 0.84375\n",
      "2017-11-05T12:57:27.590231: step 910, loss 0.396328, acc 0.78125\n",
      "2017-11-05T12:57:31.640609: step 911, loss 0.389758, acc 0.875\n",
      "2017-11-05T12:57:35.734518: step 912, loss 0.401376, acc 0.84375\n",
      "2017-11-05T12:57:39.845439: step 913, loss 0.527931, acc 0.78125\n",
      "2017-11-05T12:57:43.866796: step 914, loss 0.511447, acc 0.875\n",
      "2017-11-05T12:57:47.975216: step 915, loss 0.251235, acc 0.90625\n",
      "2017-11-05T12:57:52.108653: step 916, loss 0.101447, acc 0.9375\n",
      "2017-11-05T12:57:56.197905: step 917, loss 0.240587, acc 0.84375\n",
      "2017-11-05T12:58:00.348044: step 918, loss 0.189089, acc 0.90625\n",
      "2017-11-05T12:58:04.454962: step 919, loss 0.563242, acc 0.84375\n",
      "2017-11-05T12:58:08.557729: step 920, loss 0.312333, acc 0.875\n",
      "2017-11-05T12:58:12.600102: step 921, loss 0.0582456, acc 0.96875\n",
      "2017-11-05T12:58:16.732037: step 922, loss 0.22797, acc 0.9375\n",
      "2017-11-05T12:58:20.766456: step 923, loss 0.417304, acc 0.875\n",
      "2017-11-05T12:58:25.109761: step 924, loss 0.0273577, acc 0.96875\n",
      "2017-11-05T12:58:29.372289: step 925, loss 0.298338, acc 0.9375\n",
      "2017-11-05T12:58:33.853361: step 926, loss 0.283772, acc 0.84375\n",
      "2017-11-05T12:58:38.105431: step 927, loss 0.00395167, acc 1\n",
      "2017-11-05T12:58:42.212225: step 928, loss 0.189558, acc 0.90625\n",
      "2017-11-05T12:58:46.259409: step 929, loss 0.549209, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T12:58:50.261753: step 930, loss 0.348167, acc 0.875\n",
      "2017-11-05T12:58:54.364668: step 931, loss 0.571143, acc 0.75\n",
      "2017-11-05T12:58:58.388331: step 932, loss 0.25808, acc 0.84375\n",
      "2017-11-05T12:59:02.429703: step 933, loss 0.14233, acc 0.9375\n",
      "2017-11-05T12:59:06.618180: step 934, loss 0.0597177, acc 0.96875\n",
      "2017-11-05T12:59:10.696580: step 935, loss 0.168825, acc 0.90625\n",
      "2017-11-05T12:59:13.381935: step 936, loss 0.0370832, acc 1\n",
      "2017-11-05T12:59:17.449471: step 937, loss 0.276756, acc 0.875\n",
      "2017-11-05T12:59:21.494871: step 938, loss 0.103985, acc 0.9375\n",
      "2017-11-05T12:59:25.544943: step 939, loss 0.120156, acc 0.96875\n",
      "2017-11-05T12:59:29.557596: step 940, loss 0.382727, acc 0.8125\n",
      "2017-11-05T12:59:33.613612: step 941, loss 0.128827, acc 0.96875\n",
      "2017-11-05T12:59:37.645094: step 942, loss 0.388905, acc 0.84375\n",
      "2017-11-05T12:59:41.711755: step 943, loss 0.203773, acc 0.96875\n",
      "2017-11-05T12:59:45.762648: step 944, loss 0.206725, acc 0.9375\n",
      "2017-11-05T12:59:49.843048: step 945, loss 0.495139, acc 0.78125\n",
      "2017-11-05T12:59:53.845391: step 946, loss 0.341656, acc 0.875\n",
      "2017-11-05T12:59:57.891766: step 947, loss 0.290429, acc 0.90625\n",
      "2017-11-05T13:00:02.285888: step 948, loss 0.347347, acc 0.875\n",
      "2017-11-05T13:00:06.326760: step 949, loss 0.308033, acc 0.8125\n",
      "2017-11-05T13:00:10.400824: step 950, loss 0.200642, acc 0.9375\n",
      "2017-11-05T13:00:14.541329: step 951, loss 0.212998, acc 0.90625\n",
      "2017-11-05T13:00:18.717950: step 952, loss 0.39585, acc 0.84375\n",
      "2017-11-05T13:00:22.858393: step 953, loss 0.201751, acc 0.90625\n",
      "2017-11-05T13:00:26.890635: step 954, loss 0.232723, acc 0.90625\n",
      "2017-11-05T13:00:30.985930: step 955, loss 0.300063, acc 0.875\n",
      "2017-11-05T13:00:35.293991: step 956, loss 0.204604, acc 0.9375\n",
      "2017-11-05T13:00:39.347372: step 957, loss 0.330593, acc 0.90625\n",
      "2017-11-05T13:00:43.498321: step 958, loss 0.0330024, acc 1\n",
      "2017-11-05T13:00:47.549634: step 959, loss 0.0857399, acc 0.9375\n",
      "2017-11-05T13:00:51.630316: step 960, loss 0.025724, acc 1\n",
      "2017-11-05T13:00:55.766261: step 961, loss 0.247198, acc 0.875\n",
      "2017-11-05T13:00:59.829763: step 962, loss 0.523191, acc 0.8125\n",
      "2017-11-05T13:01:03.879784: step 963, loss 0.18477, acc 0.875\n",
      "2017-11-05T13:01:07.965066: step 964, loss 0.332787, acc 0.9375\n",
      "2017-11-05T13:01:12.048896: step 965, loss 0.301152, acc 0.84375\n",
      "2017-11-05T13:01:16.191907: step 966, loss 0.258987, acc 0.90625\n",
      "2017-11-05T13:01:20.246674: step 967, loss 0.0494914, acc 1\n",
      "2017-11-05T13:01:24.338487: step 968, loss 0.274995, acc 0.90625\n",
      "2017-11-05T13:01:28.864981: step 969, loss 0.0962087, acc 1\n",
      "2017-11-05T13:01:32.991683: step 970, loss 0.0931203, acc 0.9375\n",
      "2017-11-05T13:01:37.106345: step 971, loss 0.228006, acc 0.9375\n",
      "2017-11-05T13:01:39.714441: step 972, loss 0.0582087, acc 1\n",
      "2017-11-05T13:01:43.789134: step 973, loss 0.0743998, acc 0.9375\n",
      "2017-11-05T13:01:47.859700: step 974, loss 0.234615, acc 0.875\n",
      "2017-11-05T13:01:51.870909: step 975, loss 0.165663, acc 0.9375\n",
      "2017-11-05T13:01:56.027175: step 976, loss 0.0389105, acc 1\n",
      "2017-11-05T13:02:00.158865: step 977, loss 0.449948, acc 0.90625\n",
      "2017-11-05T13:02:04.351508: step 978, loss 0.0812872, acc 0.96875\n",
      "2017-11-05T13:02:08.463378: step 979, loss 0.0845685, acc 0.96875\n",
      "2017-11-05T13:02:12.472058: step 980, loss 0.123824, acc 0.90625\n",
      "2017-11-05T13:02:16.588831: step 981, loss 0.409736, acc 0.84375\n",
      "2017-11-05T13:02:20.601533: step 982, loss 0.157159, acc 0.9375\n",
      "2017-11-05T13:02:24.627238: step 983, loss 0.364987, acc 0.90625\n",
      "2017-11-05T13:02:28.745128: step 984, loss 0.11713, acc 0.90625\n",
      "2017-11-05T13:02:32.939211: step 985, loss 0.272671, acc 0.875\n",
      "2017-11-05T13:02:37.129319: step 986, loss 0.175222, acc 0.9375\n",
      "2017-11-05T13:02:41.155192: step 987, loss 0.529687, acc 0.78125\n",
      "2017-11-05T13:02:45.273717: step 988, loss 0.0709905, acc 0.96875\n",
      "2017-11-05T13:02:49.348473: step 989, loss 0.205218, acc 0.9375\n",
      "2017-11-05T13:02:53.466188: step 990, loss 0.27409, acc 0.84375\n",
      "2017-11-05T13:02:57.568086: step 991, loss 0.125654, acc 0.96875\n",
      "2017-11-05T13:03:01.695466: step 992, loss 0.151152, acc 0.9375\n",
      "2017-11-05T13:03:05.831916: step 993, loss 0.260138, acc 0.875\n",
      "2017-11-05T13:03:09.827821: step 994, loss 0.430325, acc 0.875\n",
      "2017-11-05T13:03:13.924651: step 995, loss 0.325986, acc 0.84375\n",
      "2017-11-05T13:03:18.007404: step 996, loss 0.241043, acc 0.90625\n",
      "2017-11-05T13:03:22.148133: step 997, loss 0.370712, acc 0.84375\n",
      "2017-11-05T13:03:26.325022: step 998, loss 0.116056, acc 0.9375\n",
      "2017-11-05T13:03:30.416143: step 999, loss 0.189559, acc 0.875\n",
      "2017-11-05T13:03:34.402766: step 1000, loss 0.0858734, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:03:37.085536: step 1000, loss 0.823292, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-05T13:03:42.780724: step 1001, loss 0.236227, acc 0.96875\n",
      "2017-11-05T13:03:46.827572: step 1002, loss 0.14424, acc 0.9375\n",
      "2017-11-05T13:03:50.909318: step 1003, loss 0.312583, acc 0.90625\n",
      "2017-11-05T13:03:54.935197: step 1004, loss 0.29354, acc 0.875\n",
      "2017-11-05T13:03:59.029781: step 1005, loss 0.136132, acc 0.90625\n",
      "2017-11-05T13:04:03.108600: step 1006, loss 0.209875, acc 0.90625\n",
      "2017-11-05T13:04:07.193606: step 1007, loss 0.181536, acc 0.9375\n",
      "2017-11-05T13:04:09.758996: step 1008, loss 0.327166, acc 0.9\n",
      "2017-11-05T13:04:13.858913: step 1009, loss 0.337693, acc 0.84375\n",
      "2017-11-05T13:04:18.026218: step 1010, loss 0.124161, acc 0.9375\n",
      "2017-11-05T13:04:22.082790: step 1011, loss 0.222556, acc 0.96875\n",
      "2017-11-05T13:04:26.192318: step 1012, loss 0.323907, acc 0.84375\n",
      "2017-11-05T13:04:30.196118: step 1013, loss 0.336146, acc 0.84375\n",
      "2017-11-05T13:04:34.332641: step 1014, loss 0.307353, acc 0.90625\n",
      "2017-11-05T13:04:38.412999: step 1015, loss 0.123395, acc 0.9375\n",
      "2017-11-05T13:04:42.414049: step 1016, loss 0.346406, acc 0.90625\n",
      "2017-11-05T13:04:46.393940: step 1017, loss 0.108282, acc 0.96875\n",
      "2017-11-05T13:04:50.802994: step 1018, loss 0.2702, acc 0.875\n",
      "2017-11-05T13:04:54.983398: step 1019, loss 0.28098, acc 0.84375\n",
      "2017-11-05T13:04:59.110868: step 1020, loss 0.219651, acc 0.90625\n",
      "2017-11-05T13:05:03.192655: step 1021, loss 0.126784, acc 0.9375\n",
      "2017-11-05T13:05:07.322553: step 1022, loss 0.0675388, acc 0.96875\n",
      "2017-11-05T13:05:11.464465: step 1023, loss 0.151808, acc 0.9375\n",
      "2017-11-05T13:05:15.545229: step 1024, loss 0.116075, acc 0.9375\n",
      "2017-11-05T13:05:19.668714: step 1025, loss 0.464722, acc 0.78125\n",
      "2017-11-05T13:05:23.737903: step 1026, loss 0.0490711, acc 0.96875\n",
      "2017-11-05T13:05:27.792822: step 1027, loss 0.322678, acc 0.875\n",
      "2017-11-05T13:05:31.896516: step 1028, loss 0.123214, acc 0.96875\n",
      "2017-11-05T13:05:35.963927: step 1029, loss 0.294784, acc 0.84375\n",
      "2017-11-05T13:05:40.058138: step 1030, loss 0.292983, acc 0.90625\n",
      "2017-11-05T13:05:44.171575: step 1031, loss 0.0847105, acc 0.9375\n",
      "2017-11-05T13:05:48.272664: step 1032, loss 0.297185, acc 0.9375\n",
      "2017-11-05T13:05:52.383442: step 1033, loss 0.077418, acc 0.96875\n",
      "2017-11-05T13:05:56.461744: step 1034, loss 0.269084, acc 0.875\n",
      "2017-11-05T13:06:00.534209: step 1035, loss 0.0435715, acc 1\n",
      "2017-11-05T13:06:04.680761: step 1036, loss 0.339602, acc 0.84375\n",
      "2017-11-05T13:06:08.786094: step 1037, loss 0.410744, acc 0.90625\n",
      "2017-11-05T13:06:13.002124: step 1038, loss 0.181951, acc 0.90625\n",
      "2017-11-05T13:06:17.144645: step 1039, loss 0.231092, acc 0.875\n",
      "2017-11-05T13:06:21.344487: step 1040, loss 0.150838, acc 0.90625\n",
      "2017-11-05T13:06:25.574252: step 1041, loss 0.158761, acc 0.90625\n",
      "2017-11-05T13:06:29.739940: step 1042, loss 0.381583, acc 0.84375\n",
      "2017-11-05T13:06:33.868322: step 1043, loss 0.149494, acc 0.9375\n",
      "2017-11-05T13:06:36.581962: step 1044, loss 0.328837, acc 0.85\n",
      "2017-11-05T13:06:40.797998: step 1045, loss 0.220042, acc 0.875\n",
      "2017-11-05T13:06:44.945742: step 1046, loss 0.281289, acc 0.84375\n",
      "2017-11-05T13:06:49.062351: step 1047, loss 0.393203, acc 0.875\n",
      "2017-11-05T13:06:53.128601: step 1048, loss 0.16865, acc 0.90625\n",
      "2017-11-05T13:06:57.564261: step 1049, loss 0.283097, acc 0.90625\n",
      "2017-11-05T13:07:01.634552: step 1050, loss 0.135192, acc 0.9375\n",
      "2017-11-05T13:07:05.609372: step 1051, loss 0.214566, acc 0.9375\n",
      "2017-11-05T13:07:09.630945: step 1052, loss 0.727637, acc 0.84375\n",
      "2017-11-05T13:07:13.694477: step 1053, loss 0.328943, acc 0.84375\n",
      "2017-11-05T13:07:17.716808: step 1054, loss 0.189565, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T13:07:21.818268: step 1055, loss 0.0684789, acc 0.96875\n",
      "2017-11-05T13:07:25.997859: step 1056, loss 0.390563, acc 0.875\n",
      "2017-11-05T13:07:30.122580: step 1057, loss 0.180119, acc 0.9375\n",
      "2017-11-05T13:07:34.284542: step 1058, loss 0.248291, acc 0.875\n",
      "2017-11-05T13:07:38.328477: step 1059, loss 0.0587594, acc 1\n",
      "2017-11-05T13:07:42.473448: step 1060, loss 0.194869, acc 0.90625\n",
      "2017-11-05T13:07:46.618731: step 1061, loss 0.168615, acc 0.90625\n",
      "2017-11-05T13:07:50.730132: step 1062, loss 0.285667, acc 0.90625\n",
      "2017-11-05T13:07:54.925066: step 1063, loss 0.0234543, acc 1\n",
      "2017-11-05T13:07:59.030046: step 1064, loss 0.455291, acc 0.84375\n",
      "2017-11-05T13:08:03.116265: step 1065, loss 0.124028, acc 0.9375\n",
      "2017-11-05T13:08:07.279681: step 1066, loss 0.338631, acc 0.9375\n",
      "2017-11-05T13:08:11.456133: step 1067, loss 0.230612, acc 0.9375\n",
      "2017-11-05T13:08:15.580783: step 1068, loss 0.385435, acc 0.875\n",
      "2017-11-05T13:08:19.695921: step 1069, loss 0.111655, acc 0.90625\n",
      "2017-11-05T13:08:23.985542: step 1070, loss 0.475438, acc 0.875\n",
      "2017-11-05T13:08:28.300481: step 1071, loss 0.164578, acc 0.90625\n",
      "2017-11-05T13:08:32.429148: step 1072, loss 0.113408, acc 0.96875\n",
      "2017-11-05T13:08:36.656981: step 1073, loss 0.222026, acc 0.90625\n",
      "2017-11-05T13:08:40.779555: step 1074, loss 0.136848, acc 0.96875\n",
      "2017-11-05T13:08:44.925077: step 1075, loss 0.242565, acc 0.875\n",
      "2017-11-05T13:08:49.007191: step 1076, loss 0.474839, acc 0.8125\n",
      "2017-11-05T13:08:52.981409: step 1077, loss 0.361684, acc 0.8125\n",
      "2017-11-05T13:08:56.975415: step 1078, loss 0.300704, acc 0.875\n",
      "2017-11-05T13:09:01.064122: step 1079, loss 0.382766, acc 0.8125\n",
      "2017-11-05T13:09:03.640022: step 1080, loss 0.244669, acc 0.85\n",
      "2017-11-05T13:09:07.729977: step 1081, loss 0.0989353, acc 0.9375\n",
      "2017-11-05T13:09:11.763853: step 1082, loss 0.358026, acc 0.875\n",
      "2017-11-05T13:09:15.822677: step 1083, loss 0.222362, acc 0.90625\n",
      "2017-11-05T13:09:19.938716: step 1084, loss 0.131624, acc 0.96875\n",
      "2017-11-05T13:09:24.038825: step 1085, loss 0.198355, acc 0.9375\n",
      "2017-11-05T13:09:28.186722: step 1086, loss 0.376928, acc 0.90625\n",
      "2017-11-05T13:09:32.227325: step 1087, loss 0.542247, acc 0.84375\n",
      "2017-11-05T13:09:36.304198: step 1088, loss 0.577839, acc 0.8125\n",
      "2017-11-05T13:09:40.452800: step 1089, loss 0.139979, acc 0.90625\n",
      "2017-11-05T13:09:44.465233: step 1090, loss 0.271424, acc 0.84375\n",
      "2017-11-05T13:09:48.475099: step 1091, loss 0.0871643, acc 0.96875\n",
      "2017-11-05T13:09:52.558249: step 1092, loss 0.262106, acc 0.875\n",
      "2017-11-05T13:09:56.594411: step 1093, loss 0.287021, acc 0.84375\n",
      "2017-11-05T13:10:00.851816: step 1094, loss 0.11503, acc 0.96875\n",
      "2017-11-05T13:10:05.073945: step 1095, loss 0.271857, acc 0.875\n",
      "2017-11-05T13:10:09.118992: step 1096, loss 0.190558, acc 0.875\n",
      "2017-11-05T13:10:13.218823: step 1097, loss 0.25498, acc 0.875\n",
      "2017-11-05T13:10:17.260055: step 1098, loss 0.145854, acc 0.9375\n",
      "2017-11-05T13:10:21.357998: step 1099, loss 0.276773, acc 0.875\n",
      "2017-11-05T13:10:25.414357: step 1100, loss 0.34745, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:10:28.109499: step 1100, loss 0.934841, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-05T13:10:34.001321: step 1101, loss 0.0251344, acc 1\n",
      "2017-11-05T13:10:38.281336: step 1102, loss 0.224988, acc 0.84375\n",
      "2017-11-05T13:10:42.361960: step 1103, loss 0.0860137, acc 1\n",
      "2017-11-05T13:10:46.443997: step 1104, loss 0.174087, acc 0.9375\n",
      "2017-11-05T13:10:50.438706: step 1105, loss 0.0967643, acc 0.96875\n",
      "2017-11-05T13:10:54.582671: step 1106, loss 0.233395, acc 0.90625\n",
      "2017-11-05T13:10:58.755184: step 1107, loss 0.158974, acc 0.96875\n",
      "2017-11-05T13:11:02.869942: step 1108, loss 0.0460083, acc 0.96875\n",
      "2017-11-05T13:11:06.973818: step 1109, loss 0.290914, acc 0.90625\n",
      "2017-11-05T13:11:11.122475: step 1110, loss 0.184776, acc 0.9375\n",
      "2017-11-05T13:11:15.208516: step 1111, loss 0.200534, acc 0.90625\n",
      "2017-11-05T13:11:19.253430: step 1112, loss 0.10054, acc 0.9375\n",
      "2017-11-05T13:11:23.395480: step 1113, loss 0.232966, acc 0.875\n",
      "2017-11-05T13:11:27.552623: step 1114, loss 0.189724, acc 0.90625\n",
      "2017-11-05T13:11:31.621974: step 1115, loss 0.255165, acc 0.875\n",
      "2017-11-05T13:11:34.257847: step 1116, loss 0.117593, acc 0.95\n",
      "2017-11-05T13:11:38.413015: step 1117, loss 0.119065, acc 0.9375\n",
      "2017-11-05T13:11:42.549204: step 1118, loss 0.151727, acc 0.9375\n",
      "2017-11-05T13:11:46.858191: step 1119, loss 0.154455, acc 0.90625\n",
      "2017-11-05T13:11:50.923730: step 1120, loss 0.309079, acc 0.9375\n",
      "2017-11-05T13:11:54.917511: step 1121, loss 0.113641, acc 0.96875\n",
      "2017-11-05T13:11:58.927425: step 1122, loss 0.0695836, acc 0.96875\n",
      "2017-11-05T13:12:02.915375: step 1123, loss 0.141896, acc 0.90625\n",
      "2017-11-05T13:12:06.914060: step 1124, loss 0.323007, acc 0.90625\n",
      "2017-11-05T13:12:10.866116: step 1125, loss 0.295447, acc 0.9375\n",
      "2017-11-05T13:12:14.823932: step 1126, loss 0.272894, acc 0.875\n",
      "2017-11-05T13:12:18.783455: step 1127, loss 0.147436, acc 0.875\n",
      "2017-11-05T13:12:22.737056: step 1128, loss 0.221945, acc 0.96875\n",
      "2017-11-05T13:12:26.676630: step 1129, loss 0.267175, acc 0.90625\n",
      "2017-11-05T13:12:30.615902: step 1130, loss 0.0911944, acc 0.9375\n",
      "2017-11-05T13:12:34.830856: step 1131, loss 0.0938397, acc 0.96875\n",
      "2017-11-05T13:12:38.837266: step 1132, loss 0.0969262, acc 0.96875\n",
      "2017-11-05T13:12:42.762602: step 1133, loss 0.234128, acc 0.875\n",
      "2017-11-05T13:12:46.747481: step 1134, loss 0.139151, acc 0.9375\n",
      "2017-11-05T13:12:50.770111: step 1135, loss 0.51704, acc 0.78125\n",
      "2017-11-05T13:12:54.784974: step 1136, loss 0.156725, acc 0.90625\n",
      "2017-11-05T13:12:58.783745: step 1137, loss 0.0573941, acc 1\n",
      "2017-11-05T13:13:02.753837: step 1138, loss 0.0733164, acc 0.9375\n",
      "2017-11-05T13:13:06.732780: step 1139, loss 0.334136, acc 0.84375\n",
      "2017-11-05T13:13:10.794649: step 1140, loss 0.138712, acc 0.90625\n",
      "2017-11-05T13:13:14.845353: step 1141, loss 0.0895906, acc 0.96875\n",
      "2017-11-05T13:13:18.842771: step 1142, loss 0.219305, acc 0.90625\n",
      "2017-11-05T13:13:23.001655: step 1143, loss 0.249451, acc 0.96875\n",
      "2017-11-05T13:13:27.075382: step 1144, loss 0.0930761, acc 0.9375\n",
      "2017-11-05T13:13:31.139981: step 1145, loss 0.224963, acc 0.9375\n",
      "2017-11-05T13:13:35.129521: step 1146, loss 0.345262, acc 0.84375\n",
      "2017-11-05T13:13:39.199645: step 1147, loss 0.107997, acc 0.9375\n",
      "2017-11-05T13:13:43.223466: step 1148, loss 0.26023, acc 0.90625\n",
      "2017-11-05T13:13:47.217783: step 1149, loss 0.158816, acc 0.9375\n",
      "2017-11-05T13:13:51.182239: step 1150, loss 0.323353, acc 0.875\n",
      "2017-11-05T13:13:55.217541: step 1151, loss 0.192605, acc 0.875\n",
      "2017-11-05T13:13:57.769478: step 1152, loss 0.170663, acc 0.9\n",
      "2017-11-05T13:14:01.785811: step 1153, loss 0.371208, acc 0.875\n",
      "2017-11-05T13:14:05.797482: step 1154, loss 0.321214, acc 0.90625\n",
      "2017-11-05T13:14:09.825070: step 1155, loss 0.293522, acc 0.9375\n",
      "2017-11-05T13:14:13.818844: step 1156, loss 0.303826, acc 0.9375\n",
      "2017-11-05T13:14:17.829909: step 1157, loss 0.0591911, acc 0.96875\n",
      "2017-11-05T13:14:21.869842: step 1158, loss 0.302206, acc 0.875\n",
      "2017-11-05T13:14:25.898417: step 1159, loss 0.291307, acc 0.875\n",
      "2017-11-05T13:14:30.015479: step 1160, loss 0.251839, acc 0.875\n",
      "2017-11-05T13:14:34.084780: step 1161, loss 0.522431, acc 0.875\n",
      "2017-11-05T13:14:38.222195: step 1162, loss 0.203452, acc 0.90625\n",
      "2017-11-05T13:14:42.283063: step 1163, loss 0.399376, acc 0.8125\n",
      "2017-11-05T13:14:46.295044: step 1164, loss 0.167385, acc 0.9375\n",
      "2017-11-05T13:14:50.274810: step 1165, loss 0.173424, acc 0.9375\n",
      "2017-11-05T13:14:54.316690: step 1166, loss 0.181452, acc 0.9375\n",
      "2017-11-05T13:14:58.398908: step 1167, loss 0.445515, acc 0.78125\n",
      "2017-11-05T13:15:02.487095: step 1168, loss 0.208201, acc 0.9375\n",
      "2017-11-05T13:15:06.500383: step 1169, loss 0.513229, acc 0.875\n",
      "2017-11-05T13:15:10.622155: step 1170, loss 0.461444, acc 0.90625\n",
      "2017-11-05T13:15:14.663967: step 1171, loss 0.221637, acc 0.84375\n",
      "2017-11-05T13:15:18.683087: step 1172, loss 0.168614, acc 0.96875\n",
      "2017-11-05T13:15:22.681266: step 1173, loss 0.238932, acc 0.90625\n",
      "2017-11-05T13:15:26.666126: step 1174, loss 0.061024, acc 0.96875\n",
      "2017-11-05T13:15:30.751274: step 1175, loss 0.196353, acc 0.9375\n",
      "2017-11-05T13:15:34.940540: step 1176, loss 0.075228, acc 0.90625\n",
      "2017-11-05T13:15:39.029791: step 1177, loss 0.174414, acc 0.875\n",
      "2017-11-05T13:15:43.006428: step 1178, loss 0.0644325, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T13:15:47.032726: step 1179, loss 0.562166, acc 0.78125\n",
      "2017-11-05T13:15:51.031228: step 1180, loss 0.0800592, acc 0.96875\n",
      "2017-11-05T13:15:55.104759: step 1181, loss 0.153193, acc 0.9375\n",
      "2017-11-05T13:15:59.152748: step 1182, loss 0.246901, acc 0.90625\n",
      "2017-11-05T13:16:03.179728: step 1183, loss 0.264258, acc 0.875\n",
      "2017-11-05T13:16:07.247048: step 1184, loss 0.310708, acc 0.875\n",
      "2017-11-05T13:16:11.551423: step 1185, loss 0.270997, acc 0.875\n",
      "2017-11-05T13:16:15.753894: step 1186, loss 0.160135, acc 0.90625\n",
      "2017-11-05T13:16:19.742832: step 1187, loss 0.053624, acc 0.96875\n",
      "2017-11-05T13:16:22.376851: step 1188, loss 0.00781655, acc 1\n",
      "2017-11-05T13:16:26.479201: step 1189, loss 0.0870148, acc 0.96875\n",
      "2017-11-05T13:16:30.628714: step 1190, loss 0.171611, acc 0.90625\n",
      "2017-11-05T13:16:34.966185: step 1191, loss 0.17676, acc 0.90625\n",
      "2017-11-05T13:16:39.134390: step 1192, loss 0.161717, acc 0.9375\n",
      "2017-11-05T13:16:43.303629: step 1193, loss 0.191109, acc 0.90625\n",
      "2017-11-05T13:16:47.360281: step 1194, loss 0.132207, acc 0.96875\n",
      "2017-11-05T13:16:51.428271: step 1195, loss 0.134331, acc 0.9375\n",
      "2017-11-05T13:16:55.456030: step 1196, loss 0.111695, acc 0.9375\n",
      "2017-11-05T13:16:59.453564: step 1197, loss 0.0727351, acc 0.96875\n",
      "2017-11-05T13:17:03.409058: step 1198, loss 0.0357566, acc 1\n",
      "2017-11-05T13:17:07.406716: step 1199, loss 0.131062, acc 0.9375\n",
      "2017-11-05T13:17:11.749226: step 1200, loss 0.209901, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:17:14.524491: step 1200, loss 0.84986, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-05T13:17:20.207519: step 1201, loss 0.133354, acc 0.90625\n",
      "2017-11-05T13:17:24.239613: step 1202, loss 0.287335, acc 0.90625\n",
      "2017-11-05T13:17:28.268775: step 1203, loss 0.0757862, acc 0.96875\n",
      "2017-11-05T13:17:32.296134: step 1204, loss 0.107828, acc 0.96875\n",
      "2017-11-05T13:17:36.329122: step 1205, loss 0.19903, acc 0.90625\n",
      "2017-11-05T13:17:40.464595: step 1206, loss 0.315038, acc 0.875\n",
      "2017-11-05T13:17:44.789331: step 1207, loss 0.154399, acc 0.90625\n",
      "2017-11-05T13:17:48.943764: step 1208, loss 0.109227, acc 0.96875\n",
      "2017-11-05T13:17:52.999025: step 1209, loss 0.410417, acc 0.8125\n",
      "2017-11-05T13:17:57.075869: step 1210, loss 0.245025, acc 0.90625\n",
      "2017-11-05T13:18:01.109641: step 1211, loss 0.542971, acc 0.84375\n",
      "2017-11-05T13:18:05.305135: step 1212, loss 0.463803, acc 0.875\n",
      "2017-11-05T13:18:09.388683: step 1213, loss 0.204015, acc 0.90625\n",
      "2017-11-05T13:18:13.491022: step 1214, loss 0.182406, acc 0.875\n",
      "2017-11-05T13:18:17.566469: step 1215, loss 0.0744041, acc 0.96875\n",
      "2017-11-05T13:18:21.627870: step 1216, loss 0.347069, acc 0.90625\n",
      "2017-11-05T13:18:25.714967: step 1217, loss 0.285474, acc 0.875\n",
      "2017-11-05T13:18:29.966857: step 1218, loss 0.146037, acc 0.9375\n",
      "2017-11-05T13:18:34.121096: step 1219, loss 0.128573, acc 0.96875\n",
      "2017-11-05T13:18:38.226061: step 1220, loss 0.27462, acc 0.84375\n",
      "2017-11-05T13:18:42.383098: step 1221, loss 0.158913, acc 0.875\n",
      "2017-11-05T13:18:46.522320: step 1222, loss 0.159411, acc 0.96875\n",
      "2017-11-05T13:18:50.807438: step 1223, loss 0.336179, acc 0.84375\n",
      "2017-11-05T13:18:53.497706: step 1224, loss 0.267659, acc 0.85\n",
      "2017-11-05T13:18:57.647157: step 1225, loss 0.12491, acc 0.96875\n",
      "2017-11-05T13:19:01.739029: step 1226, loss 0.18358, acc 0.9375\n",
      "2017-11-05T13:19:05.982696: step 1227, loss 0.0655777, acc 0.96875\n",
      "2017-11-05T13:19:10.161017: step 1228, loss 0.111902, acc 0.96875\n",
      "2017-11-05T13:19:14.248354: step 1229, loss 0.269587, acc 0.90625\n",
      "2017-11-05T13:19:18.405324: step 1230, loss 0.0620293, acc 0.96875\n",
      "2017-11-05T13:19:22.546178: step 1231, loss 0.187869, acc 0.875\n",
      "2017-11-05T13:19:26.793480: step 1232, loss 0.636668, acc 0.8125\n",
      "2017-11-05T13:19:30.854980: step 1233, loss 0.201141, acc 0.9375\n",
      "2017-11-05T13:19:35.164306: step 1234, loss 0.249663, acc 0.875\n",
      "2017-11-05T13:19:39.511567: step 1235, loss 0.204379, acc 0.875\n",
      "2017-11-05T13:19:43.585367: step 1236, loss 0.256683, acc 0.875\n",
      "2017-11-05T13:19:47.638975: step 1237, loss 0.180766, acc 0.96875\n",
      "2017-11-05T13:19:51.725558: step 1238, loss 0.207905, acc 0.9375\n",
      "2017-11-05T13:19:55.789633: step 1239, loss 0.323291, acc 0.84375\n",
      "2017-11-05T13:19:59.875541: step 1240, loss 0.101466, acc 0.90625\n",
      "2017-11-05T13:20:04.217858: step 1241, loss 0.261003, acc 0.84375\n",
      "2017-11-05T13:20:08.375769: step 1242, loss 0.171873, acc 0.9375\n",
      "2017-11-05T13:20:12.469868: step 1243, loss 0.240629, acc 0.875\n",
      "2017-11-05T13:20:16.484494: step 1244, loss 0.215834, acc 0.875\n",
      "2017-11-05T13:20:20.523784: step 1245, loss 0.574511, acc 0.84375\n",
      "2017-11-05T13:20:24.566878: step 1246, loss 0.3583, acc 0.875\n",
      "2017-11-05T13:20:28.631892: step 1247, loss 0.0291444, acc 0.96875\n",
      "2017-11-05T13:20:32.715696: step 1248, loss 0.152122, acc 0.9375\n",
      "2017-11-05T13:20:36.816576: step 1249, loss 0.0181276, acc 1\n",
      "2017-11-05T13:20:40.982444: step 1250, loss 0.177561, acc 0.90625\n",
      "2017-11-05T13:20:45.005616: step 1251, loss 0.457594, acc 0.78125\n",
      "2017-11-05T13:20:49.201656: step 1252, loss 0.43357, acc 0.875\n",
      "2017-11-05T13:20:53.412192: step 1253, loss 0.126779, acc 0.96875\n",
      "2017-11-05T13:20:57.614753: step 1254, loss 0.260609, acc 0.84375\n",
      "2017-11-05T13:21:01.762827: step 1255, loss 0.162436, acc 0.9375\n",
      "2017-11-05T13:21:05.983546: step 1256, loss 0.123193, acc 0.9375\n",
      "2017-11-05T13:21:10.161436: step 1257, loss 0.442068, acc 0.84375\n",
      "2017-11-05T13:21:14.360874: step 1258, loss 0.28636, acc 0.84375\n",
      "2017-11-05T13:21:18.483402: step 1259, loss 0.371048, acc 0.84375\n",
      "2017-11-05T13:21:21.173488: step 1260, loss 0.527136, acc 0.8\n",
      "2017-11-05T13:21:25.361879: step 1261, loss 0.193569, acc 0.90625\n",
      "2017-11-05T13:21:29.458790: step 1262, loss 0.10832, acc 0.96875\n",
      "2017-11-05T13:21:33.548208: step 1263, loss 0.0800017, acc 0.96875\n",
      "2017-11-05T13:21:37.680873: step 1264, loss 0.134349, acc 0.9375\n",
      "2017-11-05T13:21:41.800714: step 1265, loss 0.374918, acc 0.90625\n",
      "2017-11-05T13:21:45.875523: step 1266, loss 0.076016, acc 1\n",
      "2017-11-05T13:21:49.960660: step 1267, loss 0.232031, acc 0.90625\n",
      "2017-11-05T13:21:54.031333: step 1268, loss 0.103155, acc 0.96875\n",
      "2017-11-05T13:21:58.213787: step 1269, loss 0.375049, acc 0.8125\n",
      "2017-11-05T13:22:02.300308: step 1270, loss 0.116292, acc 0.96875\n",
      "2017-11-05T13:22:06.416517: step 1271, loss 0.354456, acc 0.875\n",
      "2017-11-05T13:22:10.537592: step 1272, loss 0.450801, acc 0.8125\n",
      "2017-11-05T13:22:14.630884: step 1273, loss 0.420911, acc 0.8125\n",
      "2017-11-05T13:22:18.703957: step 1274, loss 0.261864, acc 0.875\n",
      "2017-11-05T13:22:22.719098: step 1275, loss 0.14575, acc 0.90625\n",
      "2017-11-05T13:22:26.818690: step 1276, loss 0.101001, acc 0.9375\n",
      "2017-11-05T13:22:30.911417: step 1277, loss 0.286258, acc 0.9375\n",
      "2017-11-05T13:22:35.258160: step 1278, loss 0.308038, acc 0.90625\n",
      "2017-11-05T13:22:39.333046: step 1279, loss 0.226927, acc 0.90625\n",
      "2017-11-05T13:22:43.418490: step 1280, loss 0.159346, acc 0.90625\n",
      "2017-11-05T13:22:47.520999: step 1281, loss 0.17934, acc 0.90625\n",
      "2017-11-05T13:22:51.578762: step 1282, loss 0.267678, acc 0.90625\n",
      "2017-11-05T13:22:55.710916: step 1283, loss 0.314667, acc 0.84375\n",
      "2017-11-05T13:22:59.871353: step 1284, loss 0.272008, acc 0.84375\n",
      "2017-11-05T13:23:03.977051: step 1285, loss 0.177072, acc 0.9375\n",
      "2017-11-05T13:23:08.105090: step 1286, loss 0.254105, acc 0.875\n",
      "2017-11-05T13:23:12.146938: step 1287, loss 0.111374, acc 0.90625\n",
      "2017-11-05T13:23:16.157153: step 1288, loss 0.103149, acc 0.96875\n",
      "2017-11-05T13:23:20.165677: step 1289, loss 0.247175, acc 0.90625\n",
      "2017-11-05T13:23:24.469671: step 1290, loss 0.197541, acc 0.9375\n",
      "2017-11-05T13:23:28.559073: step 1291, loss 0.310199, acc 0.84375\n",
      "2017-11-05T13:23:32.663206: step 1292, loss 0.191972, acc 0.875\n",
      "2017-11-05T13:23:36.781132: step 1293, loss 0.111477, acc 0.96875\n",
      "2017-11-05T13:23:40.807400: step 1294, loss 0.282351, acc 0.875\n",
      "2017-11-05T13:23:44.893122: step 1295, loss 0.499601, acc 0.8125\n",
      "2017-11-05T13:23:47.507626: step 1296, loss 0.113316, acc 0.95\n",
      "2017-11-05T13:23:51.594832: step 1297, loss 0.225061, acc 0.9375\n",
      "2017-11-05T13:23:55.712621: step 1298, loss 0.0823052, acc 0.96875\n",
      "2017-11-05T13:23:59.808433: step 1299, loss 0.0563482, acc 1\n",
      "2017-11-05T13:24:03.877781: step 1300, loss 0.207892, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:24:06.555429: step 1300, loss 0.817193, acc 0.783333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-05T13:24:13.273810: step 1301, loss 0.172935, acc 0.875\n",
      "2017-11-05T13:24:17.202006: step 1302, loss 0.168813, acc 0.875\n",
      "2017-11-05T13:24:21.181736: step 1303, loss 0.191049, acc 0.875\n",
      "2017-11-05T13:24:25.135602: step 1304, loss 0.404979, acc 0.84375\n",
      "2017-11-05T13:24:29.106410: step 1305, loss 0.279535, acc 0.9375\n",
      "2017-11-05T13:24:33.060533: step 1306, loss 0.157807, acc 0.9375\n",
      "2017-11-05T13:24:37.142309: step 1307, loss 0.162056, acc 0.9375\n",
      "2017-11-05T13:24:41.240510: step 1308, loss 0.174374, acc 0.9375\n",
      "2017-11-05T13:24:45.368541: step 1309, loss 0.185149, acc 0.9375\n",
      "2017-11-05T13:24:49.415381: step 1310, loss 0.218796, acc 0.9375\n",
      "2017-11-05T13:24:53.429970: step 1311, loss 0.336345, acc 0.8125\n",
      "2017-11-05T13:24:57.470679: step 1312, loss 0.06528, acc 0.96875\n",
      "2017-11-05T13:25:01.482728: step 1313, loss 0.155839, acc 0.9375\n",
      "2017-11-05T13:25:05.550698: step 1314, loss 0.170222, acc 0.875\n",
      "2017-11-05T13:25:09.547359: step 1315, loss 0.0853539, acc 0.9375\n",
      "2017-11-05T13:25:13.596543: step 1316, loss 0.231753, acc 0.90625\n",
      "2017-11-05T13:25:17.637742: step 1317, loss 0.33321, acc 0.84375\n",
      "2017-11-05T13:25:21.732650: step 1318, loss 0.147477, acc 0.9375\n",
      "2017-11-05T13:25:25.819388: step 1319, loss 0.3119, acc 0.90625\n",
      "2017-11-05T13:25:29.825260: step 1320, loss 0.186199, acc 0.9375\n",
      "2017-11-05T13:25:33.876392: step 1321, loss 0.484275, acc 0.78125\n",
      "2017-11-05T13:25:37.946756: step 1322, loss 0.07922, acc 0.96875\n",
      "2017-11-05T13:25:41.952889: step 1323, loss 0.338211, acc 0.84375\n",
      "2017-11-05T13:25:46.054913: step 1324, loss 0.367051, acc 0.84375\n",
      "2017-11-05T13:25:50.187759: step 1325, loss 0.0942138, acc 0.96875\n",
      "2017-11-05T13:25:54.269565: step 1326, loss 0.39745, acc 0.8125\n",
      "2017-11-05T13:25:58.330363: step 1327, loss 0.331701, acc 0.875\n",
      "2017-11-05T13:26:02.429422: step 1328, loss 0.143128, acc 0.90625\n",
      "2017-11-05T13:26:06.500276: step 1329, loss 0.255835, acc 0.90625\n",
      "2017-11-05T13:26:10.538793: step 1330, loss 0.329128, acc 0.8125\n",
      "2017-11-05T13:26:14.677288: step 1331, loss 0.0840512, acc 0.9375\n",
      "2017-11-05T13:26:17.253664: step 1332, loss 0.158086, acc 0.95\n",
      "2017-11-05T13:26:21.248762: step 1333, loss 0.209477, acc 0.90625\n",
      "2017-11-05T13:26:25.298927: step 1334, loss 0.34345, acc 0.84375\n",
      "2017-11-05T13:26:29.399464: step 1335, loss 0.229627, acc 0.90625\n",
      "2017-11-05T13:26:33.546735: step 1336, loss 0.180624, acc 0.9375\n",
      "2017-11-05T13:26:37.701213: step 1337, loss 0.197969, acc 0.96875\n",
      "2017-11-05T13:26:41.687702: step 1338, loss 0.414489, acc 0.8125\n",
      "2017-11-05T13:26:45.780006: step 1339, loss 0.0559385, acc 1\n",
      "2017-11-05T13:26:49.845172: step 1340, loss 0.103298, acc 0.96875\n",
      "2017-11-05T13:26:53.994824: step 1341, loss 0.419032, acc 0.8125\n",
      "2017-11-05T13:26:58.030181: step 1342, loss 0.154419, acc 0.9375\n",
      "2017-11-05T13:27:02.085910: step 1343, loss 0.119327, acc 0.9375\n",
      "2017-11-05T13:27:06.112195: step 1344, loss 0.10631, acc 0.90625\n",
      "2017-11-05T13:27:10.149993: step 1345, loss 0.328206, acc 0.875\n",
      "2017-11-05T13:27:14.210350: step 1346, loss 0.127599, acc 0.96875\n",
      "2017-11-05T13:27:18.300271: step 1347, loss 0.333935, acc 0.84375\n",
      "2017-11-05T13:27:22.785211: step 1348, loss 0.0440144, acc 1\n",
      "2017-11-05T13:27:26.903604: step 1349, loss 0.158587, acc 0.9375\n",
      "2017-11-05T13:27:30.930004: step 1350, loss 0.1284, acc 0.9375\n",
      "2017-11-05T13:27:34.961724: step 1351, loss 0.332211, acc 0.875\n",
      "2017-11-05T13:27:39.041608: step 1352, loss 0.208527, acc 0.9375\n",
      "2017-11-05T13:27:43.134276: step 1353, loss 0.119493, acc 0.9375\n",
      "2017-11-05T13:27:47.194137: step 1354, loss 0.12141, acc 0.90625\n",
      "2017-11-05T13:27:51.183650: step 1355, loss 0.09297, acc 0.96875\n",
      "2017-11-05T13:27:55.246206: step 1356, loss 0.133759, acc 0.96875\n",
      "2017-11-05T13:27:59.248622: step 1357, loss 0.202983, acc 0.9375\n",
      "2017-11-05T13:28:03.319965: step 1358, loss 0.11724, acc 0.96875\n",
      "2017-11-05T13:28:07.377552: step 1359, loss 0.26184, acc 0.90625\n",
      "2017-11-05T13:28:11.402481: step 1360, loss 0.199783, acc 0.96875\n",
      "2017-11-05T13:28:15.448114: step 1361, loss 0.318787, acc 0.875\n",
      "2017-11-05T13:28:19.549936: step 1362, loss 0.00631227, acc 1\n",
      "2017-11-05T13:28:23.795647: step 1363, loss 0.204278, acc 0.90625\n",
      "2017-11-05T13:28:28.091996: step 1364, loss 0.225034, acc 0.9375\n",
      "2017-11-05T13:28:32.133005: step 1365, loss 0.274283, acc 0.84375\n",
      "2017-11-05T13:28:36.305424: step 1366, loss 0.367055, acc 0.84375\n",
      "2017-11-05T13:28:40.329616: step 1367, loss 0.227054, acc 0.90625\n",
      "2017-11-05T13:28:42.966321: step 1368, loss 0.146727, acc 0.95\n",
      "2017-11-05T13:28:47.054582: step 1369, loss 0.442507, acc 0.84375\n",
      "2017-11-05T13:28:51.124000: step 1370, loss 0.126029, acc 0.96875\n",
      "2017-11-05T13:28:55.599129: step 1371, loss 0.437247, acc 0.875\n",
      "2017-11-05T13:28:59.683639: step 1372, loss 0.160573, acc 0.875\n",
      "2017-11-05T13:29:03.715581: step 1373, loss 0.310668, acc 0.875\n",
      "2017-11-05T13:29:07.833593: step 1374, loss 0.20795, acc 0.90625\n",
      "2017-11-05T13:29:11.855086: step 1375, loss 0.193309, acc 0.875\n",
      "2017-11-05T13:29:15.851935: step 1376, loss 0.134479, acc 0.90625\n",
      "2017-11-05T13:29:19.922573: step 1377, loss 0.267009, acc 0.90625\n",
      "2017-11-05T13:29:24.011524: step 1378, loss 0.163677, acc 0.96875\n",
      "2017-11-05T13:29:28.041413: step 1379, loss 0.496428, acc 0.875\n",
      "2017-11-05T13:29:32.103112: step 1380, loss 0.134467, acc 0.9375\n",
      "2017-11-05T13:29:36.116121: step 1381, loss 0.271615, acc 0.90625\n",
      "2017-11-05T13:29:40.190904: step 1382, loss 0.133723, acc 0.96875\n",
      "2017-11-05T13:29:44.233350: step 1383, loss 0.119325, acc 0.9375\n",
      "2017-11-05T13:29:48.295954: step 1384, loss 0.110596, acc 0.9375\n",
      "2017-11-05T13:29:52.380406: step 1385, loss 0.238425, acc 0.90625\n",
      "2017-11-05T13:29:56.405448: step 1386, loss 0.210364, acc 0.875\n",
      "2017-11-05T13:30:00.520011: step 1387, loss 0.105334, acc 0.96875\n",
      "2017-11-05T13:30:04.762205: step 1388, loss 0.347516, acc 0.8125\n",
      "2017-11-05T13:30:08.879031: step 1389, loss 0.138447, acc 0.96875\n",
      "2017-11-05T13:30:12.937344: step 1390, loss 0.300602, acc 0.875\n",
      "2017-11-05T13:30:17.032429: step 1391, loss 0.229955, acc 0.90625\n",
      "2017-11-05T13:30:21.016255: step 1392, loss 0.0691086, acc 0.9375\n",
      "2017-11-05T13:30:25.188331: step 1393, loss 0.163441, acc 0.90625\n",
      "2017-11-05T13:30:29.251708: step 1394, loss 0.0704586, acc 1\n",
      "2017-11-05T13:30:33.447033: step 1395, loss 0.0760262, acc 0.96875\n",
      "2017-11-05T13:30:37.534090: step 1396, loss 0.215507, acc 0.9375\n",
      "2017-11-05T13:30:41.547580: step 1397, loss 0.0602504, acc 0.96875\n",
      "2017-11-05T13:30:45.594212: step 1398, loss 0.279585, acc 0.90625\n",
      "2017-11-05T13:30:49.670548: step 1399, loss 0.294437, acc 0.90625\n",
      "2017-11-05T13:30:53.764699: step 1400, loss 0.149574, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:30:56.411083: step 1400, loss 0.983463, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-05T13:31:02.648639: step 1401, loss 0.57178, acc 0.8125\n",
      "2017-11-05T13:31:06.755682: step 1402, loss 0.0385508, acc 1\n",
      "2017-11-05T13:31:10.794886: step 1403, loss 0.105838, acc 0.96875\n",
      "2017-11-05T13:31:13.384738: step 1404, loss 0.284894, acc 0.9\n",
      "2017-11-05T13:31:17.402149: step 1405, loss 0.26091, acc 0.875\n",
      "2017-11-05T13:31:21.359834: step 1406, loss 0.24627, acc 0.90625\n",
      "2017-11-05T13:31:25.391299: step 1407, loss 0.356014, acc 0.8125\n",
      "2017-11-05T13:31:29.436950: step 1408, loss 0.155744, acc 0.96875\n",
      "2017-11-05T13:31:33.548123: step 1409, loss 0.0975456, acc 0.96875\n",
      "2017-11-05T13:31:37.696935: step 1410, loss 0.151337, acc 0.90625\n",
      "2017-11-05T13:31:41.733420: step 1411, loss 0.194816, acc 0.9375\n",
      "2017-11-05T13:31:45.776591: step 1412, loss 0.197369, acc 0.9375\n",
      "2017-11-05T13:31:49.818178: step 1413, loss 0.231822, acc 0.90625\n",
      "2017-11-05T13:31:54.017812: step 1414, loss 0.151644, acc 0.9375\n",
      "2017-11-05T13:31:58.198427: step 1415, loss 0.204196, acc 0.9375\n",
      "2017-11-05T13:32:02.253442: step 1416, loss 0.539156, acc 0.84375\n",
      "2017-11-05T13:32:06.450614: step 1417, loss 0.445275, acc 0.8125\n",
      "2017-11-05T13:32:10.548945: step 1418, loss 0.341363, acc 0.875\n",
      "2017-11-05T13:32:14.672691: step 1419, loss 0.134382, acc 0.9375\n",
      "2017-11-05T13:32:18.738864: step 1420, loss 0.39634, acc 0.875\n",
      "2017-11-05T13:32:22.884738: step 1421, loss 0.432006, acc 0.84375\n",
      "2017-11-05T13:32:26.952402: step 1422, loss 0.166965, acc 0.9375\n",
      "2017-11-05T13:32:30.963913: step 1423, loss 0.205554, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T13:32:35.080178: step 1424, loss 0.327952, acc 0.875\n",
      "2017-11-05T13:32:39.295498: step 1425, loss 0.209891, acc 0.875\n",
      "2017-11-05T13:32:43.364943: step 1426, loss 0.0986728, acc 0.9375\n",
      "2017-11-05T13:32:47.378139: step 1427, loss 0.21344, acc 0.90625\n",
      "2017-11-05T13:32:51.433923: step 1428, loss 0.216289, acc 0.90625\n",
      "2017-11-05T13:32:55.565916: step 1429, loss 0.225933, acc 0.875\n",
      "2017-11-05T13:32:59.573030: step 1430, loss 0.146844, acc 0.90625\n",
      "2017-11-05T13:33:03.612728: step 1431, loss 0.103993, acc 0.9375\n",
      "2017-11-05T13:33:07.773166: step 1432, loss 0.297107, acc 0.875\n",
      "2017-11-05T13:33:11.830539: step 1433, loss 0.208212, acc 0.90625\n",
      "2017-11-05T13:33:15.867753: step 1434, loss 0.379071, acc 0.875\n",
      "2017-11-05T13:33:19.896079: step 1435, loss 0.147598, acc 0.90625\n",
      "2017-11-05T13:33:24.031915: step 1436, loss 0.201088, acc 0.9375\n",
      "2017-11-05T13:33:28.117173: step 1437, loss 0.183182, acc 0.90625\n",
      "2017-11-05T13:33:32.488297: step 1438, loss 0.336178, acc 0.78125\n",
      "2017-11-05T13:33:36.526259: step 1439, loss 0.250449, acc 0.9375\n",
      "2017-11-05T13:33:39.114840: step 1440, loss 0.648615, acc 0.8\n",
      "2017-11-05T13:33:43.085654: step 1441, loss 0.063458, acc 0.96875\n",
      "2017-11-05T13:33:47.104541: step 1442, loss 0.0959289, acc 0.9375\n",
      "2017-11-05T13:33:51.092643: step 1443, loss 0.470041, acc 0.8125\n",
      "2017-11-05T13:33:55.076244: step 1444, loss 0.28746, acc 0.875\n",
      "2017-11-05T13:33:59.429630: step 1445, loss 0.119214, acc 0.9375\n",
      "2017-11-05T13:34:03.499754: step 1446, loss 0.0527938, acc 0.96875\n",
      "2017-11-05T13:34:07.603051: step 1447, loss 0.0472961, acc 0.96875\n",
      "2017-11-05T13:34:11.722175: step 1448, loss 0.186986, acc 0.875\n",
      "2017-11-05T13:34:15.768094: step 1449, loss 0.412748, acc 0.875\n",
      "2017-11-05T13:34:19.747611: step 1450, loss 0.250266, acc 0.90625\n",
      "2017-11-05T13:34:23.811309: step 1451, loss 0.360293, acc 0.84375\n",
      "2017-11-05T13:34:28.315237: step 1452, loss 0.139276, acc 0.9375\n",
      "2017-11-05T13:34:32.462215: step 1453, loss 0.270104, acc 0.84375\n",
      "2017-11-05T13:34:36.655600: step 1454, loss 0.215288, acc 0.875\n",
      "2017-11-05T13:34:40.714324: step 1455, loss 0.0868438, acc 0.96875\n",
      "2017-11-05T13:34:44.768190: step 1456, loss 0.336337, acc 0.84375\n",
      "2017-11-05T13:34:48.797373: step 1457, loss 0.32488, acc 0.9375\n",
      "2017-11-05T13:34:52.831205: step 1458, loss 0.275186, acc 0.84375\n",
      "2017-11-05T13:34:56.932290: step 1459, loss 0.243042, acc 0.875\n",
      "2017-11-05T13:35:00.942242: step 1460, loss 0.068735, acc 0.96875\n",
      "2017-11-05T13:35:05.061698: step 1461, loss 0.480918, acc 0.84375\n",
      "2017-11-05T13:35:09.080046: step 1462, loss 0.61526, acc 0.78125\n",
      "2017-11-05T13:35:13.133903: step 1463, loss 0.0380516, acc 0.96875\n",
      "2017-11-05T13:35:17.209784: step 1464, loss 0.204943, acc 0.875\n",
      "2017-11-05T13:35:21.259906: step 1465, loss 0.0656078, acc 0.96875\n",
      "2017-11-05T13:35:25.251823: step 1466, loss 0.0373248, acc 1\n",
      "2017-11-05T13:35:29.326930: step 1467, loss 0.181859, acc 0.9375\n",
      "2017-11-05T13:35:33.315298: step 1468, loss 0.272038, acc 0.875\n",
      "2017-11-05T13:35:37.352618: step 1469, loss 0.278156, acc 0.90625\n",
      "2017-11-05T13:35:41.388281: step 1470, loss 0.0830114, acc 0.96875\n",
      "2017-11-05T13:35:45.357737: step 1471, loss 0.118729, acc 0.9375\n",
      "2017-11-05T13:35:49.412949: step 1472, loss 0.100518, acc 0.9375\n",
      "2017-11-05T13:35:53.403630: step 1473, loss 0.361196, acc 0.875\n",
      "2017-11-05T13:35:57.406761: step 1474, loss 0.183493, acc 0.90625\n",
      "2017-11-05T13:36:01.535506: step 1475, loss 0.0971055, acc 0.9375\n",
      "2017-11-05T13:36:04.178673: step 1476, loss 0.0966035, acc 0.95\n",
      "2017-11-05T13:36:08.293266: step 1477, loss 0.212931, acc 0.90625\n",
      "2017-11-05T13:36:12.468259: step 1478, loss 0.244829, acc 0.875\n",
      "2017-11-05T13:36:16.599101: step 1479, loss 0.130564, acc 0.90625\n",
      "2017-11-05T13:36:20.717898: step 1480, loss 0.267186, acc 0.84375\n",
      "2017-11-05T13:36:24.808495: step 1481, loss 0.179662, acc 0.90625\n",
      "2017-11-05T13:36:28.895424: step 1482, loss 0.120237, acc 0.96875\n",
      "2017-11-05T13:36:33.088329: step 1483, loss 0.13462, acc 0.96875\n",
      "2017-11-05T13:36:37.324806: step 1484, loss 0.124912, acc 0.9375\n",
      "2017-11-05T13:36:41.306966: step 1485, loss 0.217671, acc 0.90625\n",
      "2017-11-05T13:36:45.340175: step 1486, loss 0.342687, acc 0.875\n",
      "2017-11-05T13:36:49.375268: step 1487, loss 0.176167, acc 0.96875\n",
      "2017-11-05T13:36:53.366440: step 1488, loss 0.385417, acc 0.90625\n",
      "2017-11-05T13:36:57.409812: step 1489, loss 0.077342, acc 0.9375\n",
      "2017-11-05T13:37:01.495923: step 1490, loss 0.209718, acc 0.875\n",
      "2017-11-05T13:37:05.524155: step 1491, loss 0.0612, acc 1\n",
      "2017-11-05T13:37:09.595331: step 1492, loss 0.188275, acc 0.90625\n",
      "2017-11-05T13:37:13.650365: step 1493, loss 0.166623, acc 0.90625\n",
      "2017-11-05T13:37:17.651384: step 1494, loss 0.229583, acc 0.84375\n",
      "2017-11-05T13:37:21.701243: step 1495, loss 0.185457, acc 0.90625\n",
      "2017-11-05T13:37:25.710385: step 1496, loss 0.285431, acc 0.875\n",
      "2017-11-05T13:37:29.777333: step 1497, loss 0.214318, acc 0.875\n",
      "2017-11-05T13:37:33.895235: step 1498, loss 0.10039, acc 0.96875\n",
      "2017-11-05T13:37:38.048435: step 1499, loss 0.234461, acc 0.9375\n",
      "2017-11-05T13:37:42.174752: step 1500, loss 0.414562, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:37:45.072851: step 1500, loss 0.831399, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-05T13:37:50.987436: step 1501, loss 0.130695, acc 0.9375\n",
      "2017-11-05T13:37:55.088119: step 1502, loss 0.144882, acc 0.9375\n",
      "2017-11-05T13:37:59.258435: step 1503, loss 0.16536, acc 0.875\n",
      "2017-11-05T13:38:03.380573: step 1504, loss 0.376772, acc 0.8125\n",
      "2017-11-05T13:38:07.457896: step 1505, loss 0.276097, acc 0.90625\n",
      "2017-11-05T13:38:11.995822: step 1506, loss 0.165645, acc 0.9375\n",
      "2017-11-05T13:38:16.680860: step 1507, loss 0.272721, acc 0.84375\n",
      "2017-11-05T13:38:21.533361: step 1508, loss 0.169203, acc 0.96875\n",
      "2017-11-05T13:38:26.342023: step 1509, loss 0.142799, acc 0.9375\n",
      "2017-11-05T13:38:31.325391: step 1510, loss 0.0871508, acc 0.96875\n",
      "2017-11-05T13:38:36.135820: step 1511, loss 0.0792847, acc 0.9375\n",
      "2017-11-05T13:38:39.207787: step 1512, loss 0.263977, acc 0.9\n",
      "2017-11-05T13:38:44.033999: step 1513, loss 0.0269972, acc 1\n",
      "2017-11-05T13:38:48.772523: step 1514, loss 0.348647, acc 0.90625\n",
      "2017-11-05T13:38:53.515810: step 1515, loss 0.420902, acc 0.84375\n",
      "2017-11-05T13:38:58.111484: step 1516, loss 0.288416, acc 0.84375\n",
      "2017-11-05T13:39:03.591565: step 1517, loss 0.146787, acc 0.9375\n",
      "2017-11-05T13:39:08.387879: step 1518, loss 0.396984, acc 0.8125\n",
      "2017-11-05T13:39:13.122623: step 1519, loss 0.183509, acc 0.875\n",
      "2017-11-05T13:39:17.835814: step 1520, loss 0.204399, acc 0.9375\n",
      "2017-11-05T13:39:22.558597: step 1521, loss 0.128976, acc 0.96875\n",
      "2017-11-05T13:39:27.421094: step 1522, loss 0.270612, acc 0.84375\n",
      "2017-11-05T13:39:32.156535: step 1523, loss 0.180181, acc 0.90625\n",
      "2017-11-05T13:39:36.838020: step 1524, loss 0.13725, acc 0.9375\n",
      "2017-11-05T13:39:41.571056: step 1525, loss 0.17596, acc 0.9375\n",
      "2017-11-05T13:39:46.355698: step 1526, loss 0.481327, acc 0.84375\n",
      "2017-11-05T13:39:51.104401: step 1527, loss 0.163973, acc 0.90625\n",
      "2017-11-05T13:39:55.804573: step 1528, loss 0.193041, acc 0.875\n",
      "2017-11-05T13:40:00.599279: step 1529, loss 0.18767, acc 0.90625\n",
      "2017-11-05T13:40:05.587299: step 1530, loss 0.214028, acc 0.90625\n",
      "2017-11-05T13:40:10.400010: step 1531, loss 0.00681675, acc 1\n",
      "2017-11-05T13:40:15.204465: step 1532, loss 0.291813, acc 0.90625\n",
      "2017-11-05T13:40:20.011373: step 1533, loss 0.078422, acc 0.96875\n",
      "2017-11-05T13:40:24.748942: step 1534, loss 0.10219, acc 0.96875\n",
      "2017-11-05T13:40:29.358151: step 1535, loss 0.177852, acc 0.90625\n",
      "2017-11-05T13:40:34.124539: step 1536, loss 0.527552, acc 0.8125\n",
      "2017-11-05T13:40:39.026013: step 1537, loss 0.110805, acc 0.9375\n",
      "2017-11-05T13:40:43.695706: step 1538, loss 0.113951, acc 0.96875\n",
      "2017-11-05T13:40:48.416504: step 1539, loss 0.445256, acc 0.8125\n",
      "2017-11-05T13:40:53.202027: step 1540, loss 0.410436, acc 0.875\n",
      "2017-11-05T13:40:57.993432: step 1541, loss 0.147307, acc 0.9375\n",
      "2017-11-05T13:41:02.762546: step 1542, loss 0.0999372, acc 0.96875\n",
      "2017-11-05T13:41:08.046602: step 1543, loss 0.24808, acc 0.90625\n",
      "2017-11-05T13:41:13.942466: step 1544, loss 0.419654, acc 0.8125\n",
      "2017-11-05T13:41:19.390392: step 1545, loss 0.250525, acc 0.90625\n",
      "2017-11-05T13:41:24.398634: step 1546, loss 0.149468, acc 0.90625\n",
      "2017-11-05T13:41:28.454270: step 1547, loss 0.0822258, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T13:41:31.019727: step 1548, loss 0.0387874, acc 1\n",
      "2017-11-05T13:41:35.075422: step 1549, loss 0.263714, acc 0.90625\n",
      "2017-11-05T13:41:39.085462: step 1550, loss 0.595135, acc 0.84375\n",
      "2017-11-05T13:41:43.125977: step 1551, loss 0.118898, acc 0.9375\n",
      "2017-11-05T13:41:47.186894: step 1552, loss 0.101762, acc 0.96875\n",
      "2017-11-05T13:41:51.245046: step 1553, loss 0.34679, acc 0.84375\n",
      "2017-11-05T13:41:55.399181: step 1554, loss 0.3612, acc 0.875\n",
      "2017-11-05T13:41:59.562326: step 1555, loss 0.163188, acc 0.90625\n",
      "2017-11-05T13:42:03.629448: step 1556, loss 0.138086, acc 0.9375\n",
      "2017-11-05T13:42:07.801490: step 1557, loss 0.0898108, acc 0.9375\n",
      "2017-11-05T13:42:11.895918: step 1558, loss 0.101519, acc 0.96875\n",
      "2017-11-05T13:42:15.980627: step 1559, loss 0.427913, acc 0.84375\n",
      "2017-11-05T13:42:20.016399: step 1560, loss 0.174538, acc 0.90625\n",
      "2017-11-05T13:42:24.060233: step 1561, loss 0.243662, acc 0.90625\n",
      "2017-11-05T13:42:28.284741: step 1562, loss 0.126437, acc 0.9375\n",
      "2017-11-05T13:42:32.388231: step 1563, loss 0.221087, acc 0.875\n",
      "2017-11-05T13:42:36.553525: step 1564, loss 0.262848, acc 0.90625\n",
      "2017-11-05T13:42:40.798969: step 1565, loss 0.0604153, acc 0.96875\n",
      "2017-11-05T13:42:44.852993: step 1566, loss 0.253303, acc 0.875\n",
      "2017-11-05T13:42:49.014589: step 1567, loss 0.219205, acc 0.9375\n",
      "2017-11-05T13:42:53.024131: step 1568, loss 0.18887, acc 0.875\n",
      "2017-11-05T13:42:57.109062: step 1569, loss 0.443467, acc 0.875\n",
      "2017-11-05T13:43:01.164759: step 1570, loss 0.00561761, acc 1\n",
      "2017-11-05T13:43:05.200983: step 1571, loss 0.107075, acc 0.96875\n",
      "2017-11-05T13:43:09.333682: step 1572, loss 0.230811, acc 0.90625\n",
      "2017-11-05T13:43:13.406340: step 1573, loss 0.438879, acc 0.875\n",
      "2017-11-05T13:43:17.365887: step 1574, loss 0.249942, acc 0.90625\n",
      "2017-11-05T13:43:21.404057: step 1575, loss 0.562936, acc 0.84375\n",
      "2017-11-05T13:43:25.562373: step 1576, loss 0.543924, acc 0.84375\n",
      "2017-11-05T13:43:29.939246: step 1577, loss 0.172333, acc 0.90625\n",
      "2017-11-05T13:43:34.238235: step 1578, loss 0.370468, acc 0.84375\n",
      "2017-11-05T13:43:38.460217: step 1579, loss 0.396703, acc 0.8125\n",
      "2017-11-05T13:43:42.593963: step 1580, loss 0.314847, acc 0.84375\n",
      "2017-11-05T13:43:46.574797: step 1581, loss 0.379951, acc 0.84375\n",
      "2017-11-05T13:43:50.643734: step 1582, loss 0.181829, acc 0.90625\n",
      "2017-11-05T13:43:54.575650: step 1583, loss 0.244772, acc 0.90625\n",
      "2017-11-05T13:43:57.181135: step 1584, loss 0.0389246, acc 1\n",
      "2017-11-05T13:44:01.275604: step 1585, loss 0.577654, acc 0.8125\n",
      "2017-11-05T13:44:05.417448: step 1586, loss 0.368196, acc 0.84375\n",
      "2017-11-05T13:44:09.507660: step 1587, loss 0.464617, acc 0.90625\n",
      "2017-11-05T13:44:13.548835: step 1588, loss 0.2124, acc 0.875\n",
      "2017-11-05T13:44:17.501383: step 1589, loss 0.20276, acc 0.90625\n",
      "2017-11-05T13:44:21.478279: step 1590, loss 0.10859, acc 0.9375\n",
      "2017-11-05T13:44:25.496256: step 1591, loss 0.248443, acc 0.90625\n",
      "2017-11-05T13:44:29.570418: step 1592, loss 0.241839, acc 0.84375\n",
      "2017-11-05T13:44:33.612125: step 1593, loss 0.248213, acc 0.90625\n",
      "2017-11-05T13:44:38.050859: step 1594, loss 0.132631, acc 0.9375\n",
      "2017-11-05T13:44:42.190947: step 1595, loss 0.166057, acc 0.875\n",
      "2017-11-05T13:44:46.289116: step 1596, loss 0.172992, acc 0.90625\n",
      "2017-11-05T13:44:50.259749: step 1597, loss 0.125267, acc 0.9375\n",
      "2017-11-05T13:44:54.283262: step 1598, loss 0.130372, acc 0.90625\n",
      "2017-11-05T13:44:58.345563: step 1599, loss 0.462123, acc 0.78125\n",
      "2017-11-05T13:45:02.370965: step 1600, loss 0.145983, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:45:05.040917: step 1600, loss 0.981216, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-05T13:45:11.154863: step 1601, loss 0.126326, acc 0.96875\n",
      "2017-11-05T13:45:15.310235: step 1602, loss 0.550471, acc 0.90625\n",
      "2017-11-05T13:45:19.302014: step 1603, loss 0.0574525, acc 0.96875\n",
      "2017-11-05T13:45:23.293764: step 1604, loss 0.340074, acc 0.90625\n",
      "2017-11-05T13:45:27.416564: step 1605, loss 0.309406, acc 0.875\n",
      "2017-11-05T13:45:31.432415: step 1606, loss 0.215748, acc 0.90625\n",
      "2017-11-05T13:45:35.832514: step 1607, loss 0.155837, acc 0.9375\n",
      "2017-11-05T13:45:39.919928: step 1608, loss 0.0450207, acc 1\n",
      "2017-11-05T13:45:43.958781: step 1609, loss 0.245278, acc 0.875\n",
      "2017-11-05T13:45:47.979969: step 1610, loss 0.0936356, acc 0.9375\n",
      "2017-11-05T13:45:52.030234: step 1611, loss 0.144062, acc 0.9375\n",
      "2017-11-05T13:45:56.063450: step 1612, loss 0.326835, acc 0.875\n",
      "2017-11-05T13:46:00.074161: step 1613, loss 0.097189, acc 0.9375\n",
      "2017-11-05T13:46:04.087785: step 1614, loss 0.197895, acc 0.90625\n",
      "2017-11-05T13:46:08.231518: step 1615, loss 0.371424, acc 0.78125\n",
      "2017-11-05T13:46:12.346151: step 1616, loss 0.1363, acc 0.90625\n",
      "2017-11-05T13:46:16.333330: step 1617, loss 0.218324, acc 0.875\n",
      "2017-11-05T13:46:20.273070: step 1618, loss 0.173778, acc 0.90625\n",
      "2017-11-05T13:46:24.214791: step 1619, loss 0.110277, acc 0.9375\n",
      "2017-11-05T13:46:26.827153: step 1620, loss 0.083497, acc 0.95\n",
      "2017-11-05T13:46:30.850864: step 1621, loss 0.343396, acc 0.90625\n",
      "2017-11-05T13:46:35.108290: step 1622, loss 0.162493, acc 0.96875\n",
      "2017-11-05T13:46:39.249441: step 1623, loss 0.199828, acc 0.90625\n",
      "2017-11-05T13:46:43.267483: step 1624, loss 0.290707, acc 0.90625\n",
      "2017-11-05T13:46:47.238041: step 1625, loss 0.116018, acc 0.96875\n",
      "2017-11-05T13:46:51.223221: step 1626, loss 0.0865912, acc 0.96875\n",
      "2017-11-05T13:46:55.247837: step 1627, loss 0.207203, acc 0.90625\n",
      "2017-11-05T13:46:59.198867: step 1628, loss 0.121557, acc 0.9375\n",
      "2017-11-05T13:47:03.537718: step 1629, loss 0.12937, acc 0.96875\n",
      "2017-11-05T13:47:07.693103: step 1630, loss 0.309014, acc 0.84375\n",
      "2017-11-05T13:47:11.687413: step 1631, loss 0.279264, acc 0.90625\n",
      "2017-11-05T13:47:15.742956: step 1632, loss 0.0759799, acc 1\n",
      "2017-11-05T13:47:19.629161: step 1633, loss 0.276504, acc 0.90625\n",
      "2017-11-05T13:47:23.669801: step 1634, loss 0.336575, acc 0.90625\n",
      "2017-11-05T13:47:27.674055: step 1635, loss 0.116295, acc 0.9375\n",
      "2017-11-05T13:47:31.735799: step 1636, loss 0.306983, acc 0.84375\n",
      "2017-11-05T13:47:35.780415: step 1637, loss 0.0987799, acc 0.96875\n",
      "2017-11-05T13:47:39.738974: step 1638, loss 0.0668517, acc 0.96875\n",
      "2017-11-05T13:47:43.680981: step 1639, loss 0.169455, acc 0.9375\n",
      "2017-11-05T13:47:47.634902: step 1640, loss 0.0397711, acc 0.96875\n",
      "2017-11-05T13:47:51.620632: step 1641, loss 0.460522, acc 0.875\n",
      "2017-11-05T13:47:55.549175: step 1642, loss 0.148088, acc 0.9375\n",
      "2017-11-05T13:47:59.502944: step 1643, loss 0.467526, acc 0.90625\n",
      "2017-11-05T13:48:03.428816: step 1644, loss 0.121192, acc 0.9375\n",
      "2017-11-05T13:48:07.495718: step 1645, loss 0.149864, acc 0.9375\n",
      "2017-11-05T13:48:11.428771: step 1646, loss 0.293862, acc 0.90625\n",
      "2017-11-05T13:48:15.450767: step 1647, loss 0.0470722, acc 0.96875\n",
      "2017-11-05T13:48:19.440705: step 1648, loss 0.1019, acc 0.9375\n",
      "2017-11-05T13:48:23.383773: step 1649, loss 0.168681, acc 0.9375\n",
      "2017-11-05T13:48:27.335286: step 1650, loss 0.124727, acc 0.96875\n",
      "2017-11-05T13:48:31.280538: step 1651, loss 0.063029, acc 0.96875\n",
      "2017-11-05T13:48:35.369968: step 1652, loss 0.247149, acc 0.875\n",
      "2017-11-05T13:48:39.301309: step 1653, loss 0.284034, acc 0.84375\n",
      "2017-11-05T13:48:43.450956: step 1654, loss 0.18858, acc 0.9375\n",
      "2017-11-05T13:48:47.463348: step 1655, loss 0.487908, acc 0.75\n",
      "2017-11-05T13:48:50.088215: step 1656, loss 0.352267, acc 0.8\n",
      "2017-11-05T13:48:54.180903: step 1657, loss 0.133314, acc 0.9375\n",
      "2017-11-05T13:48:58.226154: step 1658, loss 0.372629, acc 0.78125\n",
      "2017-11-05T13:49:02.334258: step 1659, loss 0.0785554, acc 0.96875\n",
      "2017-11-05T13:49:06.421109: step 1660, loss 0.0709601, acc 0.9375\n",
      "2017-11-05T13:49:10.540267: step 1661, loss 0.174601, acc 0.875\n",
      "2017-11-05T13:49:14.581450: step 1662, loss 0.0430078, acc 1\n",
      "2017-11-05T13:49:18.810161: step 1663, loss 0.133304, acc 0.9375\n",
      "2017-11-05T13:49:22.928827: step 1664, loss 0.0886486, acc 0.96875\n",
      "2017-11-05T13:49:27.332446: step 1665, loss 0.198343, acc 0.90625\n",
      "2017-11-05T13:49:31.503549: step 1666, loss 0.324633, acc 0.90625\n",
      "2017-11-05T13:49:36.061754: step 1667, loss 0.139513, acc 0.96875\n",
      "2017-11-05T13:49:40.186774: step 1668, loss 0.322308, acc 0.8125\n",
      "2017-11-05T13:49:44.132839: step 1669, loss 0.193087, acc 0.96875\n",
      "2017-11-05T13:49:48.121905: step 1670, loss 0.151288, acc 0.9375\n",
      "2017-11-05T13:49:52.078500: step 1671, loss 0.087927, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T13:49:56.031233: step 1672, loss 0.197628, acc 0.9375\n",
      "2017-11-05T13:49:59.986308: step 1673, loss 0.223428, acc 0.90625\n",
      "2017-11-05T13:50:04.230398: step 1674, loss 0.231808, acc 0.875\n",
      "2017-11-05T13:50:08.209851: step 1675, loss 0.292485, acc 0.9375\n",
      "2017-11-05T13:50:12.158146: step 1676, loss 0.134599, acc 0.9375\n",
      "2017-11-05T13:50:16.148135: step 1677, loss 0.214108, acc 0.90625\n",
      "2017-11-05T13:50:20.147003: step 1678, loss 0.252162, acc 0.84375\n",
      "2017-11-05T13:50:24.111331: step 1679, loss 0.105516, acc 0.9375\n",
      "2017-11-05T13:50:28.007380: step 1680, loss 0.178322, acc 0.96875\n",
      "2017-11-05T13:50:31.950537: step 1681, loss 0.0682794, acc 0.96875\n",
      "2017-11-05T13:50:36.099293: step 1682, loss 0.24448, acc 0.875\n",
      "2017-11-05T13:50:40.065336: step 1683, loss 0.246854, acc 0.84375\n",
      "2017-11-05T13:50:44.008153: step 1684, loss 0.18272, acc 0.90625\n",
      "2017-11-05T13:50:47.961238: step 1685, loss 0.207452, acc 0.875\n",
      "2017-11-05T13:50:51.895037: step 1686, loss 0.190467, acc 0.90625\n",
      "2017-11-05T13:50:55.830052: step 1687, loss 0.0749455, acc 0.9375\n",
      "2017-11-05T13:50:59.790192: step 1688, loss 0.311009, acc 0.84375\n",
      "2017-11-05T13:51:03.743242: step 1689, loss 0.416559, acc 0.90625\n",
      "2017-11-05T13:51:07.683937: step 1690, loss 0.0864956, acc 0.96875\n",
      "2017-11-05T13:51:11.659998: step 1691, loss 0.437309, acc 0.8125\n",
      "2017-11-05T13:51:14.225612: step 1692, loss 0.174767, acc 0.95\n",
      "2017-11-05T13:51:18.098664: step 1693, loss 0.170018, acc 0.96875\n",
      "2017-11-05T13:51:22.283877: step 1694, loss 0.042949, acc 1\n",
      "2017-11-05T13:51:26.265382: step 1695, loss 0.265936, acc 0.875\n",
      "2017-11-05T13:51:30.245901: step 1696, loss 0.228353, acc 0.90625\n",
      "2017-11-05T13:51:34.168111: step 1697, loss 0.130767, acc 0.9375\n",
      "2017-11-05T13:51:38.089112: step 1698, loss 0.228717, acc 0.90625\n",
      "2017-11-05T13:51:42.030539: step 1699, loss 0.331495, acc 0.8125\n",
      "2017-11-05T13:51:45.916644: step 1700, loss 0.182692, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:51:48.436507: step 1700, loss 0.910314, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-05T13:51:54.360310: step 1701, loss 0.160653, acc 0.9375\n",
      "2017-11-05T13:51:58.362985: step 1702, loss 0.398619, acc 0.8125\n",
      "2017-11-05T13:52:02.299349: step 1703, loss 0.258003, acc 0.875\n",
      "2017-11-05T13:52:06.227902: step 1704, loss 0.108667, acc 0.9375\n",
      "2017-11-05T13:52:10.204731: step 1705, loss 0.0910844, acc 0.96875\n",
      "2017-11-05T13:52:14.092316: step 1706, loss 0.136308, acc 0.90625\n",
      "2017-11-05T13:52:18.020377: step 1707, loss 0.0977303, acc 0.96875\n",
      "2017-11-05T13:52:21.962903: step 1708, loss 0.25884, acc 0.875\n",
      "2017-11-05T13:52:25.896362: step 1709, loss 0.258064, acc 0.8125\n",
      "2017-11-05T13:52:29.852427: step 1710, loss 0.218247, acc 0.90625\n",
      "2017-11-05T13:52:33.893080: step 1711, loss 0.155158, acc 0.96875\n",
      "2017-11-05T13:52:37.859444: step 1712, loss 0.112255, acc 0.90625\n",
      "2017-11-05T13:52:41.929261: step 1713, loss 0.207897, acc 0.90625\n",
      "2017-11-05T13:52:45.887816: step 1714, loss 0.0628123, acc 0.96875\n",
      "2017-11-05T13:52:49.788247: step 1715, loss 0.272761, acc 0.875\n",
      "2017-11-05T13:52:53.703702: step 1716, loss 0.125346, acc 0.9375\n",
      "2017-11-05T13:52:57.632692: step 1717, loss 0.198997, acc 0.90625\n",
      "2017-11-05T13:53:01.543374: step 1718, loss 0.0471393, acc 1\n",
      "2017-11-05T13:53:05.505899: step 1719, loss 0.290421, acc 0.84375\n",
      "2017-11-05T13:53:09.459979: step 1720, loss 0.134603, acc 0.9375\n",
      "2017-11-05T13:53:13.395776: step 1721, loss 0.141824, acc 0.9375\n",
      "2017-11-05T13:53:17.320677: step 1722, loss 0.28868, acc 0.875\n",
      "2017-11-05T13:53:21.275943: step 1723, loss 0.340242, acc 0.84375\n",
      "2017-11-05T13:53:25.264643: step 1724, loss 0.0335408, acc 1\n",
      "2017-11-05T13:53:29.203574: step 1725, loss 0.147531, acc 0.875\n",
      "2017-11-05T13:53:33.148853: step 1726, loss 0.204044, acc 0.84375\n",
      "2017-11-05T13:53:37.097465: step 1727, loss 0.0957783, acc 0.96875\n",
      "2017-11-05T13:53:39.600155: step 1728, loss 0.381531, acc 0.85\n",
      "2017-11-05T13:53:43.544554: step 1729, loss 0.247303, acc 0.875\n",
      "2017-11-05T13:53:47.492090: step 1730, loss 0.270669, acc 0.875\n",
      "2017-11-05T13:53:51.453437: step 1731, loss 0.192218, acc 0.9375\n",
      "2017-11-05T13:53:55.416203: step 1732, loss 0.117011, acc 0.96875\n",
      "2017-11-05T13:53:59.361353: step 1733, loss 0.317058, acc 0.875\n",
      "2017-11-05T13:54:03.345089: step 1734, loss 0.315653, acc 0.90625\n",
      "2017-11-05T13:54:07.358715: step 1735, loss 0.108886, acc 0.96875\n",
      "2017-11-05T13:54:11.316918: step 1736, loss 0.0832213, acc 0.9375\n",
      "2017-11-05T13:54:15.264278: step 1737, loss 0.0925118, acc 0.96875\n",
      "2017-11-05T13:54:19.229086: step 1738, loss 0.44697, acc 0.8125\n",
      "2017-11-05T13:54:23.349193: step 1739, loss 0.151972, acc 0.9375\n",
      "2017-11-05T13:54:27.355315: step 1740, loss 0.241047, acc 0.9375\n",
      "2017-11-05T13:54:31.299040: step 1741, loss 0.44613, acc 0.78125\n",
      "2017-11-05T13:54:35.359562: step 1742, loss 0.318863, acc 0.875\n",
      "2017-11-05T13:54:39.319592: step 1743, loss 0.160576, acc 0.90625\n",
      "2017-11-05T13:54:43.284042: step 1744, loss 0.28274, acc 0.84375\n",
      "2017-11-05T13:54:47.214354: step 1745, loss 0.218723, acc 0.875\n",
      "2017-11-05T13:54:51.187713: step 1746, loss 0.246951, acc 0.90625\n",
      "2017-11-05T13:54:55.136460: step 1747, loss 0.222326, acc 0.90625\n",
      "2017-11-05T13:54:59.101994: step 1748, loss 0.449586, acc 0.78125\n",
      "2017-11-05T13:55:02.992753: step 1749, loss 0.455078, acc 0.84375\n",
      "2017-11-05T13:55:06.901802: step 1750, loss 0.0714151, acc 0.96875\n",
      "2017-11-05T13:55:10.874904: step 1751, loss 0.0516243, acc 0.96875\n",
      "2017-11-05T13:55:14.845870: step 1752, loss 0.290677, acc 0.875\n",
      "2017-11-05T13:55:18.802616: step 1753, loss 0.188634, acc 0.90625\n",
      "2017-11-05T13:55:22.734143: step 1754, loss 0.226774, acc 0.9375\n",
      "2017-11-05T13:55:26.767829: step 1755, loss 0.412169, acc 0.8125\n",
      "2017-11-05T13:55:30.738017: step 1756, loss 0.0975455, acc 0.9375\n",
      "2017-11-05T13:55:34.649063: step 1757, loss 0.437626, acc 0.78125\n",
      "2017-11-05T13:55:38.566624: step 1758, loss 0.10059, acc 0.90625\n",
      "2017-11-05T13:55:42.530385: step 1759, loss 0.223635, acc 0.90625\n",
      "2017-11-05T13:55:46.444064: step 1760, loss 0.289128, acc 0.875\n",
      "2017-11-05T13:55:50.426937: step 1761, loss 0.165983, acc 0.9375\n",
      "2017-11-05T13:55:54.365172: step 1762, loss 0.132306, acc 0.96875\n",
      "2017-11-05T13:55:58.368833: step 1763, loss 0.0618991, acc 0.96875\n",
      "2017-11-05T13:56:00.893529: step 1764, loss 0.165794, acc 0.9\n",
      "2017-11-05T13:56:04.880798: step 1765, loss 0.0748201, acc 0.96875\n",
      "2017-11-05T13:56:08.873773: step 1766, loss 0.430229, acc 0.78125\n",
      "2017-11-05T13:56:12.831075: step 1767, loss 0.0747665, acc 0.96875\n",
      "2017-11-05T13:56:16.780263: step 1768, loss 0.202901, acc 0.96875\n",
      "2017-11-05T13:56:20.754627: step 1769, loss 0.197181, acc 0.9375\n",
      "2017-11-05T13:56:24.715635: step 1770, loss 0.315426, acc 0.84375\n",
      "2017-11-05T13:56:28.727373: step 1771, loss 0.23591, acc 0.875\n",
      "2017-11-05T13:56:32.698435: step 1772, loss 0.1434, acc 0.90625\n",
      "2017-11-05T13:56:36.715027: step 1773, loss 0.23746, acc 0.9375\n",
      "2017-11-05T13:56:40.673934: step 1774, loss 0.15982, acc 0.9375\n",
      "2017-11-05T13:56:44.610421: step 1775, loss 0.0525979, acc 1\n",
      "2017-11-05T13:56:48.516664: step 1776, loss 0.19132, acc 0.90625\n",
      "2017-11-05T13:56:52.451726: step 1777, loss 0.175386, acc 0.9375\n",
      "2017-11-05T13:56:56.424572: step 1778, loss 0.152965, acc 0.90625\n",
      "2017-11-05T13:57:00.326197: step 1779, loss 0.0439336, acc 1\n",
      "2017-11-05T13:57:04.280509: step 1780, loss 0.0664689, acc 0.96875\n",
      "2017-11-05T13:57:08.308621: step 1781, loss 0.301655, acc 0.875\n",
      "2017-11-05T13:57:12.244734: step 1782, loss 0.137033, acc 0.90625\n",
      "2017-11-05T13:57:16.210155: step 1783, loss 0.267754, acc 0.90625\n",
      "2017-11-05T13:57:20.130177: step 1784, loss 0.07274, acc 0.96875\n",
      "2017-11-05T13:57:24.100492: step 1785, loss 0.133097, acc 0.9375\n",
      "2017-11-05T13:57:28.073114: step 1786, loss 0.151397, acc 0.9375\n",
      "2017-11-05T13:57:32.054141: step 1787, loss 0.108991, acc 0.96875\n",
      "2017-11-05T13:57:35.997994: step 1788, loss 0.210857, acc 0.90625\n",
      "2017-11-05T13:57:39.989765: step 1789, loss 0.0949292, acc 0.9375\n",
      "2017-11-05T13:57:44.009067: step 1790, loss 0.216126, acc 0.90625\n",
      "2017-11-05T13:57:47.958045: step 1791, loss 0.180706, acc 0.9375\n",
      "2017-11-05T13:57:51.933705: step 1792, loss 0.14698, acc 0.875\n",
      "2017-11-05T13:57:55.940238: step 1793, loss 0.0570577, acc 0.96875\n",
      "2017-11-05T13:57:59.920140: step 1794, loss 0.0824838, acc 0.96875\n",
      "2017-11-05T13:58:03.895611: step 1795, loss 0.144462, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T13:58:07.825748: step 1796, loss 0.279532, acc 0.90625\n",
      "2017-11-05T13:58:11.823814: step 1797, loss 0.111256, acc 0.96875\n",
      "2017-11-05T13:58:15.828780: step 1798, loss 0.538552, acc 0.8125\n",
      "2017-11-05T13:58:19.823710: step 1799, loss 0.25163, acc 0.9375\n",
      "2017-11-05T13:58:22.446695: step 1800, loss 0.315204, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T13:58:25.226666: step 1800, loss 0.990067, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-05T13:58:30.464979: step 1801, loss 0.283168, acc 0.875\n",
      "2017-11-05T13:58:34.555835: step 1802, loss 0.231174, acc 0.875\n",
      "2017-11-05T13:58:38.553441: step 1803, loss 0.212102, acc 0.90625\n",
      "2017-11-05T13:58:42.473895: step 1804, loss 0.202907, acc 0.84375\n",
      "2017-11-05T13:58:46.421087: step 1805, loss 0.177624, acc 0.90625\n",
      "2017-11-05T13:58:50.359518: step 1806, loss 0.273151, acc 0.84375\n",
      "2017-11-05T13:58:54.342643: step 1807, loss 0.136242, acc 0.9375\n",
      "2017-11-05T13:58:58.277264: step 1808, loss 0.0238613, acc 1\n",
      "2017-11-05T13:59:02.212338: step 1809, loss 0.211677, acc 0.875\n",
      "2017-11-05T13:59:06.198025: step 1810, loss 0.151741, acc 0.9375\n",
      "2017-11-05T13:59:10.220574: step 1811, loss 0.202058, acc 0.90625\n",
      "2017-11-05T13:59:14.179341: step 1812, loss 0.15442, acc 0.875\n",
      "2017-11-05T13:59:18.081001: step 1813, loss 0.241294, acc 0.90625\n",
      "2017-11-05T13:59:22.047873: step 1814, loss 0.103945, acc 0.9375\n",
      "2017-11-05T13:59:26.004258: step 1815, loss 0.268559, acc 0.875\n",
      "2017-11-05T13:59:30.043367: step 1816, loss 0.0934104, acc 0.96875\n",
      "2017-11-05T13:59:34.034642: step 1817, loss 0.316133, acc 0.90625\n",
      "2017-11-05T13:59:37.983482: step 1818, loss 0.142083, acc 0.90625\n",
      "2017-11-05T13:59:41.990333: step 1819, loss 0.140482, acc 0.96875\n",
      "2017-11-05T13:59:45.956038: step 1820, loss 0.223071, acc 0.84375\n",
      "2017-11-05T13:59:49.985339: step 1821, loss 0.27143, acc 0.875\n",
      "2017-11-05T13:59:53.954657: step 1822, loss 0.249338, acc 0.90625\n",
      "2017-11-05T13:59:57.900642: step 1823, loss 0.206262, acc 0.9375\n",
      "2017-11-05T14:00:02.154127: step 1824, loss 0.113124, acc 0.9375\n",
      "2017-11-05T14:00:06.119321: step 1825, loss 0.354482, acc 0.84375\n",
      "2017-11-05T14:00:10.099649: step 1826, loss 0.140742, acc 0.9375\n",
      "2017-11-05T14:00:14.048991: step 1827, loss 0.288248, acc 0.84375\n",
      "2017-11-05T14:00:18.080856: step 1828, loss 0.179914, acc 0.9375\n",
      "2017-11-05T14:00:22.073558: step 1829, loss 0.161682, acc 0.90625\n",
      "2017-11-05T14:00:26.008869: step 1830, loss 0.290353, acc 0.875\n",
      "2017-11-05T14:00:30.018120: step 1831, loss 0.086464, acc 0.96875\n",
      "2017-11-05T14:00:34.095845: step 1832, loss 0.19143, acc 0.90625\n",
      "2017-11-05T14:00:38.111285: step 1833, loss 0.190562, acc 0.9375\n",
      "2017-11-05T14:00:42.109053: step 1834, loss 0.0702935, acc 0.96875\n",
      "2017-11-05T14:00:46.071319: step 1835, loss 0.25284, acc 0.90625\n",
      "2017-11-05T14:00:48.528865: step 1836, loss 0.0369844, acc 1\n",
      "2017-11-05T14:00:52.507258: step 1837, loss 0.129682, acc 0.90625\n",
      "2017-11-05T14:00:56.509229: step 1838, loss 0.201605, acc 0.9375\n",
      "2017-11-05T14:01:00.461830: step 1839, loss 0.131779, acc 0.9375\n",
      "2017-11-05T14:01:04.497781: step 1840, loss 0.272068, acc 0.875\n",
      "2017-11-05T14:01:08.483437: step 1841, loss 0.331874, acc 0.875\n",
      "2017-11-05T14:01:12.482218: step 1842, loss 0.185682, acc 0.9375\n",
      "2017-11-05T14:01:16.467822: step 1843, loss 0.261318, acc 0.875\n",
      "2017-11-05T14:01:20.420609: step 1844, loss 0.361083, acc 0.8125\n",
      "2017-11-05T14:01:24.399072: step 1845, loss 0.126541, acc 0.96875\n",
      "2017-11-05T14:01:28.331878: step 1846, loss 0.212914, acc 0.90625\n",
      "2017-11-05T14:01:32.305452: step 1847, loss 0.109189, acc 0.96875\n",
      "2017-11-05T14:01:36.419515: step 1848, loss 0.216447, acc 0.90625\n",
      "2017-11-05T14:01:40.415016: step 1849, loss 0.196643, acc 0.9375\n",
      "2017-11-05T14:01:44.350010: step 1850, loss 0.13646, acc 0.90625\n",
      "2017-11-05T14:01:48.342004: step 1851, loss 0.195407, acc 0.875\n",
      "2017-11-05T14:01:52.286655: step 1852, loss 0.332256, acc 0.84375\n",
      "2017-11-05T14:01:56.250157: step 1853, loss 0.25165, acc 0.84375\n",
      "2017-11-05T14:02:00.331716: step 1854, loss 0.206024, acc 0.90625\n",
      "2017-11-05T14:02:04.314687: step 1855, loss 0.103302, acc 0.96875\n",
      "2017-11-05T14:02:08.327314: step 1856, loss 0.130963, acc 0.96875\n",
      "2017-11-05T14:02:12.292566: step 1857, loss 0.220139, acc 0.90625\n",
      "2017-11-05T14:02:16.263458: step 1858, loss 0.302179, acc 0.90625\n",
      "2017-11-05T14:02:20.271997: step 1859, loss 0.255163, acc 0.875\n",
      "2017-11-05T14:02:24.248917: step 1860, loss 0.041405, acc 1\n",
      "2017-11-05T14:02:28.211122: step 1861, loss 0.096209, acc 0.9375\n",
      "2017-11-05T14:02:32.180869: step 1862, loss 0.195563, acc 0.90625\n",
      "2017-11-05T14:02:36.342230: step 1863, loss 0.156536, acc 0.90625\n",
      "2017-11-05T14:02:40.380339: step 1864, loss 0.160821, acc 0.96875\n",
      "2017-11-05T14:02:44.443434: step 1865, loss 0.227948, acc 0.875\n",
      "2017-11-05T14:02:48.403266: step 1866, loss 0.179249, acc 0.90625\n",
      "2017-11-05T14:02:52.354194: step 1867, loss 0.171159, acc 0.9375\n",
      "2017-11-05T14:02:56.405540: step 1868, loss 0.0959723, acc 0.9375\n",
      "2017-11-05T14:03:00.415602: step 1869, loss 0.117706, acc 0.9375\n",
      "2017-11-05T14:03:04.387926: step 1870, loss 0.160613, acc 0.9375\n",
      "2017-11-05T14:03:08.443127: step 1871, loss 0.247415, acc 0.90625\n",
      "2017-11-05T14:03:11.018881: step 1872, loss 0.188123, acc 0.9\n",
      "2017-11-05T14:03:14.984052: step 1873, loss 0.101882, acc 0.9375\n",
      "2017-11-05T14:03:19.027992: step 1874, loss 0.0696595, acc 0.96875\n",
      "2017-11-05T14:03:23.115297: step 1875, loss 0.117801, acc 0.90625\n",
      "2017-11-05T14:03:27.477596: step 1876, loss 0.0701124, acc 0.96875\n",
      "2017-11-05T14:03:31.486322: step 1877, loss 0.183378, acc 0.90625\n",
      "2017-11-05T14:03:35.547284: step 1878, loss 0.224108, acc 0.9375\n",
      "2017-11-05T14:03:39.559637: step 1879, loss 0.159502, acc 0.9375\n",
      "2017-11-05T14:03:43.490272: step 1880, loss 0.126468, acc 0.9375\n",
      "2017-11-05T14:03:47.508159: step 1881, loss 0.186341, acc 0.90625\n",
      "2017-11-05T14:03:51.493239: step 1882, loss 0.119545, acc 0.96875\n",
      "2017-11-05T14:03:55.483576: step 1883, loss 0.0597475, acc 0.96875\n",
      "2017-11-05T14:03:59.496711: step 1884, loss 0.157257, acc 0.9375\n",
      "2017-11-05T14:04:03.447445: step 1885, loss 0.26229, acc 0.875\n",
      "2017-11-05T14:04:07.431498: step 1886, loss 0.219088, acc 0.9375\n",
      "2017-11-05T14:04:11.381167: step 1887, loss 0.0363552, acc 1\n",
      "2017-11-05T14:04:15.349993: step 1888, loss 0.243158, acc 0.90625\n",
      "2017-11-05T14:04:19.295238: step 1889, loss 0.175734, acc 0.9375\n",
      "2017-11-05T14:04:23.265429: step 1890, loss 0.154373, acc 0.96875\n",
      "2017-11-05T14:04:27.228233: step 1891, loss 0.269558, acc 0.84375\n",
      "2017-11-05T14:04:31.177689: step 1892, loss 0.200168, acc 0.90625\n",
      "2017-11-05T14:04:35.285928: step 1893, loss 0.224343, acc 0.90625\n",
      "2017-11-05T14:04:39.306413: step 1894, loss 0.384094, acc 0.8125\n",
      "2017-11-05T14:04:43.329336: step 1895, loss 0.175673, acc 0.90625\n",
      "2017-11-05T14:04:47.285156: step 1896, loss 0.190876, acc 0.90625\n",
      "2017-11-05T14:04:51.217848: step 1897, loss 0.121412, acc 0.96875\n",
      "2017-11-05T14:04:55.210890: step 1898, loss 0.369424, acc 0.875\n",
      "2017-11-05T14:04:59.189970: step 1899, loss 0.230371, acc 0.875\n",
      "2017-11-05T14:05:03.199468: step 1900, loss 0.164925, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:05:05.800738: step 1900, loss 0.81697, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-05T14:05:11.229668: step 1901, loss 0.0821026, acc 0.96875\n",
      "2017-11-05T14:05:15.213829: step 1902, loss 0.183117, acc 0.9375\n",
      "2017-11-05T14:05:19.187532: step 1903, loss 0.164499, acc 0.90625\n",
      "2017-11-05T14:05:23.193654: step 1904, loss 0.368962, acc 0.8125\n",
      "2017-11-05T14:05:27.222170: step 1905, loss 0.322189, acc 0.84375\n",
      "2017-11-05T14:05:31.274219: step 1906, loss 0.189431, acc 0.9375\n",
      "2017-11-05T14:05:35.334595: step 1907, loss 0.108535, acc 0.9375\n",
      "2017-11-05T14:05:37.952750: step 1908, loss 0.0424984, acc 1\n",
      "2017-11-05T14:05:41.984268: step 1909, loss 0.141027, acc 0.9375\n",
      "2017-11-05T14:05:45.997455: step 1910, loss 0.374465, acc 0.84375\n",
      "2017-11-05T14:05:50.083842: step 1911, loss 0.152887, acc 0.90625\n",
      "2017-11-05T14:05:54.080325: step 1912, loss 0.103824, acc 0.9375\n",
      "2017-11-05T14:05:58.055478: step 1913, loss 0.21809, acc 0.90625\n",
      "2017-11-05T14:06:02.120967: step 1914, loss 0.13182, acc 0.9375\n",
      "2017-11-05T14:06:06.096556: step 1915, loss 0.190705, acc 0.875\n",
      "2017-11-05T14:06:10.141695: step 1916, loss 0.151137, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T14:06:14.184524: step 1917, loss 0.260755, acc 0.875\n",
      "2017-11-05T14:06:18.221789: step 1918, loss 0.437339, acc 0.78125\n",
      "2017-11-05T14:06:22.249852: step 1919, loss 0.259192, acc 0.90625\n",
      "2017-11-05T14:06:26.222349: step 1920, loss 0.163655, acc 0.9375\n",
      "2017-11-05T14:06:30.179794: step 1921, loss 0.0565419, acc 0.96875\n",
      "2017-11-05T14:06:34.325101: step 1922, loss 0.0840334, acc 0.96875\n",
      "2017-11-05T14:06:38.352463: step 1923, loss 0.09513, acc 0.9375\n",
      "2017-11-05T14:06:42.392040: step 1924, loss 0.124822, acc 0.9375\n",
      "2017-11-05T14:06:46.376264: step 1925, loss 0.0391646, acc 1\n",
      "2017-11-05T14:06:50.425452: step 1926, loss 0.154749, acc 0.9375\n",
      "2017-11-05T14:06:54.391298: step 1927, loss 0.233326, acc 0.9375\n",
      "2017-11-05T14:06:58.417696: step 1928, loss 0.07285, acc 0.96875\n",
      "2017-11-05T14:07:02.394011: step 1929, loss 0.512325, acc 0.78125\n",
      "2017-11-05T14:07:06.439794: step 1930, loss 0.113285, acc 0.9375\n",
      "2017-11-05T14:07:10.421682: step 1931, loss 0.203272, acc 0.84375\n",
      "2017-11-05T14:07:14.431228: step 1932, loss 0.158393, acc 0.9375\n",
      "2017-11-05T14:07:18.416486: step 1933, loss 0.131412, acc 0.90625\n",
      "2017-11-05T14:07:22.441077: step 1934, loss 0.316693, acc 0.84375\n",
      "2017-11-05T14:07:26.424744: step 1935, loss 0.22967, acc 0.90625\n",
      "2017-11-05T14:07:30.447973: step 1936, loss 0.1617, acc 0.9375\n",
      "2017-11-05T14:07:34.487730: step 1937, loss 0.0801647, acc 0.9375\n",
      "2017-11-05T14:07:38.534017: step 1938, loss 0.224012, acc 0.875\n",
      "2017-11-05T14:07:42.575750: step 1939, loss 0.128705, acc 0.96875\n",
      "2017-11-05T14:07:46.581587: step 1940, loss 0.111858, acc 0.96875\n",
      "2017-11-05T14:07:50.646759: step 1941, loss 0.155959, acc 0.90625\n",
      "2017-11-05T14:07:54.754080: step 1942, loss 0.25876, acc 0.9375\n",
      "2017-11-05T14:07:58.761548: step 1943, loss 0.141678, acc 0.9375\n",
      "2017-11-05T14:08:01.472226: step 1944, loss 0.164379, acc 0.9\n",
      "2017-11-05T14:08:05.484116: step 1945, loss 0.254002, acc 0.9375\n",
      "2017-11-05T14:08:09.494669: step 1946, loss 0.0797075, acc 0.96875\n",
      "2017-11-05T14:08:13.521151: step 1947, loss 0.0375556, acc 1\n",
      "2017-11-05T14:08:17.564361: step 1948, loss 0.0692079, acc 0.96875\n",
      "2017-11-05T14:08:21.615967: step 1949, loss 0.239043, acc 0.90625\n",
      "2017-11-05T14:08:25.630402: step 1950, loss 0.294037, acc 0.8125\n",
      "2017-11-05T14:08:29.727601: step 1951, loss 0.222624, acc 0.90625\n",
      "2017-11-05T14:08:33.771268: step 1952, loss 0.279077, acc 0.90625\n",
      "2017-11-05T14:08:37.820746: step 1953, loss 0.21961, acc 0.875\n",
      "2017-11-05T14:08:41.886800: step 1954, loss 0.180068, acc 0.96875\n",
      "2017-11-05T14:08:45.980105: step 1955, loss 0.278017, acc 0.84375\n",
      "2017-11-05T14:08:49.935273: step 1956, loss 0.378985, acc 0.8125\n",
      "2017-11-05T14:08:53.956846: step 1957, loss 0.502227, acc 0.84375\n",
      "2017-11-05T14:08:57.983019: step 1958, loss 0.250055, acc 0.8125\n",
      "2017-11-05T14:09:01.885018: step 1959, loss 0.25539, acc 0.84375\n",
      "2017-11-05T14:09:05.907791: step 1960, loss 0.217361, acc 0.875\n",
      "2017-11-05T14:09:09.979333: step 1961, loss 0.0797866, acc 0.96875\n",
      "2017-11-05T14:09:13.995573: step 1962, loss 0.377203, acc 0.78125\n",
      "2017-11-05T14:09:18.021739: step 1963, loss 0.137659, acc 0.9375\n",
      "2017-11-05T14:09:22.149835: step 1964, loss 0.232651, acc 0.90625\n",
      "2017-11-05T14:09:26.261107: step 1965, loss 0.0596162, acc 0.96875\n",
      "2017-11-05T14:09:30.247424: step 1966, loss 0.213689, acc 0.9375\n",
      "2017-11-05T14:09:34.242317: step 1967, loss 0.118082, acc 0.9375\n",
      "2017-11-05T14:09:38.275207: step 1968, loss 0.338548, acc 0.8125\n",
      "2017-11-05T14:09:42.374851: step 1969, loss 0.283195, acc 0.875\n",
      "2017-11-05T14:09:46.387058: step 1970, loss 0.288903, acc 0.84375\n",
      "2017-11-05T14:09:50.475245: step 1971, loss 0.275881, acc 0.875\n",
      "2017-11-05T14:09:54.424976: step 1972, loss 0.0707019, acc 0.96875\n",
      "2017-11-05T14:09:58.400966: step 1973, loss 0.224173, acc 0.90625\n",
      "2017-11-05T14:10:02.722342: step 1974, loss 0.100161, acc 0.96875\n",
      "2017-11-05T14:10:06.842672: step 1975, loss 0.103606, acc 0.9375\n",
      "2017-11-05T14:10:10.862186: step 1976, loss 0.118425, acc 0.9375\n",
      "2017-11-05T14:10:14.880362: step 1977, loss 0.191333, acc 0.90625\n",
      "2017-11-05T14:10:18.899039: step 1978, loss 0.0206855, acc 1\n",
      "2017-11-05T14:10:22.897729: step 1979, loss 0.0942749, acc 0.96875\n",
      "2017-11-05T14:10:25.446366: step 1980, loss 0.443231, acc 0.9\n",
      "2017-11-05T14:10:29.452753: step 1981, loss 0.17014, acc 0.90625\n",
      "2017-11-05T14:10:33.455738: step 1982, loss 0.294366, acc 0.84375\n",
      "2017-11-05T14:10:37.526015: step 1983, loss 0.21696, acc 0.9375\n",
      "2017-11-05T14:10:41.548194: step 1984, loss 0.135941, acc 0.96875\n",
      "2017-11-05T14:10:45.628378: step 1985, loss 0.149209, acc 0.9375\n",
      "2017-11-05T14:10:49.634100: step 1986, loss 0.142302, acc 0.9375\n",
      "2017-11-05T14:10:53.610239: step 1987, loss 0.477621, acc 0.8125\n",
      "2017-11-05T14:10:57.545942: step 1988, loss 0.292335, acc 0.875\n",
      "2017-11-05T14:11:01.519387: step 1989, loss 0.233787, acc 0.875\n",
      "2017-11-05T14:11:05.540559: step 1990, loss 0.481168, acc 0.78125\n",
      "2017-11-05T14:11:09.569098: step 1991, loss 0.142149, acc 0.9375\n",
      "2017-11-05T14:11:13.608643: step 1992, loss 0.197493, acc 0.90625\n",
      "2017-11-05T14:11:17.580717: step 1993, loss 0.106284, acc 0.96875\n",
      "2017-11-05T14:11:21.577975: step 1994, loss 0.277656, acc 0.875\n",
      "2017-11-05T14:11:25.595107: step 1995, loss 0.190928, acc 0.875\n",
      "2017-11-05T14:11:29.629187: step 1996, loss 0.201212, acc 0.90625\n",
      "2017-11-05T14:11:33.658611: step 1997, loss 0.238677, acc 0.9375\n",
      "2017-11-05T14:11:37.647781: step 1998, loss 0.053821, acc 1\n",
      "2017-11-05T14:11:41.730896: step 1999, loss 0.276596, acc 0.90625\n",
      "2017-11-05T14:11:45.696228: step 2000, loss 0.223705, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:11:48.262425: step 2000, loss 1.09734, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-05T14:11:53.488153: step 2001, loss 0.388505, acc 0.875\n",
      "2017-11-05T14:11:57.478837: step 2002, loss 0.314797, acc 0.90625\n",
      "2017-11-05T14:12:01.555563: step 2003, loss 0.134709, acc 0.90625\n",
      "2017-11-05T14:12:05.577391: step 2004, loss 0.153349, acc 0.96875\n",
      "2017-11-05T14:12:09.650447: step 2005, loss 0.414368, acc 0.875\n",
      "2017-11-05T14:12:13.646538: step 2006, loss 0.119105, acc 0.96875\n",
      "2017-11-05T14:12:17.624440: step 2007, loss 0.169726, acc 0.90625\n",
      "2017-11-05T14:12:21.663173: step 2008, loss 0.158511, acc 0.90625\n",
      "2017-11-05T14:12:25.667020: step 2009, loss 0.235045, acc 0.875\n",
      "2017-11-05T14:12:29.665993: step 2010, loss 0.0681066, acc 0.96875\n",
      "2017-11-05T14:12:33.671658: step 2011, loss 0.293684, acc 0.875\n",
      "2017-11-05T14:12:37.682845: step 2012, loss 0.172302, acc 0.90625\n",
      "2017-11-05T14:12:41.701236: step 2013, loss 0.0970804, acc 1\n",
      "2017-11-05T14:12:45.784677: step 2014, loss 0.240824, acc 0.90625\n",
      "2017-11-05T14:12:49.777540: step 2015, loss 0.0854369, acc 0.96875\n",
      "2017-11-05T14:12:52.361627: step 2016, loss 0.240823, acc 0.9\n",
      "2017-11-05T14:12:56.420166: step 2017, loss 0.107808, acc 0.9375\n",
      "2017-11-05T14:13:00.443191: step 2018, loss 0.255992, acc 0.84375\n",
      "2017-11-05T14:13:04.422774: step 2019, loss 0.012169, acc 1\n",
      "2017-11-05T14:13:08.390311: step 2020, loss 0.128288, acc 0.9375\n",
      "2017-11-05T14:13:12.317238: step 2021, loss 0.232076, acc 0.9375\n",
      "2017-11-05T14:13:16.242982: step 2022, loss 0.24469, acc 0.875\n",
      "2017-11-05T14:13:20.229593: step 2023, loss 0.0908207, acc 0.96875\n",
      "2017-11-05T14:13:24.399661: step 2024, loss 0.295113, acc 0.9375\n",
      "2017-11-05T14:13:28.326424: step 2025, loss 0.0849424, acc 0.96875\n",
      "2017-11-05T14:13:32.273270: step 2026, loss 0.289799, acc 0.90625\n",
      "2017-11-05T14:13:36.203848: step 2027, loss 0.219437, acc 0.90625\n",
      "2017-11-05T14:13:40.160884: step 2028, loss 0.151267, acc 0.90625\n",
      "2017-11-05T14:13:44.102035: step 2029, loss 0.189109, acc 0.875\n",
      "2017-11-05T14:13:48.059760: step 2030, loss 0.112735, acc 0.9375\n",
      "2017-11-05T14:13:52.078213: step 2031, loss 0.105835, acc 0.96875\n",
      "2017-11-05T14:13:55.972477: step 2032, loss 0.0705528, acc 0.96875\n",
      "2017-11-05T14:13:59.887773: step 2033, loss 0.257462, acc 0.90625\n",
      "2017-11-05T14:14:03.836759: step 2034, loss 0.228635, acc 0.875\n",
      "2017-11-05T14:14:07.749616: step 2035, loss 0.251172, acc 0.84375\n",
      "2017-11-05T14:14:11.678824: step 2036, loss 0.148354, acc 0.90625\n",
      "2017-11-05T14:14:15.605278: step 2037, loss 0.0765045, acc 0.96875\n",
      "2017-11-05T14:14:19.504117: step 2038, loss 0.188286, acc 0.875\n",
      "2017-11-05T14:14:23.407093: step 2039, loss 0.385589, acc 0.84375\n",
      "2017-11-05T14:14:27.352317: step 2040, loss 0.159399, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T14:14:31.257182: step 2041, loss 0.173781, acc 0.9375\n",
      "2017-11-05T14:14:35.271431: step 2042, loss 0.0661873, acc 0.96875\n",
      "2017-11-05T14:14:39.155356: step 2043, loss 0.161336, acc 0.90625\n",
      "2017-11-05T14:14:43.039708: step 2044, loss 0.238201, acc 0.9375\n",
      "2017-11-05T14:14:47.005134: step 2045, loss 0.139373, acc 0.90625\n",
      "2017-11-05T14:14:50.916672: step 2046, loss 0.285168, acc 0.875\n",
      "2017-11-05T14:14:54.837355: step 2047, loss 0.156414, acc 0.90625\n",
      "2017-11-05T14:14:58.710898: step 2048, loss 0.338554, acc 0.84375\n",
      "2017-11-05T14:15:02.637127: step 2049, loss 0.280695, acc 0.84375\n",
      "2017-11-05T14:15:06.581158: step 2050, loss 0.333211, acc 0.90625\n",
      "2017-11-05T14:15:10.470120: step 2051, loss 0.166272, acc 0.90625\n",
      "2017-11-05T14:15:12.982785: step 2052, loss 0.135093, acc 0.9\n",
      "2017-11-05T14:15:16.904868: step 2053, loss 0.215176, acc 0.9375\n",
      "2017-11-05T14:15:20.813410: step 2054, loss 0.263286, acc 0.90625\n",
      "2017-11-05T14:15:24.701616: step 2055, loss 0.173572, acc 0.9375\n",
      "2017-11-05T14:15:28.631650: step 2056, loss 0.373905, acc 0.875\n",
      "2017-11-05T14:15:32.504726: step 2057, loss 0.188505, acc 0.875\n",
      "2017-11-05T14:15:36.437508: step 2058, loss 0.107737, acc 0.96875\n",
      "2017-11-05T14:15:40.365623: step 2059, loss 0.434995, acc 0.8125\n",
      "2017-11-05T14:15:44.329347: step 2060, loss 0.292315, acc 0.875\n",
      "2017-11-05T14:15:48.210532: step 2061, loss 0.0967103, acc 0.96875\n",
      "2017-11-05T14:15:52.206175: step 2062, loss 0.0454098, acc 0.96875\n",
      "2017-11-05T14:15:56.170159: step 2063, loss 0.120026, acc 0.96875\n",
      "2017-11-05T14:16:00.077074: step 2064, loss 0.214719, acc 0.90625\n",
      "2017-11-05T14:16:04.046436: step 2065, loss 0.204977, acc 0.9375\n",
      "2017-11-05T14:16:08.088936: step 2066, loss 0.14544, acc 0.96875\n",
      "2017-11-05T14:16:12.096985: step 2067, loss 0.356734, acc 0.84375\n",
      "2017-11-05T14:16:16.062600: step 2068, loss 0.203373, acc 0.84375\n",
      "2017-11-05T14:16:19.987832: step 2069, loss 0.270501, acc 0.875\n",
      "2017-11-05T14:16:23.893873: step 2070, loss 0.124009, acc 0.90625\n",
      "2017-11-05T14:16:27.874784: step 2071, loss 0.212302, acc 0.875\n",
      "2017-11-05T14:16:31.806918: step 2072, loss 0.24928, acc 0.875\n",
      "2017-11-05T14:16:36.049648: step 2073, loss 0.276814, acc 0.875\n",
      "2017-11-05T14:16:40.088583: step 2074, loss 0.16576, acc 0.90625\n",
      "2017-11-05T14:16:44.006085: step 2075, loss 0.228077, acc 0.875\n",
      "2017-11-05T14:16:47.928032: step 2076, loss 0.150319, acc 0.9375\n",
      "2017-11-05T14:16:51.811384: step 2077, loss 0.0454594, acc 0.96875\n",
      "2017-11-05T14:16:55.776538: step 2078, loss 0.222823, acc 0.9375\n",
      "2017-11-05T14:16:59.667878: step 2079, loss 0.116598, acc 0.9375\n",
      "2017-11-05T14:17:03.611819: step 2080, loss 0.632399, acc 0.8125\n",
      "2017-11-05T14:17:07.532413: step 2081, loss 0.0957407, acc 0.9375\n",
      "2017-11-05T14:17:11.451534: step 2082, loss 0.32614, acc 0.90625\n",
      "2017-11-05T14:17:15.413782: step 2083, loss 0.370495, acc 0.8125\n",
      "2017-11-05T14:17:19.310836: step 2084, loss 0.182304, acc 0.90625\n",
      "2017-11-05T14:17:23.245429: step 2085, loss 0.140982, acc 0.9375\n",
      "2017-11-05T14:17:27.118561: step 2086, loss 0.221801, acc 0.875\n",
      "2017-11-05T14:17:31.072134: step 2087, loss 0.259627, acc 0.875\n",
      "2017-11-05T14:17:33.581198: step 2088, loss 0.248366, acc 0.9\n",
      "2017-11-05T14:17:37.508077: step 2089, loss 0.241768, acc 0.875\n",
      "2017-11-05T14:17:41.430734: step 2090, loss 0.108875, acc 0.96875\n",
      "2017-11-05T14:17:45.377948: step 2091, loss 0.235726, acc 0.90625\n",
      "2017-11-05T14:17:49.303061: step 2092, loss 0.183992, acc 0.875\n",
      "2017-11-05T14:17:53.262048: step 2093, loss 0.0972995, acc 0.9375\n",
      "2017-11-05T14:17:57.191374: step 2094, loss 0.206305, acc 0.875\n",
      "2017-11-05T14:18:01.120586: step 2095, loss 0.252889, acc 0.875\n",
      "2017-11-05T14:18:05.098610: step 2096, loss 0.0167598, acc 1\n",
      "2017-11-05T14:18:09.061909: step 2097, loss 0.228648, acc 0.9375\n",
      "2017-11-05T14:18:13.028034: step 2098, loss 0.24464, acc 0.9375\n",
      "2017-11-05T14:18:16.992528: step 2099, loss 0.0746705, acc 0.96875\n",
      "2017-11-05T14:18:20.888716: step 2100, loss 0.399556, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:18:23.527720: step 2100, loss 1.0409, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-05T14:18:29.661243: step 2101, loss 0.00842101, acc 1\n",
      "2017-11-05T14:18:33.682477: step 2102, loss 0.675387, acc 0.78125\n",
      "2017-11-05T14:18:37.712109: step 2103, loss 0.249146, acc 0.875\n",
      "2017-11-05T14:18:41.817036: step 2104, loss 0.197661, acc 0.90625\n",
      "2017-11-05T14:18:45.779821: step 2105, loss 0.172006, acc 0.9375\n",
      "2017-11-05T14:18:49.837611: step 2106, loss 0.142324, acc 0.96875\n",
      "2017-11-05T14:18:53.926570: step 2107, loss 0.361266, acc 0.875\n",
      "2017-11-05T14:18:57.956432: step 2108, loss 0.174733, acc 0.875\n",
      "2017-11-05T14:19:02.071926: step 2109, loss 0.535139, acc 0.75\n",
      "2017-11-05T14:19:06.130682: step 2110, loss 0.296423, acc 0.90625\n",
      "2017-11-05T14:19:10.226641: step 2111, loss 0.291524, acc 0.90625\n",
      "2017-11-05T14:19:14.325438: step 2112, loss 0.238746, acc 0.90625\n",
      "2017-11-05T14:19:18.337664: step 2113, loss 0.267803, acc 0.875\n",
      "2017-11-05T14:19:22.476218: step 2114, loss 0.397499, acc 0.875\n",
      "2017-11-05T14:19:26.530955: step 2115, loss 0.272566, acc 0.875\n",
      "2017-11-05T14:19:30.662545: step 2116, loss 0.172013, acc 0.9375\n",
      "2017-11-05T14:19:34.754070: step 2117, loss 0.127973, acc 0.9375\n",
      "2017-11-05T14:19:38.892668: step 2118, loss 0.125414, acc 0.90625\n",
      "2017-11-05T14:19:42.976491: step 2119, loss 0.101011, acc 0.9375\n",
      "2017-11-05T14:19:47.252847: step 2120, loss 0.251824, acc 0.90625\n",
      "2017-11-05T14:19:51.392417: step 2121, loss 0.117866, acc 0.9375\n",
      "2017-11-05T14:19:55.476979: step 2122, loss 0.308817, acc 0.875\n",
      "2017-11-05T14:19:59.439618: step 2123, loss 0.226243, acc 0.90625\n",
      "2017-11-05T14:20:02.338459: step 2124, loss 0.202468, acc 0.9\n",
      "2017-11-05T14:20:06.335009: step 2125, loss 0.134782, acc 0.90625\n",
      "2017-11-05T14:20:10.332563: step 2126, loss 0.259147, acc 0.875\n",
      "2017-11-05T14:20:14.288059: step 2127, loss 0.140923, acc 0.90625\n",
      "2017-11-05T14:20:18.222662: step 2128, loss 0.126977, acc 0.9375\n",
      "2017-11-05T14:20:22.170276: step 2129, loss 0.151375, acc 0.9375\n",
      "2017-11-05T14:20:26.210065: step 2130, loss 0.270848, acc 0.84375\n",
      "2017-11-05T14:20:30.173292: step 2131, loss 0.149889, acc 0.9375\n",
      "2017-11-05T14:20:34.171094: step 2132, loss 0.195734, acc 0.90625\n",
      "2017-11-05T14:20:38.159807: step 2133, loss 0.143552, acc 0.90625\n",
      "2017-11-05T14:20:42.202516: step 2134, loss 0.0796792, acc 0.96875\n",
      "2017-11-05T14:20:46.199796: step 2135, loss 0.274261, acc 0.84375\n",
      "2017-11-05T14:20:50.209347: step 2136, loss 0.0461187, acc 0.96875\n",
      "2017-11-05T14:20:54.191702: step 2137, loss 0.149635, acc 0.9375\n",
      "2017-11-05T14:20:58.182193: step 2138, loss 0.265296, acc 0.84375\n",
      "2017-11-05T14:21:02.204993: step 2139, loss 0.117251, acc 0.9375\n",
      "2017-11-05T14:21:06.254430: step 2140, loss 0.155392, acc 0.96875\n",
      "2017-11-05T14:21:10.210303: step 2141, loss 0.22056, acc 0.90625\n",
      "2017-11-05T14:21:14.158481: step 2142, loss 0.425958, acc 0.8125\n",
      "2017-11-05T14:21:18.158658: step 2143, loss 0.173144, acc 0.9375\n",
      "2017-11-05T14:21:22.130572: step 2144, loss 0.378476, acc 0.8125\n",
      "2017-11-05T14:21:26.154417: step 2145, loss 0.163037, acc 0.90625\n",
      "2017-11-05T14:21:30.124033: step 2146, loss 0.122043, acc 0.9375\n",
      "2017-11-05T14:21:34.114628: step 2147, loss 0.25577, acc 0.84375\n",
      "2017-11-05T14:21:38.088207: step 2148, loss 0.271261, acc 0.90625\n",
      "2017-11-05T14:21:42.035949: step 2149, loss 0.143422, acc 0.9375\n",
      "2017-11-05T14:21:46.016704: step 2150, loss 0.308398, acc 0.84375\n",
      "2017-11-05T14:21:50.043495: step 2151, loss 0.172667, acc 0.9375\n",
      "2017-11-05T14:21:54.044914: step 2152, loss 0.070614, acc 0.96875\n",
      "2017-11-05T14:21:58.014276: step 2153, loss 0.129299, acc 0.9375\n",
      "2017-11-05T14:22:02.196495: step 2154, loss 0.331662, acc 0.8125\n",
      "2017-11-05T14:22:06.290692: step 2155, loss 0.103995, acc 0.96875\n",
      "2017-11-05T14:22:10.280138: step 2156, loss 0.243163, acc 0.875\n",
      "2017-11-05T14:22:14.245613: step 2157, loss 0.268707, acc 0.875\n",
      "2017-11-05T14:22:18.253243: step 2158, loss 0.162455, acc 0.96875\n",
      "2017-11-05T14:22:22.192926: step 2159, loss 0.184038, acc 0.96875\n",
      "2017-11-05T14:22:24.781247: step 2160, loss 0.571002, acc 0.75\n",
      "2017-11-05T14:22:28.797630: step 2161, loss 0.20303, acc 0.875\n",
      "2017-11-05T14:22:32.812414: step 2162, loss 0.339168, acc 0.8125\n",
      "2017-11-05T14:22:36.894670: step 2163, loss 0.212278, acc 0.90625\n",
      "2017-11-05T14:22:40.882534: step 2164, loss 0.131824, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T14:22:45.005828: step 2165, loss 0.249726, acc 0.90625\n",
      "2017-11-05T14:22:49.000284: step 2166, loss 0.146783, acc 0.9375\n",
      "2017-11-05T14:22:52.948879: step 2167, loss 0.114229, acc 0.9375\n",
      "2017-11-05T14:22:56.927065: step 2168, loss 0.195259, acc 0.90625\n",
      "2017-11-05T14:23:00.891980: step 2169, loss 0.153942, acc 0.90625\n",
      "2017-11-05T14:23:04.936403: step 2170, loss 0.223639, acc 0.875\n",
      "2017-11-05T14:23:08.927526: step 2171, loss 0.141855, acc 0.9375\n",
      "2017-11-05T14:23:12.918520: step 2172, loss 0.10431, acc 0.9375\n",
      "2017-11-05T14:23:16.898677: step 2173, loss 0.26729, acc 0.875\n",
      "2017-11-05T14:23:20.911969: step 2174, loss 0.204923, acc 0.9375\n",
      "2017-11-05T14:23:25.029761: step 2175, loss 0.0374021, acc 0.96875\n",
      "2017-11-05T14:23:29.161060: step 2176, loss 0.261919, acc 0.875\n",
      "2017-11-05T14:23:33.185085: step 2177, loss 0.143094, acc 0.9375\n",
      "2017-11-05T14:23:37.129572: step 2178, loss 0.368179, acc 0.84375\n",
      "2017-11-05T14:23:41.060362: step 2179, loss 0.126981, acc 0.9375\n",
      "2017-11-05T14:23:45.019874: step 2180, loss 0.116788, acc 0.96875\n",
      "2017-11-05T14:23:49.033616: step 2181, loss 0.251169, acc 0.875\n",
      "2017-11-05T14:23:53.007736: step 2182, loss 0.175715, acc 0.9375\n",
      "2017-11-05T14:23:57.015984: step 2183, loss 0.0442374, acc 1\n",
      "2017-11-05T14:24:01.018935: step 2184, loss 0.257729, acc 0.875\n",
      "2017-11-05T14:24:05.036369: step 2185, loss 0.276017, acc 0.8125\n",
      "2017-11-05T14:24:09.046914: step 2186, loss 0.313969, acc 0.875\n",
      "2017-11-05T14:24:13.026167: step 2187, loss 0.391188, acc 0.84375\n",
      "2017-11-05T14:24:16.969456: step 2188, loss 0.102438, acc 0.9375\n",
      "2017-11-05T14:24:20.962804: step 2189, loss 0.0604586, acc 0.96875\n",
      "2017-11-05T14:24:24.957702: step 2190, loss 0.06796, acc 0.96875\n",
      "2017-11-05T14:24:29.004077: step 2191, loss 0.173168, acc 0.9375\n",
      "2017-11-05T14:24:33.057339: step 2192, loss 0.184715, acc 0.875\n",
      "2017-11-05T14:24:37.045721: step 2193, loss 0.323811, acc 0.84375\n",
      "2017-11-05T14:24:41.069656: step 2194, loss 0.193944, acc 0.90625\n",
      "2017-11-05T14:24:45.015690: step 2195, loss 0.327162, acc 0.875\n",
      "2017-11-05T14:24:47.569919: step 2196, loss 0.271765, acc 0.85\n",
      "2017-11-05T14:24:51.515045: step 2197, loss 0.247224, acc 0.875\n",
      "2017-11-05T14:24:55.562089: step 2198, loss 0.216421, acc 0.90625\n",
      "2017-11-05T14:24:59.539151: step 2199, loss 0.27147, acc 0.84375\n",
      "2017-11-05T14:25:03.530973: step 2200, loss 0.121457, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:25:06.108154: step 2200, loss 0.738788, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-05T14:25:11.382354: step 2201, loss 0.226751, acc 0.90625\n",
      "2017-11-05T14:25:15.320309: step 2202, loss 0.12466, acc 0.90625\n",
      "2017-11-05T14:25:19.229698: step 2203, loss 0.289026, acc 0.84375\n",
      "2017-11-05T14:25:23.182845: step 2204, loss 0.263378, acc 0.875\n",
      "2017-11-05T14:25:27.146957: step 2205, loss 0.126625, acc 0.875\n",
      "2017-11-05T14:25:31.130383: step 2206, loss 0.244835, acc 0.875\n",
      "2017-11-05T14:25:35.121072: step 2207, loss 0.0642707, acc 0.96875\n",
      "2017-11-05T14:25:39.086795: step 2208, loss 0.141109, acc 0.9375\n",
      "2017-11-05T14:25:43.025994: step 2209, loss 0.290508, acc 0.875\n",
      "2017-11-05T14:25:46.999946: step 2210, loss 0.0944235, acc 0.9375\n",
      "2017-11-05T14:25:50.961878: step 2211, loss 0.0831067, acc 0.96875\n",
      "2017-11-05T14:25:54.988010: step 2212, loss 0.221203, acc 0.90625\n",
      "2017-11-05T14:25:59.051292: step 2213, loss 0.252256, acc 0.875\n",
      "2017-11-05T14:26:03.107546: step 2214, loss 0.35857, acc 0.8125\n",
      "2017-11-05T14:26:07.111084: step 2215, loss 0.136893, acc 0.96875\n",
      "2017-11-05T14:26:11.031230: step 2216, loss 0.157872, acc 0.9375\n",
      "2017-11-05T14:26:14.979705: step 2217, loss 0.259901, acc 0.875\n",
      "2017-11-05T14:26:18.997936: step 2218, loss 0.333133, acc 0.84375\n",
      "2017-11-05T14:26:22.982664: step 2219, loss 0.123467, acc 0.9375\n",
      "2017-11-05T14:26:26.953461: step 2220, loss 0.202205, acc 0.90625\n",
      "2017-11-05T14:26:30.906977: step 2221, loss 0.186873, acc 0.90625\n",
      "2017-11-05T14:26:34.996793: step 2222, loss 0.417567, acc 0.8125\n",
      "2017-11-05T14:26:38.966337: step 2223, loss 0.155995, acc 0.9375\n",
      "2017-11-05T14:26:42.950414: step 2224, loss 0.175529, acc 0.90625\n",
      "2017-11-05T14:26:46.983166: step 2225, loss 0.295017, acc 0.875\n",
      "2017-11-05T14:26:50.942347: step 2226, loss 0.15167, acc 0.90625\n",
      "2017-11-05T14:26:54.998008: step 2227, loss 0.262007, acc 0.90625\n",
      "2017-11-05T14:26:58.931341: step 2228, loss 0.511891, acc 0.78125\n",
      "2017-11-05T14:27:02.893166: step 2229, loss 0.193575, acc 0.84375\n",
      "2017-11-05T14:27:06.935285: step 2230, loss 0.130095, acc 0.96875\n",
      "2017-11-05T14:27:11.031105: step 2231, loss 0.141481, acc 0.9375\n",
      "2017-11-05T14:27:13.493241: step 2232, loss 0.227009, acc 0.9\n",
      "2017-11-05T14:27:17.524161: step 2233, loss 0.0941364, acc 0.9375\n",
      "2017-11-05T14:27:21.475860: step 2234, loss 0.226458, acc 0.90625\n",
      "2017-11-05T14:27:25.402831: step 2235, loss 0.0704271, acc 0.96875\n",
      "2017-11-05T14:27:29.425242: step 2236, loss 0.0532284, acc 1\n",
      "2017-11-05T14:27:33.448298: step 2237, loss 0.173049, acc 0.9375\n",
      "2017-11-05T14:27:37.396973: step 2238, loss 0.0681337, acc 1\n",
      "2017-11-05T14:27:41.438456: step 2239, loss 0.183725, acc 0.90625\n",
      "2017-11-05T14:27:45.342517: step 2240, loss 0.0832162, acc 0.9375\n",
      "2017-11-05T14:27:49.339665: step 2241, loss 0.0587368, acc 0.96875\n",
      "2017-11-05T14:27:53.323060: step 2242, loss 0.461841, acc 0.875\n",
      "2017-11-05T14:27:57.315572: step 2243, loss 0.272639, acc 0.84375\n",
      "2017-11-05T14:28:01.305465: step 2244, loss 0.287482, acc 0.84375\n",
      "2017-11-05T14:28:05.335393: step 2245, loss 0.20355, acc 0.90625\n",
      "2017-11-05T14:28:09.372368: step 2246, loss 0.111449, acc 0.9375\n",
      "2017-11-05T14:28:13.373269: step 2247, loss 0.108994, acc 0.9375\n",
      "2017-11-05T14:28:17.403463: step 2248, loss 0.193281, acc 0.9375\n",
      "2017-11-05T14:28:21.428057: step 2249, loss 0.185214, acc 0.875\n",
      "2017-11-05T14:28:25.709170: step 2250, loss 0.196698, acc 0.90625\n",
      "2017-11-05T14:28:29.741135: step 2251, loss 0.152604, acc 0.9375\n",
      "2017-11-05T14:28:33.778299: step 2252, loss 0.294116, acc 0.84375\n",
      "2017-11-05T14:28:37.853995: step 2253, loss 0.196468, acc 0.90625\n",
      "2017-11-05T14:28:41.845740: step 2254, loss 0.204609, acc 0.875\n",
      "2017-11-05T14:28:45.825618: step 2255, loss 0.233511, acc 0.9375\n",
      "2017-11-05T14:28:49.771407: step 2256, loss 0.378167, acc 0.875\n",
      "2017-11-05T14:28:53.751562: step 2257, loss 0.0905269, acc 0.9375\n",
      "2017-11-05T14:28:57.709631: step 2258, loss 0.0887043, acc 0.9375\n",
      "2017-11-05T14:29:01.790961: step 2259, loss 0.281016, acc 0.875\n",
      "2017-11-05T14:29:05.781047: step 2260, loss 0.195313, acc 0.875\n",
      "2017-11-05T14:29:09.815642: step 2261, loss 0.16684, acc 0.90625\n",
      "2017-11-05T14:29:13.791907: step 2262, loss 0.229898, acc 0.90625\n",
      "2017-11-05T14:29:17.756957: step 2263, loss 0.23158, acc 0.875\n",
      "2017-11-05T14:29:21.715256: step 2264, loss 0.225775, acc 0.875\n",
      "2017-11-05T14:29:25.741040: step 2265, loss 0.0829567, acc 0.9375\n",
      "2017-11-05T14:29:29.723743: step 2266, loss 0.111521, acc 0.96875\n",
      "2017-11-05T14:29:33.777410: step 2267, loss 0.221646, acc 0.875\n",
      "2017-11-05T14:29:36.294866: step 2268, loss 0.155924, acc 0.9\n",
      "2017-11-05T14:29:40.304522: step 2269, loss 0.145745, acc 0.9375\n",
      "2017-11-05T14:29:44.266864: step 2270, loss 0.0338258, acc 1\n",
      "2017-11-05T14:29:48.241314: step 2271, loss 0.30602, acc 0.875\n",
      "2017-11-05T14:29:52.302838: step 2272, loss 0.224601, acc 0.84375\n",
      "2017-11-05T14:29:56.245455: step 2273, loss 0.0976688, acc 0.96875\n",
      "2017-11-05T14:30:00.261981: step 2274, loss 0.0625394, acc 0.96875\n",
      "2017-11-05T14:30:04.490410: step 2275, loss 0.177693, acc 0.90625\n",
      "2017-11-05T14:30:08.519982: step 2276, loss 0.156399, acc 0.9375\n",
      "2017-11-05T14:30:12.552439: step 2277, loss 0.149022, acc 0.9375\n",
      "2017-11-05T14:30:16.510746: step 2278, loss 0.217343, acc 0.84375\n",
      "2017-11-05T14:30:20.468829: step 2279, loss 0.343656, acc 0.84375\n",
      "2017-11-05T14:30:24.476286: step 2280, loss 0.0730185, acc 0.9375\n",
      "2017-11-05T14:30:28.468788: step 2281, loss 0.151552, acc 0.90625\n",
      "2017-11-05T14:30:32.445472: step 2282, loss 0.193084, acc 0.90625\n",
      "2017-11-05T14:30:36.563098: step 2283, loss 0.128905, acc 0.90625\n",
      "2017-11-05T14:30:40.557515: step 2284, loss 0.239785, acc 0.90625\n",
      "2017-11-05T14:30:44.596350: step 2285, loss 0.162319, acc 0.9375\n",
      "2017-11-05T14:30:48.590760: step 2286, loss 0.280596, acc 0.84375\n",
      "2017-11-05T14:30:52.536149: step 2287, loss 0.172561, acc 0.90625\n",
      "2017-11-05T14:30:56.554370: step 2288, loss 0.155604, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T14:31:00.591449: step 2289, loss 0.136082, acc 0.9375\n",
      "2017-11-05T14:31:04.661724: step 2290, loss 0.279068, acc 0.84375\n",
      "2017-11-05T14:31:08.799679: step 2291, loss 0.0928374, acc 0.96875\n",
      "2017-11-05T14:31:12.810507: step 2292, loss 0.247529, acc 0.875\n",
      "2017-11-05T14:31:16.813407: step 2293, loss 0.148948, acc 0.90625\n",
      "2017-11-05T14:31:20.798522: step 2294, loss 0.504685, acc 0.8125\n",
      "2017-11-05T14:31:24.743220: step 2295, loss 0.296163, acc 0.90625\n",
      "2017-11-05T14:31:28.791231: step 2296, loss 0.108945, acc 0.96875\n",
      "2017-11-05T14:31:32.733125: step 2297, loss 0.361552, acc 0.84375\n",
      "2017-11-05T14:31:36.787959: step 2298, loss 0.120232, acc 0.96875\n",
      "2017-11-05T14:31:40.792335: step 2299, loss 0.084566, acc 0.9375\n",
      "2017-11-05T14:31:44.826444: step 2300, loss 0.0984419, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:31:47.377632: step 2300, loss 0.802355, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-05T14:31:52.606196: step 2301, loss 0.231202, acc 0.90625\n",
      "2017-11-05T14:31:56.709330: step 2302, loss 0.092776, acc 0.9375\n",
      "2017-11-05T14:32:00.890438: step 2303, loss 0.139552, acc 0.90625\n",
      "2017-11-05T14:32:03.398745: step 2304, loss 0.478044, acc 0.8\n",
      "2017-11-05T14:32:07.408600: step 2305, loss 0.148966, acc 0.90625\n",
      "2017-11-05T14:32:11.341655: step 2306, loss 0.0553706, acc 0.96875\n",
      "2017-11-05T14:32:15.366456: step 2307, loss 0.0843343, acc 0.96875\n",
      "2017-11-05T14:32:19.332291: step 2308, loss 0.216748, acc 0.875\n",
      "2017-11-05T14:32:23.294423: step 2309, loss 0.222974, acc 0.875\n",
      "2017-11-05T14:32:27.268025: step 2310, loss 0.179154, acc 0.90625\n",
      "2017-11-05T14:32:31.231964: step 2311, loss 0.32065, acc 0.84375\n",
      "2017-11-05T14:32:35.356054: step 2312, loss 0.128115, acc 0.9375\n",
      "2017-11-05T14:32:39.346837: step 2313, loss 0.267689, acc 0.90625\n",
      "2017-11-05T14:32:43.288795: step 2314, loss 0.122001, acc 0.96875\n",
      "2017-11-05T14:32:47.422339: step 2315, loss 0.0887665, acc 0.96875\n",
      "2017-11-05T14:32:51.352359: step 2316, loss 0.111542, acc 0.9375\n",
      "2017-11-05T14:32:55.361236: step 2317, loss 0.155522, acc 0.90625\n",
      "2017-11-05T14:32:59.412317: step 2318, loss 0.0697208, acc 1\n",
      "2017-11-05T14:33:03.441386: step 2319, loss 0.102448, acc 0.9375\n",
      "2017-11-05T14:33:07.436472: step 2320, loss 0.0439849, acc 0.96875\n",
      "2017-11-05T14:33:11.470908: step 2321, loss 0.11292, acc 0.96875\n",
      "2017-11-05T14:33:15.552088: step 2322, loss 0.286015, acc 0.90625\n",
      "2017-11-05T14:33:19.559138: step 2323, loss 0.161595, acc 0.90625\n",
      "2017-11-05T14:33:23.705651: step 2324, loss 0.460531, acc 0.84375\n",
      "2017-11-05T14:33:27.925890: step 2325, loss 0.226585, acc 0.90625\n",
      "2017-11-05T14:33:31.919626: step 2326, loss 0.271203, acc 0.875\n",
      "2017-11-05T14:33:35.911990: step 2327, loss 0.0937843, acc 0.96875\n",
      "2017-11-05T14:33:39.937423: step 2328, loss 0.121518, acc 0.96875\n",
      "2017-11-05T14:33:43.916500: step 2329, loss 0.208368, acc 0.90625\n",
      "2017-11-05T14:33:47.923557: step 2330, loss 0.106379, acc 0.9375\n",
      "2017-11-05T14:33:51.950776: step 2331, loss 0.274845, acc 0.84375\n",
      "2017-11-05T14:33:55.927591: step 2332, loss 0.19007, acc 0.9375\n",
      "2017-11-05T14:33:59.969589: step 2333, loss 0.207222, acc 0.90625\n",
      "2017-11-05T14:34:04.026367: step 2334, loss 0.160715, acc 0.90625\n",
      "2017-11-05T14:34:08.052256: step 2335, loss 0.36142, acc 0.8125\n",
      "2017-11-05T14:34:11.993185: step 2336, loss 0.316846, acc 0.8125\n",
      "2017-11-05T14:34:15.976639: step 2337, loss 0.131918, acc 0.90625\n",
      "2017-11-05T14:34:19.974250: step 2338, loss 0.118649, acc 0.90625\n",
      "2017-11-05T14:34:24.027647: step 2339, loss 0.382838, acc 0.84375\n",
      "2017-11-05T14:34:26.576318: step 2340, loss 0.0605834, acc 0.95\n",
      "2017-11-05T14:34:30.505120: step 2341, loss 0.314388, acc 0.875\n",
      "2017-11-05T14:34:34.648483: step 2342, loss 0.202162, acc 0.875\n",
      "2017-11-05T14:34:38.671629: step 2343, loss 0.274479, acc 0.84375\n",
      "2017-11-05T14:34:42.688581: step 2344, loss 0.309476, acc 0.84375\n",
      "2017-11-05T14:34:46.651946: step 2345, loss 0.228212, acc 0.9375\n",
      "2017-11-05T14:34:50.666147: step 2346, loss 0.0777611, acc 1\n",
      "2017-11-05T14:34:54.688983: step 2347, loss 0.0572618, acc 1\n",
      "2017-11-05T14:34:58.773403: step 2348, loss 0.248846, acc 0.875\n",
      "2017-11-05T14:35:02.722621: step 2349, loss 0.058303, acc 0.96875\n",
      "2017-11-05T14:35:06.729375: step 2350, loss 0.125763, acc 0.9375\n",
      "2017-11-05T14:35:10.810225: step 2351, loss 0.303184, acc 0.90625\n",
      "2017-11-05T14:35:14.832953: step 2352, loss 0.12583, acc 0.9375\n",
      "2017-11-05T14:35:18.838431: step 2353, loss 0.188304, acc 0.90625\n",
      "2017-11-05T14:35:22.884793: step 2354, loss 0.225299, acc 0.9375\n",
      "2017-11-05T14:35:26.822814: step 2355, loss 0.20954, acc 0.875\n",
      "2017-11-05T14:35:30.865472: step 2356, loss 0.236231, acc 0.875\n",
      "2017-11-05T14:35:34.835172: step 2357, loss 0.180838, acc 0.9375\n",
      "2017-11-05T14:35:38.806909: step 2358, loss 0.0865217, acc 0.96875\n",
      "2017-11-05T14:35:42.839017: step 2359, loss 0.116774, acc 0.9375\n",
      "2017-11-05T14:35:46.796385: step 2360, loss 0.191822, acc 0.90625\n",
      "2017-11-05T14:35:50.794507: step 2361, loss 0.125912, acc 0.9375\n",
      "2017-11-05T14:35:54.752960: step 2362, loss 0.294182, acc 0.8125\n",
      "2017-11-05T14:35:58.762697: step 2363, loss 0.0896849, acc 0.9375\n",
      "2017-11-05T14:36:02.755010: step 2364, loss 0.28189, acc 0.84375\n",
      "2017-11-05T14:36:06.819071: step 2365, loss 0.137185, acc 0.9375\n",
      "2017-11-05T14:36:10.851336: step 2366, loss 0.320672, acc 0.84375\n",
      "2017-11-05T14:36:14.839687: step 2367, loss 0.109702, acc 0.96875\n",
      "2017-11-05T14:36:18.898056: step 2368, loss 0.212586, acc 0.84375\n",
      "2017-11-05T14:36:22.902844: step 2369, loss 0.300623, acc 0.8125\n",
      "2017-11-05T14:36:27.070615: step 2370, loss 0.0947365, acc 0.9375\n",
      "2017-11-05T14:36:31.043620: step 2371, loss 0.0928208, acc 0.96875\n",
      "2017-11-05T14:36:35.277477: step 2372, loss 0.0946318, acc 0.96875\n",
      "2017-11-05T14:36:39.288228: step 2373, loss 0.110091, acc 0.9375\n",
      "2017-11-05T14:36:43.265590: step 2374, loss 0.0889634, acc 0.96875\n",
      "2017-11-05T14:36:47.252307: step 2375, loss 0.191954, acc 0.90625\n",
      "2017-11-05T14:36:49.859453: step 2376, loss 0.121745, acc 0.9\n",
      "2017-11-05T14:36:53.884302: step 2377, loss 0.119076, acc 0.9375\n",
      "2017-11-05T14:36:57.878914: step 2378, loss 0.0728669, acc 0.9375\n",
      "2017-11-05T14:37:01.889972: step 2379, loss 0.164952, acc 0.90625\n",
      "2017-11-05T14:37:05.914168: step 2380, loss 0.178411, acc 0.90625\n",
      "2017-11-05T14:37:09.960387: step 2381, loss 0.0437553, acc 1\n",
      "2017-11-05T14:37:13.929431: step 2382, loss 0.209767, acc 0.90625\n",
      "2017-11-05T14:37:17.955979: step 2383, loss 0.197219, acc 0.9375\n",
      "2017-11-05T14:37:21.949684: step 2384, loss 0.16682, acc 0.90625\n",
      "2017-11-05T14:37:25.911357: step 2385, loss 0.169505, acc 0.90625\n",
      "2017-11-05T14:37:29.988597: step 2386, loss 0.162525, acc 0.9375\n",
      "2017-11-05T14:37:33.947491: step 2387, loss 0.355171, acc 0.84375\n",
      "2017-11-05T14:37:37.957148: step 2388, loss 0.124446, acc 0.9375\n",
      "2017-11-05T14:37:41.963561: step 2389, loss 0.227547, acc 0.9375\n",
      "2017-11-05T14:37:46.003705: step 2390, loss 0.147239, acc 0.9375\n",
      "2017-11-05T14:37:50.015219: step 2391, loss 0.328516, acc 0.8125\n",
      "2017-11-05T14:37:54.075330: step 2392, loss 0.091527, acc 0.96875\n",
      "2017-11-05T14:37:58.114278: step 2393, loss 0.210125, acc 0.90625\n",
      "2017-11-05T14:38:02.166575: step 2394, loss 0.19488, acc 0.90625\n",
      "2017-11-05T14:38:06.315346: step 2395, loss 0.148341, acc 0.90625\n",
      "2017-11-05T14:38:10.295298: step 2396, loss 0.336852, acc 0.8125\n",
      "2017-11-05T14:38:14.315369: step 2397, loss 0.17041, acc 0.9375\n",
      "2017-11-05T14:38:18.292230: step 2398, loss 0.310812, acc 0.84375\n",
      "2017-11-05T14:38:22.311131: step 2399, loss 0.0968107, acc 0.96875\n",
      "2017-11-05T14:38:26.444022: step 2400, loss 0.204034, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:38:28.951293: step 2400, loss 0.852536, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-05T14:38:34.336192: step 2401, loss 0.131033, acc 0.96875\n",
      "2017-11-05T14:38:38.409150: step 2402, loss 0.214707, acc 0.90625\n",
      "2017-11-05T14:38:42.413800: step 2403, loss 0.152893, acc 0.90625\n",
      "2017-11-05T14:38:46.475224: step 2404, loss 0.0992287, acc 0.96875\n",
      "2017-11-05T14:38:50.439626: step 2405, loss 0.392407, acc 0.78125\n",
      "2017-11-05T14:38:54.449496: step 2406, loss 0.194701, acc 0.90625\n",
      "2017-11-05T14:38:58.495595: step 2407, loss 0.184346, acc 0.9375\n",
      "2017-11-05T14:39:02.488424: step 2408, loss 0.35785, acc 0.875\n",
      "2017-11-05T14:39:06.479646: step 2409, loss 0.196411, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T14:39:10.549081: step 2410, loss 0.0946703, acc 0.96875\n",
      "2017-11-05T14:39:14.550972: step 2411, loss 0.191716, acc 0.875\n",
      "2017-11-05T14:39:17.161609: step 2412, loss 0.149182, acc 0.9\n",
      "2017-11-05T14:39:21.211491: step 2413, loss 0.166164, acc 0.90625\n",
      "2017-11-05T14:39:25.469991: step 2414, loss 0.132967, acc 0.9375\n",
      "2017-11-05T14:39:29.594131: step 2415, loss 0.237315, acc 0.90625\n",
      "2017-11-05T14:39:33.575077: step 2416, loss 0.25116, acc 0.875\n",
      "2017-11-05T14:39:37.561382: step 2417, loss 0.173795, acc 0.90625\n",
      "2017-11-05T14:39:41.597192: step 2418, loss 0.118321, acc 0.9375\n",
      "2017-11-05T14:39:45.644167: step 2419, loss 0.149691, acc 0.9375\n",
      "2017-11-05T14:39:49.711647: step 2420, loss 0.218124, acc 0.9375\n",
      "2017-11-05T14:39:53.650765: step 2421, loss 0.192687, acc 0.90625\n",
      "2017-11-05T14:39:57.639133: step 2422, loss 0.208897, acc 0.96875\n",
      "2017-11-05T14:40:01.921800: step 2423, loss 0.151766, acc 0.9375\n",
      "2017-11-05T14:40:06.004353: step 2424, loss 0.153713, acc 0.90625\n",
      "2017-11-05T14:40:10.076190: step 2425, loss 0.175598, acc 0.90625\n",
      "2017-11-05T14:40:14.116840: step 2426, loss 0.108301, acc 0.96875\n",
      "2017-11-05T14:40:18.112200: step 2427, loss 0.0901741, acc 0.90625\n",
      "2017-11-05T14:40:22.089385: step 2428, loss 0.0986021, acc 0.96875\n",
      "2017-11-05T14:40:26.114382: step 2429, loss 0.0416081, acc 0.96875\n",
      "2017-11-05T14:40:30.124264: step 2430, loss 0.115118, acc 0.96875\n",
      "2017-11-05T14:40:34.296383: step 2431, loss 0.236792, acc 0.90625\n",
      "2017-11-05T14:40:38.390994: step 2432, loss 0.138846, acc 0.9375\n",
      "2017-11-05T14:40:42.344474: step 2433, loss 0.261496, acc 0.875\n",
      "2017-11-05T14:40:46.319796: step 2434, loss 0.0762729, acc 0.96875\n",
      "2017-11-05T14:40:50.360792: step 2435, loss 0.265412, acc 0.90625\n",
      "2017-11-05T14:40:54.406297: step 2436, loss 0.160374, acc 0.9375\n",
      "2017-11-05T14:40:58.427528: step 2437, loss 0.468596, acc 0.84375\n",
      "2017-11-05T14:41:02.453104: step 2438, loss 0.0509584, acc 1\n",
      "2017-11-05T14:41:06.499097: step 2439, loss 0.0874455, acc 0.9375\n",
      "2017-11-05T14:41:10.992270: step 2440, loss 0.181244, acc 0.9375\n",
      "2017-11-05T14:41:15.979337: step 2441, loss 0.279717, acc 0.875\n",
      "2017-11-05T14:41:19.985903: step 2442, loss 0.214746, acc 0.875\n",
      "2017-11-05T14:41:24.070710: step 2443, loss 0.212845, acc 0.90625\n",
      "2017-11-05T14:41:28.124089: step 2444, loss 0.290965, acc 0.84375\n",
      "2017-11-05T14:41:32.132176: step 2445, loss 0.201113, acc 0.84375\n",
      "2017-11-05T14:41:36.123364: step 2446, loss 0.189225, acc 0.90625\n",
      "2017-11-05T14:41:40.150482: step 2447, loss 0.189941, acc 0.9375\n",
      "2017-11-05T14:41:42.645773: step 2448, loss 0.403562, acc 0.8\n",
      "2017-11-05T14:41:46.669553: step 2449, loss 0.0890593, acc 0.96875\n",
      "2017-11-05T14:41:50.714541: step 2450, loss 0.108848, acc 0.96875\n",
      "2017-11-05T14:41:54.727191: step 2451, loss 0.153829, acc 0.90625\n",
      "2017-11-05T14:41:58.735526: step 2452, loss 0.125448, acc 0.90625\n",
      "2017-11-05T14:42:02.817429: step 2453, loss 0.0453858, acc 1\n",
      "2017-11-05T14:42:06.877130: step 2454, loss 0.176818, acc 0.90625\n",
      "2017-11-05T14:42:10.921664: step 2455, loss 0.147217, acc 0.96875\n",
      "2017-11-05T14:42:14.974252: step 2456, loss 0.161515, acc 0.90625\n",
      "2017-11-05T14:42:18.985265: step 2457, loss 0.13731, acc 0.9375\n",
      "2017-11-05T14:42:23.096844: step 2458, loss 0.26748, acc 0.90625\n",
      "2017-11-05T14:42:27.127769: step 2459, loss 0.340246, acc 0.8125\n",
      "2017-11-05T14:42:31.099303: step 2460, loss 0.139866, acc 0.9375\n",
      "2017-11-05T14:42:35.284310: step 2461, loss 0.210368, acc 0.84375\n",
      "2017-11-05T14:42:39.353696: step 2462, loss 0.222622, acc 0.875\n",
      "2017-11-05T14:42:43.363557: step 2463, loss 0.131784, acc 0.9375\n",
      "2017-11-05T14:42:47.452039: step 2464, loss 0.259442, acc 0.8125\n",
      "2017-11-05T14:42:51.605342: step 2465, loss 0.179007, acc 0.9375\n",
      "2017-11-05T14:42:55.645542: step 2466, loss 0.204405, acc 0.875\n",
      "2017-11-05T14:42:59.552207: step 2467, loss 0.23038, acc 0.90625\n",
      "2017-11-05T14:43:03.492097: step 2468, loss 0.172241, acc 0.90625\n",
      "2017-11-05T14:43:07.422323: step 2469, loss 0.129837, acc 0.9375\n",
      "2017-11-05T14:43:11.409943: step 2470, loss 0.190311, acc 0.90625\n",
      "2017-11-05T14:43:15.313343: step 2471, loss 0.184068, acc 0.90625\n",
      "2017-11-05T14:43:19.205004: step 2472, loss 0.213985, acc 0.84375\n",
      "2017-11-05T14:43:23.234679: step 2473, loss 0.359571, acc 0.875\n",
      "2017-11-05T14:43:27.202505: step 2474, loss 0.175632, acc 0.90625\n",
      "2017-11-05T14:43:31.130728: step 2475, loss 0.245647, acc 0.90625\n",
      "2017-11-05T14:43:35.069417: step 2476, loss 0.0681616, acc 0.96875\n",
      "2017-11-05T14:43:39.051189: step 2477, loss 0.0702028, acc 0.9375\n",
      "2017-11-05T14:43:42.998952: step 2478, loss 0.269448, acc 0.875\n",
      "2017-11-05T14:43:46.973862: step 2479, loss 0.226837, acc 0.90625\n",
      "2017-11-05T14:43:50.892976: step 2480, loss 0.219949, acc 0.9375\n",
      "2017-11-05T14:43:54.809135: step 2481, loss 0.219993, acc 0.9375\n",
      "2017-11-05T14:43:58.687755: step 2482, loss 0.299925, acc 0.875\n",
      "2017-11-05T14:44:02.583126: step 2483, loss 0.134415, acc 0.9375\n",
      "2017-11-05T14:44:05.143313: step 2484, loss 0.521337, acc 0.75\n",
      "2017-11-05T14:44:09.062486: step 2485, loss 0.194556, acc 0.90625\n",
      "2017-11-05T14:44:12.969143: step 2486, loss 0.327519, acc 0.84375\n",
      "2017-11-05T14:44:16.864702: step 2487, loss 0.0853391, acc 0.96875\n",
      "2017-11-05T14:44:20.790623: step 2488, loss 0.120392, acc 0.9375\n",
      "2017-11-05T14:44:24.762818: step 2489, loss 0.149214, acc 0.90625\n",
      "2017-11-05T14:44:28.730860: step 2490, loss 0.136087, acc 0.90625\n",
      "2017-11-05T14:44:32.701762: step 2491, loss 0.324026, acc 0.84375\n",
      "2017-11-05T14:44:36.782359: step 2492, loss 0.168347, acc 0.9375\n",
      "2017-11-05T14:44:40.686764: step 2493, loss 0.139187, acc 0.9375\n",
      "2017-11-05T14:44:44.634054: step 2494, loss 0.114215, acc 0.9375\n",
      "2017-11-05T14:44:48.535221: step 2495, loss 0.345669, acc 0.78125\n",
      "2017-11-05T14:44:52.484574: step 2496, loss 0.114036, acc 0.9375\n",
      "2017-11-05T14:44:56.390679: step 2497, loss 0.139426, acc 0.90625\n",
      "2017-11-05T14:45:00.348733: step 2498, loss 0.210845, acc 0.9375\n",
      "2017-11-05T14:45:04.251724: step 2499, loss 0.192969, acc 0.84375\n",
      "2017-11-05T14:45:08.226379: step 2500, loss 0.247516, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:45:10.788259: step 2500, loss 0.878237, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-05T14:45:16.043204: step 2501, loss 0.0789423, acc 0.96875\n",
      "2017-11-05T14:45:20.028380: step 2502, loss 0.226922, acc 0.875\n",
      "2017-11-05T14:45:23.982969: step 2503, loss 0.0973873, acc 0.9375\n",
      "2017-11-05T14:45:27.945460: step 2504, loss 0.06275, acc 0.96875\n",
      "2017-11-05T14:45:31.909976: step 2505, loss 0.0353934, acc 1\n",
      "2017-11-05T14:45:35.828002: step 2506, loss 0.258697, acc 0.84375\n",
      "2017-11-05T14:45:39.746550: step 2507, loss 0.130495, acc 0.9375\n",
      "2017-11-05T14:45:43.687581: step 2508, loss 0.337063, acc 0.90625\n",
      "2017-11-05T14:45:47.606891: step 2509, loss 0.230016, acc 0.875\n",
      "2017-11-05T14:45:51.520847: step 2510, loss 0.152833, acc 0.90625\n",
      "2017-11-05T14:45:55.453714: step 2511, loss 0.201324, acc 0.875\n",
      "2017-11-05T14:45:59.382181: step 2512, loss 0.215165, acc 0.875\n",
      "2017-11-05T14:46:03.312856: step 2513, loss 0.217205, acc 0.875\n",
      "2017-11-05T14:46:07.244760: step 2514, loss 0.320427, acc 0.78125\n",
      "2017-11-05T14:46:11.205983: step 2515, loss 0.0595096, acc 1\n",
      "2017-11-05T14:46:15.154450: step 2516, loss 0.265109, acc 0.875\n",
      "2017-11-05T14:46:19.105892: step 2517, loss 0.105161, acc 0.96875\n",
      "2017-11-05T14:46:23.033430: step 2518, loss 0.147866, acc 0.9375\n",
      "2017-11-05T14:46:27.028812: step 2519, loss 0.234632, acc 0.90625\n",
      "2017-11-05T14:46:29.530142: step 2520, loss 0.342397, acc 0.9\n",
      "2017-11-05T14:46:33.568851: step 2521, loss 0.0958929, acc 0.96875\n",
      "2017-11-05T14:46:37.776468: step 2522, loss 0.38657, acc 0.875\n",
      "2017-11-05T14:46:41.856025: step 2523, loss 0.444018, acc 0.75\n",
      "2017-11-05T14:46:45.734000: step 2524, loss 0.120038, acc 0.9375\n",
      "2017-11-05T14:46:49.701956: step 2525, loss 0.276307, acc 0.875\n",
      "2017-11-05T14:46:53.626521: step 2526, loss 0.253906, acc 0.875\n",
      "2017-11-05T14:46:57.543496: step 2527, loss 0.291241, acc 0.875\n",
      "2017-11-05T14:47:01.480755: step 2528, loss 0.218707, acc 0.9375\n",
      "2017-11-05T14:47:05.435528: step 2529, loss 0.0309221, acc 1\n",
      "2017-11-05T14:47:09.417525: step 2530, loss 0.20009, acc 0.875\n",
      "2017-11-05T14:47:13.333737: step 2531, loss 0.190102, acc 0.90625\n",
      "2017-11-05T14:47:17.308368: step 2532, loss 0.0902637, acc 0.96875\n",
      "2017-11-05T14:47:21.241111: step 2533, loss 0.060801, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T14:47:25.195180: step 2534, loss 0.101188, acc 0.96875\n",
      "2017-11-05T14:47:29.119152: step 2535, loss 0.185998, acc 0.9375\n",
      "2017-11-05T14:47:33.020752: step 2536, loss 0.240967, acc 0.90625\n",
      "2017-11-05T14:47:36.913837: step 2537, loss 0.305978, acc 0.78125\n",
      "2017-11-05T14:47:40.882190: step 2538, loss 0.0630063, acc 0.96875\n",
      "2017-11-05T14:47:44.772814: step 2539, loss 0.354172, acc 0.84375\n",
      "2017-11-05T14:47:48.715377: step 2540, loss 0.112883, acc 0.96875\n",
      "2017-11-05T14:47:52.650290: step 2541, loss 0.138737, acc 0.875\n",
      "2017-11-05T14:47:56.575247: step 2542, loss 0.213518, acc 0.90625\n",
      "2017-11-05T14:48:00.509773: step 2543, loss 0.144024, acc 0.96875\n",
      "2017-11-05T14:48:04.540508: step 2544, loss 0.146872, acc 0.9375\n",
      "2017-11-05T14:48:08.522468: step 2545, loss 0.393107, acc 0.78125\n",
      "2017-11-05T14:48:12.544993: step 2546, loss 0.114212, acc 0.9375\n",
      "2017-11-05T14:48:16.486126: step 2547, loss 0.16231, acc 0.90625\n",
      "2017-11-05T14:48:20.401333: step 2548, loss 0.147105, acc 0.9375\n",
      "2017-11-05T14:48:24.551214: step 2549, loss 0.340156, acc 0.8125\n",
      "2017-11-05T14:48:28.584290: step 2550, loss 0.225546, acc 0.9375\n",
      "2017-11-05T14:48:32.538981: step 2551, loss 0.149384, acc 0.90625\n",
      "2017-11-05T14:48:36.645566: step 2552, loss 0.363499, acc 0.8125\n",
      "2017-11-05T14:48:40.781669: step 2553, loss 0.0201416, acc 1\n",
      "2017-11-05T14:48:44.883624: step 2554, loss 0.17314, acc 0.9375\n",
      "2017-11-05T14:48:49.052548: step 2555, loss 0.491351, acc 0.8125\n",
      "2017-11-05T14:48:51.678001: step 2556, loss 0.184655, acc 0.95\n",
      "2017-11-05T14:48:55.740172: step 2557, loss 0.251156, acc 0.90625\n",
      "2017-11-05T14:48:59.838788: step 2558, loss 0.113001, acc 0.9375\n",
      "2017-11-05T14:49:03.913925: step 2559, loss 0.217799, acc 0.90625\n",
      "2017-11-05T14:49:08.070872: step 2560, loss 0.16985, acc 0.9375\n",
      "2017-11-05T14:49:12.114707: step 2561, loss 0.122587, acc 0.90625\n",
      "2017-11-05T14:49:16.120688: step 2562, loss 0.157524, acc 0.9375\n",
      "2017-11-05T14:49:20.179389: step 2563, loss 0.16717, acc 0.90625\n",
      "2017-11-05T14:49:24.344816: step 2564, loss 0.104168, acc 0.96875\n",
      "2017-11-05T14:49:28.327754: step 2565, loss 0.0760006, acc 0.96875\n",
      "2017-11-05T14:49:32.460347: step 2566, loss 0.305467, acc 0.875\n",
      "2017-11-05T14:49:36.487325: step 2567, loss 0.0594645, acc 0.96875\n",
      "2017-11-05T14:49:40.606182: step 2568, loss 0.325975, acc 0.84375\n",
      "2017-11-05T14:49:44.674019: step 2569, loss 0.281722, acc 0.875\n",
      "2017-11-05T14:49:48.898131: step 2570, loss 0.115751, acc 0.90625\n",
      "2017-11-05T14:49:53.001311: step 2571, loss 0.132681, acc 0.90625\n",
      "2017-11-05T14:49:56.950067: step 2572, loss 0.230324, acc 0.84375\n",
      "2017-11-05T14:50:01.155670: step 2573, loss 0.147978, acc 0.90625\n",
      "2017-11-05T14:50:05.183493: step 2574, loss 0.191759, acc 0.90625\n",
      "2017-11-05T14:50:09.108560: step 2575, loss 0.158354, acc 0.90625\n",
      "2017-11-05T14:50:13.022112: step 2576, loss 0.209867, acc 0.9375\n",
      "2017-11-05T14:50:17.032399: step 2577, loss 0.186734, acc 0.90625\n",
      "2017-11-05T14:50:20.980970: step 2578, loss 0.112129, acc 0.96875\n",
      "2017-11-05T14:50:24.879435: step 2579, loss 0.0407684, acc 1\n",
      "2017-11-05T14:50:28.860757: step 2580, loss 0.262625, acc 0.9375\n",
      "2017-11-05T14:50:32.870556: step 2581, loss 0.0443322, acc 0.96875\n",
      "2017-11-05T14:50:36.851998: step 2582, loss 0.167551, acc 0.90625\n",
      "2017-11-05T14:50:40.907820: step 2583, loss 0.100107, acc 0.9375\n",
      "2017-11-05T14:50:44.835596: step 2584, loss 0.251734, acc 0.875\n",
      "2017-11-05T14:50:48.811075: step 2585, loss 0.14563, acc 0.9375\n",
      "2017-11-05T14:50:52.757334: step 2586, loss 0.187571, acc 0.90625\n",
      "2017-11-05T14:50:56.708118: step 2587, loss 0.0885497, acc 0.96875\n",
      "2017-11-05T14:51:00.726665: step 2588, loss 0.0562199, acc 0.96875\n",
      "2017-11-05T14:51:04.646359: step 2589, loss 0.204194, acc 0.90625\n",
      "2017-11-05T14:51:08.601947: step 2590, loss 0.100855, acc 0.96875\n",
      "2017-11-05T14:51:12.506259: step 2591, loss 0.105267, acc 0.9375\n",
      "2017-11-05T14:51:15.049735: step 2592, loss 0.374536, acc 0.8\n",
      "2017-11-05T14:51:18.955828: step 2593, loss 0.0888335, acc 0.96875\n",
      "2017-11-05T14:51:22.915592: step 2594, loss 0.174015, acc 0.9375\n",
      "2017-11-05T14:51:26.816602: step 2595, loss 0.142857, acc 0.9375\n",
      "2017-11-05T14:51:30.810516: step 2596, loss 0.13061, acc 0.9375\n",
      "2017-11-05T14:51:34.760304: step 2597, loss 0.237202, acc 0.84375\n",
      "2017-11-05T14:51:38.705123: step 2598, loss 0.13496, acc 0.96875\n",
      "2017-11-05T14:51:42.656946: step 2599, loss 0.14408, acc 0.96875\n",
      "2017-11-05T14:51:46.600217: step 2600, loss 0.119144, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:51:49.152274: step 2600, loss 0.916896, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-05T14:51:54.430582: step 2601, loss 0.310974, acc 0.8125\n",
      "2017-11-05T14:51:58.450541: step 2602, loss 0.130663, acc 0.90625\n",
      "2017-11-05T14:52:02.476934: step 2603, loss 0.126184, acc 0.90625\n",
      "2017-11-05T14:52:06.423551: step 2604, loss 0.225385, acc 0.875\n",
      "2017-11-05T14:52:10.484097: step 2605, loss 0.143176, acc 0.9375\n",
      "2017-11-05T14:52:14.498515: step 2606, loss 0.147095, acc 0.9375\n",
      "2017-11-05T14:52:18.522611: step 2607, loss 0.190056, acc 0.90625\n",
      "2017-11-05T14:52:22.516781: step 2608, loss 0.163883, acc 0.9375\n",
      "2017-11-05T14:52:26.453650: step 2609, loss 0.276017, acc 0.875\n",
      "2017-11-05T14:52:30.560603: step 2610, loss 0.191024, acc 0.90625\n",
      "2017-11-05T14:52:34.616267: step 2611, loss 0.082142, acc 0.96875\n",
      "2017-11-05T14:52:38.558263: step 2612, loss 0.316256, acc 0.8125\n",
      "2017-11-05T14:52:42.517271: step 2613, loss 0.145259, acc 0.9375\n",
      "2017-11-05T14:52:46.447343: step 2614, loss 0.256279, acc 0.875\n",
      "2017-11-05T14:52:50.612766: step 2615, loss 0.295111, acc 0.90625\n",
      "2017-11-05T14:52:54.610532: step 2616, loss 0.265418, acc 0.90625\n",
      "2017-11-05T14:52:58.527514: step 2617, loss 0.339485, acc 0.875\n",
      "2017-11-05T14:53:02.596184: step 2618, loss 0.302353, acc 0.90625\n",
      "2017-11-05T14:53:06.561336: step 2619, loss 0.332084, acc 0.875\n",
      "2017-11-05T14:53:10.503377: step 2620, loss 0.104106, acc 0.9375\n",
      "2017-11-05T14:53:14.550271: step 2621, loss 0.184985, acc 0.875\n",
      "2017-11-05T14:53:18.503643: step 2622, loss 0.263545, acc 0.875\n",
      "2017-11-05T14:53:22.570673: step 2623, loss 0.191526, acc 0.90625\n",
      "2017-11-05T14:53:26.609557: step 2624, loss 0.296108, acc 0.875\n",
      "2017-11-05T14:53:30.534678: step 2625, loss 0.274334, acc 0.90625\n",
      "2017-11-05T14:53:34.530565: step 2626, loss 0.252037, acc 0.90625\n",
      "2017-11-05T14:53:38.530863: step 2627, loss 0.0157113, acc 1\n",
      "2017-11-05T14:53:41.044853: step 2628, loss 0.090388, acc 0.95\n",
      "2017-11-05T14:53:44.990280: step 2629, loss 0.131061, acc 0.9375\n",
      "2017-11-05T14:53:48.951552: step 2630, loss 0.446316, acc 0.78125\n",
      "2017-11-05T14:53:52.928466: step 2631, loss 0.194013, acc 0.90625\n",
      "2017-11-05T14:53:56.852798: step 2632, loss 0.200624, acc 0.90625\n",
      "2017-11-05T14:54:00.855439: step 2633, loss 0.0852484, acc 0.96875\n",
      "2017-11-05T14:54:04.881132: step 2634, loss 0.0812694, acc 0.96875\n",
      "2017-11-05T14:54:08.830824: step 2635, loss 0.283554, acc 0.90625\n",
      "2017-11-05T14:54:12.830096: step 2636, loss 0.165615, acc 0.90625\n",
      "2017-11-05T14:54:16.756606: step 2637, loss 0.0544598, acc 1\n",
      "2017-11-05T14:54:20.707877: step 2638, loss 0.0117349, acc 1\n",
      "2017-11-05T14:54:24.663697: step 2639, loss 0.307774, acc 0.84375\n",
      "2017-11-05T14:54:28.621250: step 2640, loss 0.102723, acc 0.9375\n",
      "2017-11-05T14:54:32.647673: step 2641, loss 0.144931, acc 0.96875\n",
      "2017-11-05T14:54:36.722020: step 2642, loss 0.121224, acc 0.96875\n",
      "2017-11-05T14:54:40.687215: step 2643, loss 0.270658, acc 0.875\n",
      "2017-11-05T14:54:44.682102: step 2644, loss 0.373329, acc 0.84375\n",
      "2017-11-05T14:54:48.636219: step 2645, loss 0.42786, acc 0.78125\n",
      "2017-11-05T14:54:52.607281: step 2646, loss 0.0567073, acc 0.96875\n",
      "2017-11-05T14:54:56.586844: step 2647, loss 0.0895548, acc 0.9375\n",
      "2017-11-05T14:55:00.575305: step 2648, loss 0.152125, acc 0.9375\n",
      "2017-11-05T14:55:04.487454: step 2649, loss 0.286071, acc 0.90625\n",
      "2017-11-05T14:55:08.495458: step 2650, loss 0.224119, acc 0.90625\n",
      "2017-11-05T14:55:12.474606: step 2651, loss 0.328423, acc 0.84375\n",
      "2017-11-05T14:55:16.433665: step 2652, loss 0.380368, acc 0.84375\n",
      "2017-11-05T14:55:20.358261: step 2653, loss 0.260051, acc 0.84375\n",
      "2017-11-05T14:55:24.355751: step 2654, loss 0.242512, acc 0.90625\n",
      "2017-11-05T14:55:28.288592: step 2655, loss 0.217653, acc 0.875\n",
      "2017-11-05T14:55:32.238821: step 2656, loss 0.166662, acc 0.96875\n",
      "2017-11-05T14:55:36.213852: step 2657, loss 0.601667, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T14:55:40.194234: step 2658, loss 0.224325, acc 0.9375\n",
      "2017-11-05T14:55:44.192089: step 2659, loss 0.221914, acc 0.9375\n",
      "2017-11-05T14:55:48.125971: step 2660, loss 0.137286, acc 0.9375\n",
      "2017-11-05T14:55:52.135231: step 2661, loss 0.116045, acc 0.9375\n",
      "2017-11-05T14:55:56.101651: step 2662, loss 0.0605742, acc 0.96875\n",
      "2017-11-05T14:56:00.056732: step 2663, loss 0.297593, acc 0.875\n",
      "2017-11-05T14:56:02.644888: step 2664, loss 0.1757, acc 0.95\n",
      "2017-11-05T14:56:06.653731: step 2665, loss 0.0985912, acc 0.9375\n",
      "2017-11-05T14:56:10.607392: step 2666, loss 0.0869366, acc 0.96875\n",
      "2017-11-05T14:56:14.570130: step 2667, loss 0.101162, acc 0.9375\n",
      "2017-11-05T14:56:18.526719: step 2668, loss 0.141198, acc 0.90625\n",
      "2017-11-05T14:56:22.432656: step 2669, loss 0.0646064, acc 0.96875\n",
      "2017-11-05T14:56:26.448615: step 2670, loss 0.102616, acc 0.96875\n",
      "2017-11-05T14:56:30.405270: step 2671, loss 0.186838, acc 0.90625\n",
      "2017-11-05T14:56:34.539550: step 2672, loss 0.255131, acc 0.90625\n",
      "2017-11-05T14:56:38.472812: step 2673, loss 0.0483451, acc 0.96875\n",
      "2017-11-05T14:56:42.469431: step 2674, loss 0.484013, acc 0.75\n",
      "2017-11-05T14:56:46.434740: step 2675, loss 0.115604, acc 0.9375\n",
      "2017-11-05T14:56:50.452409: step 2676, loss 0.0853246, acc 0.9375\n",
      "2017-11-05T14:56:54.452781: step 2677, loss 0.112112, acc 0.96875\n",
      "2017-11-05T14:56:58.433255: step 2678, loss 0.106123, acc 0.9375\n",
      "2017-11-05T14:57:02.423785: step 2679, loss 0.282749, acc 0.875\n",
      "2017-11-05T14:57:06.402582: step 2680, loss 0.271165, acc 0.8125\n",
      "2017-11-05T14:57:10.396196: step 2681, loss 0.196702, acc 0.90625\n",
      "2017-11-05T14:57:14.361897: step 2682, loss 0.165952, acc 0.9375\n",
      "2017-11-05T14:57:18.281823: step 2683, loss 0.191663, acc 0.875\n",
      "2017-11-05T14:57:22.239236: step 2684, loss 0.255254, acc 0.875\n",
      "2017-11-05T14:57:26.225644: step 2685, loss 0.221963, acc 0.84375\n",
      "2017-11-05T14:57:30.185528: step 2686, loss 0.392808, acc 0.8125\n",
      "2017-11-05T14:57:34.131494: step 2687, loss 0.050525, acc 1\n",
      "2017-11-05T14:57:38.056957: step 2688, loss 0.074128, acc 0.9375\n",
      "2017-11-05T14:57:42.028719: step 2689, loss 0.0923191, acc 0.9375\n",
      "2017-11-05T14:57:45.988564: step 2690, loss 0.410483, acc 0.875\n",
      "2017-11-05T14:57:49.988948: step 2691, loss 0.54125, acc 0.8125\n",
      "2017-11-05T14:57:53.984116: step 2692, loss 0.243586, acc 0.875\n",
      "2017-11-05T14:57:57.884848: step 2693, loss 0.170255, acc 0.9375\n",
      "2017-11-05T14:58:01.836423: step 2694, loss 0.167115, acc 0.90625\n",
      "2017-11-05T14:58:05.808996: step 2695, loss 0.141513, acc 0.90625\n",
      "2017-11-05T14:58:09.847266: step 2696, loss 0.156917, acc 0.9375\n",
      "2017-11-05T14:58:13.813350: step 2697, loss 0.247158, acc 0.875\n",
      "2017-11-05T14:58:17.793731: step 2698, loss 0.151618, acc 0.9375\n",
      "2017-11-05T14:58:21.888454: step 2699, loss 0.124632, acc 0.96875\n",
      "2017-11-05T14:58:24.770818: step 2700, loss 0.216493, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T14:58:27.428143: step 2700, loss 0.811048, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-05T14:58:33.143764: step 2701, loss 0.217075, acc 0.875\n",
      "2017-11-05T14:58:37.225613: step 2702, loss 0.085843, acc 0.9375\n",
      "2017-11-05T14:58:41.231971: step 2703, loss 0.151948, acc 0.90625\n",
      "2017-11-05T14:58:45.286975: step 2704, loss 0.206038, acc 0.875\n",
      "2017-11-05T14:58:49.310298: step 2705, loss 0.0964471, acc 0.9375\n",
      "2017-11-05T14:58:53.337026: step 2706, loss 0.156655, acc 0.9375\n",
      "2017-11-05T14:58:57.400231: step 2707, loss 0.0773046, acc 0.96875\n",
      "2017-11-05T14:59:01.434149: step 2708, loss 0.255326, acc 0.875\n",
      "2017-11-05T14:59:05.365459: step 2709, loss 0.0857306, acc 0.9375\n",
      "2017-11-05T14:59:09.386680: step 2710, loss 0.222879, acc 0.90625\n",
      "2017-11-05T14:59:13.456155: step 2711, loss 0.171466, acc 0.9375\n",
      "2017-11-05T14:59:17.455449: step 2712, loss 0.408062, acc 0.875\n",
      "2017-11-05T14:59:21.397818: step 2713, loss 0.334819, acc 0.78125\n",
      "2017-11-05T14:59:25.417447: step 2714, loss 0.0645919, acc 0.96875\n",
      "2017-11-05T14:59:29.493926: step 2715, loss 0.196263, acc 0.9375\n",
      "2017-11-05T14:59:33.525749: step 2716, loss 0.340652, acc 0.84375\n",
      "2017-11-05T14:59:37.495936: step 2717, loss 0.145608, acc 0.9375\n",
      "2017-11-05T14:59:41.510167: step 2718, loss 0.185554, acc 0.84375\n",
      "2017-11-05T14:59:45.539800: step 2719, loss 0.165233, acc 0.90625\n",
      "2017-11-05T14:59:49.578753: step 2720, loss 0.213164, acc 0.90625\n",
      "2017-11-05T14:59:53.594706: step 2721, loss 0.057693, acc 0.96875\n",
      "2017-11-05T14:59:57.563356: step 2722, loss 0.190293, acc 0.9375\n",
      "2017-11-05T15:00:01.859383: step 2723, loss 0.101769, acc 0.9375\n",
      "2017-11-05T15:00:05.920541: step 2724, loss 0.0893728, acc 0.9375\n",
      "2017-11-05T15:00:09.930655: step 2725, loss 0.204553, acc 0.875\n",
      "2017-11-05T15:00:14.205223: step 2726, loss 0.254506, acc 0.90625\n",
      "2017-11-05T15:00:18.261607: step 2727, loss 0.4012, acc 0.8125\n",
      "2017-11-05T15:00:22.494097: step 2728, loss 0.157307, acc 0.90625\n",
      "2017-11-05T15:00:26.601525: step 2729, loss 0.20276, acc 0.875\n",
      "2017-11-05T15:00:30.637536: step 2730, loss 0.0660333, acc 0.9375\n",
      "2017-11-05T15:00:34.812682: step 2731, loss 0.14969, acc 0.9375\n",
      "2017-11-05T15:00:38.918607: step 2732, loss 0.285086, acc 0.84375\n",
      "2017-11-05T15:00:42.918778: step 2733, loss 0.095912, acc 0.9375\n",
      "2017-11-05T15:00:46.962501: step 2734, loss 0.129554, acc 0.96875\n",
      "2017-11-05T15:00:50.975996: step 2735, loss 0.220369, acc 0.875\n",
      "2017-11-05T15:00:53.606940: step 2736, loss 0.530097, acc 0.8\n",
      "2017-11-05T15:00:57.621419: step 2737, loss 0.26996, acc 0.875\n",
      "2017-11-05T15:01:01.649025: step 2738, loss 0.425825, acc 0.84375\n",
      "2017-11-05T15:01:05.702682: step 2739, loss 0.104968, acc 0.9375\n",
      "2017-11-05T15:01:09.776822: step 2740, loss 0.27869, acc 0.84375\n",
      "2017-11-05T15:01:13.756895: step 2741, loss 0.282518, acc 0.875\n",
      "2017-11-05T15:01:17.756204: step 2742, loss 0.388036, acc 0.84375\n",
      "2017-11-05T15:01:21.805235: step 2743, loss 0.174668, acc 0.90625\n",
      "2017-11-05T15:01:25.768718: step 2744, loss 0.140347, acc 0.9375\n",
      "2017-11-05T15:01:29.831614: step 2745, loss 0.271802, acc 0.90625\n",
      "2017-11-05T15:01:33.867391: step 2746, loss 0.21564, acc 0.90625\n",
      "2017-11-05T15:01:37.875349: step 2747, loss 0.143535, acc 0.90625\n",
      "2017-11-05T15:01:41.883604: step 2748, loss 0.169823, acc 0.90625\n",
      "2017-11-05T15:01:45.836585: step 2749, loss 0.0491807, acc 0.96875\n",
      "2017-11-05T15:01:49.842567: step 2750, loss 0.189054, acc 0.9375\n",
      "2017-11-05T15:01:53.865529: step 2751, loss 0.208486, acc 0.875\n",
      "2017-11-05T15:01:57.839348: step 2752, loss 0.10809, acc 0.96875\n",
      "2017-11-05T15:02:02.024393: step 2753, loss 0.119437, acc 0.9375\n",
      "2017-11-05T15:02:06.077895: step 2754, loss 0.143945, acc 0.9375\n",
      "2017-11-05T15:02:10.103301: step 2755, loss 0.119345, acc 0.9375\n",
      "2017-11-05T15:02:14.093441: step 2756, loss 0.132484, acc 0.9375\n",
      "2017-11-05T15:02:18.087016: step 2757, loss 0.303872, acc 0.875\n",
      "2017-11-05T15:02:22.083959: step 2758, loss 0.22531, acc 0.875\n",
      "2017-11-05T15:02:26.061610: step 2759, loss 0.273283, acc 0.875\n",
      "2017-11-05T15:02:30.085901: step 2760, loss 0.149114, acc 0.90625\n",
      "2017-11-05T15:02:34.182687: step 2761, loss 0.347266, acc 0.875\n",
      "2017-11-05T15:02:38.351905: step 2762, loss 0.0613417, acc 0.96875\n",
      "2017-11-05T15:02:42.367660: step 2763, loss 0.110573, acc 0.9375\n",
      "2017-11-05T15:02:46.394527: step 2764, loss 0.363453, acc 0.8125\n",
      "2017-11-05T15:02:50.545556: step 2765, loss 0.279408, acc 0.875\n",
      "2017-11-05T15:02:54.618476: step 2766, loss 0.275502, acc 0.875\n",
      "2017-11-05T15:02:58.640387: step 2767, loss 0.119142, acc 0.9375\n",
      "2017-11-05T15:03:02.585248: step 2768, loss 0.095243, acc 0.96875\n",
      "2017-11-05T15:03:06.581233: step 2769, loss 0.101803, acc 0.96875\n",
      "2017-11-05T15:03:10.584080: step 2770, loss 0.190597, acc 0.90625\n",
      "2017-11-05T15:03:14.662217: step 2771, loss 0.276833, acc 0.84375\n",
      "2017-11-05T15:03:17.250675: step 2772, loss 0.0224234, acc 1\n",
      "2017-11-05T15:03:21.247904: step 2773, loss 0.0485163, acc 1\n",
      "2017-11-05T15:03:25.382060: step 2774, loss 0.20029, acc 0.875\n",
      "2017-11-05T15:03:29.546959: step 2775, loss 0.11223, acc 0.96875\n",
      "2017-11-05T15:03:33.572696: step 2776, loss 0.210558, acc 0.875\n",
      "2017-11-05T15:03:37.582082: step 2777, loss 0.317068, acc 0.84375\n",
      "2017-11-05T15:03:41.626212: step 2778, loss 0.191508, acc 0.9375\n",
      "2017-11-05T15:03:45.619299: step 2779, loss 0.212339, acc 0.90625\n",
      "2017-11-05T15:03:49.631311: step 2780, loss 0.0972746, acc 0.9375\n",
      "2017-11-05T15:03:53.634809: step 2781, loss 0.123313, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:03:57.670360: step 2782, loss 0.162, acc 0.9375\n",
      "2017-11-05T15:04:01.694538: step 2783, loss 0.214171, acc 0.84375\n",
      "2017-11-05T15:04:05.740948: step 2784, loss 0.137693, acc 0.90625\n",
      "2017-11-05T15:04:09.763221: step 2785, loss 0.107804, acc 0.96875\n",
      "2017-11-05T15:04:13.722790: step 2786, loss 0.0620141, acc 0.96875\n",
      "2017-11-05T15:04:17.669147: step 2787, loss 0.135109, acc 0.9375\n",
      "2017-11-05T15:04:21.586815: step 2788, loss 0.150713, acc 0.90625\n",
      "2017-11-05T15:04:25.516044: step 2789, loss 0.294093, acc 0.84375\n",
      "2017-11-05T15:04:29.437917: step 2790, loss 0.161373, acc 0.90625\n",
      "2017-11-05T15:04:33.444373: step 2791, loss 0.245956, acc 0.90625\n",
      "2017-11-05T15:04:37.452850: step 2792, loss 0.137058, acc 0.90625\n",
      "2017-11-05T15:04:41.384614: step 2793, loss 0.225807, acc 0.9375\n",
      "2017-11-05T15:04:45.305956: step 2794, loss 0.235982, acc 0.875\n",
      "2017-11-05T15:04:49.220907: step 2795, loss 0.101408, acc 0.96875\n",
      "2017-11-05T15:04:53.159352: step 2796, loss 0.348824, acc 0.8125\n",
      "2017-11-05T15:04:57.088852: step 2797, loss 0.129928, acc 0.9375\n",
      "2017-11-05T15:05:01.011690: step 2798, loss 0.213066, acc 0.90625\n",
      "2017-11-05T15:05:04.969510: step 2799, loss 0.149194, acc 0.90625\n",
      "2017-11-05T15:05:08.927411: step 2800, loss 0.161316, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:05:11.461326: step 2800, loss 0.841394, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-05T15:05:16.970193: step 2801, loss 0.271752, acc 0.875\n",
      "2017-11-05T15:05:20.909060: step 2802, loss 0.172397, acc 0.9375\n",
      "2017-11-05T15:05:24.835805: step 2803, loss 0.169074, acc 0.9375\n",
      "2017-11-05T15:05:28.771481: step 2804, loss 0.214779, acc 0.90625\n",
      "2017-11-05T15:05:32.684020: step 2805, loss 0.294844, acc 0.8125\n",
      "2017-11-05T15:05:36.590769: step 2806, loss 0.175588, acc 0.875\n",
      "2017-11-05T15:05:40.507428: step 2807, loss 0.32603, acc 0.84375\n",
      "2017-11-05T15:05:42.994835: step 2808, loss 0.40213, acc 0.9\n",
      "2017-11-05T15:05:46.940075: step 2809, loss 0.203109, acc 0.90625\n",
      "2017-11-05T15:05:50.841577: step 2810, loss 0.202074, acc 0.875\n",
      "2017-11-05T15:05:54.781642: step 2811, loss 0.262987, acc 0.90625\n",
      "2017-11-05T15:05:58.742426: step 2812, loss 0.0873657, acc 0.9375\n",
      "2017-11-05T15:06:02.641835: step 2813, loss 0.0936851, acc 0.9375\n",
      "2017-11-05T15:06:06.571209: step 2814, loss 0.0852362, acc 0.9375\n",
      "2017-11-05T15:06:10.504367: step 2815, loss 0.353818, acc 0.84375\n",
      "2017-11-05T15:06:14.463428: step 2816, loss 0.196916, acc 0.9375\n",
      "2017-11-05T15:06:18.447078: step 2817, loss 0.113874, acc 0.9375\n",
      "2017-11-05T15:06:22.315858: step 2818, loss 0.317822, acc 0.8125\n",
      "2017-11-05T15:06:26.222474: step 2819, loss 0.139758, acc 0.9375\n",
      "2017-11-05T15:06:30.097309: step 2820, loss 0.243764, acc 0.84375\n",
      "2017-11-05T15:06:34.181143: step 2821, loss 0.22842, acc 0.875\n",
      "2017-11-05T15:06:38.153444: step 2822, loss 0.123374, acc 0.96875\n",
      "2017-11-05T15:06:42.060728: step 2823, loss 0.103139, acc 0.96875\n",
      "2017-11-05T15:06:46.040005: step 2824, loss 0.297564, acc 0.84375\n",
      "2017-11-05T15:06:49.940846: step 2825, loss 0.1334, acc 0.96875\n",
      "2017-11-05T15:06:53.843009: step 2826, loss 0.366425, acc 0.875\n",
      "2017-11-05T15:06:57.811546: step 2827, loss 0.106262, acc 0.9375\n",
      "2017-11-05T15:07:01.771666: step 2828, loss 0.355134, acc 0.8125\n",
      "2017-11-05T15:07:05.723098: step 2829, loss 0.202433, acc 0.875\n",
      "2017-11-05T15:07:09.646860: step 2830, loss 0.0881157, acc 0.96875\n",
      "2017-11-05T15:07:13.594318: step 2831, loss 0.171343, acc 0.90625\n",
      "2017-11-05T15:07:17.524290: step 2832, loss 0.18474, acc 0.90625\n",
      "2017-11-05T15:07:21.451384: step 2833, loss 0.111034, acc 0.9375\n",
      "2017-11-05T15:07:25.400922: step 2834, loss 0.154997, acc 0.90625\n",
      "2017-11-05T15:07:29.264824: step 2835, loss 0.112568, acc 0.96875\n",
      "2017-11-05T15:07:33.177191: step 2836, loss 0.13939, acc 0.9375\n",
      "2017-11-05T15:07:37.123338: step 2837, loss 0.334163, acc 0.875\n",
      "2017-11-05T15:07:41.071839: step 2838, loss 0.0506559, acc 1\n",
      "2017-11-05T15:07:44.956701: step 2839, loss 0.236584, acc 0.90625\n",
      "2017-11-05T15:07:48.885187: step 2840, loss 0.139913, acc 0.96875\n",
      "2017-11-05T15:07:52.861142: step 2841, loss 0.431756, acc 0.78125\n",
      "2017-11-05T15:07:56.777138: step 2842, loss 0.04421, acc 0.96875\n",
      "2017-11-05T15:08:00.751811: step 2843, loss 0.153691, acc 0.9375\n",
      "2017-11-05T15:08:03.295952: step 2844, loss 0.256078, acc 0.9\n",
      "2017-11-05T15:08:07.268684: step 2845, loss 0.233754, acc 0.875\n",
      "2017-11-05T15:08:11.181768: step 2846, loss 0.226318, acc 0.90625\n",
      "2017-11-05T15:08:15.153841: step 2847, loss 0.152559, acc 0.9375\n",
      "2017-11-05T15:08:19.091520: step 2848, loss 0.162132, acc 0.90625\n",
      "2017-11-05T15:08:23.023385: step 2849, loss 0.13466, acc 0.9375\n",
      "2017-11-05T15:08:26.930159: step 2850, loss 0.0379473, acc 1\n",
      "2017-11-05T15:08:30.841840: step 2851, loss 0.29687, acc 0.84375\n",
      "2017-11-05T15:08:34.841987: step 2852, loss 0.277759, acc 0.875\n",
      "2017-11-05T15:08:38.798842: step 2853, loss 0.211844, acc 0.90625\n",
      "2017-11-05T15:08:42.699498: step 2854, loss 0.136502, acc 0.9375\n",
      "2017-11-05T15:08:46.753868: step 2855, loss 0.159222, acc 0.90625\n",
      "2017-11-05T15:08:50.715845: step 2856, loss 0.242061, acc 0.84375\n",
      "2017-11-05T15:08:54.633967: step 2857, loss 0.132004, acc 0.96875\n",
      "2017-11-05T15:08:58.564498: step 2858, loss 0.200087, acc 0.875\n",
      "2017-11-05T15:09:02.528392: step 2859, loss 0.229003, acc 0.90625\n",
      "2017-11-05T15:09:06.554334: step 2860, loss 0.203811, acc 0.9375\n",
      "2017-11-05T15:09:10.525449: step 2861, loss 0.0910516, acc 0.9375\n",
      "2017-11-05T15:09:14.509552: step 2862, loss 0.19916, acc 0.9375\n",
      "2017-11-05T15:09:18.489836: step 2863, loss 0.258705, acc 0.78125\n",
      "2017-11-05T15:09:22.459663: step 2864, loss 0.073242, acc 0.96875\n",
      "2017-11-05T15:09:26.787252: step 2865, loss 0.208846, acc 0.875\n",
      "2017-11-05T15:09:30.736766: step 2866, loss 0.124505, acc 0.96875\n",
      "2017-11-05T15:09:34.629563: step 2867, loss 0.248254, acc 0.875\n",
      "2017-11-05T15:09:38.589648: step 2868, loss 0.235362, acc 0.875\n",
      "2017-11-05T15:09:42.488633: step 2869, loss 0.0526936, acc 0.96875\n",
      "2017-11-05T15:09:46.453280: step 2870, loss 0.15605, acc 0.9375\n",
      "2017-11-05T15:09:50.385066: step 2871, loss 0.148599, acc 0.90625\n",
      "2017-11-05T15:09:54.280759: step 2872, loss 0.407239, acc 0.84375\n",
      "2017-11-05T15:09:58.178742: step 2873, loss 0.208863, acc 0.84375\n",
      "2017-11-05T15:10:02.422806: step 2874, loss 0.164968, acc 0.9375\n",
      "2017-11-05T15:10:06.416456: step 2875, loss 0.217804, acc 0.90625\n",
      "2017-11-05T15:10:10.381776: step 2876, loss 0.200296, acc 0.9375\n",
      "2017-11-05T15:10:14.276308: step 2877, loss 0.115285, acc 0.9375\n",
      "2017-11-05T15:10:18.244412: step 2878, loss 0.289811, acc 0.90625\n",
      "2017-11-05T15:10:22.152964: step 2879, loss 0.274608, acc 0.90625\n",
      "2017-11-05T15:10:24.719871: step 2880, loss 0.355712, acc 0.95\n",
      "2017-11-05T15:10:28.624276: step 2881, loss 0.0569276, acc 0.9375\n",
      "2017-11-05T15:10:32.558321: step 2882, loss 0.091366, acc 0.96875\n",
      "2017-11-05T15:10:36.605469: step 2883, loss 0.286599, acc 0.84375\n",
      "2017-11-05T15:10:40.509187: step 2884, loss 0.144288, acc 0.9375\n",
      "2017-11-05T15:10:44.445671: step 2885, loss 0.233021, acc 0.84375\n",
      "2017-11-05T15:10:48.470540: step 2886, loss 0.119233, acc 0.9375\n",
      "2017-11-05T15:10:52.417177: step 2887, loss 0.264389, acc 0.9375\n",
      "2017-11-05T15:10:56.361745: step 2888, loss 0.107643, acc 0.96875\n",
      "2017-11-05T15:11:00.332150: step 2889, loss 0.192319, acc 0.90625\n",
      "2017-11-05T15:11:04.244848: step 2890, loss 0.178235, acc 0.90625\n",
      "2017-11-05T15:11:08.177526: step 2891, loss 0.235719, acc 0.84375\n",
      "2017-11-05T15:11:12.064565: step 2892, loss 0.203321, acc 0.875\n",
      "2017-11-05T15:11:15.967930: step 2893, loss 0.168963, acc 0.9375\n",
      "2017-11-05T15:11:19.936753: step 2894, loss 0.328909, acc 0.84375\n",
      "2017-11-05T15:11:23.899140: step 2895, loss 0.0519386, acc 0.96875\n",
      "2017-11-05T15:11:27.847816: step 2896, loss 0.0350819, acc 0.96875\n",
      "2017-11-05T15:11:31.761241: step 2897, loss 0.102418, acc 0.9375\n",
      "2017-11-05T15:11:35.712044: step 2898, loss 0.147639, acc 0.9375\n",
      "2017-11-05T15:11:39.657450: step 2899, loss 0.320992, acc 0.875\n",
      "2017-11-05T15:11:43.587040: step 2900, loss 0.129629, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:11:46.119892: step 2900, loss 0.911921, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-05T15:11:51.415769: step 2901, loss 0.176666, acc 0.9375\n",
      "2017-11-05T15:11:55.353438: step 2902, loss 0.222467, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:11:59.336753: step 2903, loss 0.155371, acc 0.90625\n",
      "2017-11-05T15:12:03.268609: step 2904, loss 0.0837035, acc 0.9375\n",
      "2017-11-05T15:12:07.299252: step 2905, loss 0.121979, acc 0.9375\n",
      "2017-11-05T15:12:11.242615: step 2906, loss 0.258608, acc 0.875\n",
      "2017-11-05T15:12:15.199132: step 2907, loss 0.251165, acc 0.875\n",
      "2017-11-05T15:12:19.131688: step 2908, loss 0.240565, acc 0.84375\n",
      "2017-11-05T15:12:23.078571: step 2909, loss 0.293319, acc 0.90625\n",
      "2017-11-05T15:12:27.068471: step 2910, loss 0.233264, acc 0.875\n",
      "2017-11-05T15:12:30.980492: step 2911, loss 0.362332, acc 0.84375\n",
      "2017-11-05T15:12:35.030640: step 2912, loss 0.0702336, acc 1\n",
      "2017-11-05T15:12:39.139582: step 2913, loss 0.141019, acc 0.875\n",
      "2017-11-05T15:12:43.101687: step 2914, loss 0.0906749, acc 0.90625\n",
      "2017-11-05T15:12:47.016682: step 2915, loss 0.357344, acc 0.8125\n",
      "2017-11-05T15:12:49.760306: step 2916, loss 0.196145, acc 0.9\n",
      "2017-11-05T15:12:53.689375: step 2917, loss 0.242862, acc 0.84375\n",
      "2017-11-05T15:12:57.622791: step 2918, loss 0.164312, acc 0.9375\n",
      "2017-11-05T15:13:01.545659: step 2919, loss 0.248132, acc 0.90625\n",
      "2017-11-05T15:13:05.470286: step 2920, loss 0.219756, acc 0.875\n",
      "2017-11-05T15:13:09.429787: step 2921, loss 0.077798, acc 0.96875\n",
      "2017-11-05T15:13:13.427321: step 2922, loss 0.176413, acc 0.875\n",
      "2017-11-05T15:13:17.437056: step 2923, loss 0.127756, acc 0.9375\n",
      "2017-11-05T15:13:21.363343: step 2924, loss 0.0958445, acc 0.96875\n",
      "2017-11-05T15:13:25.464120: step 2925, loss 0.175429, acc 0.9375\n",
      "2017-11-05T15:13:29.483789: step 2926, loss 0.0900536, acc 0.96875\n",
      "2017-11-05T15:13:33.398438: step 2927, loss 0.0721456, acc 0.96875\n",
      "2017-11-05T15:13:37.357165: step 2928, loss 0.186955, acc 0.9375\n",
      "2017-11-05T15:13:41.288562: step 2929, loss 0.139044, acc 0.90625\n",
      "2017-11-05T15:13:45.188431: step 2930, loss 0.0953442, acc 0.96875\n",
      "2017-11-05T15:13:49.181658: step 2931, loss 0.181691, acc 0.875\n",
      "2017-11-05T15:13:53.115429: step 2932, loss 0.252647, acc 0.90625\n",
      "2017-11-05T15:13:57.035817: step 2933, loss 0.305889, acc 0.90625\n",
      "2017-11-05T15:14:00.969047: step 2934, loss 0.211054, acc 0.875\n",
      "2017-11-05T15:14:04.897310: step 2935, loss 0.0544595, acc 0.96875\n",
      "2017-11-05T15:14:08.860768: step 2936, loss 0.106504, acc 0.96875\n",
      "2017-11-05T15:14:12.816211: step 2937, loss 0.177361, acc 0.96875\n",
      "2017-11-05T15:14:16.767999: step 2938, loss 0.0932628, acc 0.9375\n",
      "2017-11-05T15:14:20.761195: step 2939, loss 0.410448, acc 0.8125\n",
      "2017-11-05T15:14:24.771735: step 2940, loss 0.286626, acc 0.875\n",
      "2017-11-05T15:14:28.733572: step 2941, loss 0.202122, acc 0.9375\n",
      "2017-11-05T15:14:32.696888: step 2942, loss 0.307725, acc 0.875\n",
      "2017-11-05T15:14:36.752531: step 2943, loss 0.350499, acc 0.875\n",
      "2017-11-05T15:14:40.727489: step 2944, loss 0.230485, acc 0.84375\n",
      "2017-11-05T15:14:44.685441: step 2945, loss 0.100317, acc 0.9375\n",
      "2017-11-05T15:14:48.628457: step 2946, loss 0.242988, acc 0.84375\n",
      "2017-11-05T15:14:52.562841: step 2947, loss 0.298878, acc 0.8125\n",
      "2017-11-05T15:14:56.488176: step 2948, loss 0.106573, acc 0.9375\n",
      "2017-11-05T15:15:00.520377: step 2949, loss 0.184616, acc 0.875\n",
      "2017-11-05T15:15:04.492796: step 2950, loss 0.0614112, acc 1\n",
      "2017-11-05T15:15:08.436551: step 2951, loss 0.127252, acc 0.90625\n",
      "2017-11-05T15:15:10.962498: step 2952, loss 0.216699, acc 0.85\n",
      "2017-11-05T15:15:14.971208: step 2953, loss 0.122368, acc 0.90625\n",
      "2017-11-05T15:15:18.944933: step 2954, loss 0.124855, acc 0.90625\n",
      "2017-11-05T15:15:23.034781: step 2955, loss 0.206755, acc 0.90625\n",
      "2017-11-05T15:15:27.017611: step 2956, loss 0.127946, acc 0.9375\n",
      "2017-11-05T15:15:30.971094: step 2957, loss 0.317539, acc 0.875\n",
      "2017-11-05T15:15:34.904593: step 2958, loss 0.217655, acc 0.875\n",
      "2017-11-05T15:15:38.892778: step 2959, loss 0.280267, acc 0.90625\n",
      "2017-11-05T15:15:42.855417: step 2960, loss 0.162025, acc 0.9375\n",
      "2017-11-05T15:15:46.833447: step 2961, loss 0.325521, acc 0.875\n",
      "2017-11-05T15:15:50.736820: step 2962, loss 0.125854, acc 0.9375\n",
      "2017-11-05T15:15:54.664684: step 2963, loss 0.0721957, acc 0.9375\n",
      "2017-11-05T15:15:58.713535: step 2964, loss 0.227935, acc 0.90625\n",
      "2017-11-05T15:16:02.670592: step 2965, loss 0.122823, acc 0.90625\n",
      "2017-11-05T15:16:06.607537: step 2966, loss 0.244275, acc 0.9375\n",
      "2017-11-05T15:16:10.634500: step 2967, loss 0.0791764, acc 0.96875\n",
      "2017-11-05T15:16:14.657302: step 2968, loss 0.281566, acc 0.90625\n",
      "2017-11-05T15:16:18.574472: step 2969, loss 0.311806, acc 0.875\n",
      "2017-11-05T15:16:22.516327: step 2970, loss 0.0847493, acc 0.9375\n",
      "2017-11-05T15:16:26.489951: step 2971, loss 0.212792, acc 0.875\n",
      "2017-11-05T15:16:30.412206: step 2972, loss 0.188672, acc 0.875\n",
      "2017-11-05T15:16:34.518588: step 2973, loss 0.107153, acc 0.9375\n",
      "2017-11-05T15:16:38.683771: step 2974, loss 0.110469, acc 0.96875\n",
      "2017-11-05T15:16:42.736683: step 2975, loss 0.184585, acc 0.90625\n",
      "2017-11-05T15:16:46.670039: step 2976, loss 0.304699, acc 0.8125\n",
      "2017-11-05T15:16:50.697950: step 2977, loss 0.203182, acc 0.90625\n",
      "2017-11-05T15:16:54.783793: step 2978, loss 0.0218252, acc 1\n",
      "2017-11-05T15:16:58.737332: step 2979, loss 0.254141, acc 0.875\n",
      "2017-11-05T15:17:02.742048: step 2980, loss 0.213314, acc 0.875\n",
      "2017-11-05T15:17:06.690526: step 2981, loss 0.0798437, acc 1\n",
      "2017-11-05T15:17:10.670729: step 2982, loss 0.221479, acc 0.875\n",
      "2017-11-05T15:17:14.751161: step 2983, loss 0.293278, acc 0.8125\n",
      "2017-11-05T15:17:18.704691: step 2984, loss 0.0963787, acc 0.96875\n",
      "2017-11-05T15:17:22.789861: step 2985, loss 0.177254, acc 0.9375\n",
      "2017-11-05T15:17:26.815598: step 2986, loss 0.226813, acc 0.90625\n",
      "2017-11-05T15:17:30.760330: step 2987, loss 0.241355, acc 0.90625\n",
      "2017-11-05T15:17:33.364918: step 2988, loss 0.10469, acc 0.95\n",
      "2017-11-05T15:17:37.354702: step 2989, loss 0.161375, acc 0.9375\n",
      "2017-11-05T15:17:41.346581: step 2990, loss 0.298466, acc 0.90625\n",
      "2017-11-05T15:17:45.358671: step 2991, loss 0.257389, acc 0.875\n",
      "2017-11-05T15:17:49.387228: step 2992, loss 0.147856, acc 0.9375\n",
      "2017-11-05T15:17:53.392489: step 2993, loss 0.0878628, acc 0.9375\n",
      "2017-11-05T15:17:57.357180: step 2994, loss 0.265839, acc 0.875\n",
      "2017-11-05T15:18:01.319943: step 2995, loss 0.12432, acc 0.90625\n",
      "2017-11-05T15:18:05.330824: step 2996, loss 0.208051, acc 0.875\n",
      "2017-11-05T15:18:09.359189: step 2997, loss 0.103273, acc 0.9375\n",
      "2017-11-05T15:18:13.460816: step 2998, loss 0.0866409, acc 0.96875\n",
      "2017-11-05T15:18:17.471545: step 2999, loss 0.373543, acc 0.78125\n",
      "2017-11-05T15:18:21.534774: step 3000, loss 0.100558, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:18:24.217387: step 3000, loss 0.757267, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-05T15:18:29.881947: step 3001, loss 0.302091, acc 0.8125\n",
      "2017-11-05T15:18:34.042622: step 3002, loss 0.181758, acc 0.90625\n",
      "2017-11-05T15:18:38.141235: step 3003, loss 0.318856, acc 0.875\n",
      "2017-11-05T15:18:42.426796: step 3004, loss 0.164551, acc 0.90625\n",
      "2017-11-05T15:18:46.477994: step 3005, loss 0.1087, acc 0.90625\n",
      "2017-11-05T15:18:50.689521: step 3006, loss 0.0895697, acc 0.96875\n",
      "2017-11-05T15:18:54.816531: step 3007, loss 0.312897, acc 0.8125\n",
      "2017-11-05T15:18:58.991575: step 3008, loss 0.29295, acc 0.875\n",
      "2017-11-05T15:19:03.095637: step 3009, loss 0.21496, acc 0.875\n",
      "2017-11-05T15:19:07.307349: step 3010, loss 0.206332, acc 0.875\n",
      "2017-11-05T15:19:11.492529: step 3011, loss 0.125517, acc 0.9375\n",
      "2017-11-05T15:19:15.705692: step 3012, loss 0.122306, acc 0.96875\n",
      "2017-11-05T15:19:19.824456: step 3013, loss 0.0922031, acc 0.96875\n",
      "2017-11-05T15:19:24.023109: step 3014, loss 0.204585, acc 0.90625\n",
      "2017-11-05T15:19:28.187930: step 3015, loss 0.170834, acc 0.90625\n",
      "2017-11-05T15:19:32.441807: step 3016, loss 0.24073, acc 0.875\n",
      "2017-11-05T15:19:36.607966: step 3017, loss 0.172526, acc 0.9375\n",
      "2017-11-05T15:19:40.909411: step 3018, loss 0.163004, acc 0.90625\n",
      "2017-11-05T15:19:45.099012: step 3019, loss 0.166888, acc 0.9375\n",
      "2017-11-05T15:19:49.094101: step 3020, loss 0.126633, acc 0.9375\n",
      "2017-11-05T15:19:53.182543: step 3021, loss 0.048871, acc 0.96875\n",
      "2017-11-05T15:19:57.172249: step 3022, loss 0.14788, acc 0.9375\n",
      "2017-11-05T15:20:01.528021: step 3023, loss 0.154778, acc 0.96875\n",
      "2017-11-05T15:20:04.128842: step 3024, loss 0.0207924, acc 1\n",
      "2017-11-05T15:20:08.224591: step 3025, loss 0.347517, acc 0.90625\n",
      "2017-11-05T15:20:12.245826: step 3026, loss 0.0431485, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:20:16.269532: step 3027, loss 0.172088, acc 0.9375\n",
      "2017-11-05T15:20:20.328612: step 3028, loss 0.059747, acc 0.96875\n",
      "2017-11-05T15:20:24.371527: step 3029, loss 0.412307, acc 0.8125\n",
      "2017-11-05T15:20:28.393270: step 3030, loss 0.150786, acc 0.9375\n",
      "2017-11-05T15:20:32.409716: step 3031, loss 0.12901, acc 0.9375\n",
      "2017-11-05T15:20:36.552615: step 3032, loss 0.121593, acc 0.9375\n",
      "2017-11-05T15:20:40.592784: step 3033, loss 0.234716, acc 0.84375\n",
      "2017-11-05T15:20:44.605928: step 3034, loss 0.169988, acc 0.9375\n",
      "2017-11-05T15:20:48.613050: step 3035, loss 0.0557493, acc 0.96875\n",
      "2017-11-05T15:20:52.639713: step 3036, loss 0.206261, acc 0.90625\n",
      "2017-11-05T15:20:56.679568: step 3037, loss 0.0994715, acc 0.96875\n",
      "2017-11-05T15:21:00.686602: step 3038, loss 0.441395, acc 0.8125\n",
      "2017-11-05T15:21:04.712760: step 3039, loss 0.127839, acc 0.90625\n",
      "2017-11-05T15:21:08.765005: step 3040, loss 0.11758, acc 0.96875\n",
      "2017-11-05T15:21:12.816856: step 3041, loss 0.141709, acc 0.90625\n",
      "2017-11-05T15:21:16.909664: step 3042, loss 0.118837, acc 0.9375\n",
      "2017-11-05T15:21:20.992730: step 3043, loss 0.139127, acc 0.90625\n",
      "2017-11-05T15:21:25.069851: step 3044, loss 0.112923, acc 0.9375\n",
      "2017-11-05T15:21:29.080016: step 3045, loss 0.0564121, acc 0.96875\n",
      "2017-11-05T15:21:33.117895: step 3046, loss 0.431124, acc 0.875\n",
      "2017-11-05T15:21:37.162674: step 3047, loss 0.171541, acc 0.9375\n",
      "2017-11-05T15:21:41.229456: step 3048, loss 0.188364, acc 0.90625\n",
      "2017-11-05T15:21:45.268380: step 3049, loss 0.181229, acc 0.90625\n",
      "2017-11-05T15:21:49.288846: step 3050, loss 0.130461, acc 0.9375\n",
      "2017-11-05T15:21:53.287986: step 3051, loss 0.316578, acc 0.8125\n",
      "2017-11-05T15:21:57.342121: step 3052, loss 0.390911, acc 0.875\n",
      "2017-11-05T15:22:01.445296: step 3053, loss 0.266347, acc 0.875\n",
      "2017-11-05T15:22:05.554810: step 3054, loss 0.169443, acc 0.90625\n",
      "2017-11-05T15:22:09.600827: step 3055, loss 0.0775515, acc 0.9375\n",
      "2017-11-05T15:22:13.611822: step 3056, loss 0.299551, acc 0.78125\n",
      "2017-11-05T15:22:17.685135: step 3057, loss 0.0889273, acc 0.96875\n",
      "2017-11-05T15:22:21.721610: step 3058, loss 0.321422, acc 0.90625\n",
      "2017-11-05T15:22:25.747475: step 3059, loss 0.193993, acc 0.90625\n",
      "2017-11-05T15:22:28.392927: step 3060, loss 0.267763, acc 0.9\n",
      "2017-11-05T15:22:32.434035: step 3061, loss 0.152803, acc 0.90625\n",
      "2017-11-05T15:22:36.599854: step 3062, loss 0.184447, acc 0.875\n",
      "2017-11-05T15:22:40.642453: step 3063, loss 0.146124, acc 0.90625\n",
      "2017-11-05T15:22:44.685788: step 3064, loss 0.050981, acc 0.96875\n",
      "2017-11-05T15:22:48.720878: step 3065, loss 0.491668, acc 0.8125\n",
      "2017-11-05T15:22:52.857927: step 3066, loss 0.145344, acc 0.9375\n",
      "2017-11-05T15:22:56.868280: step 3067, loss 0.317583, acc 0.875\n",
      "2017-11-05T15:23:00.924608: step 3068, loss 0.0962898, acc 0.9375\n",
      "2017-11-05T15:23:04.985268: step 3069, loss 0.100005, acc 0.9375\n",
      "2017-11-05T15:23:09.055348: step 3070, loss 0.237377, acc 0.90625\n",
      "2017-11-05T15:23:13.065168: step 3071, loss 0.312285, acc 0.78125\n",
      "2017-11-05T15:23:17.142409: step 3072, loss 0.17845, acc 0.9375\n",
      "2017-11-05T15:23:21.194931: step 3073, loss 0.115478, acc 0.9375\n",
      "2017-11-05T15:23:25.336326: step 3074, loss 0.323206, acc 0.875\n",
      "2017-11-05T15:23:29.654380: step 3075, loss 0.150743, acc 0.9375\n",
      "2017-11-05T15:23:33.744540: step 3076, loss 0.152569, acc 0.90625\n",
      "2017-11-05T15:23:37.893406: step 3077, loss 0.168929, acc 0.90625\n",
      "2017-11-05T15:23:41.886984: step 3078, loss 0.179881, acc 0.90625\n",
      "2017-11-05T15:23:45.896544: step 3079, loss 0.166903, acc 0.9375\n",
      "2017-11-05T15:23:49.923025: step 3080, loss 0.229581, acc 0.84375\n",
      "2017-11-05T15:23:54.001262: step 3081, loss 0.332091, acc 0.84375\n",
      "2017-11-05T15:23:58.006587: step 3082, loss 0.178123, acc 0.90625\n",
      "2017-11-05T15:24:02.109937: step 3083, loss 0.152952, acc 0.90625\n",
      "2017-11-05T15:24:06.172817: step 3084, loss 0.0241524, acc 1\n",
      "2017-11-05T15:24:10.169103: step 3085, loss 0.12663, acc 0.90625\n",
      "2017-11-05T15:24:14.173134: step 3086, loss 0.169108, acc 0.9375\n",
      "2017-11-05T15:24:18.213618: step 3087, loss 0.350608, acc 0.84375\n",
      "2017-11-05T15:24:22.253196: step 3088, loss 0.148024, acc 0.96875\n",
      "2017-11-05T15:24:26.267619: step 3089, loss 0.446951, acc 0.78125\n",
      "2017-11-05T15:24:30.277347: step 3090, loss 0.0786536, acc 0.9375\n",
      "2017-11-05T15:24:34.378673: step 3091, loss 0.144045, acc 0.96875\n",
      "2017-11-05T15:24:38.444353: step 3092, loss 0.18667, acc 0.90625\n",
      "2017-11-05T15:24:42.459103: step 3093, loss 0.325461, acc 0.875\n",
      "2017-11-05T15:24:46.519200: step 3094, loss 0.256834, acc 0.90625\n",
      "2017-11-05T15:24:50.525616: step 3095, loss 0.409187, acc 0.84375\n",
      "2017-11-05T15:24:53.159795: step 3096, loss 0.0641765, acc 1\n",
      "2017-11-05T15:24:57.223599: step 3097, loss 0.217412, acc 0.875\n",
      "2017-11-05T15:25:01.214586: step 3098, loss 0.33852, acc 0.875\n",
      "2017-11-05T15:25:05.234427: step 3099, loss 0.0421653, acc 1\n",
      "2017-11-05T15:25:09.321555: step 3100, loss 0.102061, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:25:11.907113: step 3100, loss 1.04209, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-05T15:25:17.060641: step 3101, loss 0.0666643, acc 0.96875\n",
      "2017-11-05T15:25:21.066148: step 3102, loss 0.33923, acc 0.875\n",
      "2017-11-05T15:25:25.121536: step 3103, loss 0.347829, acc 0.875\n",
      "2017-11-05T15:25:29.177911: step 3104, loss 0.0814838, acc 0.9375\n",
      "2017-11-05T15:25:33.168229: step 3105, loss 0.0925644, acc 1\n",
      "2017-11-05T15:25:37.162641: step 3106, loss 0.231916, acc 0.90625\n",
      "2017-11-05T15:25:41.156717: step 3107, loss 0.200047, acc 0.875\n",
      "2017-11-05T15:25:45.185207: step 3108, loss 0.125152, acc 0.9375\n",
      "2017-11-05T15:25:49.227539: step 3109, loss 0.0430416, acc 1\n",
      "2017-11-05T15:25:53.270143: step 3110, loss 0.175999, acc 0.90625\n",
      "2017-11-05T15:25:57.309667: step 3111, loss 0.150181, acc 0.875\n",
      "2017-11-05T15:26:01.367249: step 3112, loss 0.213276, acc 0.9375\n",
      "2017-11-05T15:26:05.409652: step 3113, loss 0.208296, acc 0.875\n",
      "2017-11-05T15:26:09.508715: step 3114, loss 0.347403, acc 0.78125\n",
      "2017-11-05T15:26:13.540750: step 3115, loss 0.330606, acc 0.90625\n",
      "2017-11-05T15:26:17.662676: step 3116, loss 0.160705, acc 0.9375\n",
      "2017-11-05T15:26:21.651531: step 3117, loss 0.200641, acc 0.875\n",
      "2017-11-05T15:26:25.750295: step 3118, loss 0.164198, acc 0.9375\n",
      "2017-11-05T15:26:29.799786: step 3119, loss 0.170564, acc 0.84375\n",
      "2017-11-05T15:26:33.927804: step 3120, loss 0.557773, acc 0.8125\n",
      "2017-11-05T15:26:37.998411: step 3121, loss 0.251472, acc 0.9375\n",
      "2017-11-05T15:26:42.054949: step 3122, loss 0.427497, acc 0.84375\n",
      "2017-11-05T15:26:46.080160: step 3123, loss 0.0756176, acc 0.96875\n",
      "2017-11-05T15:26:50.130261: step 3124, loss 0.177878, acc 0.9375\n",
      "2017-11-05T15:26:54.110114: step 3125, loss 0.121663, acc 0.9375\n",
      "2017-11-05T15:26:58.194425: step 3126, loss 0.315253, acc 0.84375\n",
      "2017-11-05T15:27:02.135803: step 3127, loss 0.100093, acc 0.9375\n",
      "2017-11-05T15:27:06.194524: step 3128, loss 0.0833656, acc 0.96875\n",
      "2017-11-05T15:27:10.225500: step 3129, loss 0.0844308, acc 0.9375\n",
      "2017-11-05T15:27:14.226068: step 3130, loss 0.12743, acc 0.90625\n",
      "2017-11-05T15:27:18.225125: step 3131, loss 0.164799, acc 0.90625\n",
      "2017-11-05T15:27:20.900467: step 3132, loss 0.219851, acc 0.85\n",
      "2017-11-05T15:27:24.964252: step 3133, loss 0.177793, acc 0.875\n",
      "2017-11-05T15:27:29.003243: step 3134, loss 0.211355, acc 0.90625\n",
      "2017-11-05T15:27:32.998897: step 3135, loss 0.102219, acc 0.9375\n",
      "2017-11-05T15:27:37.094035: step 3136, loss 0.227889, acc 0.90625\n",
      "2017-11-05T15:27:41.053099: step 3137, loss 0.0969295, acc 0.96875\n",
      "2017-11-05T15:27:45.050778: step 3138, loss 0.222664, acc 0.90625\n",
      "2017-11-05T15:27:49.009050: step 3139, loss 0.0597827, acc 0.96875\n",
      "2017-11-05T15:27:53.074205: step 3140, loss 0.278603, acc 0.84375\n",
      "2017-11-05T15:27:56.980682: step 3141, loss 0.104674, acc 0.9375\n",
      "2017-11-05T15:28:00.994676: step 3142, loss 0.118704, acc 0.96875\n",
      "2017-11-05T15:28:05.115545: step 3143, loss 0.0947621, acc 0.9375\n",
      "2017-11-05T15:28:09.172262: step 3144, loss 0.0236725, acc 1\n",
      "2017-11-05T15:28:13.205696: step 3145, loss 0.166567, acc 0.90625\n",
      "2017-11-05T15:28:17.184393: step 3146, loss 0.312253, acc 0.84375\n",
      "2017-11-05T15:28:21.226372: step 3147, loss 0.117939, acc 0.9375\n",
      "2017-11-05T15:28:25.450395: step 3148, loss 0.123852, acc 0.9375\n",
      "2017-11-05T15:28:29.530069: step 3149, loss 0.200053, acc 0.90625\n",
      "2017-11-05T15:28:33.619640: step 3150, loss 0.18101, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:28:37.645339: step 3151, loss 0.109708, acc 0.9375\n",
      "2017-11-05T15:28:41.662565: step 3152, loss 0.195609, acc 0.90625\n",
      "2017-11-05T15:28:45.738498: step 3153, loss 0.079545, acc 0.96875\n",
      "2017-11-05T15:28:49.700798: step 3154, loss 0.339997, acc 0.8125\n",
      "2017-11-05T15:28:53.711158: step 3155, loss 0.113082, acc 0.9375\n",
      "2017-11-05T15:28:57.691666: step 3156, loss 0.260523, acc 0.875\n",
      "2017-11-05T15:29:01.664807: step 3157, loss 0.0702571, acc 0.96875\n",
      "2017-11-05T15:29:05.756489: step 3158, loss 0.0853769, acc 0.96875\n",
      "2017-11-05T15:29:09.791135: step 3159, loss 0.20176, acc 0.875\n",
      "2017-11-05T15:29:13.824058: step 3160, loss 0.241642, acc 0.90625\n",
      "2017-11-05T15:29:17.919454: step 3161, loss 0.231234, acc 0.90625\n",
      "2017-11-05T15:29:21.926501: step 3162, loss 0.202696, acc 0.90625\n",
      "2017-11-05T15:29:25.940941: step 3163, loss 0.217801, acc 0.90625\n",
      "2017-11-05T15:29:29.971450: step 3164, loss 0.380287, acc 0.8125\n",
      "2017-11-05T15:29:33.915048: step 3165, loss 0.240473, acc 0.84375\n",
      "2017-11-05T15:29:37.946653: step 3166, loss 0.247242, acc 0.875\n",
      "2017-11-05T15:29:42.061117: step 3167, loss 0.152457, acc 0.9375\n",
      "2017-11-05T15:29:44.626045: step 3168, loss 0.0917408, acc 0.95\n",
      "2017-11-05T15:29:48.665307: step 3169, loss 0.182559, acc 0.90625\n",
      "2017-11-05T15:29:52.613772: step 3170, loss 0.110179, acc 0.96875\n",
      "2017-11-05T15:29:56.587608: step 3171, loss 0.17246, acc 0.9375\n",
      "2017-11-05T15:30:00.711403: step 3172, loss 0.0781409, acc 0.96875\n",
      "2017-11-05T15:30:04.800629: step 3173, loss 0.264253, acc 0.875\n",
      "2017-11-05T15:30:08.784838: step 3174, loss 0.211997, acc 0.9375\n",
      "2017-11-05T15:30:12.754326: step 3175, loss 0.30667, acc 0.875\n",
      "2017-11-05T15:30:16.744288: step 3176, loss 0.322525, acc 0.84375\n",
      "2017-11-05T15:30:20.674887: step 3177, loss 0.181755, acc 0.90625\n",
      "2017-11-05T15:30:24.599185: step 3178, loss 0.257512, acc 0.90625\n",
      "2017-11-05T15:30:28.485842: step 3179, loss 0.179551, acc 0.9375\n",
      "2017-11-05T15:30:32.447021: step 3180, loss 0.184786, acc 0.875\n",
      "2017-11-05T15:30:36.500945: step 3181, loss 0.174555, acc 0.9375\n",
      "2017-11-05T15:30:40.414741: step 3182, loss 0.35014, acc 0.84375\n",
      "2017-11-05T15:30:44.356668: step 3183, loss 0.214613, acc 0.9375\n",
      "2017-11-05T15:30:48.282582: step 3184, loss 0.172525, acc 0.90625\n",
      "2017-11-05T15:30:52.221912: step 3185, loss 0.304279, acc 0.8125\n",
      "2017-11-05T15:30:56.158613: step 3186, loss 0.186274, acc 0.90625\n",
      "2017-11-05T15:31:00.053415: step 3187, loss 0.163155, acc 0.9375\n",
      "2017-11-05T15:31:04.036955: step 3188, loss 0.0446582, acc 1\n",
      "2017-11-05T15:31:08.030143: step 3189, loss 0.153435, acc 0.96875\n",
      "2017-11-05T15:31:11.958637: step 3190, loss 0.075241, acc 0.96875\n",
      "2017-11-05T15:31:15.913587: step 3191, loss 0.254497, acc 0.90625\n",
      "2017-11-05T15:31:19.829098: step 3192, loss 0.333445, acc 0.90625\n",
      "2017-11-05T15:31:23.745949: step 3193, loss 0.256399, acc 0.9375\n",
      "2017-11-05T15:31:27.719282: step 3194, loss 0.226117, acc 0.90625\n",
      "2017-11-05T15:31:31.621058: step 3195, loss 0.429906, acc 0.875\n",
      "2017-11-05T15:31:35.604663: step 3196, loss 0.105024, acc 0.9375\n",
      "2017-11-05T15:31:39.611053: step 3197, loss 0.246655, acc 0.875\n",
      "2017-11-05T15:31:43.524366: step 3198, loss 0.0720857, acc 0.9375\n",
      "2017-11-05T15:31:47.453665: step 3199, loss 0.171374, acc 0.90625\n",
      "2017-11-05T15:31:51.444100: step 3200, loss 0.0122983, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:31:53.971077: step 3200, loss 0.819417, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-05T15:31:59.302559: step 3201, loss 0.270234, acc 0.90625\n",
      "2017-11-05T15:32:03.289196: step 3202, loss 0.13516, acc 0.9375\n",
      "2017-11-05T15:32:07.231367: step 3203, loss 0.207264, acc 0.84375\n",
      "2017-11-05T15:32:09.733932: step 3204, loss 0.404734, acc 0.75\n",
      "2017-11-05T15:32:13.678169: step 3205, loss 0.122433, acc 0.90625\n",
      "2017-11-05T15:32:17.616024: step 3206, loss 0.0554193, acc 0.96875\n",
      "2017-11-05T15:32:21.554288: step 3207, loss 0.0308404, acc 1\n",
      "2017-11-05T15:32:25.468500: step 3208, loss 0.114495, acc 0.90625\n",
      "2017-11-05T15:32:29.400362: step 3209, loss 0.130063, acc 0.9375\n",
      "2017-11-05T15:32:33.408168: step 3210, loss 0.193231, acc 0.9375\n",
      "2017-11-05T15:32:37.453845: step 3211, loss 0.38672, acc 0.875\n",
      "2017-11-05T15:32:41.493995: step 3212, loss 0.259268, acc 0.875\n",
      "2017-11-05T15:32:45.435099: step 3213, loss 0.249678, acc 0.90625\n",
      "2017-11-05T15:32:49.363994: step 3214, loss 0.318346, acc 0.875\n",
      "2017-11-05T15:32:53.486481: step 3215, loss 0.0547993, acc 0.96875\n",
      "2017-11-05T15:32:57.471833: step 3216, loss 0.141112, acc 0.90625\n",
      "2017-11-05T15:33:01.418210: step 3217, loss 0.249511, acc 0.90625\n",
      "2017-11-05T15:33:05.349790: step 3218, loss 0.129695, acc 0.90625\n",
      "2017-11-05T15:33:09.316550: step 3219, loss 0.202622, acc 0.90625\n",
      "2017-11-05T15:33:13.255124: step 3220, loss 0.230795, acc 0.875\n",
      "2017-11-05T15:33:17.250536: step 3221, loss 0.252794, acc 0.875\n",
      "2017-11-05T15:33:21.162347: step 3222, loss 0.116642, acc 0.9375\n",
      "2017-11-05T15:33:25.302921: step 3223, loss 0.142643, acc 0.9375\n",
      "2017-11-05T15:33:29.385747: step 3224, loss 0.073345, acc 0.9375\n",
      "2017-11-05T15:33:33.299060: step 3225, loss 0.0576867, acc 1\n",
      "2017-11-05T15:33:37.273218: step 3226, loss 0.237676, acc 0.84375\n",
      "2017-11-05T15:33:41.207993: step 3227, loss 0.163853, acc 0.90625\n",
      "2017-11-05T15:33:45.116833: step 3228, loss 0.288389, acc 0.90625\n",
      "2017-11-05T15:33:49.102644: step 3229, loss 0.144718, acc 0.9375\n",
      "2017-11-05T15:33:52.999468: step 3230, loss 0.174193, acc 0.9375\n",
      "2017-11-05T15:33:56.900568: step 3231, loss 0.244609, acc 0.9375\n",
      "2017-11-05T15:34:00.907743: step 3232, loss 0.389736, acc 0.875\n",
      "2017-11-05T15:34:04.842561: step 3233, loss 0.219594, acc 0.875\n",
      "2017-11-05T15:34:08.800792: step 3234, loss 0.348737, acc 0.84375\n",
      "2017-11-05T15:34:12.719139: step 3235, loss 0.0818899, acc 0.96875\n",
      "2017-11-05T15:34:16.648608: step 3236, loss 0.0830993, acc 0.9375\n",
      "2017-11-05T15:34:20.628850: step 3237, loss 0.380272, acc 0.78125\n",
      "2017-11-05T15:34:24.541531: step 3238, loss 0.380458, acc 0.8125\n",
      "2017-11-05T15:34:28.491096: step 3239, loss 0.302097, acc 0.875\n",
      "2017-11-05T15:34:30.996905: step 3240, loss 0.0293984, acc 1\n",
      "2017-11-05T15:34:35.063926: step 3241, loss 0.4437, acc 0.8125\n",
      "2017-11-05T15:34:39.002439: step 3242, loss 0.265279, acc 0.90625\n",
      "2017-11-05T15:34:42.964223: step 3243, loss 0.140767, acc 0.9375\n",
      "2017-11-05T15:34:46.916235: step 3244, loss 0.0806199, acc 0.96875\n",
      "2017-11-05T15:34:50.859400: step 3245, loss 0.0940952, acc 0.96875\n",
      "2017-11-05T15:34:54.807899: step 3246, loss 0.10477, acc 0.96875\n",
      "2017-11-05T15:34:58.741666: step 3247, loss 0.110559, acc 0.96875\n",
      "2017-11-05T15:35:02.654744: step 3248, loss 0.0982864, acc 0.96875\n",
      "2017-11-05T15:35:06.634363: step 3249, loss 0.17058, acc 0.9375\n",
      "2017-11-05T15:35:10.597543: step 3250, loss 0.177601, acc 0.9375\n",
      "2017-11-05T15:35:14.592323: step 3251, loss 0.24865, acc 0.9375\n",
      "2017-11-05T15:35:18.511885: step 3252, loss 0.429867, acc 0.8125\n",
      "2017-11-05T15:35:22.399132: step 3253, loss 0.0784107, acc 0.96875\n",
      "2017-11-05T15:35:26.336042: step 3254, loss 0.026609, acc 1\n",
      "2017-11-05T15:35:30.233828: step 3255, loss 0.163284, acc 0.9375\n",
      "2017-11-05T15:35:34.215751: step 3256, loss 0.152592, acc 0.90625\n",
      "2017-11-05T15:35:38.180747: step 3257, loss 0.167842, acc 0.90625\n",
      "2017-11-05T15:35:42.097892: step 3258, loss 0.246422, acc 0.90625\n",
      "2017-11-05T15:35:46.076008: step 3259, loss 0.178183, acc 0.9375\n",
      "2017-11-05T15:35:49.975189: step 3260, loss 0.21121, acc 0.90625\n",
      "2017-11-05T15:35:53.952383: step 3261, loss 0.350426, acc 0.84375\n",
      "2017-11-05T15:35:57.897902: step 3262, loss 0.368086, acc 0.84375\n",
      "2017-11-05T15:36:01.816665: step 3263, loss 0.184473, acc 0.9375\n",
      "2017-11-05T15:36:05.728041: step 3264, loss 0.280596, acc 0.90625\n",
      "2017-11-05T15:36:09.699677: step 3265, loss 0.157922, acc 0.875\n",
      "2017-11-05T15:36:13.653140: step 3266, loss 0.183565, acc 0.90625\n",
      "2017-11-05T15:36:17.571224: step 3267, loss 0.127329, acc 0.9375\n",
      "2017-11-05T15:36:21.469609: step 3268, loss 0.297856, acc 0.84375\n",
      "2017-11-05T15:36:25.402629: step 3269, loss 0.35738, acc 0.84375\n",
      "2017-11-05T15:36:29.349932: step 3270, loss 0.265893, acc 0.875\n",
      "2017-11-05T15:36:33.327118: step 3271, loss 0.382641, acc 0.8125\n",
      "2017-11-05T15:36:37.342537: step 3272, loss 0.185021, acc 0.9375\n",
      "2017-11-05T15:36:41.300009: step 3273, loss 0.112107, acc 0.90625\n",
      "2017-11-05T15:36:45.265259: step 3274, loss 0.201911, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:36:49.213627: step 3275, loss 0.111031, acc 0.96875\n",
      "2017-11-05T15:36:51.736260: step 3276, loss 0.497734, acc 0.8\n",
      "2017-11-05T15:36:55.633509: step 3277, loss 0.0886961, acc 0.96875\n",
      "2017-11-05T15:36:59.566779: step 3278, loss 0.458026, acc 0.875\n",
      "2017-11-05T15:37:03.506528: step 3279, loss 0.172718, acc 0.9375\n",
      "2017-11-05T15:37:07.499713: step 3280, loss 0.380597, acc 0.875\n",
      "2017-11-05T15:37:11.435342: step 3281, loss 0.0897705, acc 0.9375\n",
      "2017-11-05T15:37:15.363593: step 3282, loss 0.193964, acc 0.875\n",
      "2017-11-05T15:37:19.249207: step 3283, loss 0.364546, acc 0.84375\n",
      "2017-11-05T15:37:23.182806: step 3284, loss 0.166214, acc 0.9375\n",
      "2017-11-05T15:37:27.090098: step 3285, loss 0.211687, acc 0.875\n",
      "2017-11-05T15:37:31.007280: step 3286, loss 0.239441, acc 0.90625\n",
      "2017-11-05T15:37:34.948231: step 3287, loss 0.218433, acc 0.84375\n",
      "2017-11-05T15:37:38.920641: step 3288, loss 0.154978, acc 0.90625\n",
      "2017-11-05T15:37:42.867532: step 3289, loss 0.273009, acc 0.875\n",
      "2017-11-05T15:37:46.787123: step 3290, loss 0.246593, acc 0.84375\n",
      "2017-11-05T15:37:50.720831: step 3291, loss 0.136995, acc 0.9375\n",
      "2017-11-05T15:37:54.670843: step 3292, loss 0.0756738, acc 0.9375\n",
      "2017-11-05T15:37:58.652360: step 3293, loss 0.110761, acc 0.9375\n",
      "2017-11-05T15:38:02.601350: step 3294, loss 0.540316, acc 0.78125\n",
      "2017-11-05T15:38:06.542968: step 3295, loss 0.153962, acc 0.90625\n",
      "2017-11-05T15:38:10.431999: step 3296, loss 0.215347, acc 0.90625\n",
      "2017-11-05T15:38:14.363710: step 3297, loss 0.277254, acc 0.875\n",
      "2017-11-05T15:38:18.250548: step 3298, loss 0.179456, acc 0.9375\n",
      "2017-11-05T15:38:22.323302: step 3299, loss 0.134858, acc 0.9375\n",
      "2017-11-05T15:38:26.441069: step 3300, loss 0.150829, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:38:29.008888: step 3300, loss 0.750965, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-05T15:38:34.767547: step 3301, loss 0.219317, acc 0.875\n",
      "2017-11-05T15:38:38.847875: step 3302, loss 0.229687, acc 0.90625\n",
      "2017-11-05T15:38:42.863685: step 3303, loss 0.171901, acc 0.90625\n",
      "2017-11-05T15:38:46.809577: step 3304, loss 0.158847, acc 0.90625\n",
      "2017-11-05T15:38:50.770071: step 3305, loss 0.0689737, acc 0.96875\n",
      "2017-11-05T15:38:54.748970: step 3306, loss 0.0948098, acc 0.96875\n",
      "2017-11-05T15:38:58.665947: step 3307, loss 0.194541, acc 0.875\n",
      "2017-11-05T15:39:02.601712: step 3308, loss 0.0904333, acc 0.96875\n",
      "2017-11-05T15:39:06.553374: step 3309, loss 0.340381, acc 0.875\n",
      "2017-11-05T15:39:10.535237: step 3310, loss 0.376809, acc 0.8125\n",
      "2017-11-05T15:39:14.445493: step 3311, loss 0.127838, acc 0.9375\n",
      "2017-11-05T15:39:16.977515: step 3312, loss 0.190828, acc 0.9\n",
      "2017-11-05T15:39:20.918609: step 3313, loss 0.036343, acc 1\n",
      "2017-11-05T15:39:24.812369: step 3314, loss 0.355314, acc 0.84375\n",
      "2017-11-05T15:39:28.762762: step 3315, loss 0.122073, acc 0.9375\n",
      "2017-11-05T15:39:32.741442: step 3316, loss 0.113577, acc 0.96875\n",
      "2017-11-05T15:39:36.665188: step 3317, loss 0.026138, acc 1\n",
      "2017-11-05T15:39:40.630292: step 3318, loss 0.238428, acc 0.9375\n",
      "2017-11-05T15:39:44.627326: step 3319, loss 0.0776047, acc 0.96875\n",
      "2017-11-05T15:39:48.585231: step 3320, loss 0.207593, acc 0.875\n",
      "2017-11-05T15:39:52.530345: step 3321, loss 0.0425929, acc 0.96875\n",
      "2017-11-05T15:39:56.496579: step 3322, loss 0.176466, acc 0.9375\n",
      "2017-11-05T15:40:00.583954: step 3323, loss 0.219079, acc 0.90625\n",
      "2017-11-05T15:40:04.700724: step 3324, loss 0.303264, acc 0.84375\n",
      "2017-11-05T15:40:08.715577: step 3325, loss 0.158168, acc 0.90625\n",
      "2017-11-05T15:40:12.651204: step 3326, loss 0.225442, acc 0.90625\n",
      "2017-11-05T15:40:16.606440: step 3327, loss 0.204888, acc 0.90625\n",
      "2017-11-05T15:40:20.529397: step 3328, loss 0.258716, acc 0.84375\n",
      "2017-11-05T15:40:24.449141: step 3329, loss 0.179575, acc 0.90625\n",
      "2017-11-05T15:40:28.479986: step 3330, loss 0.274542, acc 0.8125\n",
      "2017-11-05T15:40:32.399273: step 3331, loss 0.304842, acc 0.8125\n",
      "2017-11-05T15:40:36.517743: step 3332, loss 0.120437, acc 0.90625\n",
      "2017-11-05T15:40:40.555859: step 3333, loss 0.341225, acc 0.84375\n",
      "2017-11-05T15:40:44.470629: step 3334, loss 0.0292728, acc 1\n",
      "2017-11-05T15:40:48.462802: step 3335, loss 0.379765, acc 0.84375\n",
      "2017-11-05T15:40:52.367089: step 3336, loss 0.285502, acc 0.9375\n",
      "2017-11-05T15:40:56.370206: step 3337, loss 0.145372, acc 0.9375\n",
      "2017-11-05T15:41:00.332807: step 3338, loss 0.525084, acc 0.90625\n",
      "2017-11-05T15:41:04.325227: step 3339, loss 0.217688, acc 0.96875\n",
      "2017-11-05T15:41:08.294948: step 3340, loss 0.201336, acc 0.875\n",
      "2017-11-05T15:41:13.161257: step 3341, loss 0.369091, acc 0.875\n",
      "2017-11-05T15:41:17.593111: step 3342, loss 0.303714, acc 0.875\n",
      "2017-11-05T15:41:21.572833: step 3343, loss 0.18155, acc 0.875\n",
      "2017-11-05T15:41:25.629421: step 3344, loss 0.290842, acc 0.875\n",
      "2017-11-05T15:41:29.639771: step 3345, loss 0.121596, acc 0.96875\n",
      "2017-11-05T15:41:33.544558: step 3346, loss 0.513876, acc 0.875\n",
      "2017-11-05T15:41:37.523191: step 3347, loss 0.269641, acc 0.875\n",
      "2017-11-05T15:41:40.019520: step 3348, loss 0.120038, acc 0.95\n",
      "2017-11-05T15:41:43.983067: step 3349, loss 0.13516, acc 0.90625\n",
      "2017-11-05T15:41:47.926432: step 3350, loss 0.328044, acc 0.84375\n",
      "2017-11-05T15:41:51.899270: step 3351, loss 0.171066, acc 0.875\n",
      "2017-11-05T15:41:55.833165: step 3352, loss 0.0473313, acc 1\n",
      "2017-11-05T15:41:59.805032: step 3353, loss 0.177749, acc 0.875\n",
      "2017-11-05T15:42:03.793673: step 3354, loss 0.222441, acc 0.90625\n",
      "2017-11-05T15:42:07.782738: step 3355, loss 0.191059, acc 0.90625\n",
      "2017-11-05T15:42:11.743707: step 3356, loss 0.0639156, acc 0.96875\n",
      "2017-11-05T15:42:15.751703: step 3357, loss 0.106535, acc 0.9375\n",
      "2017-11-05T15:42:19.667482: step 3358, loss 0.0705209, acc 0.96875\n",
      "2017-11-05T15:42:23.580399: step 3359, loss 0.0484972, acc 0.96875\n",
      "2017-11-05T15:42:27.493620: step 3360, loss 0.0555548, acc 0.96875\n",
      "2017-11-05T15:42:31.564245: step 3361, loss 0.205733, acc 0.9375\n",
      "2017-11-05T15:42:35.695923: step 3362, loss 0.374277, acc 0.84375\n",
      "2017-11-05T15:42:39.655894: step 3363, loss 0.19813, acc 0.90625\n",
      "2017-11-05T15:42:43.586212: step 3364, loss 0.0949898, acc 0.96875\n",
      "2017-11-05T15:42:47.628957: step 3365, loss 0.183553, acc 0.875\n",
      "2017-11-05T15:42:51.561316: step 3366, loss 0.0533355, acc 0.96875\n",
      "2017-11-05T15:42:55.679073: step 3367, loss 0.280661, acc 0.84375\n",
      "2017-11-05T15:42:59.664776: step 3368, loss 0.248621, acc 0.875\n",
      "2017-11-05T15:43:03.626904: step 3369, loss 0.246812, acc 0.84375\n",
      "2017-11-05T15:43:07.599628: step 3370, loss 0.271357, acc 0.84375\n",
      "2017-11-05T15:43:11.544928: step 3371, loss 0.166856, acc 0.90625\n",
      "2017-11-05T15:43:15.567585: step 3372, loss 0.151777, acc 0.90625\n",
      "2017-11-05T15:43:19.568012: step 3373, loss 0.0373217, acc 1\n",
      "2017-11-05T15:43:23.690877: step 3374, loss 0.178474, acc 0.90625\n",
      "2017-11-05T15:43:27.860795: step 3375, loss 0.501268, acc 0.8125\n",
      "2017-11-05T15:43:31.818499: step 3376, loss 0.17634, acc 0.9375\n",
      "2017-11-05T15:43:35.738518: step 3377, loss 0.176489, acc 0.875\n",
      "2017-11-05T15:43:39.702679: step 3378, loss 0.152194, acc 0.90625\n",
      "2017-11-05T15:43:43.670861: step 3379, loss 0.26406, acc 0.875\n",
      "2017-11-05T15:43:47.613823: step 3380, loss 0.152553, acc 0.96875\n",
      "2017-11-05T15:43:51.639034: step 3381, loss 0.335169, acc 0.84375\n",
      "2017-11-05T15:43:55.572906: step 3382, loss 0.0721895, acc 0.96875\n",
      "2017-11-05T15:43:59.518851: step 3383, loss 0.216136, acc 0.90625\n",
      "2017-11-05T15:44:02.079642: step 3384, loss 0.33444, acc 0.8\n",
      "2017-11-05T15:44:06.048316: step 3385, loss 0.291683, acc 0.875\n",
      "2017-11-05T15:44:10.100569: step 3386, loss 0.0881142, acc 0.96875\n",
      "2017-11-05T15:44:14.079724: step 3387, loss 0.0964599, acc 0.9375\n",
      "2017-11-05T15:44:18.008513: step 3388, loss 0.171704, acc 0.875\n",
      "2017-11-05T15:44:22.005188: step 3389, loss 0.0656177, acc 0.9375\n",
      "2017-11-05T15:44:26.031832: step 3390, loss 0.110974, acc 0.9375\n",
      "2017-11-05T15:44:30.040786: step 3391, loss 0.315047, acc 0.875\n",
      "2017-11-05T15:44:34.155008: step 3392, loss 0.103315, acc 0.9375\n",
      "2017-11-05T15:44:38.250200: step 3393, loss 0.111991, acc 0.96875\n",
      "2017-11-05T15:44:42.191067: step 3394, loss 0.13388, acc 0.9375\n",
      "2017-11-05T15:44:46.147620: step 3395, loss 0.124514, acc 0.96875\n",
      "2017-11-05T15:44:50.233496: step 3396, loss 0.0882491, acc 0.96875\n",
      "2017-11-05T15:44:54.186019: step 3397, loss 0.196817, acc 0.90625\n",
      "2017-11-05T15:44:58.177452: step 3398, loss 0.16709, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:45:02.249485: step 3399, loss 0.0757445, acc 0.96875\n",
      "2017-11-05T15:45:06.291295: step 3400, loss 0.193943, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:45:08.817679: step 3400, loss 0.916041, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-05T15:45:14.148135: step 3401, loss 0.371015, acc 0.875\n",
      "2017-11-05T15:45:18.349167: step 3402, loss 0.102871, acc 0.9375\n",
      "2017-11-05T15:45:22.295463: step 3403, loss 0.166067, acc 0.875\n",
      "2017-11-05T15:45:26.237631: step 3404, loss 0.325767, acc 0.875\n",
      "2017-11-05T15:45:30.201232: step 3405, loss 0.0817221, acc 1\n",
      "2017-11-05T15:45:34.135780: step 3406, loss 0.073205, acc 0.96875\n",
      "2017-11-05T15:45:38.099421: step 3407, loss 0.308025, acc 0.875\n",
      "2017-11-05T15:45:42.061370: step 3408, loss 0.0929665, acc 0.9375\n",
      "2017-11-05T15:45:46.012985: step 3409, loss 0.10572, acc 0.9375\n",
      "2017-11-05T15:45:50.003310: step 3410, loss 0.152194, acc 0.9375\n",
      "2017-11-05T15:45:54.018439: step 3411, loss 0.149023, acc 0.9375\n",
      "2017-11-05T15:45:57.985606: step 3412, loss 0.160339, acc 0.9375\n",
      "2017-11-05T15:46:01.971720: step 3413, loss 0.068361, acc 0.96875\n",
      "2017-11-05T15:46:05.980925: step 3414, loss 0.151224, acc 0.9375\n",
      "2017-11-05T15:46:09.941119: step 3415, loss 0.226404, acc 0.9375\n",
      "2017-11-05T15:46:13.922716: step 3416, loss 0.29196, acc 0.875\n",
      "2017-11-05T15:46:17.917067: step 3417, loss 0.0624584, acc 0.96875\n",
      "2017-11-05T15:46:21.869751: step 3418, loss 0.121037, acc 0.9375\n",
      "2017-11-05T15:46:25.840196: step 3419, loss 0.237263, acc 0.9375\n",
      "2017-11-05T15:46:28.308242: step 3420, loss 0.507024, acc 0.7\n",
      "2017-11-05T15:46:32.283146: step 3421, loss 0.126746, acc 0.9375\n",
      "2017-11-05T15:46:36.460920: step 3422, loss 0.1565, acc 0.9375\n",
      "2017-11-05T15:46:40.628091: step 3423, loss 0.155707, acc 0.90625\n",
      "2017-11-05T15:46:44.621187: step 3424, loss 0.121472, acc 0.90625\n",
      "2017-11-05T15:46:48.586243: step 3425, loss 0.140489, acc 0.9375\n",
      "2017-11-05T15:46:52.556819: step 3426, loss 0.23289, acc 0.9375\n",
      "2017-11-05T15:46:56.521488: step 3427, loss 0.134797, acc 0.90625\n",
      "2017-11-05T15:47:00.484189: step 3428, loss 0.087644, acc 0.96875\n",
      "2017-11-05T15:47:04.444500: step 3429, loss 0.418034, acc 0.8125\n",
      "2017-11-05T15:47:08.456650: step 3430, loss 0.114192, acc 0.96875\n",
      "2017-11-05T15:47:12.388121: step 3431, loss 0.167461, acc 0.9375\n",
      "2017-11-05T15:47:16.415498: step 3432, loss 0.0926779, acc 0.96875\n",
      "2017-11-05T15:47:20.421239: step 3433, loss 0.241753, acc 0.875\n",
      "2017-11-05T15:47:24.398383: step 3434, loss 0.114398, acc 0.9375\n",
      "2017-11-05T15:47:28.326993: step 3435, loss 0.0377857, acc 0.96875\n",
      "2017-11-05T15:47:32.305147: step 3436, loss 0.212888, acc 0.9375\n",
      "2017-11-05T15:47:36.232400: step 3437, loss 0.123968, acc 0.9375\n",
      "2017-11-05T15:47:40.232715: step 3438, loss 0.172679, acc 0.90625\n",
      "2017-11-05T15:47:44.165288: step 3439, loss 0.290509, acc 0.875\n",
      "2017-11-05T15:47:48.168702: step 3440, loss 0.256861, acc 0.875\n",
      "2017-11-05T15:47:52.189911: step 3441, loss 0.414889, acc 0.8125\n",
      "2017-11-05T15:47:56.150131: step 3442, loss 0.0903738, acc 0.9375\n",
      "2017-11-05T15:48:00.136327: step 3443, loss 0.145815, acc 0.9375\n",
      "2017-11-05T15:48:04.053658: step 3444, loss 0.129108, acc 0.96875\n",
      "2017-11-05T15:48:08.089465: step 3445, loss 0.129191, acc 0.9375\n",
      "2017-11-05T15:48:12.070872: step 3446, loss 0.356581, acc 0.875\n",
      "2017-11-05T15:48:16.105093: step 3447, loss 0.227022, acc 0.90625\n",
      "2017-11-05T15:48:20.111815: step 3448, loss 0.272593, acc 0.84375\n",
      "2017-11-05T15:48:24.183012: step 3449, loss 0.399049, acc 0.8125\n",
      "2017-11-05T15:48:28.503610: step 3450, loss 0.30904, acc 0.84375\n",
      "2017-11-05T15:48:32.520039: step 3451, loss 0.225479, acc 0.90625\n",
      "2017-11-05T15:48:36.608414: step 3452, loss 0.249922, acc 0.875\n",
      "2017-11-05T15:48:40.704953: step 3453, loss 0.123937, acc 0.9375\n",
      "2017-11-05T15:48:44.801630: step 3454, loss 0.184565, acc 0.90625\n",
      "2017-11-05T15:48:48.878028: step 3455, loss 0.213592, acc 0.90625\n",
      "2017-11-05T15:48:51.556798: step 3456, loss 0.241026, acc 0.95\n",
      "2017-11-05T15:48:55.636375: step 3457, loss 0.0646019, acc 0.96875\n",
      "2017-11-05T15:48:59.792291: step 3458, loss 0.21311, acc 0.90625\n",
      "2017-11-05T15:49:03.981753: step 3459, loss 0.203761, acc 0.90625\n",
      "2017-11-05T15:49:08.006153: step 3460, loss 0.085138, acc 0.96875\n",
      "2017-11-05T15:49:12.159773: step 3461, loss 0.145387, acc 0.90625\n",
      "2017-11-05T15:49:16.180636: step 3462, loss 0.0727309, acc 1\n",
      "2017-11-05T15:49:20.395003: step 3463, loss 0.0812199, acc 0.96875\n",
      "2017-11-05T15:49:24.464653: step 3464, loss 0.18136, acc 0.90625\n",
      "2017-11-05T15:49:28.658230: step 3465, loss 0.317253, acc 0.8125\n",
      "2017-11-05T15:49:32.712114: step 3466, loss 0.189272, acc 0.875\n",
      "2017-11-05T15:49:36.809427: step 3467, loss 0.194979, acc 0.90625\n",
      "2017-11-05T15:49:40.869505: step 3468, loss 0.479609, acc 0.78125\n",
      "2017-11-05T15:49:45.006099: step 3469, loss 0.168309, acc 0.90625\n",
      "2017-11-05T15:49:49.062300: step 3470, loss 0.0489589, acc 1\n",
      "2017-11-05T15:49:53.210138: step 3471, loss 0.361735, acc 0.84375\n",
      "2017-11-05T15:49:57.217852: step 3472, loss 0.172946, acc 0.9375\n",
      "2017-11-05T15:50:01.554744: step 3473, loss 0.206743, acc 0.90625\n",
      "2017-11-05T15:50:05.627823: step 3474, loss 0.0941109, acc 0.96875\n",
      "2017-11-05T15:50:09.703089: step 3475, loss 0.153989, acc 0.9375\n",
      "2017-11-05T15:50:13.716215: step 3476, loss 0.107085, acc 0.96875\n",
      "2017-11-05T15:50:17.723675: step 3477, loss 0.131356, acc 0.96875\n",
      "2017-11-05T15:50:21.726975: step 3478, loss 0.334299, acc 0.84375\n",
      "2017-11-05T15:50:25.660445: step 3479, loss 0.145626, acc 0.90625\n",
      "2017-11-05T15:50:29.608572: step 3480, loss 0.134263, acc 0.90625\n",
      "2017-11-05T15:50:33.658595: step 3481, loss 0.219964, acc 0.90625\n",
      "2017-11-05T15:50:37.706788: step 3482, loss 0.312094, acc 0.875\n",
      "2017-11-05T15:50:41.657327: step 3483, loss 0.28903, acc 0.84375\n",
      "2017-11-05T15:50:45.681567: step 3484, loss 0.253966, acc 0.875\n",
      "2017-11-05T15:50:49.674739: step 3485, loss 0.172299, acc 0.9375\n",
      "2017-11-05T15:50:53.666859: step 3486, loss 0.291168, acc 0.84375\n",
      "2017-11-05T15:50:57.629485: step 3487, loss 0.196984, acc 0.90625\n",
      "2017-11-05T15:51:01.648356: step 3488, loss 0.114789, acc 0.9375\n",
      "2017-11-05T15:51:05.601537: step 3489, loss 0.189705, acc 0.875\n",
      "2017-11-05T15:51:09.605277: step 3490, loss 0.137454, acc 0.90625\n",
      "2017-11-05T15:51:13.588230: step 3491, loss 0.0913496, acc 0.96875\n",
      "2017-11-05T15:51:16.059349: step 3492, loss 0.127494, acc 0.9\n",
      "2017-11-05T15:51:20.054724: step 3493, loss 0.137233, acc 0.9375\n",
      "2017-11-05T15:51:24.062297: step 3494, loss 0.280802, acc 0.84375\n",
      "2017-11-05T15:51:28.038664: step 3495, loss 0.0812206, acc 0.9375\n",
      "2017-11-05T15:51:31.962848: step 3496, loss 0.282087, acc 0.84375\n",
      "2017-11-05T15:51:35.986061: step 3497, loss 0.111395, acc 0.96875\n",
      "2017-11-05T15:51:39.947003: step 3498, loss 0.163148, acc 0.9375\n",
      "2017-11-05T15:51:43.936522: step 3499, loss 0.293985, acc 0.875\n",
      "2017-11-05T15:51:47.932391: step 3500, loss 0.0688243, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:51:50.453313: step 3500, loss 0.969621, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-05T15:51:55.681005: step 3501, loss 0.378415, acc 0.8125\n",
      "2017-11-05T15:51:59.680618: step 3502, loss 0.011805, acc 1\n",
      "2017-11-05T15:52:03.736064: step 3503, loss 0.0815474, acc 0.96875\n",
      "2017-11-05T15:52:07.789930: step 3504, loss 0.138974, acc 0.96875\n",
      "2017-11-05T15:52:11.736903: step 3505, loss 0.0331724, acc 1\n",
      "2017-11-05T15:52:15.732454: step 3506, loss 0.0946883, acc 0.96875\n",
      "2017-11-05T15:52:19.699407: step 3507, loss 0.218535, acc 0.9375\n",
      "2017-11-05T15:52:23.687813: step 3508, loss 0.250539, acc 0.9375\n",
      "2017-11-05T15:52:27.603727: step 3509, loss 0.420412, acc 0.84375\n",
      "2017-11-05T15:52:31.536021: step 3510, loss 0.238801, acc 0.90625\n",
      "2017-11-05T15:52:35.639203: step 3511, loss 0.301566, acc 0.8125\n",
      "2017-11-05T15:52:39.650841: step 3512, loss 0.0459459, acc 0.96875\n",
      "2017-11-05T15:52:43.569484: step 3513, loss 0.0906101, acc 0.9375\n",
      "2017-11-05T15:52:47.528609: step 3514, loss 0.0470184, acc 1\n",
      "2017-11-05T15:52:51.555160: step 3515, loss 0.257836, acc 0.90625\n",
      "2017-11-05T15:52:55.704320: step 3516, loss 0.248872, acc 0.875\n",
      "2017-11-05T15:52:59.721877: step 3517, loss 0.367384, acc 0.875\n",
      "2017-11-05T15:53:03.760987: step 3518, loss 0.523551, acc 0.78125\n",
      "2017-11-05T15:53:07.792790: step 3519, loss 0.15103, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:53:11.773000: step 3520, loss 0.187071, acc 0.9375\n",
      "2017-11-05T15:53:15.782654: step 3521, loss 0.171758, acc 0.90625\n",
      "2017-11-05T15:53:19.726989: step 3522, loss 0.127388, acc 0.96875\n",
      "2017-11-05T15:53:23.937365: step 3523, loss 0.197461, acc 0.9375\n",
      "2017-11-05T15:53:28.101417: step 3524, loss 0.421724, acc 0.90625\n",
      "2017-11-05T15:53:32.075773: step 3525, loss 0.479282, acc 0.84375\n",
      "2017-11-05T15:53:36.103055: step 3526, loss 0.258701, acc 0.875\n",
      "2017-11-05T15:53:40.110403: step 3527, loss 0.0946961, acc 0.9375\n",
      "2017-11-05T15:53:42.650964: step 3528, loss 0.10472, acc 0.9\n",
      "2017-11-05T15:53:46.696805: step 3529, loss 0.0938227, acc 0.96875\n",
      "2017-11-05T15:53:50.700334: step 3530, loss 0.149364, acc 0.90625\n",
      "2017-11-05T15:53:54.692371: step 3531, loss 0.156119, acc 0.9375\n",
      "2017-11-05T15:53:58.702269: step 3532, loss 0.188542, acc 0.90625\n",
      "2017-11-05T15:54:02.746174: step 3533, loss 0.223578, acc 0.90625\n",
      "2017-11-05T15:54:06.728476: step 3534, loss 0.126984, acc 0.9375\n",
      "2017-11-05T15:54:10.689047: step 3535, loss 0.142241, acc 0.9375\n",
      "2017-11-05T15:54:14.692297: step 3536, loss 0.220279, acc 0.90625\n",
      "2017-11-05T15:54:18.733743: step 3537, loss 0.400591, acc 0.8125\n",
      "2017-11-05T15:54:22.747289: step 3538, loss 0.146457, acc 0.90625\n",
      "2017-11-05T15:54:26.767758: step 3539, loss 0.287579, acc 0.875\n",
      "2017-11-05T15:54:30.772635: step 3540, loss 0.109671, acc 0.90625\n",
      "2017-11-05T15:54:34.833410: step 3541, loss 0.250075, acc 0.9375\n",
      "2017-11-05T15:54:38.792138: step 3542, loss 0.144052, acc 0.90625\n",
      "2017-11-05T15:54:42.775496: step 3543, loss 0.291728, acc 0.90625\n",
      "2017-11-05T15:54:46.764261: step 3544, loss 0.11061, acc 0.96875\n",
      "2017-11-05T15:54:50.742307: step 3545, loss 0.108477, acc 0.96875\n",
      "2017-11-05T15:54:54.657197: step 3546, loss 0.366228, acc 0.84375\n",
      "2017-11-05T15:54:58.632327: step 3547, loss 0.248874, acc 0.9375\n",
      "2017-11-05T15:55:02.624801: step 3548, loss 0.168136, acc 0.9375\n",
      "2017-11-05T15:55:06.654656: step 3549, loss 0.251428, acc 0.84375\n",
      "2017-11-05T15:55:10.664823: step 3550, loss 0.184734, acc 0.90625\n",
      "2017-11-05T15:55:14.655108: step 3551, loss 0.234643, acc 0.875\n",
      "2017-11-05T15:55:18.645693: step 3552, loss 0.0841793, acc 0.96875\n",
      "2017-11-05T15:55:22.626274: step 3553, loss 0.0743843, acc 0.9375\n",
      "2017-11-05T15:55:26.588047: step 3554, loss 0.107753, acc 0.9375\n",
      "2017-11-05T15:55:30.535473: step 3555, loss 0.148392, acc 0.90625\n",
      "2017-11-05T15:55:34.668871: step 3556, loss 0.0687402, acc 0.96875\n",
      "2017-11-05T15:55:38.596656: step 3557, loss 0.154509, acc 0.875\n",
      "2017-11-05T15:55:42.595301: step 3558, loss 0.338798, acc 0.84375\n",
      "2017-11-05T15:55:46.630991: step 3559, loss 0.102266, acc 0.96875\n",
      "2017-11-05T15:55:50.648933: step 3560, loss 0.228256, acc 0.875\n",
      "2017-11-05T15:55:54.696445: step 3561, loss 0.273319, acc 0.8125\n",
      "2017-11-05T15:55:58.669351: step 3562, loss 0.154073, acc 0.90625\n",
      "2017-11-05T15:56:02.624298: step 3563, loss 0.164085, acc 0.875\n",
      "2017-11-05T15:56:05.286313: step 3564, loss 0.309818, acc 0.85\n",
      "2017-11-05T15:56:09.325611: step 3565, loss 0.161683, acc 0.90625\n",
      "2017-11-05T15:56:13.229589: step 3566, loss 0.0716016, acc 0.96875\n",
      "2017-11-05T15:56:17.194093: step 3567, loss 0.224295, acc 0.9375\n",
      "2017-11-05T15:56:21.218649: step 3568, loss 0.235471, acc 0.90625\n",
      "2017-11-05T15:56:25.307537: step 3569, loss 0.163075, acc 0.90625\n",
      "2017-11-05T15:56:29.345101: step 3570, loss 0.130039, acc 0.9375\n",
      "2017-11-05T15:56:33.336496: step 3571, loss 0.178606, acc 0.9375\n",
      "2017-11-05T15:56:37.455462: step 3572, loss 0.225422, acc 0.90625\n",
      "2017-11-05T15:56:41.411325: step 3573, loss 0.274986, acc 0.84375\n",
      "2017-11-05T15:56:45.463043: step 3574, loss 0.0818561, acc 0.96875\n",
      "2017-11-05T15:56:49.462572: step 3575, loss 0.0751275, acc 0.96875\n",
      "2017-11-05T15:56:53.527299: step 3576, loss 0.404541, acc 0.8125\n",
      "2017-11-05T15:56:57.458129: step 3577, loss 0.0891434, acc 0.96875\n",
      "2017-11-05T15:57:01.437931: step 3578, loss 0.240675, acc 0.84375\n",
      "2017-11-05T15:57:05.461139: step 3579, loss 0.200279, acc 0.90625\n",
      "2017-11-05T15:57:09.522723: step 3580, loss 0.230209, acc 0.90625\n",
      "2017-11-05T15:57:13.475963: step 3581, loss 0.177635, acc 0.90625\n",
      "2017-11-05T15:57:17.492644: step 3582, loss 0.20276, acc 0.90625\n",
      "2017-11-05T15:57:21.422324: step 3583, loss 0.0997159, acc 0.90625\n",
      "2017-11-05T15:57:25.372740: step 3584, loss 0.157648, acc 0.9375\n",
      "2017-11-05T15:57:29.287422: step 3585, loss 0.10928, acc 0.90625\n",
      "2017-11-05T15:57:33.275233: step 3586, loss 0.16783, acc 0.90625\n",
      "2017-11-05T15:57:37.315037: step 3587, loss 0.0242782, acc 1\n",
      "2017-11-05T15:57:41.325716: step 3588, loss 0.0469418, acc 0.96875\n",
      "2017-11-05T15:57:45.364756: step 3589, loss 0.199305, acc 0.875\n",
      "2017-11-05T15:57:49.405279: step 3590, loss 0.314233, acc 0.90625\n",
      "2017-11-05T15:57:53.434339: step 3591, loss 0.0872724, acc 0.9375\n",
      "2017-11-05T15:57:57.426152: step 3592, loss 0.416416, acc 0.875\n",
      "2017-11-05T15:58:01.435400: step 3593, loss 0.0710631, acc 0.96875\n",
      "2017-11-05T15:58:05.478354: step 3594, loss 0.107444, acc 0.9375\n",
      "2017-11-05T15:58:09.565646: step 3595, loss 0.117107, acc 0.96875\n",
      "2017-11-05T15:58:13.564771: step 3596, loss 0.361371, acc 0.84375\n",
      "2017-11-05T15:58:17.632800: step 3597, loss 0.199307, acc 0.84375\n",
      "2017-11-05T15:58:21.733957: step 3598, loss 0.246081, acc 0.90625\n",
      "2017-11-05T15:58:25.934565: step 3599, loss 0.276992, acc 0.84375\n",
      "2017-11-05T15:58:28.544721: step 3600, loss 0.173485, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T15:58:31.264656: step 3600, loss 0.782692, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\1\\1509857724\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C911F8D0>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\n",
      "\n",
      "2017-11-05T15:58:41.162416: step 1, loss 0.699376, acc 0.90625\n",
      "2017-11-05T15:58:45.121348: step 2, loss 1.466, acc 0.90625\n",
      "2017-11-05T15:58:49.157373: step 3, loss 1.33887, acc 0.875\n",
      "2017-11-05T15:58:53.130658: step 4, loss 1.24167, acc 0.84375\n",
      "2017-11-05T15:58:57.136674: step 5, loss 0.0337017, acc 0.96875\n",
      "2017-11-05T15:59:01.086085: step 6, loss 0.518997, acc 0.90625\n",
      "2017-11-05T15:59:05.034819: step 7, loss 1.13448, acc 0.875\n",
      "2017-11-05T15:59:09.122649: step 8, loss 1.80717, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T15:59:13.093118: step 9, loss 1.23432, acc 0.78125\n",
      "2017-11-05T15:59:17.142720: step 10, loss 2.23609, acc 0.8125\n",
      "2017-11-05T15:59:21.137975: step 11, loss 0.883088, acc 0.90625\n",
      "2017-11-05T15:59:25.177608: step 12, loss 1.66263, acc 0.78125\n",
      "2017-11-05T15:59:29.142960: step 13, loss 0.913551, acc 0.875\n",
      "2017-11-05T15:59:33.149681: step 14, loss 2.48274, acc 0.75\n",
      "2017-11-05T15:59:37.133054: step 15, loss 0.96389, acc 0.8125\n",
      "2017-11-05T15:59:41.157305: step 16, loss 1.53675, acc 0.65625\n",
      "2017-11-05T15:59:45.088754: step 17, loss 0.970711, acc 0.8125\n",
      "2017-11-05T15:59:49.126016: step 18, loss 2.07208, acc 0.65625\n",
      "2017-11-05T15:59:53.151104: step 19, loss 1.06595, acc 0.75\n",
      "2017-11-05T15:59:57.161614: step 20, loss 0.916767, acc 0.78125\n",
      "2017-11-05T16:00:01.364660: step 21, loss 1.04465, acc 0.875\n",
      "2017-11-05T16:00:05.367024: step 22, loss 0.879046, acc 0.8125\n",
      "2017-11-05T16:00:09.366231: step 23, loss 1.08691, acc 0.8125\n",
      "2017-11-05T16:00:13.407138: step 24, loss 0.572813, acc 0.90625\n",
      "2017-11-05T16:00:17.540405: step 25, loss 0.737157, acc 0.875\n",
      "2017-11-05T16:00:21.484906: step 26, loss 1.21585, acc 0.78125\n",
      "2017-11-05T16:00:25.516785: step 27, loss 0.917907, acc 0.84375\n",
      "2017-11-05T16:00:29.520226: step 28, loss 0.99077, acc 0.84375\n",
      "2017-11-05T16:00:33.636328: step 29, loss 0.561528, acc 0.90625\n",
      "2017-11-05T16:00:37.724078: step 30, loss 1.72407, acc 0.84375\n",
      "2017-11-05T16:00:41.707121: step 31, loss 1.64709, acc 0.8125\n",
      "2017-11-05T16:00:45.660822: step 32, loss 1.08469, acc 0.9375\n",
      "2017-11-05T16:00:49.648868: step 33, loss 0.807343, acc 0.96875\n",
      "2017-11-05T16:00:53.751824: step 34, loss 0.284061, acc 0.9375\n",
      "2017-11-05T16:00:57.742387: step 35, loss 1.78751, acc 0.8125\n",
      "2017-11-05T16:01:00.351452: step 36, loss 1.77984, acc 0.8\n",
      "2017-11-05T16:01:04.379106: step 37, loss 1.47592, acc 0.84375\n",
      "2017-11-05T16:01:08.427734: step 38, loss 0.766362, acc 0.75\n",
      "2017-11-05T16:01:12.446709: step 39, loss 0.920166, acc 0.875\n",
      "2017-11-05T16:01:16.447465: step 40, loss 0.541499, acc 0.84375\n",
      "2017-11-05T16:01:20.497871: step 41, loss 0.188323, acc 0.96875\n",
      "2017-11-05T16:01:24.504892: step 42, loss 0.959851, acc 0.78125\n",
      "2017-11-05T16:01:28.482605: step 43, loss 0.335081, acc 0.96875\n",
      "2017-11-05T16:01:32.397121: step 44, loss 0.581029, acc 0.875\n",
      "2017-11-05T16:01:36.411422: step 45, loss 1.85968, acc 0.53125\n",
      "2017-11-05T16:01:40.449202: step 46, loss 0.229181, acc 0.84375\n",
      "2017-11-05T16:01:44.442323: step 47, loss 0.741451, acc 0.8125\n",
      "2017-11-05T16:01:48.366173: step 48, loss 0.460887, acc 0.9375\n",
      "2017-11-05T16:01:52.383940: step 49, loss 0.924883, acc 0.875\n",
      "2017-11-05T16:01:56.358398: step 50, loss 1.81546, acc 0.84375\n",
      "2017-11-05T16:02:00.349219: step 51, loss 1.12766, acc 0.84375\n",
      "2017-11-05T16:02:04.356622: step 52, loss 0.888246, acc 0.875\n",
      "2017-11-05T16:02:08.374009: step 53, loss 0.293621, acc 0.96875\n",
      "2017-11-05T16:02:12.398578: step 54, loss 1.27033, acc 0.875\n",
      "2017-11-05T16:02:16.379690: step 55, loss 1.62492, acc 0.84375\n",
      "2017-11-05T16:02:20.325030: step 56, loss 0.623721, acc 0.8125\n",
      "2017-11-05T16:02:24.296323: step 57, loss 0.493126, acc 0.90625\n",
      "2017-11-05T16:02:28.290791: step 58, loss 0.506785, acc 0.84375\n",
      "2017-11-05T16:02:32.277989: step 59, loss 0.244325, acc 0.90625\n",
      "2017-11-05T16:02:36.415852: step 60, loss 0.441263, acc 0.9375\n",
      "2017-11-05T16:02:40.573908: step 61, loss 0.441193, acc 0.875\n",
      "2017-11-05T16:02:44.613278: step 62, loss 1.12865, acc 0.78125\n",
      "2017-11-05T16:02:48.643096: step 63, loss 1.62896, acc 0.84375\n",
      "2017-11-05T16:02:52.655584: step 64, loss 1.63405, acc 0.6875\n",
      "2017-11-05T16:02:56.846403: step 65, loss 1.54491, acc 0.875\n",
      "2017-11-05T16:03:00.890939: step 66, loss 0.456393, acc 0.84375\n",
      "2017-11-05T16:03:04.888974: step 67, loss 1.37712, acc 0.75\n",
      "2017-11-05T16:03:08.910547: step 68, loss 1.07694, acc 0.875\n",
      "2017-11-05T16:03:12.883477: step 69, loss 1.67922, acc 0.75\n",
      "2017-11-05T16:03:17.040669: step 70, loss 0.839742, acc 0.875\n",
      "2017-11-05T16:03:21.066598: step 71, loss 0.167494, acc 0.96875\n",
      "2017-11-05T16:03:23.808419: step 72, loss 0.913199, acc 0.9\n",
      "2017-11-05T16:03:27.983835: step 73, loss 0.501107, acc 0.90625\n",
      "2017-11-05T16:03:31.957459: step 74, loss 0.785726, acc 0.875\n",
      "2017-11-05T16:03:35.979073: step 75, loss 0.957162, acc 0.71875\n",
      "2017-11-05T16:03:40.003786: step 76, loss 0.670988, acc 0.84375\n",
      "2017-11-05T16:03:44.052506: step 77, loss 0.380627, acc 0.90625\n",
      "2017-11-05T16:03:47.930289: step 78, loss 0.170048, acc 0.875\n",
      "2017-11-05T16:03:51.972088: step 79, loss 0.392261, acc 0.9375\n",
      "2017-11-05T16:03:55.930958: step 80, loss 0.459541, acc 0.90625\n",
      "2017-11-05T16:03:59.953969: step 81, loss 0.672006, acc 0.90625\n",
      "2017-11-05T16:04:03.983999: step 82, loss 1.14192, acc 0.78125\n",
      "2017-11-05T16:04:07.910394: step 83, loss 0.177227, acc 0.90625\n",
      "2017-11-05T16:04:11.931531: step 84, loss 1.41567, acc 0.78125\n",
      "2017-11-05T16:04:15.957047: step 85, loss 1.34554, acc 0.84375\n",
      "2017-11-05T16:04:20.030740: step 86, loss 0.565495, acc 0.90625\n",
      "2017-11-05T16:04:24.054712: step 87, loss 0.646585, acc 0.8125\n",
      "2017-11-05T16:04:28.058841: step 88, loss 0.00671978, acc 1\n",
      "2017-11-05T16:04:31.963747: step 89, loss 0.819601, acc 0.84375\n",
      "2017-11-05T16:04:36.137702: step 90, loss 0.731802, acc 0.84375\n",
      "2017-11-05T16:04:40.173950: step 91, loss 1.11119, acc 0.84375\n",
      "2017-11-05T16:04:44.088601: step 92, loss 0.220739, acc 0.9375\n",
      "2017-11-05T16:04:48.130526: step 93, loss 1.01419, acc 0.65625\n",
      "2017-11-05T16:04:52.097665: step 94, loss 0.665496, acc 0.90625\n",
      "2017-11-05T16:04:56.076033: step 95, loss 0.550304, acc 0.9375\n",
      "2017-11-05T16:05:00.098247: step 96, loss 1.08204, acc 0.8125\n",
      "2017-11-05T16:05:04.088505: step 97, loss 0.757865, acc 0.78125\n",
      "2017-11-05T16:05:08.135309: step 98, loss 1.72181, acc 0.75\n",
      "2017-11-05T16:05:12.124623: step 99, loss 0.663091, acc 0.8125\n",
      "2017-11-05T16:05:16.040873: step 100, loss 1.24022, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:05:18.692401: step 100, loss 1.61615, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-100\n",
      "\n",
      "2017-11-05T16:05:24.093299: step 101, loss 0.467057, acc 0.875\n",
      "2017-11-05T16:05:28.002195: step 102, loss 1.30532, acc 0.84375\n",
      "2017-11-05T16:05:32.038498: step 103, loss 1.08652, acc 0.90625\n",
      "2017-11-05T16:05:35.958167: step 104, loss 0.573989, acc 0.9375\n",
      "2017-11-05T16:05:40.002318: step 105, loss 0.511615, acc 0.875\n",
      "2017-11-05T16:05:43.987103: step 106, loss 0.769002, acc 0.78125\n",
      "2017-11-05T16:05:47.966938: step 107, loss 0.451828, acc 0.90625\n",
      "2017-11-05T16:05:50.475667: step 108, loss 1.11244, acc 0.8\n",
      "2017-11-05T16:05:54.473986: step 109, loss 0.369881, acc 0.9375\n",
      "2017-11-05T16:05:58.505974: step 110, loss 0.821274, acc 0.78125\n",
      "2017-11-05T16:06:02.482239: step 111, loss 1.04255, acc 0.84375\n",
      "2017-11-05T16:06:06.390612: step 112, loss 0.55296, acc 0.875\n",
      "2017-11-05T16:06:10.355262: step 113, loss 0.441841, acc 0.875\n",
      "2017-11-05T16:06:14.330246: step 114, loss 0.512873, acc 0.84375\n",
      "2017-11-05T16:06:18.281801: step 115, loss 0.860906, acc 0.8125\n",
      "2017-11-05T16:06:22.351275: step 116, loss 0.866566, acc 0.8125\n",
      "2017-11-05T16:06:26.347928: step 117, loss 0.301655, acc 0.90625\n",
      "2017-11-05T16:06:30.312370: step 118, loss 1.07779, acc 0.78125\n",
      "2017-11-05T16:06:34.459248: step 119, loss 0.504898, acc 0.90625\n",
      "2017-11-05T16:06:38.446107: step 120, loss 0.313317, acc 0.90625\n",
      "2017-11-05T16:06:42.455030: step 121, loss 0.498584, acc 0.875\n",
      "2017-11-05T16:06:46.413297: step 122, loss 0.673326, acc 0.84375\n",
      "2017-11-05T16:06:50.381715: step 123, loss 0.365567, acc 0.90625\n",
      "2017-11-05T16:06:54.415127: step 124, loss 0.266467, acc 0.9375\n",
      "2017-11-05T16:06:58.433625: step 125, loss 0.728451, acc 0.84375\n",
      "2017-11-05T16:07:02.433714: step 126, loss 0.473772, acc 0.875\n",
      "2017-11-05T16:07:06.406692: step 127, loss 0.478106, acc 0.9375\n",
      "2017-11-05T16:07:10.400386: step 128, loss 1.12668, acc 0.8125\n",
      "2017-11-05T16:07:14.391985: step 129, loss 0.713396, acc 0.90625\n",
      "2017-11-05T16:07:18.351960: step 130, loss 0.15032, acc 0.9375\n",
      "2017-11-05T16:07:22.357139: step 131, loss 0.131807, acc 0.90625\n",
      "2017-11-05T16:07:26.374470: step 132, loss 0.819854, acc 0.875\n",
      "2017-11-05T16:07:30.290618: step 133, loss 0.0279425, acc 1\n",
      "2017-11-05T16:07:34.268466: step 134, loss 1.0637, acc 0.84375\n",
      "2017-11-05T16:07:38.256052: step 135, loss 0.200638, acc 0.9375\n",
      "2017-11-05T16:07:42.233334: step 136, loss 1.14801, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T16:07:46.260578: step 137, loss 0.440436, acc 0.90625\n",
      "2017-11-05T16:07:50.201112: step 138, loss 0.229317, acc 0.9375\n",
      "2017-11-05T16:07:54.240348: step 139, loss 0.642431, acc 0.875\n",
      "2017-11-05T16:07:58.249857: step 140, loss 0.778608, acc 0.875\n",
      "2017-11-05T16:08:02.210856: step 141, loss 0.761256, acc 0.90625\n",
      "2017-11-05T16:08:06.176414: step 142, loss 0.98954, acc 0.875\n",
      "2017-11-05T16:08:10.265703: step 143, loss 1.11455, acc 0.84375\n",
      "2017-11-05T16:08:12.779050: step 144, loss 0.318344, acc 0.85\n",
      "2017-11-05T16:08:16.757835: step 145, loss 0.664169, acc 0.875\n",
      "2017-11-05T16:08:20.790856: step 146, loss 0.412383, acc 0.875\n",
      "2017-11-05T16:08:25.005556: step 147, loss 0.976379, acc 0.71875\n",
      "2017-11-05T16:08:29.242961: step 148, loss 1.13618, acc 0.75\n",
      "2017-11-05T16:08:33.331883: step 149, loss 1.03419, acc 0.71875\n",
      "2017-11-05T16:08:37.371592: step 150, loss 1.05157, acc 0.78125\n",
      "2017-11-05T16:08:41.435993: step 151, loss 0.0737512, acc 0.96875\n",
      "2017-11-05T16:08:45.482573: step 152, loss 0.615547, acc 0.8125\n",
      "2017-11-05T16:08:49.445657: step 153, loss 0.433266, acc 0.8125\n",
      "2017-11-05T16:08:53.370777: step 154, loss 0.593763, acc 0.875\n",
      "2017-11-05T16:08:57.380943: step 155, loss 0.20591, acc 0.90625\n",
      "2017-11-05T16:09:01.511775: step 156, loss 0.440482, acc 0.90625\n",
      "2017-11-05T16:09:05.493396: step 157, loss 0.654066, acc 0.90625\n",
      "2017-11-05T16:09:09.454882: step 158, loss 0.501703, acc 0.90625\n",
      "2017-11-05T16:09:13.355634: step 159, loss 0.205662, acc 0.96875\n",
      "2017-11-05T16:09:17.308545: step 160, loss 1.96952, acc 0.8125\n",
      "2017-11-05T16:09:21.262090: step 161, loss 1.90495, acc 0.75\n",
      "2017-11-05T16:09:25.270368: step 162, loss 0.524003, acc 0.9375\n",
      "2017-11-05T16:09:29.181117: step 163, loss 0.26251, acc 0.90625\n",
      "2017-11-05T16:09:33.110526: step 164, loss 1.10795, acc 0.8125\n",
      "2017-11-05T16:09:37.044021: step 165, loss 0.151918, acc 0.96875\n",
      "2017-11-05T16:09:41.019537: step 166, loss 0.660556, acc 0.84375\n",
      "2017-11-05T16:09:44.905853: step 167, loss 0.827933, acc 0.78125\n",
      "2017-11-05T16:09:48.902141: step 168, loss 0.542216, acc 0.84375\n",
      "2017-11-05T16:09:52.817876: step 169, loss 0.677946, acc 0.78125\n",
      "2017-11-05T16:09:56.756812: step 170, loss 0.66938, acc 0.78125\n",
      "2017-11-05T16:10:00.829605: step 171, loss 0.602092, acc 0.8125\n",
      "2017-11-05T16:10:05.024185: step 172, loss 0.752942, acc 0.78125\n",
      "2017-11-05T16:10:09.020033: step 173, loss 0.581503, acc 0.875\n",
      "2017-11-05T16:10:12.977448: step 174, loss 0.61201, acc 0.84375\n",
      "2017-11-05T16:10:16.918573: step 175, loss 1.03928, acc 0.8125\n",
      "2017-11-05T16:10:20.840324: step 176, loss 0.14675, acc 0.96875\n",
      "2017-11-05T16:10:24.838687: step 177, loss 0.166047, acc 0.875\n",
      "2017-11-05T16:10:28.823943: step 178, loss 0.525763, acc 0.875\n",
      "2017-11-05T16:10:32.840107: step 179, loss 0.749685, acc 0.90625\n",
      "2017-11-05T16:10:35.442322: step 180, loss 0.0282046, acc 1\n",
      "2017-11-05T16:10:39.451344: step 181, loss 1.17255, acc 0.78125\n",
      "2017-11-05T16:10:43.434054: step 182, loss 0.706713, acc 0.875\n",
      "2017-11-05T16:10:47.369013: step 183, loss 0.560731, acc 0.90625\n",
      "2017-11-05T16:10:51.343523: step 184, loss 0.209189, acc 0.9375\n",
      "2017-11-05T16:10:55.373469: step 185, loss 0.224508, acc 0.9375\n",
      "2017-11-05T16:10:59.331507: step 186, loss 0.309453, acc 0.90625\n",
      "2017-11-05T16:11:03.370226: step 187, loss 0.439845, acc 0.8125\n",
      "2017-11-05T16:11:07.327903: step 188, loss 0.488717, acc 0.875\n",
      "2017-11-05T16:11:11.256608: step 189, loss 0.271848, acc 0.9375\n",
      "2017-11-05T16:11:15.227694: step 190, loss 0.781033, acc 0.84375\n",
      "2017-11-05T16:11:19.188482: step 191, loss 0.226664, acc 0.90625\n",
      "2017-11-05T16:11:23.168647: step 192, loss 0.132846, acc 0.9375\n",
      "2017-11-05T16:11:27.169614: step 193, loss 0.132914, acc 0.90625\n",
      "2017-11-05T16:11:31.218901: step 194, loss 0.850119, acc 0.78125\n",
      "2017-11-05T16:11:35.242971: step 195, loss 0.932038, acc 0.8125\n",
      "2017-11-05T16:11:39.221363: step 196, loss 0.39044, acc 0.875\n",
      "2017-11-05T16:11:43.179464: step 197, loss 1.67958, acc 0.71875\n",
      "2017-11-05T16:11:47.338775: step 198, loss 0.281897, acc 0.9375\n",
      "2017-11-05T16:11:51.452249: step 199, loss 0.428202, acc 0.9375\n",
      "2017-11-05T16:11:55.635629: step 200, loss 0.223453, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:11:58.415438: step 200, loss 1.69292, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-200\n",
      "\n",
      "2017-11-05T16:12:04.112784: step 201, loss 1.04261, acc 0.90625\n",
      "2017-11-05T16:12:08.255078: step 202, loss 0.0128146, acc 1\n",
      "2017-11-05T16:12:12.303169: step 203, loss 0.980793, acc 0.84375\n",
      "2017-11-05T16:12:16.613432: step 204, loss 0.467275, acc 0.875\n",
      "2017-11-05T16:12:20.792298: step 205, loss 0.200188, acc 0.96875\n",
      "2017-11-05T16:12:25.097649: step 206, loss 0.973925, acc 0.8125\n",
      "2017-11-05T16:12:29.289387: step 207, loss 0.619137, acc 0.875\n",
      "2017-11-05T16:12:33.302830: step 208, loss 0.0857948, acc 0.96875\n",
      "2017-11-05T16:12:37.375233: step 209, loss 0.279129, acc 0.9375\n",
      "2017-11-05T16:12:41.318886: step 210, loss 0.334691, acc 0.90625\n",
      "2017-11-05T16:12:45.272673: step 211, loss 0.169598, acc 0.90625\n",
      "2017-11-05T16:12:49.230726: step 212, loss 0.479149, acc 0.875\n",
      "2017-11-05T16:12:53.225428: step 213, loss 0.319926, acc 0.90625\n",
      "2017-11-05T16:12:57.256305: step 214, loss 0.703676, acc 0.8125\n",
      "2017-11-05T16:13:01.350471: step 215, loss 0.971704, acc 0.78125\n",
      "2017-11-05T16:13:03.954942: step 216, loss 0.776761, acc 0.85\n",
      "2017-11-05T16:13:07.986343: step 217, loss 0.611086, acc 0.8125\n",
      "2017-11-05T16:13:11.976965: step 218, loss 0.310585, acc 0.90625\n",
      "2017-11-05T16:13:15.907665: step 219, loss 0.491297, acc 0.90625\n",
      "2017-11-05T16:13:19.812401: step 220, loss 0.807411, acc 0.90625\n",
      "2017-11-05T16:13:23.966333: step 221, loss 0.922972, acc 0.84375\n",
      "2017-11-05T16:13:28.061741: step 222, loss 0.17396, acc 0.96875\n",
      "2017-11-05T16:13:32.020999: step 223, loss 0.304893, acc 0.96875\n",
      "2017-11-05T16:13:35.967815: step 224, loss 0.935088, acc 0.8125\n",
      "2017-11-05T16:13:39.929570: step 225, loss 0.682862, acc 0.84375\n",
      "2017-11-05T16:13:43.851205: step 226, loss 0.286445, acc 0.90625\n",
      "2017-11-05T16:13:47.780125: step 227, loss 0.412957, acc 0.875\n",
      "2017-11-05T16:13:51.759169: step 228, loss 0.242222, acc 0.875\n",
      "2017-11-05T16:13:55.810779: step 229, loss 0.579883, acc 0.8125\n",
      "2017-11-05T16:13:59.781758: step 230, loss 0.619879, acc 0.84375\n",
      "2017-11-05T16:14:03.792310: step 231, loss 0.15511, acc 0.96875\n",
      "2017-11-05T16:14:08.139481: step 232, loss 0.947373, acc 0.78125\n",
      "2017-11-05T16:14:12.102609: step 233, loss 1.21774, acc 0.78125\n",
      "2017-11-05T16:14:16.209008: step 234, loss 1.32712, acc 0.75\n",
      "2017-11-05T16:14:20.249910: step 235, loss 0.506871, acc 0.875\n",
      "2017-11-05T16:14:24.251746: step 236, loss 0.25561, acc 0.9375\n",
      "2017-11-05T16:14:28.235547: step 237, loss 0.133201, acc 0.90625\n",
      "2017-11-05T16:14:32.346439: step 238, loss 0.323715, acc 0.875\n",
      "2017-11-05T16:14:36.432645: step 239, loss 0.317382, acc 0.90625\n",
      "2017-11-05T16:14:40.471184: step 240, loss 0.38591, acc 0.875\n",
      "2017-11-05T16:14:44.563658: step 241, loss 0.485821, acc 0.9375\n",
      "2017-11-05T16:14:48.551800: step 242, loss 0.536094, acc 0.9375\n",
      "2017-11-05T16:14:52.542492: step 243, loss 0.781654, acc 0.875\n",
      "2017-11-05T16:14:56.533897: step 244, loss 0.387026, acc 0.90625\n",
      "2017-11-05T16:15:00.626710: step 245, loss 0.346245, acc 0.9375\n",
      "2017-11-05T16:15:04.622597: step 246, loss 0.357273, acc 0.90625\n",
      "2017-11-05T16:15:08.636885: step 247, loss 0.661777, acc 0.875\n",
      "2017-11-05T16:15:12.709546: step 248, loss 0.758718, acc 0.875\n",
      "2017-11-05T16:15:16.893162: step 249, loss 0.304309, acc 0.9375\n",
      "2017-11-05T16:15:20.945341: step 250, loss 0.14273, acc 0.9375\n",
      "2017-11-05T16:15:24.969387: step 251, loss 0.574986, acc 0.8125\n",
      "2017-11-05T16:15:27.582750: step 252, loss 0.211368, acc 0.95\n",
      "2017-11-05T16:15:31.642270: step 253, loss 0.767274, acc 0.8125\n",
      "2017-11-05T16:15:35.771066: step 254, loss 0.540462, acc 0.84375\n",
      "2017-11-05T16:15:39.783037: step 255, loss 1.22943, acc 0.75\n",
      "2017-11-05T16:15:43.948014: step 256, loss 0.445287, acc 0.875\n",
      "2017-11-05T16:15:47.976189: step 257, loss 0.0258065, acc 1\n",
      "2017-11-05T16:15:52.013108: step 258, loss 0.213875, acc 0.875\n",
      "2017-11-05T16:15:56.055703: step 259, loss 0.127422, acc 0.90625\n",
      "2017-11-05T16:16:00.220900: step 260, loss 0.959624, acc 0.8125\n",
      "2017-11-05T16:16:04.336134: step 261, loss 0.33181, acc 0.90625\n",
      "2017-11-05T16:16:08.510191: step 262, loss 0.548248, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T16:16:12.673199: step 263, loss 0.487072, acc 0.90625\n",
      "2017-11-05T16:16:16.945234: step 264, loss 0.0803751, acc 0.96875\n",
      "2017-11-05T16:16:21.048458: step 265, loss 0.499935, acc 0.875\n",
      "2017-11-05T16:16:25.197463: step 266, loss 0.354371, acc 0.90625\n",
      "2017-11-05T16:16:29.725583: step 267, loss 0.00384804, acc 1\n",
      "2017-11-05T16:16:34.462922: step 268, loss 0.427911, acc 0.90625\n",
      "2017-11-05T16:16:39.257436: step 269, loss 0.140007, acc 0.96875\n",
      "2017-11-05T16:16:43.796012: step 270, loss 0.407032, acc 0.84375\n",
      "2017-11-05T16:16:48.214243: step 271, loss 0.0682681, acc 0.96875\n",
      "2017-11-05T16:16:52.272242: step 272, loss 0.33728, acc 0.9375\n",
      "2017-11-05T16:16:56.770580: step 273, loss 0.506155, acc 0.90625\n",
      "2017-11-05T16:17:01.443065: step 274, loss 0.476022, acc 0.8125\n",
      "2017-11-05T16:17:05.527624: step 275, loss 0.263183, acc 0.9375\n",
      "2017-11-05T16:17:09.642455: step 276, loss 0.640061, acc 0.84375\n",
      "2017-11-05T16:17:13.574278: step 277, loss 0.580689, acc 0.84375\n",
      "2017-11-05T16:17:17.806924: step 278, loss 0.730489, acc 0.875\n",
      "2017-11-05T16:17:21.982823: step 279, loss 0.573668, acc 0.84375\n",
      "2017-11-05T16:17:26.325413: step 280, loss 0.794223, acc 0.90625\n",
      "2017-11-05T16:17:30.944348: step 281, loss 0.620608, acc 0.90625\n",
      "2017-11-05T16:17:35.171668: step 282, loss 0.430932, acc 0.96875\n",
      "2017-11-05T16:17:39.370795: step 283, loss 0.879141, acc 0.84375\n",
      "2017-11-05T16:17:43.347691: step 284, loss 0.208508, acc 0.90625\n",
      "2017-11-05T16:17:47.401233: step 285, loss 0.248091, acc 0.875\n",
      "2017-11-05T16:17:51.492460: step 286, loss 0.0443609, acc 0.96875\n",
      "2017-11-05T16:17:55.609920: step 287, loss 0.504184, acc 0.90625\n",
      "2017-11-05T16:17:58.172865: step 288, loss 0.235214, acc 0.85\n",
      "2017-11-05T16:18:02.223218: step 289, loss 0.832963, acc 0.90625\n",
      "2017-11-05T16:18:06.263508: step 290, loss 0.378666, acc 0.875\n",
      "2017-11-05T16:18:10.256912: step 291, loss 0.296964, acc 0.875\n",
      "2017-11-05T16:18:14.362120: step 292, loss 0.294588, acc 0.90625\n",
      "2017-11-05T16:18:18.468192: step 293, loss 0.439477, acc 0.875\n",
      "2017-11-05T16:18:22.660842: step 294, loss 0.339571, acc 0.84375\n",
      "2017-11-05T16:18:26.939486: step 295, loss 0.279349, acc 0.96875\n",
      "2017-11-05T16:18:30.974723: step 296, loss 0.457101, acc 0.8125\n",
      "2017-11-05T16:18:35.142956: step 297, loss 0.205674, acc 0.96875\n",
      "2017-11-05T16:18:39.253274: step 298, loss 0.33821, acc 0.90625\n",
      "2017-11-05T16:18:43.947078: step 299, loss 0.382255, acc 0.9375\n",
      "2017-11-05T16:18:48.153455: step 300, loss 0.164288, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:18:50.982018: step 300, loss 1.46794, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-300\n",
      "\n",
      "2017-11-05T16:18:56.808631: step 301, loss 0.128384, acc 0.9375\n",
      "2017-11-05T16:19:01.259170: step 302, loss 0.311893, acc 0.90625\n",
      "2017-11-05T16:19:05.557143: step 303, loss 0.121366, acc 0.96875\n",
      "2017-11-05T16:19:10.046808: step 304, loss 0.413829, acc 0.90625\n",
      "2017-11-05T16:19:14.345033: step 305, loss 0.254932, acc 0.96875\n",
      "2017-11-05T16:19:18.600431: step 306, loss 0.14228, acc 0.90625\n",
      "2017-11-05T16:19:22.819994: step 307, loss 0.450042, acc 0.84375\n",
      "2017-11-05T16:19:27.095599: step 308, loss 0.196778, acc 0.9375\n",
      "2017-11-05T16:19:31.178544: step 309, loss 0.490533, acc 0.90625\n",
      "2017-11-05T16:19:35.378351: step 310, loss 1.0442, acc 0.84375\n",
      "2017-11-05T16:19:39.481450: step 311, loss 0.337155, acc 0.875\n",
      "2017-11-05T16:19:43.456501: step 312, loss 0.159767, acc 0.9375\n",
      "2017-11-05T16:19:47.508947: step 313, loss 0.233981, acc 0.96875\n",
      "2017-11-05T16:19:51.419285: step 314, loss 0.34546, acc 0.90625\n",
      "2017-11-05T16:19:55.448007: step 315, loss 0.294064, acc 0.9375\n",
      "2017-11-05T16:19:59.502169: step 316, loss 0.394635, acc 0.84375\n",
      "2017-11-05T16:20:03.791093: step 317, loss 0.323601, acc 0.875\n",
      "2017-11-05T16:20:07.833172: step 318, loss 0.331417, acc 0.90625\n",
      "2017-11-05T16:20:11.872808: step 319, loss 0.179861, acc 0.96875\n",
      "2017-11-05T16:20:15.890408: step 320, loss 0.740381, acc 0.8125\n",
      "2017-11-05T16:20:19.919474: step 321, loss 0.327554, acc 0.90625\n",
      "2017-11-05T16:20:23.959703: step 322, loss 0.372059, acc 0.90625\n",
      "2017-11-05T16:20:28.002824: step 323, loss 0.649637, acc 0.78125\n",
      "2017-11-05T16:20:30.535958: step 324, loss 0.0785974, acc 0.95\n",
      "2017-11-05T16:20:34.704275: step 325, loss 0.38283, acc 0.9375\n",
      "2017-11-05T16:20:38.768091: step 326, loss 0.11712, acc 0.96875\n",
      "2017-11-05T16:20:42.934648: step 327, loss 0.393067, acc 0.875\n",
      "2017-11-05T16:20:46.946624: step 328, loss 0.669633, acc 0.875\n",
      "2017-11-05T16:20:51.040640: step 329, loss 0.554314, acc 0.90625\n",
      "2017-11-05T16:20:55.091364: step 330, loss 0.665722, acc 0.9375\n",
      "2017-11-05T16:20:59.227286: step 331, loss 0.0843848, acc 0.9375\n",
      "2017-11-05T16:21:03.391026: step 332, loss 0.347239, acc 0.875\n",
      "2017-11-05T16:21:07.475913: step 333, loss 0.415152, acc 0.9375\n",
      "2017-11-05T16:21:11.611912: step 334, loss 0.596612, acc 0.84375\n",
      "2017-11-05T16:21:15.682370: step 335, loss 0.189043, acc 0.9375\n",
      "2017-11-05T16:21:19.740571: step 336, loss 0.32944, acc 0.9375\n",
      "2017-11-05T16:21:23.784302: step 337, loss 0.282308, acc 0.9375\n",
      "2017-11-05T16:21:27.828994: step 338, loss 0.330162, acc 0.875\n",
      "2017-11-05T16:21:31.895726: step 339, loss 0.354814, acc 0.96875\n",
      "2017-11-05T16:21:35.991114: step 340, loss 0.542931, acc 0.90625\n",
      "2017-11-05T16:21:40.053966: step 341, loss 0.647511, acc 0.84375\n",
      "2017-11-05T16:21:44.118591: step 342, loss 0.624805, acc 0.875\n",
      "2017-11-05T16:21:48.169197: step 343, loss 0.0862533, acc 0.9375\n",
      "2017-11-05T16:21:52.106818: step 344, loss 0.701092, acc 0.875\n",
      "2017-11-05T16:21:56.083385: step 345, loss 0.377184, acc 0.90625\n",
      "2017-11-05T16:21:59.998529: step 346, loss 0.289115, acc 0.9375\n",
      "2017-11-05T16:22:03.980319: step 347, loss 0.271113, acc 0.9375\n",
      "2017-11-05T16:22:07.984047: step 348, loss 0.903641, acc 0.78125\n",
      "2017-11-05T16:22:11.933355: step 349, loss 0.31118, acc 0.875\n",
      "2017-11-05T16:22:15.876654: step 350, loss 0.905405, acc 0.8125\n",
      "2017-11-05T16:22:19.822985: step 351, loss 0.769793, acc 0.875\n",
      "2017-11-05T16:22:23.800217: step 352, loss 0.545389, acc 0.875\n",
      "2017-11-05T16:22:27.786114: step 353, loss 0.370251, acc 0.90625\n",
      "2017-11-05T16:22:31.754725: step 354, loss 0.140696, acc 0.96875\n",
      "2017-11-05T16:22:35.864578: step 355, loss 0.227601, acc 0.9375\n",
      "2017-11-05T16:22:39.821345: step 356, loss 0.224205, acc 0.9375\n",
      "2017-11-05T16:22:43.823498: step 357, loss 0.445233, acc 0.84375\n",
      "2017-11-05T16:22:47.812827: step 358, loss 0.350614, acc 0.90625\n",
      "2017-11-05T16:22:51.742467: step 359, loss 0.215861, acc 0.90625\n",
      "2017-11-05T16:22:54.304208: step 360, loss 0.0785968, acc 0.95\n",
      "2017-11-05T16:22:58.309124: step 361, loss 0.433754, acc 0.875\n",
      "2017-11-05T16:23:02.361651: step 362, loss 0.557863, acc 0.875\n",
      "2017-11-05T16:23:06.446612: step 363, loss 0.754542, acc 0.875\n",
      "2017-11-05T16:23:10.470382: step 364, loss 0.318364, acc 0.90625\n",
      "2017-11-05T16:23:14.493526: step 365, loss 0.354219, acc 0.875\n",
      "2017-11-05T16:23:18.552972: step 366, loss 0.123888, acc 0.90625\n",
      "2017-11-05T16:23:22.631457: step 367, loss 1.3066, acc 0.84375\n",
      "2017-11-05T16:23:26.832112: step 368, loss 0.0727405, acc 0.96875\n",
      "2017-11-05T16:23:31.146656: step 369, loss 0.497807, acc 0.875\n",
      "2017-11-05T16:23:35.152734: step 370, loss 0.0935864, acc 0.96875\n",
      "2017-11-05T16:23:39.183083: step 371, loss 0.428908, acc 0.9375\n",
      "2017-11-05T16:23:43.199810: step 372, loss 0.188159, acc 0.96875\n",
      "2017-11-05T16:23:47.273908: step 373, loss 0.563346, acc 0.8125\n",
      "2017-11-05T16:23:51.319460: step 374, loss 0.397332, acc 0.96875\n",
      "2017-11-05T16:23:55.273107: step 375, loss 0.157593, acc 0.9375\n",
      "2017-11-05T16:23:59.309884: step 376, loss 1.01391, acc 0.8125\n",
      "2017-11-05T16:24:03.288482: step 377, loss 0.280448, acc 0.9375\n",
      "2017-11-05T16:24:07.284488: step 378, loss 0.600329, acc 0.84375\n",
      "2017-11-05T16:24:11.273434: step 379, loss 0.317252, acc 0.90625\n",
      "2017-11-05T16:24:15.221503: step 380, loss 0.551709, acc 0.875\n",
      "2017-11-05T16:24:19.197305: step 381, loss 0.0435186, acc 1\n",
      "2017-11-05T16:24:23.168054: step 382, loss 0.164004, acc 0.9375\n",
      "2017-11-05T16:24:27.138052: step 383, loss 0.463691, acc 0.84375\n",
      "2017-11-05T16:24:31.069454: step 384, loss 0.642909, acc 0.8125\n",
      "2017-11-05T16:24:35.148799: step 385, loss 0.153533, acc 0.90625\n",
      "2017-11-05T16:24:39.186711: step 386, loss 0.00508109, acc 1\n",
      "2017-11-05T16:24:43.163667: step 387, loss 0.684816, acc 0.84375\n",
      "2017-11-05T16:24:47.115282: step 388, loss 0.538587, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T16:24:51.130065: step 389, loss 0.58807, acc 0.84375\n",
      "2017-11-05T16:24:55.092788: step 390, loss 1.00402, acc 0.84375\n",
      "2017-11-05T16:24:59.082676: step 391, loss 0.709038, acc 0.90625\n",
      "2017-11-05T16:25:02.999299: step 392, loss 0.200057, acc 0.90625\n",
      "2017-11-05T16:25:07.052466: step 393, loss 0.0305232, acc 0.96875\n",
      "2017-11-05T16:25:11.036162: step 394, loss 0.835243, acc 0.84375\n",
      "2017-11-05T16:25:14.941197: step 395, loss 0.127387, acc 0.9375\n",
      "2017-11-05T16:25:17.525767: step 396, loss 0.491333, acc 0.9\n",
      "2017-11-05T16:25:21.553012: step 397, loss 0.199794, acc 0.9375\n",
      "2017-11-05T16:25:25.500535: step 398, loss 0.348335, acc 0.9375\n",
      "2017-11-05T16:25:29.531427: step 399, loss 0.783293, acc 0.875\n",
      "2017-11-05T16:25:33.507348: step 400, loss 0.127701, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:25:36.136550: step 400, loss 1.38531, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-400\n",
      "\n",
      "2017-11-05T16:25:41.634201: step 401, loss 0.750401, acc 0.75\n",
      "2017-11-05T16:25:45.732397: step 402, loss 0.108717, acc 0.9375\n",
      "2017-11-05T16:25:49.872010: step 403, loss 0.586915, acc 0.875\n",
      "2017-11-05T16:25:53.938427: step 404, loss 0.387409, acc 0.875\n",
      "2017-11-05T16:25:57.963561: step 405, loss 0.122829, acc 0.90625\n",
      "2017-11-05T16:26:02.127417: step 406, loss 0.177379, acc 0.90625\n",
      "2017-11-05T16:26:06.153393: step 407, loss 0.330534, acc 0.90625\n",
      "2017-11-05T16:26:10.254970: step 408, loss 0.109041, acc 0.96875\n",
      "2017-11-05T16:26:14.268479: step 409, loss 0.380087, acc 0.9375\n",
      "2017-11-05T16:26:18.324526: step 410, loss 0.0173566, acc 1\n",
      "2017-11-05T16:26:22.334859: step 411, loss 0.649027, acc 0.875\n",
      "2017-11-05T16:26:26.856293: step 412, loss 0.284743, acc 0.90625\n",
      "2017-11-05T16:26:31.252364: step 413, loss 0.444665, acc 0.875\n",
      "2017-11-05T16:26:35.706288: step 414, loss 0.149924, acc 0.90625\n",
      "2017-11-05T16:26:39.765021: step 415, loss 0.377141, acc 0.90625\n",
      "2017-11-05T16:26:43.740410: step 416, loss 1.07729, acc 0.875\n",
      "2017-11-05T16:26:47.812629: step 417, loss 0.728201, acc 0.8125\n",
      "2017-11-05T16:26:51.837098: step 418, loss 0.470266, acc 0.9375\n",
      "2017-11-05T16:26:55.832103: step 419, loss 0.084875, acc 0.96875\n",
      "2017-11-05T16:26:59.822298: step 420, loss 0.267097, acc 0.90625\n",
      "2017-11-05T16:27:03.821005: step 421, loss 0.0626492, acc 0.96875\n",
      "2017-11-05T16:27:07.856414: step 422, loss 0.101369, acc 0.96875\n",
      "2017-11-05T16:27:11.856616: step 423, loss 0.224333, acc 0.90625\n",
      "2017-11-05T16:27:15.895267: step 424, loss 0.496118, acc 0.875\n",
      "2017-11-05T16:27:19.867635: step 425, loss 0.702221, acc 0.84375\n",
      "2017-11-05T16:27:23.859190: step 426, loss 0.557165, acc 0.8125\n",
      "2017-11-05T16:27:27.829069: step 427, loss 0.640492, acc 0.84375\n",
      "2017-11-05T16:27:31.805285: step 428, loss 0.629511, acc 0.84375\n",
      "2017-11-05T16:27:35.797840: step 429, loss 0.479299, acc 0.90625\n",
      "2017-11-05T16:27:39.855127: step 430, loss 0.555673, acc 0.875\n",
      "2017-11-05T16:27:43.844223: step 431, loss 0.0846485, acc 0.9375\n",
      "2017-11-05T16:27:46.352469: step 432, loss 0.931309, acc 0.85\n",
      "2017-11-05T16:27:50.416240: step 433, loss 0.228593, acc 0.96875\n",
      "2017-11-05T16:27:54.468282: step 434, loss 0.0188438, acc 1\n",
      "2017-11-05T16:27:58.436361: step 435, loss 0.257117, acc 0.9375\n",
      "2017-11-05T16:28:02.472841: step 436, loss 0.525573, acc 0.90625\n",
      "2017-11-05T16:28:06.450573: step 437, loss 0.275332, acc 0.9375\n",
      "2017-11-05T16:28:10.528976: step 438, loss 0.519084, acc 0.90625\n",
      "2017-11-05T16:28:14.506784: step 439, loss 0.178576, acc 0.96875\n",
      "2017-11-05T16:28:18.525848: step 440, loss 0.0720564, acc 0.96875\n",
      "2017-11-05T16:28:22.664917: step 441, loss 0.415943, acc 0.90625\n",
      "2017-11-05T16:28:26.797693: step 442, loss 0.213612, acc 0.9375\n",
      "2017-11-05T16:28:30.825781: step 443, loss 0.243674, acc 0.96875\n",
      "2017-11-05T16:28:34.900468: step 444, loss 0.279551, acc 0.9375\n",
      "2017-11-05T16:28:38.940795: step 445, loss 0.227233, acc 0.9375\n",
      "2017-11-05T16:28:42.916185: step 446, loss 0.572791, acc 0.875\n",
      "2017-11-05T16:28:46.929939: step 447, loss 0.212856, acc 0.875\n",
      "2017-11-05T16:28:51.066095: step 448, loss 0.754739, acc 0.8125\n",
      "2017-11-05T16:28:55.030010: step 449, loss 0.685313, acc 0.875\n",
      "2017-11-05T16:28:59.045699: step 450, loss 0.837979, acc 0.84375\n",
      "2017-11-05T16:29:03.033385: step 451, loss 0.00671861, acc 1\n",
      "2017-11-05T16:29:06.974106: step 452, loss 0.167028, acc 0.90625\n",
      "2017-11-05T16:29:11.052965: step 453, loss 0.181154, acc 0.9375\n",
      "2017-11-05T16:29:15.063131: step 454, loss 0.171686, acc 0.9375\n",
      "2017-11-05T16:29:18.977606: step 455, loss 0.82313, acc 0.78125\n",
      "2017-11-05T16:29:22.951556: step 456, loss 0.251893, acc 0.84375\n",
      "2017-11-05T16:29:26.901205: step 457, loss 0.38457, acc 0.90625\n",
      "2017-11-05T16:29:30.919080: step 458, loss 0.332903, acc 0.90625\n",
      "2017-11-05T16:29:34.879709: step 459, loss 0.292614, acc 0.90625\n",
      "2017-11-05T16:29:38.837738: step 460, loss 0.328337, acc 0.875\n",
      "2017-11-05T16:29:42.852460: step 461, loss 0.678551, acc 0.875\n",
      "2017-11-05T16:29:46.736154: step 462, loss 0.320719, acc 0.90625\n",
      "2017-11-05T16:29:50.800465: step 463, loss 0.490697, acc 0.90625\n",
      "2017-11-05T16:29:54.769167: step 464, loss 0.77023, acc 0.84375\n",
      "2017-11-05T16:29:58.760146: step 465, loss 0.0869766, acc 0.96875\n",
      "2017-11-05T16:30:03.071053: step 466, loss 0.511992, acc 0.8125\n",
      "2017-11-05T16:30:07.002587: step 467, loss 0.332086, acc 0.90625\n",
      "2017-11-05T16:30:09.571375: step 468, loss 0.105365, acc 0.95\n",
      "2017-11-05T16:30:13.577904: step 469, loss 0.450618, acc 0.84375\n",
      "2017-11-05T16:30:17.548812: step 470, loss 0.606835, acc 0.875\n",
      "2017-11-05T16:30:21.493105: step 471, loss 0.311683, acc 0.9375\n",
      "2017-11-05T16:30:25.416954: step 472, loss 0.123206, acc 0.9375\n",
      "2017-11-05T16:30:29.380361: step 473, loss 0.237436, acc 0.90625\n",
      "2017-11-05T16:30:33.420033: step 474, loss 0.620843, acc 0.875\n",
      "2017-11-05T16:30:37.464144: step 475, loss 0.768402, acc 0.875\n",
      "2017-11-05T16:30:41.497203: step 476, loss 0.37392, acc 0.90625\n",
      "2017-11-05T16:30:45.457905: step 477, loss 0.200151, acc 0.90625\n",
      "2017-11-05T16:30:49.394617: step 478, loss 0.815675, acc 0.875\n",
      "2017-11-05T16:30:53.402863: step 479, loss 0.169117, acc 0.9375\n",
      "2017-11-05T16:30:57.359108: step 480, loss 0.31564, acc 0.9375\n",
      "2017-11-05T16:31:01.310845: step 481, loss 0.251876, acc 0.9375\n",
      "2017-11-05T16:31:05.349324: step 482, loss 0.138208, acc 0.96875\n",
      "2017-11-05T16:31:09.360415: step 483, loss 0.913406, acc 0.8125\n",
      "2017-11-05T16:31:13.383306: step 484, loss 0.383274, acc 0.875\n",
      "2017-11-05T16:31:17.396126: step 485, loss 0.430909, acc 0.78125\n",
      "2017-11-05T16:31:21.388230: step 486, loss 0.423085, acc 0.90625\n",
      "2017-11-05T16:31:25.396645: step 487, loss 0.368153, acc 0.90625\n",
      "2017-11-05T16:31:29.389535: step 488, loss 0.0134569, acc 1\n",
      "2017-11-05T16:31:33.335502: step 489, loss 0.410923, acc 0.875\n",
      "2017-11-05T16:31:37.343825: step 490, loss 0.0689859, acc 0.96875\n",
      "2017-11-05T16:31:41.286152: step 491, loss 0.343852, acc 0.9375\n",
      "2017-11-05T16:31:45.234456: step 492, loss 0.610287, acc 0.9375\n",
      "2017-11-05T16:31:49.244110: step 493, loss 0.00153189, acc 1\n",
      "2017-11-05T16:31:53.179983: step 494, loss 0.132889, acc 0.96875\n",
      "2017-11-05T16:31:57.238635: step 495, loss 0.392692, acc 0.90625\n",
      "2017-11-05T16:32:01.207452: step 496, loss 0.486947, acc 0.90625\n",
      "2017-11-05T16:32:05.246959: step 497, loss 0.334399, acc 0.9375\n",
      "2017-11-05T16:32:09.230978: step 498, loss 0.611452, acc 0.90625\n",
      "2017-11-05T16:32:13.280603: step 499, loss 0.276372, acc 0.90625\n",
      "2017-11-05T16:32:17.226105: step 500, loss 0.0138086, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:32:19.820940: step 500, loss 1.53357, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-500\n",
      "\n",
      "2017-11-05T16:32:25.349044: step 501, loss 0.589399, acc 0.84375\n",
      "2017-11-05T16:32:29.311410: step 502, loss 0.692978, acc 0.78125\n",
      "2017-11-05T16:32:33.325531: step 503, loss 0.32382, acc 0.9375\n",
      "2017-11-05T16:32:35.946063: step 504, loss 0.830568, acc 0.85\n",
      "2017-11-05T16:32:39.873992: step 505, loss 0.2321, acc 0.9375\n",
      "2017-11-05T16:32:43.852197: step 506, loss 0.541564, acc 0.84375\n",
      "2017-11-05T16:32:47.795069: step 507, loss 0.0592806, acc 0.96875\n",
      "2017-11-05T16:32:51.818989: step 508, loss 0.217472, acc 0.90625\n",
      "2017-11-05T16:32:55.788769: step 509, loss 0.516968, acc 0.875\n",
      "2017-11-05T16:32:59.710315: step 510, loss 0.866101, acc 0.78125\n",
      "2017-11-05T16:33:03.797873: step 511, loss 0.420351, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T16:33:07.775744: step 512, loss 0.187506, acc 0.90625\n",
      "2017-11-05T16:33:11.737646: step 513, loss 0.0478321, acc 0.96875\n",
      "2017-11-05T16:33:15.780423: step 514, loss 0.58731, acc 0.875\n",
      "2017-11-05T16:33:19.927609: step 515, loss 0.522667, acc 0.875\n",
      "2017-11-05T16:33:24.005583: step 516, loss 0.512323, acc 0.90625\n",
      "2017-11-05T16:33:28.099530: step 517, loss 0.0222923, acc 1\n",
      "2017-11-05T16:33:32.064597: step 518, loss 0.361145, acc 0.875\n",
      "2017-11-05T16:33:36.001803: step 519, loss 0.424561, acc 0.8125\n",
      "2017-11-05T16:33:39.986341: step 520, loss 0.450777, acc 0.84375\n",
      "2017-11-05T16:33:43.957529: step 521, loss 0.489086, acc 0.8125\n",
      "2017-11-05T16:33:47.898176: step 522, loss 0.514629, acc 0.90625\n",
      "2017-11-05T16:33:51.853653: step 523, loss 0.364088, acc 0.90625\n",
      "2017-11-05T16:33:55.880726: step 524, loss 0.243327, acc 0.9375\n",
      "2017-11-05T16:33:59.870404: step 525, loss 0.640144, acc 0.875\n",
      "2017-11-05T16:34:03.854250: step 526, loss 0.255117, acc 0.9375\n",
      "2017-11-05T16:34:07.814653: step 527, loss 0.0659992, acc 0.96875\n",
      "2017-11-05T16:34:11.757641: step 528, loss 0.277933, acc 0.8125\n",
      "2017-11-05T16:34:15.766275: step 529, loss 0.045866, acc 0.96875\n",
      "2017-11-05T16:34:19.724210: step 530, loss 0.255917, acc 0.9375\n",
      "2017-11-05T16:34:23.661470: step 531, loss 0.900699, acc 0.78125\n",
      "2017-11-05T16:34:27.652658: step 532, loss 0.739466, acc 0.875\n",
      "2017-11-05T16:34:31.561276: step 533, loss 0.467704, acc 0.84375\n",
      "2017-11-05T16:34:35.617305: step 534, loss 0.308014, acc 0.90625\n",
      "2017-11-05T16:34:39.594471: step 535, loss 0.428108, acc 0.90625\n",
      "2017-11-05T16:34:43.571012: step 536, loss 0.455985, acc 0.84375\n",
      "2017-11-05T16:34:47.501689: step 537, loss 0.289111, acc 0.875\n",
      "2017-11-05T16:34:51.436521: step 538, loss 0.238687, acc 0.9375\n",
      "2017-11-05T16:34:55.393157: step 539, loss 0.19543, acc 0.875\n",
      "2017-11-05T16:34:57.912425: step 540, loss 0.469517, acc 0.9\n",
      "2017-11-05T16:35:01.874977: step 541, loss 0.0388384, acc 1\n",
      "2017-11-05T16:35:05.835538: step 542, loss 0.481721, acc 0.875\n",
      "2017-11-05T16:35:09.817528: step 543, loss 0.06728, acc 0.9375\n",
      "2017-11-05T16:35:13.822959: step 544, loss 0.4392, acc 0.875\n",
      "2017-11-05T16:35:17.923357: step 545, loss 0.425345, acc 0.875\n",
      "2017-11-05T16:35:21.820725: step 546, loss 0.132231, acc 0.9375\n",
      "2017-11-05T16:35:25.786601: step 547, loss 0.304226, acc 0.9375\n",
      "2017-11-05T16:35:29.712438: step 548, loss 0.01675, acc 1\n",
      "2017-11-05T16:35:33.689820: step 549, loss 0.197945, acc 0.9375\n",
      "2017-11-05T16:35:37.604985: step 550, loss 0.495077, acc 0.90625\n",
      "2017-11-05T16:35:41.535186: step 551, loss 0.124647, acc 0.90625\n",
      "2017-11-05T16:35:45.494411: step 552, loss 0.23667, acc 0.90625\n",
      "2017-11-05T16:35:49.417645: step 553, loss 0.190415, acc 0.90625\n",
      "2017-11-05T16:35:53.391427: step 554, loss 0.294126, acc 0.90625\n",
      "2017-11-05T16:35:57.347394: step 555, loss 0.309549, acc 0.875\n",
      "2017-11-05T16:36:01.295545: step 556, loss 0.222466, acc 0.9375\n",
      "2017-11-05T16:36:05.224326: step 557, loss 0.260341, acc 0.90625\n",
      "2017-11-05T16:36:09.176061: step 558, loss 0.349423, acc 0.875\n",
      "2017-11-05T16:36:13.196697: step 559, loss 0.364129, acc 0.875\n",
      "2017-11-05T16:36:17.189960: step 560, loss 0.551416, acc 0.875\n",
      "2017-11-05T16:36:21.146266: step 561, loss 0.587008, acc 0.875\n",
      "2017-11-05T16:36:25.125389: step 562, loss 0.326853, acc 0.875\n",
      "2017-11-05T16:36:29.100500: step 563, loss 0.282364, acc 0.90625\n",
      "2017-11-05T16:36:33.132878: step 564, loss 0.187508, acc 0.9375\n",
      "2017-11-05T16:36:37.216882: step 565, loss 0.252904, acc 0.9375\n",
      "2017-11-05T16:36:41.271325: step 566, loss 0.24003, acc 0.9375\n",
      "2017-11-05T16:36:45.326843: step 567, loss 0.300065, acc 0.96875\n",
      "2017-11-05T16:36:49.304493: step 568, loss 0.426551, acc 0.9375\n",
      "2017-11-05T16:36:53.330523: step 569, loss 0.503402, acc 0.875\n",
      "2017-11-05T16:36:57.271362: step 570, loss 0.131749, acc 0.96875\n",
      "2017-11-05T16:37:01.247220: step 571, loss 0.384446, acc 0.9375\n",
      "2017-11-05T16:37:05.180962: step 572, loss 0.365179, acc 0.9375\n",
      "2017-11-05T16:37:09.151944: step 573, loss 0.71482, acc 0.875\n",
      "2017-11-05T16:37:13.097122: step 574, loss 0.822867, acc 0.875\n",
      "2017-11-05T16:37:17.047683: step 575, loss 0.435257, acc 0.875\n",
      "2017-11-05T16:37:19.662815: step 576, loss 0.578461, acc 0.75\n",
      "2017-11-05T16:37:23.625023: step 577, loss 0.124373, acc 0.96875\n",
      "2017-11-05T16:37:27.545410: step 578, loss 0.138664, acc 0.96875\n",
      "2017-11-05T16:37:31.488491: step 579, loss 0.494728, acc 0.875\n",
      "2017-11-05T16:37:35.517363: step 580, loss 0.270626, acc 0.9375\n",
      "2017-11-05T16:37:39.471527: step 581, loss 0.319164, acc 0.90625\n",
      "2017-11-05T16:37:43.407928: step 582, loss 0.110549, acc 0.96875\n",
      "2017-11-05T16:37:47.363947: step 583, loss 0.210246, acc 0.90625\n",
      "2017-11-05T16:37:51.338709: step 584, loss 0.203701, acc 0.9375\n",
      "2017-11-05T16:37:55.303965: step 585, loss 0.0452473, acc 1\n",
      "2017-11-05T16:37:59.214900: step 586, loss 0.293211, acc 0.90625\n",
      "2017-11-05T16:38:03.238133: step 587, loss 0.181975, acc 0.90625\n",
      "2017-11-05T16:38:07.184736: step 588, loss 0.202687, acc 0.9375\n",
      "2017-11-05T16:38:11.134616: step 589, loss 0.302113, acc 0.90625\n",
      "2017-11-05T16:38:15.103110: step 590, loss 0.465168, acc 0.78125\n",
      "2017-11-05T16:38:19.030260: step 591, loss 0.228878, acc 0.90625\n",
      "2017-11-05T16:38:23.161549: step 592, loss 0.160514, acc 0.90625\n",
      "2017-11-05T16:38:27.172155: step 593, loss 0.268061, acc 0.90625\n",
      "2017-11-05T16:38:31.101668: step 594, loss 0.32908, acc 0.875\n",
      "2017-11-05T16:38:35.255543: step 595, loss 0.642351, acc 0.84375\n",
      "2017-11-05T16:38:39.244920: step 596, loss 0.256835, acc 0.84375\n",
      "2017-11-05T16:38:43.202484: step 597, loss 0.17232, acc 0.96875\n",
      "2017-11-05T16:38:47.135051: step 598, loss 0.176652, acc 0.96875\n",
      "2017-11-05T16:38:51.080475: step 599, loss 0.412998, acc 0.90625\n",
      "2017-11-05T16:38:55.155010: step 600, loss 0.381861, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:38:57.739930: step 600, loss 1.46283, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-600\n",
      "\n",
      "2017-11-05T16:39:03.267861: step 601, loss 0.320578, acc 0.90625\n",
      "2017-11-05T16:39:07.276006: step 602, loss 0.19316, acc 0.9375\n",
      "2017-11-05T16:39:11.249010: step 603, loss 0.508374, acc 0.84375\n",
      "2017-11-05T16:39:15.217049: step 604, loss 0.44912, acc 0.875\n",
      "2017-11-05T16:39:19.190812: step 605, loss 0.431515, acc 0.9375\n",
      "2017-11-05T16:39:23.190341: step 606, loss 0.247135, acc 0.875\n",
      "2017-11-05T16:39:27.180545: step 607, loss 0.106757, acc 0.9375\n",
      "2017-11-05T16:39:31.156195: step 608, loss 0.297429, acc 0.84375\n",
      "2017-11-05T16:39:35.135673: step 609, loss 0.390738, acc 0.90625\n",
      "2017-11-05T16:39:39.139644: step 610, loss 0.127767, acc 0.90625\n",
      "2017-11-05T16:39:43.152881: step 611, loss 0.350631, acc 0.90625\n",
      "2017-11-05T16:39:45.606325: step 612, loss 0.202283, acc 0.9\n",
      "2017-11-05T16:39:49.616104: step 613, loss 0.304029, acc 0.875\n",
      "2017-11-05T16:39:53.564498: step 614, loss 0.100696, acc 0.9375\n",
      "2017-11-05T16:39:57.506212: step 615, loss 0.384292, acc 0.875\n",
      "2017-11-05T16:40:01.674308: step 616, loss 0.405575, acc 0.875\n",
      "2017-11-05T16:40:05.761624: step 617, loss 0.344506, acc 0.8125\n",
      "2017-11-05T16:40:09.785897: step 618, loss 0.379786, acc 0.875\n",
      "2017-11-05T16:40:13.765178: step 619, loss 0.172478, acc 0.90625\n",
      "2017-11-05T16:40:17.770879: step 620, loss 0.18003, acc 0.9375\n",
      "2017-11-05T16:40:21.725142: step 621, loss 0.0908877, acc 0.96875\n",
      "2017-11-05T16:40:25.655783: step 622, loss 0.226412, acc 0.9375\n",
      "2017-11-05T16:40:29.584121: step 623, loss 0.191143, acc 0.96875\n",
      "2017-11-05T16:40:33.589712: step 624, loss 0.499914, acc 0.84375\n",
      "2017-11-05T16:40:37.592143: step 625, loss 0.154736, acc 0.90625\n",
      "2017-11-05T16:40:41.569985: step 626, loss 0.00752745, acc 1\n",
      "2017-11-05T16:40:45.520585: step 627, loss 0.713376, acc 0.8125\n",
      "2017-11-05T16:40:49.460848: step 628, loss 0.255594, acc 0.90625\n",
      "2017-11-05T16:40:53.437684: step 629, loss 0.946059, acc 0.8125\n",
      "2017-11-05T16:40:57.451835: step 630, loss 0.963369, acc 0.8125\n",
      "2017-11-05T16:41:01.504744: step 631, loss 0.0718554, acc 0.96875\n",
      "2017-11-05T16:41:05.541347: step 632, loss 0.31449, acc 0.875\n",
      "2017-11-05T16:41:09.679251: step 633, loss 0.469104, acc 0.90625\n",
      "2017-11-05T16:41:14.696686: step 634, loss 0.109497, acc 0.9375\n",
      "2017-11-05T16:41:19.018250: step 635, loss 0.276069, acc 0.90625\n",
      "2017-11-05T16:41:23.089660: step 636, loss 0.401093, acc 0.875\n",
      "2017-11-05T16:41:27.099934: step 637, loss 0.220567, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T16:41:31.046728: step 638, loss 0.300538, acc 0.90625\n",
      "2017-11-05T16:41:35.035291: step 639, loss 0.330688, acc 0.90625\n",
      "2017-11-05T16:41:39.045856: step 640, loss 0.107763, acc 0.9375\n",
      "2017-11-05T16:41:43.049381: step 641, loss 0.284815, acc 0.90625\n",
      "2017-11-05T16:41:47.061608: step 642, loss 0.582484, acc 0.8125\n",
      "2017-11-05T16:41:51.065599: step 643, loss 0.457302, acc 0.9375\n",
      "2017-11-05T16:41:55.028884: step 644, loss 0.242056, acc 0.90625\n",
      "2017-11-05T16:41:58.979375: step 645, loss 0.30995, acc 0.9375\n",
      "2017-11-05T16:42:02.916415: step 646, loss 0.572912, acc 0.84375\n",
      "2017-11-05T16:42:07.011074: step 647, loss 0.219991, acc 0.9375\n",
      "2017-11-05T16:42:09.590377: step 648, loss 0.0139487, acc 1\n",
      "2017-11-05T16:42:13.701325: step 649, loss 0.0502012, acc 0.96875\n",
      "2017-11-05T16:42:17.693166: step 650, loss 0.508641, acc 0.875\n",
      "2017-11-05T16:42:21.605855: step 651, loss 0.38112, acc 0.8125\n",
      "2017-11-05T16:42:25.567254: step 652, loss 0.669182, acc 0.84375\n",
      "2017-11-05T16:42:29.609755: step 653, loss 0.449665, acc 0.875\n",
      "2017-11-05T16:42:33.650403: step 654, loss 0.188271, acc 0.9375\n",
      "2017-11-05T16:42:37.764575: step 655, loss 0.324109, acc 0.84375\n",
      "2017-11-05T16:42:41.935645: step 656, loss 0.0359304, acc 1\n",
      "2017-11-05T16:42:45.974228: step 657, loss 0.34335, acc 0.90625\n",
      "2017-11-05T16:42:49.983992: step 658, loss 0.160619, acc 0.9375\n",
      "2017-11-05T16:42:54.011267: step 659, loss 0.214927, acc 0.9375\n",
      "2017-11-05T16:42:57.972037: step 660, loss 0.458265, acc 0.875\n",
      "2017-11-05T16:43:01.966025: step 661, loss 0.453243, acc 0.90625\n",
      "2017-11-05T16:43:06.069618: step 662, loss 0.231166, acc 0.9375\n",
      "2017-11-05T16:43:10.163223: step 663, loss 0.154677, acc 0.90625\n",
      "2017-11-05T16:43:14.195372: step 664, loss 0.27711, acc 0.90625\n",
      "2017-11-05T16:43:18.233719: step 665, loss 0.234011, acc 0.90625\n",
      "2017-11-05T16:43:22.385122: step 666, loss 0.17924, acc 0.90625\n",
      "2017-11-05T16:43:26.724414: step 667, loss 0.259873, acc 0.90625\n",
      "2017-11-05T16:43:30.768748: step 668, loss 0.131757, acc 0.90625\n",
      "2017-11-05T16:43:34.781390: step 669, loss 0.499492, acc 0.875\n",
      "2017-11-05T16:43:38.745186: step 670, loss 0.139725, acc 0.96875\n",
      "2017-11-05T16:43:42.691268: step 671, loss 0.551571, acc 0.875\n",
      "2017-11-05T16:43:46.665795: step 672, loss 0.484785, acc 0.84375\n",
      "2017-11-05T16:43:50.649744: step 673, loss 0.0784494, acc 0.96875\n",
      "2017-11-05T16:43:54.696295: step 674, loss 0.132509, acc 0.90625\n",
      "2017-11-05T16:43:58.687675: step 675, loss 0.121551, acc 0.96875\n",
      "2017-11-05T16:44:02.661519: step 676, loss 0.460513, acc 0.84375\n",
      "2017-11-05T16:44:07.022869: step 677, loss 0.461402, acc 0.875\n",
      "2017-11-05T16:44:11.116841: step 678, loss 0.0689142, acc 0.96875\n",
      "2017-11-05T16:44:15.163708: step 679, loss 0.340192, acc 0.90625\n",
      "2017-11-05T16:44:19.424052: step 680, loss 0.147458, acc 0.9375\n",
      "2017-11-05T16:44:23.480539: step 681, loss 0.103005, acc 0.9375\n",
      "2017-11-05T16:44:27.491151: step 682, loss 0.336695, acc 0.90625\n",
      "2017-11-05T16:44:31.598660: step 683, loss 0.290618, acc 0.875\n",
      "2017-11-05T16:44:34.720275: step 684, loss 0.213326, acc 0.95\n",
      "2017-11-05T16:44:39.127904: step 685, loss 0.248749, acc 0.90625\n",
      "2017-11-05T16:44:43.317231: step 686, loss 0.294293, acc 0.9375\n",
      "2017-11-05T16:44:47.344333: step 687, loss 0.27092, acc 0.96875\n",
      "2017-11-05T16:44:51.300681: step 688, loss 0.943441, acc 0.84375\n",
      "2017-11-05T16:44:55.333456: step 689, loss 0.25724, acc 0.9375\n",
      "2017-11-05T16:44:59.325933: step 690, loss 0.543574, acc 0.84375\n",
      "2017-11-05T16:45:03.270206: step 691, loss 0.651645, acc 0.78125\n",
      "2017-11-05T16:45:07.226411: step 692, loss 0.159065, acc 0.875\n",
      "2017-11-05T16:45:11.248334: step 693, loss 0.250144, acc 0.9375\n",
      "2017-11-05T16:45:15.234348: step 694, loss 0.19862, acc 0.84375\n",
      "2017-11-05T16:45:19.211383: step 695, loss 0.679363, acc 0.78125\n",
      "2017-11-05T16:45:23.185614: step 696, loss 0.568205, acc 0.8125\n",
      "2017-11-05T16:45:27.132958: step 697, loss 0.31369, acc 0.875\n",
      "2017-11-05T16:45:31.273303: step 698, loss 0.355081, acc 0.9375\n",
      "2017-11-05T16:45:35.221874: step 699, loss 0.14619, acc 0.96875\n",
      "2017-11-05T16:45:39.193674: step 700, loss 0.0564338, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:45:41.737044: step 700, loss 1.31536, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-700\n",
      "\n",
      "2017-11-05T16:45:47.489624: step 701, loss 0.436826, acc 0.90625\n",
      "2017-11-05T16:45:51.443586: step 702, loss 0.0259518, acc 0.96875\n",
      "2017-11-05T16:45:55.389499: step 703, loss 0.229243, acc 0.9375\n",
      "2017-11-05T16:45:59.388955: step 704, loss 0.0804217, acc 0.96875\n",
      "2017-11-05T16:46:03.404900: step 705, loss 0.400073, acc 0.96875\n",
      "2017-11-05T16:46:07.474714: step 706, loss 0.229038, acc 0.90625\n",
      "2017-11-05T16:46:11.522780: step 707, loss 0.0689917, acc 0.96875\n",
      "2017-11-05T16:46:15.508894: step 708, loss 0.445976, acc 0.90625\n",
      "2017-11-05T16:46:19.690692: step 709, loss 0.459227, acc 0.875\n",
      "2017-11-05T16:46:23.738990: step 710, loss 0.0904988, acc 0.9375\n",
      "2017-11-05T16:46:27.751843: step 711, loss 0.197422, acc 0.96875\n",
      "2017-11-05T16:46:31.681185: step 712, loss 0.321857, acc 0.875\n",
      "2017-11-05T16:46:35.956105: step 713, loss 0.289355, acc 0.84375\n",
      "2017-11-05T16:46:40.032780: step 714, loss 0.48034, acc 0.875\n",
      "2017-11-05T16:46:44.075321: step 715, loss 0.275141, acc 0.90625\n",
      "2017-11-05T16:46:48.067637: step 716, loss 0.337604, acc 0.90625\n",
      "2017-11-05T16:46:52.089020: step 717, loss 0.076783, acc 0.96875\n",
      "2017-11-05T16:46:56.114658: step 718, loss 0.446668, acc 0.90625\n",
      "2017-11-05T16:47:00.214330: step 719, loss 0.0839635, acc 0.9375\n",
      "2017-11-05T16:47:02.737369: step 720, loss 0.318713, acc 0.85\n",
      "2017-11-05T16:47:06.686161: step 721, loss 0.224241, acc 0.875\n",
      "2017-11-05T16:47:10.694329: step 722, loss 0.316099, acc 0.875\n",
      "2017-11-05T16:47:14.810310: step 723, loss 0.177268, acc 0.9375\n",
      "2017-11-05T16:47:18.796379: step 724, loss 0.126045, acc 0.96875\n",
      "2017-11-05T16:47:22.749552: step 725, loss 0.190642, acc 0.90625\n",
      "2017-11-05T16:47:26.714470: step 726, loss 0.0396827, acc 0.96875\n",
      "2017-11-05T16:47:30.691040: step 727, loss 0.25854, acc 0.90625\n",
      "2017-11-05T16:47:34.716401: step 728, loss 0.00785517, acc 1\n",
      "2017-11-05T16:47:38.693133: step 729, loss 0.099547, acc 0.96875\n",
      "2017-11-05T16:47:42.690605: step 730, loss 0.0948323, acc 0.9375\n",
      "2017-11-05T16:47:46.625531: step 731, loss 0.131256, acc 0.96875\n",
      "2017-11-05T16:47:50.564913: step 732, loss 0.439809, acc 0.875\n",
      "2017-11-05T16:47:54.517563: step 733, loss 0.312118, acc 0.90625\n",
      "2017-11-05T16:47:58.445473: step 734, loss 0.227766, acc 0.9375\n",
      "2017-11-05T16:48:02.385490: step 735, loss 0.634386, acc 0.875\n",
      "2017-11-05T16:48:06.358319: step 736, loss 0.309926, acc 0.90625\n",
      "2017-11-05T16:48:10.307978: step 737, loss 0.516016, acc 0.90625\n",
      "2017-11-05T16:48:14.267458: step 738, loss 0.312063, acc 0.875\n",
      "2017-11-05T16:48:18.248397: step 739, loss 0.273278, acc 0.9375\n",
      "2017-11-05T16:48:22.467387: step 740, loss 0.323379, acc 0.875\n",
      "2017-11-05T16:48:26.577083: step 741, loss 0.345153, acc 0.90625\n",
      "2017-11-05T16:48:30.533463: step 742, loss 0.404406, acc 0.84375\n",
      "2017-11-05T16:48:34.603698: step 743, loss 0.147672, acc 0.96875\n",
      "2017-11-05T16:48:38.657936: step 744, loss 0.112001, acc 0.9375\n",
      "2017-11-05T16:48:42.811198: step 745, loss 0.203538, acc 0.90625\n",
      "2017-11-05T16:48:46.899540: step 746, loss 0.132902, acc 0.9375\n",
      "2017-11-05T16:48:51.125263: step 747, loss 0.721264, acc 0.71875\n",
      "2017-11-05T16:48:55.122460: step 748, loss 0.451856, acc 0.875\n",
      "2017-11-05T16:48:59.255237: step 749, loss 0.610294, acc 0.84375\n",
      "2017-11-05T16:49:03.396390: step 750, loss 0.428959, acc 0.8125\n",
      "2017-11-05T16:49:07.488471: step 751, loss 0.490451, acc 0.84375\n",
      "2017-11-05T16:49:11.524643: step 752, loss 0.0683123, acc 0.9375\n",
      "2017-11-05T16:49:15.493819: step 753, loss 0.137658, acc 0.96875\n",
      "2017-11-05T16:49:19.729765: step 754, loss 0.217218, acc 0.9375\n",
      "2017-11-05T16:49:23.831449: step 755, loss 0.161986, acc 0.9375\n",
      "2017-11-05T16:49:26.451281: step 756, loss 0.611735, acc 0.85\n",
      "2017-11-05T16:49:30.525404: step 757, loss 0.370561, acc 0.8125\n",
      "2017-11-05T16:49:34.663377: step 758, loss 0.0999625, acc 0.96875\n",
      "2017-11-05T16:49:38.937267: step 759, loss 0.185194, acc 0.9375\n",
      "2017-11-05T16:49:42.927920: step 760, loss 0.494163, acc 0.875\n",
      "2017-11-05T16:49:47.062441: step 761, loss 0.118948, acc 0.96875\n",
      "2017-11-05T16:49:51.180120: step 762, loss 0.398285, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T16:49:55.334051: step 763, loss 0.148974, acc 0.9375\n",
      "2017-11-05T16:49:59.498630: step 764, loss 0.109352, acc 0.96875\n",
      "2017-11-05T16:50:06.394340: step 765, loss 0.447592, acc 0.875\n",
      "2017-11-05T16:50:14.223283: step 766, loss 0.345, acc 0.875\n",
      "2017-11-05T16:50:22.176595: step 767, loss 0.390191, acc 0.875\n",
      "2017-11-05T16:50:29.571304: step 768, loss 0.241951, acc 0.9375\n",
      "2017-11-05T16:50:36.866698: step 769, loss 0.45075, acc 0.875\n",
      "2017-11-05T16:50:43.920980: step 770, loss 0.336839, acc 0.875\n",
      "2017-11-05T16:50:50.972194: step 771, loss 0.232649, acc 0.9375\n",
      "2017-11-05T16:50:57.763276: step 772, loss 0.467985, acc 0.84375\n",
      "2017-11-05T16:51:04.425722: step 773, loss 0.116224, acc 0.96875\n",
      "2017-11-05T16:51:10.826554: step 774, loss 0.277448, acc 0.90625\n",
      "2017-11-05T16:51:15.823725: step 775, loss 0.120903, acc 0.90625\n",
      "2017-11-05T16:51:19.829288: step 776, loss 0.0753793, acc 0.96875\n",
      "2017-11-05T16:51:23.892961: step 777, loss 0.216654, acc 0.96875\n",
      "2017-11-05T16:51:28.690948: step 778, loss 0.277199, acc 0.90625\n",
      "2017-11-05T16:51:32.887377: step 779, loss 0.0727783, acc 0.96875\n",
      "2017-11-05T16:51:37.364325: step 780, loss 0.40452, acc 0.875\n",
      "2017-11-05T16:51:41.756812: step 781, loss 0.224842, acc 0.875\n",
      "2017-11-05T16:51:46.018326: step 782, loss 0.153504, acc 0.9375\n",
      "2017-11-05T16:51:50.362646: step 783, loss 0.26379, acc 0.90625\n",
      "2017-11-05T16:51:54.870365: step 784, loss 0.0554093, acc 0.96875\n",
      "2017-11-05T16:51:59.570015: step 785, loss 0.181704, acc 0.90625\n",
      "2017-11-05T16:52:03.775810: step 786, loss 0.300432, acc 0.9375\n",
      "2017-11-05T16:52:07.942097: step 787, loss 0.601597, acc 0.8125\n",
      "2017-11-05T16:52:11.953183: step 788, loss 0.478689, acc 0.84375\n",
      "2017-11-05T16:52:16.002651: step 789, loss 0.0155804, acc 1\n",
      "2017-11-05T16:52:20.103497: step 790, loss 0.303385, acc 0.84375\n",
      "2017-11-05T16:52:24.125975: step 791, loss 0.118182, acc 0.90625\n",
      "2017-11-05T16:52:26.725705: step 792, loss 0.353894, acc 0.9\n",
      "2017-11-05T16:52:30.729982: step 793, loss 0.285704, acc 0.875\n",
      "2017-11-05T16:52:34.864810: step 794, loss 0.185721, acc 0.90625\n",
      "2017-11-05T16:52:38.833255: step 795, loss 0.5036, acc 0.875\n",
      "2017-11-05T16:52:42.927974: step 796, loss 0.149873, acc 0.96875\n",
      "2017-11-05T16:52:46.888623: step 797, loss 0.123575, acc 0.90625\n",
      "2017-11-05T16:52:50.890945: step 798, loss 0.26867, acc 0.90625\n",
      "2017-11-05T16:52:54.961736: step 799, loss 0.323149, acc 0.875\n",
      "2017-11-05T16:52:58.979175: step 800, loss 0.322609, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:53:01.572034: step 800, loss 1.09141, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-800\n",
      "\n",
      "2017-11-05T16:53:07.415518: step 801, loss 0.447181, acc 0.84375\n",
      "2017-11-05T16:53:11.400423: step 802, loss 0.442872, acc 0.90625\n",
      "2017-11-05T16:53:15.348146: step 803, loss 0.133066, acc 0.9375\n",
      "2017-11-05T16:53:19.288606: step 804, loss 0.116037, acc 0.96875\n",
      "2017-11-05T16:53:23.413816: step 805, loss 0.301227, acc 0.84375\n",
      "2017-11-05T16:53:27.653267: step 806, loss 0.031968, acc 0.96875\n",
      "2017-11-05T16:53:31.864316: step 807, loss 0.170508, acc 0.9375\n",
      "2017-11-05T16:53:35.923023: step 808, loss 0.519413, acc 0.8125\n",
      "2017-11-05T16:53:39.947471: step 809, loss 0.155031, acc 0.96875\n",
      "2017-11-05T16:53:44.144751: step 810, loss 0.040295, acc 0.96875\n",
      "2017-11-05T16:53:48.121886: step 811, loss 0.0185467, acc 1\n",
      "2017-11-05T16:53:52.083135: step 812, loss 0.39187, acc 0.875\n",
      "2017-11-05T16:53:56.094690: step 813, loss 0.00717871, acc 1\n",
      "2017-11-05T16:54:00.083557: step 814, loss 0.245212, acc 0.875\n",
      "2017-11-05T16:54:04.153853: step 815, loss 0.351756, acc 0.875\n",
      "2017-11-05T16:54:08.155598: step 816, loss 0.20607, acc 0.96875\n",
      "2017-11-05T16:54:12.130719: step 817, loss 0.588916, acc 0.78125\n",
      "2017-11-05T16:54:16.071162: step 818, loss 0.2506, acc 0.875\n",
      "2017-11-05T16:54:20.015443: step 819, loss 0.298382, acc 0.9375\n",
      "2017-11-05T16:54:24.022852: step 820, loss 0.177896, acc 0.90625\n",
      "2017-11-05T16:54:27.981682: step 821, loss 0.266316, acc 0.90625\n",
      "2017-11-05T16:54:31.940124: step 822, loss 0.332345, acc 0.875\n",
      "2017-11-05T16:54:36.045063: step 823, loss 0.355374, acc 0.90625\n",
      "2017-11-05T16:54:39.935161: step 824, loss 0.237575, acc 0.90625\n",
      "2017-11-05T16:54:43.949053: step 825, loss 0.25182, acc 0.875\n",
      "2017-11-05T16:54:47.893762: step 826, loss 0.397516, acc 0.875\n",
      "2017-11-05T16:54:51.851755: step 827, loss 0.422897, acc 0.84375\n",
      "2017-11-05T16:54:54.420212: step 828, loss 0.500217, acc 0.85\n",
      "2017-11-05T16:54:58.336321: step 829, loss 0.652324, acc 0.875\n",
      "2017-11-05T16:55:02.276734: step 830, loss 0.00642413, acc 1\n",
      "2017-11-05T16:55:06.200311: step 831, loss 0.434431, acc 0.84375\n",
      "2017-11-05T16:55:10.138832: step 832, loss 0.172721, acc 0.90625\n",
      "2017-11-05T16:55:14.130418: step 833, loss 0.25264, acc 0.90625\n",
      "2017-11-05T16:55:18.084761: step 834, loss 0.231996, acc 0.9375\n",
      "2017-11-05T16:55:22.009610: step 835, loss 0.182946, acc 0.9375\n",
      "2017-11-05T16:55:25.998509: step 836, loss 0.12459, acc 0.9375\n",
      "2017-11-05T16:55:29.909326: step 837, loss 0.34374, acc 0.90625\n",
      "2017-11-05T16:55:33.876772: step 838, loss 0.0645211, acc 0.96875\n",
      "2017-11-05T16:55:37.833436: step 839, loss 0.0644753, acc 0.96875\n",
      "2017-11-05T16:55:41.767910: step 840, loss 0.271968, acc 0.875\n",
      "2017-11-05T16:55:45.853571: step 841, loss 0.144805, acc 0.9375\n",
      "2017-11-05T16:55:49.892745: step 842, loss 0.155707, acc 0.9375\n",
      "2017-11-05T16:55:53.884617: step 843, loss 0.0360623, acc 1\n",
      "2017-11-05T16:55:57.800020: step 844, loss 0.340563, acc 0.875\n",
      "2017-11-05T16:56:01.716571: step 845, loss 0.393605, acc 0.875\n",
      "2017-11-05T16:56:05.666845: step 846, loss 0.498488, acc 0.8125\n",
      "2017-11-05T16:56:09.621037: step 847, loss 0.0703495, acc 0.96875\n",
      "2017-11-05T16:56:13.597453: step 848, loss 0.274821, acc 0.90625\n",
      "2017-11-05T16:56:17.548036: step 849, loss 0.199506, acc 0.9375\n",
      "2017-11-05T16:56:21.556810: step 850, loss 0.159234, acc 0.96875\n",
      "2017-11-05T16:56:25.543491: step 851, loss 0.422376, acc 0.84375\n",
      "2017-11-05T16:56:29.535925: step 852, loss 0.658764, acc 0.8125\n",
      "2017-11-05T16:56:33.526977: step 853, loss 0.145607, acc 0.9375\n",
      "2017-11-05T16:56:37.604717: step 854, loss 0.191824, acc 0.96875\n",
      "2017-11-05T16:56:41.533785: step 855, loss 0.446635, acc 0.875\n",
      "2017-11-05T16:56:45.525364: step 856, loss 0.223153, acc 0.9375\n",
      "2017-11-05T16:56:49.468809: step 857, loss 0.106882, acc 0.96875\n",
      "2017-11-05T16:56:53.415717: step 858, loss 0.632599, acc 0.84375\n",
      "2017-11-05T16:56:57.393262: step 859, loss 0.207508, acc 0.90625\n",
      "2017-11-05T16:57:01.355673: step 860, loss 0.412973, acc 0.875\n",
      "2017-11-05T16:57:05.268387: step 861, loss 0.200675, acc 0.90625\n",
      "2017-11-05T16:57:09.303959: step 862, loss 0.0617315, acc 0.96875\n",
      "2017-11-05T16:57:13.261705: step 863, loss 0.274542, acc 0.90625\n",
      "2017-11-05T16:57:15.761933: step 864, loss 0.298987, acc 0.9\n",
      "2017-11-05T16:57:19.735071: step 865, loss 0.104937, acc 0.9375\n",
      "2017-11-05T16:57:23.701028: step 866, loss 0.294569, acc 0.9375\n",
      "2017-11-05T16:57:27.676473: step 867, loss 0.077815, acc 0.9375\n",
      "2017-11-05T16:57:31.576949: step 868, loss 0.234921, acc 0.90625\n",
      "2017-11-05T16:57:35.554147: step 869, loss 0.403896, acc 0.84375\n",
      "2017-11-05T16:57:39.497248: step 870, loss 0.184918, acc 0.90625\n",
      "2017-11-05T16:57:43.422197: step 871, loss 0.325346, acc 0.78125\n",
      "2017-11-05T16:57:47.472714: step 872, loss 0.0439629, acc 0.96875\n",
      "2017-11-05T16:57:51.429748: step 873, loss 0.0794631, acc 0.9375\n",
      "2017-11-05T16:57:55.353757: step 874, loss 0.0641211, acc 0.96875\n",
      "2017-11-05T16:57:59.335897: step 875, loss 0.408524, acc 0.90625\n",
      "2017-11-05T16:58:03.281965: step 876, loss 0.22351, acc 0.84375\n",
      "2017-11-05T16:58:07.274661: step 877, loss 0.0690083, acc 0.96875\n",
      "2017-11-05T16:58:11.244318: step 878, loss 0.120689, acc 0.9375\n",
      "2017-11-05T16:58:15.216412: step 879, loss 0.275694, acc 0.875\n",
      "2017-11-05T16:58:19.207404: step 880, loss 0.117436, acc 0.9375\n",
      "2017-11-05T16:58:23.286932: step 881, loss 0.0768493, acc 0.96875\n",
      "2017-11-05T16:58:27.398017: step 882, loss 0.187971, acc 0.90625\n",
      "2017-11-05T16:58:31.373994: step 883, loss 0.162157, acc 0.90625\n",
      "2017-11-05T16:58:35.399891: step 884, loss 0.345205, acc 0.875\n",
      "2017-11-05T16:58:39.362622: step 885, loss 0.311384, acc 0.875\n",
      "2017-11-05T16:58:43.336684: step 886, loss 0.147072, acc 0.90625\n",
      "2017-11-05T16:58:47.312258: step 887, loss 0.203792, acc 0.9375\n",
      "2017-11-05T16:58:51.246082: step 888, loss 0.0832924, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T16:58:55.237318: step 889, loss 0.178117, acc 0.875\n",
      "2017-11-05T16:58:59.181475: step 890, loss 0.504636, acc 0.875\n",
      "2017-11-05T16:59:03.173138: step 891, loss 0.18165, acc 0.9375\n",
      "2017-11-05T16:59:07.119183: step 892, loss 0.342892, acc 0.8125\n",
      "2017-11-05T16:59:11.092298: step 893, loss 0.351537, acc 0.875\n",
      "2017-11-05T16:59:15.043293: step 894, loss 0.76872, acc 0.84375\n",
      "2017-11-05T16:59:19.024786: step 895, loss 0.120098, acc 0.9375\n",
      "2017-11-05T16:59:22.932089: step 896, loss 0.682087, acc 0.8125\n",
      "2017-11-05T16:59:26.876322: step 897, loss 0.0888331, acc 0.96875\n",
      "2017-11-05T16:59:30.841541: step 898, loss 0.335575, acc 0.875\n",
      "2017-11-05T16:59:34.802381: step 899, loss 0.368945, acc 0.875\n",
      "2017-11-05T16:59:37.333872: step 900, loss 0.0885578, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T16:59:39.942687: step 900, loss 0.87731, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-900\n",
      "\n",
      "2017-11-05T16:59:45.684120: step 901, loss 0.0851927, acc 0.96875\n",
      "2017-11-05T16:59:49.996757: step 902, loss 0.157674, acc 0.90625\n",
      "2017-11-05T16:59:54.019589: step 903, loss 0.438548, acc 0.8125\n",
      "2017-11-05T16:59:58.076233: step 904, loss 0.32192, acc 0.875\n",
      "2017-11-05T17:00:02.387692: step 905, loss 0.131059, acc 0.9375\n",
      "2017-11-05T17:00:06.395893: step 906, loss 0.396302, acc 0.84375\n",
      "2017-11-05T17:00:10.423613: step 907, loss 0.065977, acc 0.96875\n",
      "2017-11-05T17:00:14.434487: step 908, loss 0.383516, acc 0.84375\n",
      "2017-11-05T17:00:18.469485: step 909, loss 0.51588, acc 0.84375\n",
      "2017-11-05T17:00:22.482348: step 910, loss 0.0252014, acc 1\n",
      "2017-11-05T17:00:26.556119: step 911, loss 0.222026, acc 0.875\n",
      "2017-11-05T17:00:30.627788: step 912, loss 0.206022, acc 0.9375\n",
      "2017-11-05T17:00:34.808614: step 913, loss 0.293893, acc 0.90625\n",
      "2017-11-05T17:00:38.775428: step 914, loss 0.368161, acc 0.78125\n",
      "2017-11-05T17:00:42.791697: step 915, loss 0.131753, acc 0.96875\n",
      "2017-11-05T17:00:46.839216: step 916, loss 0.174423, acc 0.9375\n",
      "2017-11-05T17:00:50.849903: step 917, loss 0.403491, acc 0.8125\n",
      "2017-11-05T17:00:54.826073: step 918, loss 0.293166, acc 0.90625\n",
      "2017-11-05T17:00:58.818168: step 919, loss 0.179086, acc 0.90625\n",
      "2017-11-05T17:01:02.840888: step 920, loss 0.352271, acc 0.875\n",
      "2017-11-05T17:01:06.866721: step 921, loss 0.040271, acc 0.96875\n",
      "2017-11-05T17:01:10.948464: step 922, loss 0.0334646, acc 1\n",
      "2017-11-05T17:01:14.994933: step 923, loss 0.246539, acc 0.90625\n",
      "2017-11-05T17:01:19.033088: step 924, loss 0.12092, acc 0.96875\n",
      "2017-11-05T17:01:23.077722: step 925, loss 0.232066, acc 0.9375\n",
      "2017-11-05T17:01:27.130998: step 926, loss 0.401329, acc 0.875\n",
      "2017-11-05T17:01:31.165086: step 927, loss 0.0378485, acc 1\n",
      "2017-11-05T17:01:35.217748: step 928, loss 0.172731, acc 0.9375\n",
      "2017-11-05T17:01:39.253992: step 929, loss 0.260408, acc 0.9375\n",
      "2017-11-05T17:01:43.371517: step 930, loss 0.292959, acc 0.84375\n",
      "2017-11-05T17:01:47.339780: step 931, loss 0.378157, acc 0.90625\n",
      "2017-11-05T17:01:51.316522: step 932, loss 0.138143, acc 0.96875\n",
      "2017-11-05T17:01:55.290493: step 933, loss 0.215608, acc 0.84375\n",
      "2017-11-05T17:01:59.269900: step 934, loss 0.220591, acc 0.90625\n",
      "2017-11-05T17:02:03.383785: step 935, loss 0.257251, acc 0.875\n",
      "2017-11-05T17:02:06.024068: step 936, loss 0.169849, acc 0.9\n",
      "2017-11-05T17:02:10.069990: step 937, loss 0.312663, acc 0.90625\n",
      "2017-11-05T17:02:14.105293: step 938, loss 0.0972286, acc 0.9375\n",
      "2017-11-05T17:02:18.091485: step 939, loss 0.338722, acc 0.875\n",
      "2017-11-05T17:02:22.112354: step 940, loss 0.404905, acc 0.84375\n",
      "2017-11-05T17:02:26.158021: step 941, loss 0.197631, acc 0.90625\n",
      "2017-11-05T17:02:30.262321: step 942, loss 0.255562, acc 0.84375\n",
      "2017-11-05T17:02:34.437090: step 943, loss 0.179299, acc 0.9375\n",
      "2017-11-05T17:02:38.554235: step 944, loss 0.103224, acc 0.96875\n",
      "2017-11-05T17:02:42.533343: step 945, loss 0.2016, acc 0.875\n",
      "2017-11-05T17:02:46.590133: step 946, loss 0.299821, acc 0.875\n",
      "2017-11-05T17:02:50.662881: step 947, loss 0.240246, acc 0.875\n",
      "2017-11-05T17:02:54.736731: step 948, loss 0.303422, acc 0.875\n",
      "2017-11-05T17:02:58.813155: step 949, loss 0.0933417, acc 0.96875\n",
      "2017-11-05T17:03:02.833415: step 950, loss 0.190365, acc 0.9375\n",
      "2017-11-05T17:03:06.894227: step 951, loss 0.0885114, acc 0.96875\n",
      "2017-11-05T17:03:11.025489: step 952, loss 0.0856432, acc 0.9375\n",
      "2017-11-05T17:03:15.005684: step 953, loss 0.60991, acc 0.71875\n",
      "2017-11-05T17:03:18.947869: step 954, loss 0.373765, acc 0.8125\n",
      "2017-11-05T17:03:23.167665: step 955, loss 0.358913, acc 0.875\n",
      "2017-11-05T17:03:27.468824: step 956, loss 0.110541, acc 0.9375\n",
      "2017-11-05T17:03:31.446845: step 957, loss 0.0964388, acc 0.9375\n",
      "2017-11-05T17:03:35.490577: step 958, loss 0.242186, acc 0.90625\n",
      "2017-11-05T17:03:39.514979: step 959, loss 0.105034, acc 0.9375\n",
      "2017-11-05T17:03:43.512651: step 960, loss 0.271113, acc 0.90625\n",
      "2017-11-05T17:03:47.567007: step 961, loss 0.345162, acc 0.84375\n",
      "2017-11-05T17:03:51.566287: step 962, loss 0.487976, acc 0.8125\n",
      "2017-11-05T17:03:55.671520: step 963, loss 0.311486, acc 0.9375\n",
      "2017-11-05T17:03:59.672302: step 964, loss 0.538705, acc 0.8125\n",
      "2017-11-05T17:04:03.648509: step 965, loss 0.135139, acc 0.9375\n",
      "2017-11-05T17:04:07.656480: step 966, loss 0.10902, acc 0.9375\n",
      "2017-11-05T17:04:11.665771: step 967, loss 0.355008, acc 0.875\n",
      "2017-11-05T17:04:15.691563: step 968, loss 0.233146, acc 0.9375\n",
      "2017-11-05T17:04:19.684974: step 969, loss 0.556161, acc 0.875\n",
      "2017-11-05T17:04:23.738926: step 970, loss 0.148403, acc 0.9375\n",
      "2017-11-05T17:04:27.746547: step 971, loss 0.177999, acc 0.9375\n",
      "2017-11-05T17:04:30.366312: step 972, loss 0.293023, acc 0.9\n",
      "2017-11-05T17:04:34.540449: step 973, loss 0.094756, acc 0.96875\n",
      "2017-11-05T17:04:38.615132: step 974, loss 0.377844, acc 0.875\n",
      "2017-11-05T17:04:42.627293: step 975, loss 0.227626, acc 0.9375\n",
      "2017-11-05T17:04:46.666865: step 976, loss 0.00843808, acc 1\n",
      "2017-11-05T17:04:50.692579: step 977, loss 0.720897, acc 0.8125\n",
      "2017-11-05T17:04:54.622506: step 978, loss 0.180219, acc 0.9375\n",
      "2017-11-05T17:04:58.711266: step 979, loss 0.105063, acc 0.96875\n",
      "2017-11-05T17:05:02.647086: step 980, loss 0.22632, acc 0.90625\n",
      "2017-11-05T17:05:06.644462: step 981, loss 0.173672, acc 0.90625\n",
      "2017-11-05T17:05:10.712352: step 982, loss 0.0832084, acc 0.9375\n",
      "2017-11-05T17:05:14.756060: step 983, loss 0.106703, acc 0.9375\n",
      "2017-11-05T17:05:18.792049: step 984, loss 0.246695, acc 0.96875\n",
      "2017-11-05T17:05:22.846437: step 985, loss 0.370477, acc 0.875\n",
      "2017-11-05T17:05:26.777305: step 986, loss 0.0376581, acc 1\n",
      "2017-11-05T17:05:30.775110: step 987, loss 0.126289, acc 0.9375\n",
      "2017-11-05T17:05:34.759877: step 988, loss 0.105357, acc 0.96875\n",
      "2017-11-05T17:05:38.824225: step 989, loss 0.276042, acc 0.8125\n",
      "2017-11-05T17:05:42.863505: step 990, loss 0.0888006, acc 0.96875\n",
      "2017-11-05T17:05:46.842872: step 991, loss 0.0665276, acc 0.96875\n",
      "2017-11-05T17:05:50.919622: step 992, loss 0.589151, acc 0.75\n",
      "2017-11-05T17:05:54.875846: step 993, loss 0.373862, acc 0.875\n",
      "2017-11-05T17:05:58.878047: step 994, loss 0.195719, acc 0.875\n",
      "2017-11-05T17:06:02.909430: step 995, loss 0.21406, acc 0.90625\n",
      "2017-11-05T17:06:06.925240: step 996, loss 0.156051, acc 0.9375\n",
      "2017-11-05T17:06:10.930287: step 997, loss 0.312064, acc 0.84375\n",
      "2017-11-05T17:06:15.013320: step 998, loss 0.151268, acc 0.96875\n",
      "2017-11-05T17:06:18.958455: step 999, loss 0.39031, acc 0.875\n",
      "2017-11-05T17:06:22.995206: step 1000, loss 0.409886, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:06:25.535578: step 1000, loss 0.765978, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-05T17:06:31.391962: step 1001, loss 0.410474, acc 0.90625\n",
      "2017-11-05T17:06:35.522882: step 1002, loss 0.335636, acc 0.875\n",
      "2017-11-05T17:06:39.528417: step 1003, loss 0.107329, acc 0.96875\n",
      "2017-11-05T17:06:43.538033: step 1004, loss 0.180456, acc 0.90625\n",
      "2017-11-05T17:06:47.585736: step 1005, loss 0.217517, acc 0.875\n",
      "2017-11-05T17:06:51.557756: step 1006, loss 0.20149, acc 0.875\n",
      "2017-11-05T17:06:55.571201: step 1007, loss 0.242703, acc 0.875\n",
      "2017-11-05T17:06:58.085236: step 1008, loss 0.212359, acc 0.9\n",
      "2017-11-05T17:07:02.145735: step 1009, loss 0.186381, acc 0.9375\n",
      "2017-11-05T17:07:06.124732: step 1010, loss 0.117765, acc 0.96875\n",
      "2017-11-05T17:07:10.100222: step 1011, loss 0.391765, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T17:07:14.140189: step 1012, loss 0.584492, acc 0.8125\n",
      "2017-11-05T17:07:18.132496: step 1013, loss 0.127233, acc 0.96875\n",
      "2017-11-05T17:07:22.127657: step 1014, loss 0.331907, acc 0.875\n",
      "2017-11-05T17:07:26.122540: step 1015, loss 0.0888547, acc 0.96875\n",
      "2017-11-05T17:07:30.114734: step 1016, loss 0.0668193, acc 0.96875\n",
      "2017-11-05T17:07:34.170863: step 1017, loss 0.118226, acc 0.96875\n",
      "2017-11-05T17:07:38.166967: step 1018, loss 0.104253, acc 0.9375\n",
      "2017-11-05T17:07:42.254565: step 1019, loss 0.543119, acc 0.84375\n",
      "2017-11-05T17:07:46.223723: step 1020, loss 0.272566, acc 0.90625\n",
      "2017-11-05T17:07:50.251630: step 1021, loss 0.0525652, acc 1\n",
      "2017-11-05T17:07:54.282385: step 1022, loss 0.189773, acc 0.90625\n",
      "2017-11-05T17:07:58.267881: step 1023, loss 0.333136, acc 0.90625\n",
      "2017-11-05T17:08:02.301473: step 1024, loss 0.196821, acc 0.9375\n",
      "2017-11-05T17:08:06.297781: step 1025, loss 0.365479, acc 0.875\n",
      "2017-11-05T17:08:10.342234: step 1026, loss 0.136019, acc 0.9375\n",
      "2017-11-05T17:08:14.375310: step 1027, loss 0.300853, acc 0.875\n",
      "2017-11-05T17:08:18.354493: step 1028, loss 0.0494488, acc 1\n",
      "2017-11-05T17:08:22.357067: step 1029, loss 0.26145, acc 0.875\n",
      "2017-11-05T17:08:26.351952: step 1030, loss 0.2425, acc 0.875\n",
      "2017-11-05T17:08:30.318370: step 1031, loss 0.0448465, acc 1\n",
      "2017-11-05T17:08:34.405405: step 1032, loss 0.129321, acc 0.9375\n",
      "2017-11-05T17:08:38.542829: step 1033, loss 0.162034, acc 0.90625\n",
      "2017-11-05T17:08:42.597419: step 1034, loss 0.0224597, acc 1\n",
      "2017-11-05T17:08:46.591637: step 1035, loss 0.0489273, acc 0.96875\n",
      "2017-11-05T17:08:50.576976: step 1036, loss 0.543819, acc 0.84375\n",
      "2017-11-05T17:08:54.607820: step 1037, loss 0.297413, acc 0.90625\n",
      "2017-11-05T17:08:58.607485: step 1038, loss 0.460221, acc 0.875\n",
      "2017-11-05T17:09:02.669466: step 1039, loss 0.268706, acc 0.875\n",
      "2017-11-05T17:09:06.691573: step 1040, loss 0.345058, acc 0.84375\n",
      "2017-11-05T17:09:10.747774: step 1041, loss 0.627602, acc 0.78125\n",
      "2017-11-05T17:09:14.686368: step 1042, loss 0.698699, acc 0.75\n",
      "2017-11-05T17:09:18.721437: step 1043, loss 0.334302, acc 0.875\n",
      "2017-11-05T17:09:21.209059: step 1044, loss 0.12457, acc 0.95\n",
      "2017-11-05T17:09:25.419725: step 1045, loss 0.328166, acc 0.90625\n",
      "2017-11-05T17:09:29.539462: step 1046, loss 0.149708, acc 0.90625\n",
      "2017-11-05T17:09:33.490947: step 1047, loss 0.171858, acc 0.875\n",
      "2017-11-05T17:09:37.484225: step 1048, loss 0.223433, acc 0.9375\n",
      "2017-11-05T17:09:41.510226: step 1049, loss 0.0859532, acc 0.96875\n",
      "2017-11-05T17:09:45.551829: step 1050, loss 0.194525, acc 0.9375\n",
      "2017-11-05T17:09:49.571313: step 1051, loss 0.257326, acc 0.9375\n",
      "2017-11-05T17:09:53.576834: step 1052, loss 0.285814, acc 0.84375\n",
      "2017-11-05T17:09:57.593599: step 1053, loss 0.230632, acc 0.875\n",
      "2017-11-05T17:10:02.033823: step 1054, loss 0.276505, acc 0.84375\n",
      "2017-11-05T17:10:06.064146: step 1055, loss 0.115962, acc 0.9375\n",
      "2017-11-05T17:10:10.065755: step 1056, loss 0.217444, acc 0.9375\n",
      "2017-11-05T17:10:14.066303: step 1057, loss 0.0850133, acc 0.9375\n",
      "2017-11-05T17:10:18.028410: step 1058, loss 0.199617, acc 0.90625\n",
      "2017-11-05T17:10:21.999976: step 1059, loss 0.303904, acc 0.90625\n",
      "2017-11-05T17:10:26.047084: step 1060, loss 0.346211, acc 0.875\n",
      "2017-11-05T17:10:30.082249: step 1061, loss 0.0616843, acc 0.96875\n",
      "2017-11-05T17:10:34.221461: step 1062, loss 0.196313, acc 0.96875\n",
      "2017-11-05T17:10:38.186749: step 1063, loss 0.169991, acc 0.9375\n",
      "2017-11-05T17:10:42.253750: step 1064, loss 0.240029, acc 0.9375\n",
      "2017-11-05T17:10:46.304604: step 1065, loss 0.187438, acc 0.90625\n",
      "2017-11-05T17:10:50.282159: step 1066, loss 0.172605, acc 0.96875\n",
      "2017-11-05T17:10:54.245089: step 1067, loss 0.216618, acc 0.90625\n",
      "2017-11-05T17:10:58.224579: step 1068, loss 0.350118, acc 0.84375\n",
      "2017-11-05T17:11:02.185194: step 1069, loss 0.588917, acc 0.84375\n",
      "2017-11-05T17:11:06.174658: step 1070, loss 0.224106, acc 0.9375\n",
      "2017-11-05T17:11:10.232657: step 1071, loss 0.136298, acc 0.96875\n",
      "2017-11-05T17:11:14.185418: step 1072, loss 0.318109, acc 0.875\n",
      "2017-11-05T17:11:18.158143: step 1073, loss 0.553668, acc 0.8125\n",
      "2017-11-05T17:11:22.160462: step 1074, loss 0.038449, acc 1\n",
      "2017-11-05T17:11:26.086438: step 1075, loss 0.341067, acc 0.875\n",
      "2017-11-05T17:11:30.037134: step 1076, loss 0.225856, acc 0.90625\n",
      "2017-11-05T17:11:33.966048: step 1077, loss 0.450828, acc 0.71875\n",
      "2017-11-05T17:11:37.870267: step 1078, loss 0.177554, acc 0.90625\n",
      "2017-11-05T17:11:41.798497: step 1079, loss 0.0880437, acc 0.96875\n",
      "2017-11-05T17:11:44.312101: step 1080, loss 0.223809, acc 0.9\n",
      "2017-11-05T17:11:48.265366: step 1081, loss 0.175129, acc 0.9375\n",
      "2017-11-05T17:11:52.247122: step 1082, loss 0.155205, acc 0.875\n",
      "2017-11-05T17:11:56.153616: step 1083, loss 0.207845, acc 0.9375\n",
      "2017-11-05T17:12:00.118911: step 1084, loss 0.12468, acc 0.9375\n",
      "2017-11-05T17:12:04.145298: step 1085, loss 0.130842, acc 0.90625\n",
      "2017-11-05T17:12:08.120250: step 1086, loss 0.235234, acc 0.9375\n",
      "2017-11-05T17:12:12.102552: step 1087, loss 0.341117, acc 0.875\n",
      "2017-11-05T17:12:16.080661: step 1088, loss 0.678975, acc 0.78125\n",
      "2017-11-05T17:12:20.025597: step 1089, loss 0.258891, acc 0.90625\n",
      "2017-11-05T17:12:24.006936: step 1090, loss 0.256758, acc 0.90625\n",
      "2017-11-05T17:12:27.974498: step 1091, loss 0.0864023, acc 0.96875\n",
      "2017-11-05T17:12:32.092648: step 1092, loss 0.197827, acc 0.90625\n",
      "2017-11-05T17:12:36.229090: step 1093, loss 0.112145, acc 0.96875\n",
      "2017-11-05T17:12:40.220537: step 1094, loss 0.172598, acc 0.9375\n",
      "2017-11-05T17:12:44.207442: step 1095, loss 0.20929, acc 0.9375\n",
      "2017-11-05T17:12:48.219294: step 1096, loss 0.363456, acc 0.84375\n",
      "2017-11-05T17:12:52.119352: step 1097, loss 0.263804, acc 0.90625\n",
      "2017-11-05T17:12:56.112490: step 1098, loss 0.181378, acc 0.9375\n",
      "2017-11-05T17:13:00.112274: step 1099, loss 0.273425, acc 0.84375\n",
      "2017-11-05T17:13:04.104411: step 1100, loss 0.388871, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:13:06.691763: step 1100, loss 0.995625, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-05T17:13:12.851400: step 1101, loss 0.154397, acc 0.9375\n",
      "2017-11-05T17:13:16.849536: step 1102, loss 0.412052, acc 0.875\n",
      "2017-11-05T17:13:20.810184: step 1103, loss 0.11765, acc 0.9375\n",
      "2017-11-05T17:13:24.978219: step 1104, loss 0.164138, acc 0.9375\n",
      "2017-11-05T17:13:29.020821: step 1105, loss 0.0453398, acc 0.96875\n",
      "2017-11-05T17:13:32.969546: step 1106, loss 0.270573, acc 0.90625\n",
      "2017-11-05T17:13:36.961558: step 1107, loss 0.168957, acc 0.9375\n",
      "2017-11-05T17:13:40.927366: step 1108, loss 0.0305583, acc 1\n",
      "2017-11-05T17:13:44.888836: step 1109, loss 0.330489, acc 0.8125\n",
      "2017-11-05T17:13:48.969853: step 1110, loss 0.248313, acc 0.90625\n",
      "2017-11-05T17:13:52.983239: step 1111, loss 0.124578, acc 0.96875\n",
      "2017-11-05T17:13:56.971201: step 1112, loss 0.148433, acc 0.9375\n",
      "2017-11-05T17:14:00.925940: step 1113, loss 0.0972382, acc 0.96875\n",
      "2017-11-05T17:14:04.934845: step 1114, loss 0.242448, acc 0.875\n",
      "2017-11-05T17:14:08.961036: step 1115, loss 0.164577, acc 0.90625\n",
      "2017-11-05T17:14:11.437945: step 1116, loss 0.0764037, acc 0.95\n",
      "2017-11-05T17:14:15.372031: step 1117, loss 0.101874, acc 0.9375\n",
      "2017-11-05T17:14:19.292361: step 1118, loss 0.179579, acc 0.90625\n",
      "2017-11-05T17:14:23.272808: step 1119, loss 0.261675, acc 0.9375\n",
      "2017-11-05T17:14:27.222527: step 1120, loss 0.159464, acc 0.9375\n",
      "2017-11-05T17:14:31.195069: step 1121, loss 0.120046, acc 0.96875\n",
      "2017-11-05T17:14:35.282293: step 1122, loss 0.404203, acc 0.875\n",
      "2017-11-05T17:14:39.232336: step 1123, loss 0.323125, acc 0.90625\n",
      "2017-11-05T17:14:43.243361: step 1124, loss 0.210656, acc 0.9375\n",
      "2017-11-05T17:14:47.205047: step 1125, loss 0.132098, acc 0.90625\n",
      "2017-11-05T17:14:51.218506: step 1126, loss 0.300853, acc 0.84375\n",
      "2017-11-05T17:14:55.199050: step 1127, loss 0.413451, acc 0.8125\n",
      "2017-11-05T17:14:59.166752: step 1128, loss 0.119261, acc 0.9375\n",
      "2017-11-05T17:15:03.187214: step 1129, loss 0.115083, acc 1\n",
      "2017-11-05T17:15:07.147791: step 1130, loss 0.18965, acc 0.90625\n",
      "2017-11-05T17:15:11.130451: step 1131, loss 0.267022, acc 0.875\n",
      "2017-11-05T17:15:15.181513: step 1132, loss 0.283192, acc 0.875\n",
      "2017-11-05T17:15:19.129000: step 1133, loss 0.21929, acc 0.9375\n",
      "2017-11-05T17:15:23.088413: step 1134, loss 0.181754, acc 0.9375\n",
      "2017-11-05T17:15:27.087768: step 1135, loss 0.573475, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T17:15:31.021396: step 1136, loss 0.218008, acc 0.90625\n",
      "2017-11-05T17:15:35.006388: step 1137, loss 0.123924, acc 0.9375\n",
      "2017-11-05T17:15:38.964060: step 1138, loss 0.0601019, acc 0.96875\n",
      "2017-11-05T17:15:42.899418: step 1139, loss 0.097317, acc 0.96875\n",
      "2017-11-05T17:15:46.846909: step 1140, loss 0.150351, acc 0.90625\n",
      "2017-11-05T17:15:50.867572: step 1141, loss 0.0871872, acc 0.96875\n",
      "2017-11-05T17:15:54.884638: step 1142, loss 0.393817, acc 0.875\n",
      "2017-11-05T17:15:58.875648: step 1143, loss 0.0571968, acc 0.96875\n",
      "2017-11-05T17:16:02.869149: step 1144, loss 0.195433, acc 0.90625\n",
      "2017-11-05T17:16:06.966471: step 1145, loss 0.157281, acc 0.90625\n",
      "2017-11-05T17:16:10.982790: step 1146, loss 0.363234, acc 0.84375\n",
      "2017-11-05T17:16:14.929404: step 1147, loss 0.381509, acc 0.90625\n",
      "2017-11-05T17:16:18.862044: step 1148, loss 0.256092, acc 0.84375\n",
      "2017-11-05T17:16:22.844565: step 1149, loss 0.150773, acc 0.9375\n",
      "2017-11-05T17:16:26.819896: step 1150, loss 0.243911, acc 0.90625\n",
      "2017-11-05T17:16:30.770077: step 1151, loss 0.501653, acc 0.8125\n",
      "2017-11-05T17:16:33.289392: step 1152, loss 0.457138, acc 0.85\n",
      "2017-11-05T17:16:37.568200: step 1153, loss 0.160207, acc 0.9375\n",
      "2017-11-05T17:16:41.638339: step 1154, loss 0.033126, acc 1\n",
      "2017-11-05T17:16:45.594460: step 1155, loss 0.0925378, acc 0.9375\n",
      "2017-11-05T17:16:49.530106: step 1156, loss 0.290634, acc 0.90625\n",
      "2017-11-05T17:16:53.575210: step 1157, loss 0.0808743, acc 0.96875\n",
      "2017-11-05T17:16:57.549862: step 1158, loss 0.209512, acc 0.90625\n",
      "2017-11-05T17:17:01.467380: step 1159, loss 0.270994, acc 0.875\n",
      "2017-11-05T17:17:05.452654: step 1160, loss 0.382591, acc 0.875\n",
      "2017-11-05T17:17:09.369831: step 1161, loss 0.196675, acc 0.90625\n",
      "2017-11-05T17:17:13.299623: step 1162, loss 0.399176, acc 0.8125\n",
      "2017-11-05T17:17:17.284944: step 1163, loss 0.132158, acc 0.9375\n",
      "2017-11-05T17:17:21.256535: step 1164, loss 0.142991, acc 0.9375\n",
      "2017-11-05T17:17:25.222862: step 1165, loss 0.146812, acc 0.9375\n",
      "2017-11-05T17:17:29.216573: step 1166, loss 0.165622, acc 0.90625\n",
      "2017-11-05T17:17:33.210226: step 1167, loss 0.206355, acc 0.84375\n",
      "2017-11-05T17:17:37.151866: step 1168, loss 0.172039, acc 0.9375\n",
      "2017-11-05T17:17:41.113767: step 1169, loss 0.0937607, acc 0.9375\n",
      "2017-11-05T17:17:45.087792: step 1170, loss 0.117545, acc 0.9375\n",
      "2017-11-05T17:17:49.069803: step 1171, loss 0.260765, acc 0.90625\n",
      "2017-11-05T17:17:53.061457: step 1172, loss 0.0836389, acc 0.96875\n",
      "2017-11-05T17:17:57.086265: step 1173, loss 0.189042, acc 0.90625\n",
      "2017-11-05T17:18:01.056489: step 1174, loss 0.233476, acc 0.90625\n",
      "2017-11-05T17:18:05.007592: step 1175, loss 0.105145, acc 0.9375\n",
      "2017-11-05T17:18:09.199818: step 1176, loss 0.170458, acc 0.90625\n",
      "2017-11-05T17:18:13.181239: step 1177, loss 0.168807, acc 0.90625\n",
      "2017-11-05T17:18:17.200563: step 1178, loss 0.253089, acc 0.875\n",
      "2017-11-05T17:18:21.204432: step 1179, loss 0.332369, acc 0.8125\n",
      "2017-11-05T17:18:25.478222: step 1180, loss 0.139742, acc 0.9375\n",
      "2017-11-05T17:18:29.506552: step 1181, loss 0.389935, acc 0.84375\n",
      "2017-11-05T17:18:33.610599: step 1182, loss 0.134343, acc 0.9375\n",
      "2017-11-05T17:18:37.576390: step 1183, loss 0.434229, acc 0.84375\n",
      "2017-11-05T17:18:41.783758: step 1184, loss 0.268088, acc 0.875\n",
      "2017-11-05T17:18:45.865967: step 1185, loss 0.328184, acc 0.90625\n",
      "2017-11-05T17:18:49.976895: step 1186, loss 0.428279, acc 0.84375\n",
      "2017-11-05T17:18:54.032049: step 1187, loss 0.0953432, acc 0.9375\n",
      "2017-11-05T17:18:56.631949: step 1188, loss 0.131223, acc 0.95\n",
      "2017-11-05T17:19:00.763929: step 1189, loss 0.232015, acc 0.875\n",
      "2017-11-05T17:19:04.817801: step 1190, loss 0.205657, acc 0.9375\n",
      "2017-11-05T17:19:08.844622: step 1191, loss 0.33393, acc 0.84375\n",
      "2017-11-05T17:19:12.901223: step 1192, loss 0.151508, acc 0.9375\n",
      "2017-11-05T17:19:16.913327: step 1193, loss 0.114569, acc 0.90625\n",
      "2017-11-05T17:19:20.903102: step 1194, loss 0.219765, acc 0.84375\n",
      "2017-11-05T17:19:25.057235: step 1195, loss 0.149852, acc 0.90625\n",
      "2017-11-05T17:19:29.125472: step 1196, loss 0.0483223, acc 1\n",
      "2017-11-05T17:19:33.091719: step 1197, loss 0.0589727, acc 0.96875\n",
      "2017-11-05T17:19:37.178334: step 1198, loss 0.00879702, acc 1\n",
      "2017-11-05T17:19:41.442072: step 1199, loss 0.197763, acc 0.90625\n",
      "2017-11-05T17:19:45.599413: step 1200, loss 0.410564, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:19:48.320630: step 1200, loss 0.978967, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-05T17:19:53.656040: step 1201, loss 0.178518, acc 0.90625\n",
      "2017-11-05T17:19:57.650542: step 1202, loss 0.504333, acc 0.8125\n",
      "2017-11-05T17:20:01.883763: step 1203, loss 0.0277377, acc 1\n",
      "2017-11-05T17:20:05.806064: step 1204, loss 0.076346, acc 0.96875\n",
      "2017-11-05T17:20:09.805849: step 1205, loss 0.163917, acc 0.9375\n",
      "2017-11-05T17:20:13.796854: step 1206, loss 0.250005, acc 0.875\n",
      "2017-11-05T17:20:17.745827: step 1207, loss 0.389784, acc 0.84375\n",
      "2017-11-05T17:20:21.675732: step 1208, loss 0.412786, acc 0.8125\n",
      "2017-11-05T17:20:25.602198: step 1209, loss 0.341731, acc 0.875\n",
      "2017-11-05T17:20:29.569951: step 1210, loss 0.152031, acc 0.96875\n",
      "2017-11-05T17:20:33.594302: step 1211, loss 0.362218, acc 0.8125\n",
      "2017-11-05T17:20:37.646956: step 1212, loss 0.0995255, acc 0.96875\n",
      "2017-11-05T17:20:41.629938: step 1213, loss 0.117918, acc 0.96875\n",
      "2017-11-05T17:20:45.591109: step 1214, loss 0.176769, acc 0.90625\n",
      "2017-11-05T17:20:49.539592: step 1215, loss 0.226504, acc 0.90625\n",
      "2017-11-05T17:20:53.574351: step 1216, loss 0.282126, acc 0.90625\n",
      "2017-11-05T17:20:57.489772: step 1217, loss 0.149929, acc 0.9375\n",
      "2017-11-05T17:21:01.588959: step 1218, loss 0.223244, acc 0.90625\n",
      "2017-11-05T17:21:05.516526: step 1219, loss 0.349646, acc 0.90625\n",
      "2017-11-05T17:21:09.510047: step 1220, loss 0.251804, acc 0.90625\n",
      "2017-11-05T17:21:13.436511: step 1221, loss 0.0893106, acc 0.9375\n",
      "2017-11-05T17:21:17.438116: step 1222, loss 0.0348763, acc 1\n",
      "2017-11-05T17:21:21.385191: step 1223, loss 0.655843, acc 0.84375\n",
      "2017-11-05T17:21:23.894086: step 1224, loss 0.158761, acc 0.95\n",
      "2017-11-05T17:21:27.821969: step 1225, loss 0.0621355, acc 1\n",
      "2017-11-05T17:21:31.816810: step 1226, loss 0.193477, acc 0.9375\n",
      "2017-11-05T17:21:35.789930: step 1227, loss 0.195308, acc 0.9375\n",
      "2017-11-05T17:21:39.787799: step 1228, loss 0.156222, acc 0.9375\n",
      "2017-11-05T17:21:43.766812: step 1229, loss 0.28416, acc 0.875\n",
      "2017-11-05T17:21:47.713328: step 1230, loss 0.103784, acc 0.96875\n",
      "2017-11-05T17:21:51.674179: step 1231, loss 0.10897, acc 0.96875\n",
      "2017-11-05T17:21:55.642428: step 1232, loss 0.227419, acc 0.90625\n",
      "2017-11-05T17:21:59.638835: step 1233, loss 0.137017, acc 0.9375\n",
      "2017-11-05T17:22:03.623600: step 1234, loss 0.219455, acc 0.84375\n",
      "2017-11-05T17:22:07.663611: step 1235, loss 0.152593, acc 0.90625\n",
      "2017-11-05T17:22:11.661590: step 1236, loss 0.586855, acc 0.8125\n",
      "2017-11-05T17:22:15.653519: step 1237, loss 0.0261972, acc 1\n",
      "2017-11-05T17:22:19.625977: step 1238, loss 0.0357617, acc 1\n",
      "2017-11-05T17:22:23.577252: step 1239, loss 0.108646, acc 0.96875\n",
      "2017-11-05T17:22:27.539669: step 1240, loss 0.233067, acc 0.90625\n",
      "2017-11-05T17:22:31.498601: step 1241, loss 0.26306, acc 0.84375\n",
      "2017-11-05T17:22:35.591583: step 1242, loss 0.0641387, acc 0.96875\n",
      "2017-11-05T17:22:39.608655: step 1243, loss 0.17765, acc 0.9375\n",
      "2017-11-05T17:22:43.617235: step 1244, loss 0.265962, acc 0.875\n",
      "2017-11-05T17:22:47.582195: step 1245, loss 0.237955, acc 0.875\n",
      "2017-11-05T17:22:51.603657: step 1246, loss 0.246166, acc 0.90625\n",
      "2017-11-05T17:22:55.678532: step 1247, loss 0.144999, acc 0.9375\n",
      "2017-11-05T17:22:59.637676: step 1248, loss 0.120693, acc 0.9375\n",
      "2017-11-05T17:23:03.667556: step 1249, loss 0.126898, acc 0.9375\n",
      "2017-11-05T17:23:07.687022: step 1250, loss 0.432217, acc 0.875\n",
      "2017-11-05T17:23:11.836881: step 1251, loss 0.467389, acc 0.75\n",
      "2017-11-05T17:23:15.903940: step 1252, loss 0.23771, acc 0.9375\n",
      "2017-11-05T17:23:19.903575: step 1253, loss 0.0878274, acc 0.96875\n",
      "2017-11-05T17:23:24.100292: step 1254, loss 0.258665, acc 0.875\n",
      "2017-11-05T17:23:28.424334: step 1255, loss 0.13775, acc 0.9375\n",
      "2017-11-05T17:23:32.433794: step 1256, loss 0.0361788, acc 0.96875\n",
      "2017-11-05T17:23:36.383375: step 1257, loss 0.42787, acc 0.84375\n",
      "2017-11-05T17:23:40.371595: step 1258, loss 0.235249, acc 0.90625\n",
      "2017-11-05T17:23:44.353234: step 1259, loss 0.383077, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T17:23:46.973137: step 1260, loss 0.45466, acc 0.8\n",
      "2017-11-05T17:23:50.983368: step 1261, loss 0.313757, acc 0.84375\n",
      "2017-11-05T17:23:54.992649: step 1262, loss 0.365648, acc 0.84375\n",
      "2017-11-05T17:23:58.932356: step 1263, loss 0.0725633, acc 1\n",
      "2017-11-05T17:24:02.885975: step 1264, loss 0.264988, acc 0.90625\n",
      "2017-11-05T17:24:06.863467: step 1265, loss 0.196399, acc 0.875\n",
      "2017-11-05T17:24:10.815245: step 1266, loss 0.109553, acc 0.9375\n",
      "2017-11-05T17:24:14.815699: step 1267, loss 0.311374, acc 0.875\n",
      "2017-11-05T17:24:18.788391: step 1268, loss 0.0319036, acc 1\n",
      "2017-11-05T17:24:22.721526: step 1269, loss 0.485963, acc 0.78125\n",
      "2017-11-05T17:24:26.661333: step 1270, loss 0.13828, acc 0.9375\n",
      "2017-11-05T17:24:30.639665: step 1271, loss 0.16486, acc 0.90625\n",
      "2017-11-05T17:24:34.718238: step 1272, loss 0.552755, acc 0.8125\n",
      "2017-11-05T17:24:38.696377: step 1273, loss 0.285279, acc 0.875\n",
      "2017-11-05T17:24:42.661419: step 1274, loss 0.274543, acc 0.84375\n",
      "2017-11-05T17:24:46.595690: step 1275, loss 0.0809158, acc 0.9375\n",
      "2017-11-05T17:24:50.549989: step 1276, loss 0.516254, acc 0.78125\n",
      "2017-11-05T17:24:54.463371: step 1277, loss 0.200176, acc 0.90625\n",
      "2017-11-05T17:24:58.409718: step 1278, loss 0.19385, acc 0.9375\n",
      "2017-11-05T17:25:02.348409: step 1279, loss 0.314496, acc 0.90625\n",
      "2017-11-05T17:25:06.304379: step 1280, loss 0.135654, acc 0.9375\n",
      "2017-11-05T17:25:10.297785: step 1281, loss 0.19271, acc 0.90625\n",
      "2017-11-05T17:25:14.286109: step 1282, loss 0.29773, acc 0.84375\n",
      "2017-11-05T17:25:18.247890: step 1283, loss 0.552387, acc 0.8125\n",
      "2017-11-05T17:25:22.188684: step 1284, loss 0.16041, acc 0.9375\n",
      "2017-11-05T17:25:26.137431: step 1285, loss 0.316572, acc 0.90625\n",
      "2017-11-05T17:25:30.128910: step 1286, loss 0.0604578, acc 0.96875\n",
      "2017-11-05T17:25:34.078070: step 1287, loss 0.240182, acc 0.9375\n",
      "2017-11-05T17:25:38.049518: step 1288, loss 0.17702, acc 0.96875\n",
      "2017-11-05T17:25:41.976374: step 1289, loss 0.208703, acc 0.90625\n",
      "2017-11-05T17:25:45.922731: step 1290, loss 0.108098, acc 0.96875\n",
      "2017-11-05T17:25:49.885984: step 1291, loss 0.384891, acc 0.875\n",
      "2017-11-05T17:25:53.804473: step 1292, loss 0.23216, acc 0.90625\n",
      "2017-11-05T17:25:57.745832: step 1293, loss 0.17657, acc 0.9375\n",
      "2017-11-05T17:26:01.808767: step 1294, loss 0.169255, acc 0.9375\n",
      "2017-11-05T17:26:05.777628: step 1295, loss 0.25336, acc 0.84375\n",
      "2017-11-05T17:26:08.329962: step 1296, loss 0.0330863, acc 1\n",
      "2017-11-05T17:26:12.273687: step 1297, loss 0.190181, acc 0.90625\n",
      "2017-11-05T17:26:16.303511: step 1298, loss 0.0740207, acc 0.96875\n",
      "2017-11-05T17:26:20.209008: step 1299, loss 0.01473, acc 1\n",
      "2017-11-05T17:26:24.166466: step 1300, loss 0.227799, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:26:26.711268: step 1300, loss 0.959527, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-05T17:26:32.422878: step 1301, loss 0.0997775, acc 0.9375\n",
      "2017-11-05T17:26:36.489432: step 1302, loss 0.208565, acc 0.84375\n",
      "2017-11-05T17:26:40.450944: step 1303, loss 0.061873, acc 0.9375\n",
      "2017-11-05T17:26:44.396549: step 1304, loss 0.403634, acc 0.875\n",
      "2017-11-05T17:26:48.327724: step 1305, loss 0.244042, acc 0.9375\n",
      "2017-11-05T17:26:52.287160: step 1306, loss 0.0787239, acc 0.96875\n",
      "2017-11-05T17:26:56.341749: step 1307, loss 0.120993, acc 0.875\n",
      "2017-11-05T17:27:00.320768: step 1308, loss 0.0613994, acc 0.96875\n",
      "2017-11-05T17:27:04.342453: step 1309, loss 0.203606, acc 0.875\n",
      "2017-11-05T17:27:08.330610: step 1310, loss 0.146332, acc 0.9375\n",
      "2017-11-05T17:27:12.339678: step 1311, loss 0.167652, acc 0.9375\n",
      "2017-11-05T17:27:16.308578: step 1312, loss 0.115336, acc 0.96875\n",
      "2017-11-05T17:27:20.235784: step 1313, loss 0.314912, acc 0.90625\n",
      "2017-11-05T17:27:24.170384: step 1314, loss 0.374525, acc 0.8125\n",
      "2017-11-05T17:27:28.159535: step 1315, loss 0.263543, acc 0.875\n",
      "2017-11-05T17:27:32.116548: step 1316, loss 0.0784045, acc 1\n",
      "2017-11-05T17:27:36.032384: step 1317, loss 0.166429, acc 0.9375\n",
      "2017-11-05T17:27:40.007525: step 1318, loss 0.173627, acc 0.875\n",
      "2017-11-05T17:27:43.992537: step 1319, loss 0.226123, acc 0.875\n",
      "2017-11-05T17:27:47.936105: step 1320, loss 0.251246, acc 0.9375\n",
      "2017-11-05T17:27:51.942852: step 1321, loss 0.288042, acc 0.875\n",
      "2017-11-05T17:27:55.886912: step 1322, loss 0.0922885, acc 0.96875\n",
      "2017-11-05T17:27:59.811844: step 1323, loss 0.372487, acc 0.875\n",
      "2017-11-05T17:28:03.767477: step 1324, loss 0.165849, acc 0.875\n",
      "2017-11-05T17:28:07.705124: step 1325, loss 0.163735, acc 0.90625\n",
      "2017-11-05T17:28:11.690153: step 1326, loss 0.253293, acc 0.90625\n",
      "2017-11-05T17:28:15.588005: step 1327, loss 0.479793, acc 0.875\n",
      "2017-11-05T17:28:19.620945: step 1328, loss 0.0567963, acc 0.96875\n",
      "2017-11-05T17:28:23.717718: step 1329, loss 0.471481, acc 0.8125\n",
      "2017-11-05T17:28:27.663746: step 1330, loss 0.388691, acc 0.84375\n",
      "2017-11-05T17:28:31.626766: step 1331, loss 0.206039, acc 0.875\n",
      "2017-11-05T17:28:34.285201: step 1332, loss 0.376964, acc 0.9\n",
      "2017-11-05T17:28:38.225914: step 1333, loss 0.27131, acc 0.84375\n",
      "2017-11-05T17:28:42.229324: step 1334, loss 0.145614, acc 0.9375\n",
      "2017-11-05T17:28:46.142953: step 1335, loss 0.109034, acc 0.96875\n",
      "2017-11-05T17:28:50.088392: step 1336, loss 0.0956733, acc 0.96875\n",
      "2017-11-05T17:28:54.049375: step 1337, loss 0.182358, acc 0.90625\n",
      "2017-11-05T17:28:57.987265: step 1338, loss 0.239606, acc 0.84375\n",
      "2017-11-05T17:29:02.001539: step 1339, loss 0.119609, acc 0.9375\n",
      "2017-11-05T17:29:05.924013: step 1340, loss 0.0866261, acc 0.96875\n",
      "2017-11-05T17:29:09.948126: step 1341, loss 0.35098, acc 0.875\n",
      "2017-11-05T17:29:13.873615: step 1342, loss 0.127224, acc 0.90625\n",
      "2017-11-05T17:29:17.853309: step 1343, loss 0.216353, acc 0.90625\n",
      "2017-11-05T17:29:21.801669: step 1344, loss 0.066481, acc 0.96875\n",
      "2017-11-05T17:29:25.754623: step 1345, loss 0.205866, acc 0.90625\n",
      "2017-11-05T17:29:29.715483: step 1346, loss 0.0894453, acc 0.96875\n",
      "2017-11-05T17:29:33.680309: step 1347, loss 0.137666, acc 0.9375\n",
      "2017-11-05T17:29:37.619563: step 1348, loss 0.164378, acc 0.9375\n",
      "2017-11-05T17:29:41.548728: step 1349, loss 0.242016, acc 0.90625\n",
      "2017-11-05T17:29:45.510842: step 1350, loss 0.26219, acc 0.875\n",
      "2017-11-05T17:29:49.455379: step 1351, loss 0.1862, acc 0.875\n",
      "2017-11-05T17:29:53.359264: step 1352, loss 0.444481, acc 0.8125\n",
      "2017-11-05T17:29:57.298565: step 1353, loss 0.139816, acc 0.9375\n",
      "2017-11-05T17:30:01.448913: step 1354, loss 0.215212, acc 0.9375\n",
      "2017-11-05T17:30:05.457401: step 1355, loss 0.12148, acc 0.9375\n",
      "2017-11-05T17:30:09.420946: step 1356, loss 0.176818, acc 0.9375\n",
      "2017-11-05T17:30:13.385204: step 1357, loss 0.17194, acc 0.9375\n",
      "2017-11-05T17:30:17.394827: step 1358, loss 0.202557, acc 0.9375\n",
      "2017-11-05T17:30:21.413286: step 1359, loss 0.277691, acc 0.90625\n",
      "2017-11-05T17:30:25.335099: step 1360, loss 0.202447, acc 0.9375\n",
      "2017-11-05T17:30:29.320907: step 1361, loss 0.435596, acc 0.84375\n",
      "2017-11-05T17:30:33.405299: step 1362, loss 0.04589, acc 0.96875\n",
      "2017-11-05T17:30:37.402168: step 1363, loss 0.310643, acc 0.84375\n",
      "2017-11-05T17:30:41.406574: step 1364, loss 0.228794, acc 0.90625\n",
      "2017-11-05T17:30:45.362562: step 1365, loss 0.286431, acc 0.875\n",
      "2017-11-05T17:30:49.356226: step 1366, loss 0.429226, acc 0.84375\n",
      "2017-11-05T17:30:53.317813: step 1367, loss 0.288628, acc 0.90625\n",
      "2017-11-05T17:30:55.822511: step 1368, loss 0.159395, acc 0.95\n",
      "2017-11-05T17:30:59.784680: step 1369, loss 0.161323, acc 0.9375\n",
      "2017-11-05T17:31:03.729621: step 1370, loss 0.247673, acc 0.84375\n",
      "2017-11-05T17:31:07.708874: step 1371, loss 0.431207, acc 0.84375\n",
      "2017-11-05T17:31:11.701989: step 1372, loss 0.43337, acc 0.84375\n",
      "2017-11-05T17:31:15.678562: step 1373, loss 0.284029, acc 0.875\n",
      "2017-11-05T17:31:19.660665: step 1374, loss 0.181368, acc 0.90625\n",
      "2017-11-05T17:31:23.566933: step 1375, loss 0.224387, acc 0.875\n",
      "2017-11-05T17:31:27.589296: step 1376, loss 0.154081, acc 0.9375\n",
      "2017-11-05T17:31:31.535499: step 1377, loss 0.471914, acc 0.84375\n",
      "2017-11-05T17:31:35.515595: step 1378, loss 0.093691, acc 0.9375\n",
      "2017-11-05T17:31:39.503254: step 1379, loss 0.361288, acc 0.84375\n",
      "2017-11-05T17:31:43.515128: step 1380, loss 0.181538, acc 0.9375\n",
      "2017-11-05T17:31:47.461472: step 1381, loss 0.143128, acc 0.9375\n",
      "2017-11-05T17:31:51.450472: step 1382, loss 0.166298, acc 0.9375\n",
      "2017-11-05T17:31:55.352687: step 1383, loss 0.124294, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T17:31:59.437526: step 1384, loss 0.16922, acc 0.9375\n",
      "2017-11-05T17:32:03.415275: step 1385, loss 0.304765, acc 0.90625\n",
      "2017-11-05T17:32:07.463045: step 1386, loss 0.108888, acc 0.96875\n",
      "2017-11-05T17:32:11.467928: step 1387, loss 0.0683651, acc 0.96875\n",
      "2017-11-05T17:32:15.491650: step 1388, loss 0.223056, acc 0.90625\n",
      "2017-11-05T17:32:19.415976: step 1389, loss 0.203155, acc 0.90625\n",
      "2017-11-05T17:32:23.388427: step 1390, loss 0.239082, acc 0.90625\n",
      "2017-11-05T17:32:27.362078: step 1391, loss 0.0927753, acc 0.96875\n",
      "2017-11-05T17:32:31.331326: step 1392, loss 0.158572, acc 0.9375\n",
      "2017-11-05T17:32:35.420663: step 1393, loss 0.145236, acc 0.9375\n",
      "2017-11-05T17:32:39.437641: step 1394, loss 0.258814, acc 0.90625\n",
      "2017-11-05T17:32:43.388937: step 1395, loss 0.0923931, acc 0.9375\n",
      "2017-11-05T17:32:47.371395: step 1396, loss 0.12731, acc 0.96875\n",
      "2017-11-05T17:32:51.412010: step 1397, loss 0.025223, acc 1\n",
      "2017-11-05T17:32:55.494266: step 1398, loss 0.247014, acc 0.90625\n",
      "2017-11-05T17:32:59.537279: step 1399, loss 0.335682, acc 0.90625\n",
      "2017-11-05T17:33:03.514990: step 1400, loss 0.0490403, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:33:06.021332: step 1400, loss 1.03361, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-05T17:33:11.408631: step 1401, loss 0.700857, acc 0.71875\n",
      "2017-11-05T17:33:15.535734: step 1402, loss 0.0386971, acc 0.96875\n",
      "2017-11-05T17:33:19.515690: step 1403, loss 0.155678, acc 0.9375\n",
      "2017-11-05T17:33:22.162757: step 1404, loss 0.146262, acc 0.95\n",
      "2017-11-05T17:33:26.333442: step 1405, loss 0.370892, acc 0.875\n",
      "2017-11-05T17:33:30.247360: step 1406, loss 0.172257, acc 0.90625\n",
      "2017-11-05T17:33:34.287352: step 1407, loss 0.315101, acc 0.875\n",
      "2017-11-05T17:33:38.311655: step 1408, loss 0.162017, acc 0.9375\n",
      "2017-11-05T17:33:42.398803: step 1409, loss 0.126405, acc 0.9375\n",
      "2017-11-05T17:33:46.335496: step 1410, loss 0.217117, acc 0.90625\n",
      "2017-11-05T17:33:50.352479: step 1411, loss 0.262493, acc 0.84375\n",
      "2017-11-05T17:33:54.419224: step 1412, loss 0.0275031, acc 1\n",
      "2017-11-05T17:33:58.416283: step 1413, loss 0.181083, acc 0.90625\n",
      "2017-11-05T17:34:02.425558: step 1414, loss 0.0646458, acc 0.96875\n",
      "2017-11-05T17:34:06.420069: step 1415, loss 0.0696015, acc 0.96875\n",
      "2017-11-05T17:34:10.459147: step 1416, loss 0.384964, acc 0.84375\n",
      "2017-11-05T17:34:14.453181: step 1417, loss 0.460392, acc 0.8125\n",
      "2017-11-05T17:34:18.424561: step 1418, loss 0.252467, acc 0.90625\n",
      "2017-11-05T17:34:22.426956: step 1419, loss 0.137345, acc 0.9375\n",
      "2017-11-05T17:34:26.438895: step 1420, loss 0.151788, acc 0.90625\n",
      "2017-11-05T17:34:30.478217: step 1421, loss 0.0648088, acc 1\n",
      "2017-11-05T17:34:34.607366: step 1422, loss 0.136849, acc 0.96875\n",
      "2017-11-05T17:34:38.540880: step 1423, loss 0.0897646, acc 0.96875\n",
      "2017-11-05T17:34:42.579219: step 1424, loss 0.113983, acc 0.9375\n",
      "2017-11-05T17:34:46.548388: step 1425, loss 0.219794, acc 0.875\n",
      "2017-11-05T17:34:50.489338: step 1426, loss 0.224605, acc 0.90625\n",
      "2017-11-05T17:34:54.458153: step 1427, loss 0.230643, acc 0.9375\n",
      "2017-11-05T17:34:58.442013: step 1428, loss 0.217314, acc 0.875\n",
      "2017-11-05T17:35:02.428241: step 1429, loss 0.210458, acc 0.9375\n",
      "2017-11-05T17:35:06.392941: step 1430, loss 0.232559, acc 0.9375\n",
      "2017-11-05T17:35:10.406853: step 1431, loss 0.0546862, acc 0.96875\n",
      "2017-11-05T17:35:14.353729: step 1432, loss 0.17913, acc 0.90625\n",
      "2017-11-05T17:35:18.376869: step 1433, loss 0.262832, acc 0.875\n",
      "2017-11-05T17:35:22.337951: step 1434, loss 0.305634, acc 0.875\n",
      "2017-11-05T17:35:26.330556: step 1435, loss 0.350197, acc 0.875\n",
      "2017-11-05T17:35:30.287820: step 1436, loss 0.234963, acc 0.90625\n",
      "2017-11-05T17:35:34.274646: step 1437, loss 0.0284015, acc 1\n",
      "2017-11-05T17:35:38.232098: step 1438, loss 0.353797, acc 0.8125\n",
      "2017-11-05T17:35:42.216782: step 1439, loss 0.071819, acc 0.96875\n",
      "2017-11-05T17:35:44.721576: step 1440, loss 0.390122, acc 0.75\n",
      "2017-11-05T17:35:48.722727: step 1441, loss 0.208473, acc 0.90625\n",
      "2017-11-05T17:35:52.737689: step 1442, loss 0.175487, acc 0.90625\n",
      "2017-11-05T17:35:56.702929: step 1443, loss 0.484289, acc 0.78125\n",
      "2017-11-05T17:36:00.658272: step 1444, loss 0.180341, acc 0.90625\n",
      "2017-11-05T17:36:04.618322: step 1445, loss 0.200459, acc 0.90625\n",
      "2017-11-05T17:36:08.612897: step 1446, loss 0.124157, acc 0.9375\n",
      "2017-11-05T17:36:12.587749: step 1447, loss 0.255291, acc 0.875\n",
      "2017-11-05T17:36:16.567853: step 1448, loss 0.100276, acc 0.9375\n",
      "2017-11-05T17:36:20.638365: step 1449, loss 0.201991, acc 0.875\n",
      "2017-11-05T17:36:24.650930: step 1450, loss 0.415663, acc 0.8125\n",
      "2017-11-05T17:36:28.696492: step 1451, loss 0.258188, acc 0.90625\n",
      "2017-11-05T17:36:32.619498: step 1452, loss 0.105616, acc 0.96875\n",
      "2017-11-05T17:36:36.707996: step 1453, loss 0.238047, acc 0.9375\n",
      "2017-11-05T17:36:40.685783: step 1454, loss 0.166514, acc 0.9375\n",
      "2017-11-05T17:36:44.661344: step 1455, loss 0.215356, acc 0.9375\n",
      "2017-11-05T17:36:48.643097: step 1456, loss 0.104813, acc 0.96875\n",
      "2017-11-05T17:36:52.650594: step 1457, loss 0.228281, acc 0.875\n",
      "2017-11-05T17:36:56.613050: step 1458, loss 0.16051, acc 0.90625\n",
      "2017-11-05T17:37:00.621782: step 1459, loss 0.232353, acc 0.90625\n",
      "2017-11-05T17:37:04.623881: step 1460, loss 0.24684, acc 0.9375\n",
      "2017-11-05T17:37:08.624659: step 1461, loss 0.381506, acc 0.875\n",
      "2017-11-05T17:37:12.583852: step 1462, loss 0.315624, acc 0.875\n",
      "2017-11-05T17:37:16.551706: step 1463, loss 0.102387, acc 0.9375\n",
      "2017-11-05T17:37:20.553179: step 1464, loss 0.389774, acc 0.875\n",
      "2017-11-05T17:37:24.472594: step 1465, loss 0.156016, acc 0.9375\n",
      "2017-11-05T17:37:28.434458: step 1466, loss 0.0826966, acc 0.96875\n",
      "2017-11-05T17:37:32.413985: step 1467, loss 0.0902802, acc 0.96875\n",
      "2017-11-05T17:37:36.399946: step 1468, loss 0.138579, acc 0.9375\n",
      "2017-11-05T17:37:40.334776: step 1469, loss 0.273834, acc 0.90625\n",
      "2017-11-05T17:37:44.296388: step 1470, loss 0.0756976, acc 0.96875\n",
      "2017-11-05T17:37:48.228914: step 1471, loss 0.20584, acc 0.9375\n",
      "2017-11-05T17:37:52.200097: step 1472, loss 0.258383, acc 0.90625\n",
      "2017-11-05T17:37:56.229870: step 1473, loss 0.431922, acc 0.8125\n",
      "2017-11-05T17:38:00.260404: step 1474, loss 0.154221, acc 0.9375\n",
      "2017-11-05T17:38:04.187850: step 1475, loss 0.238303, acc 0.9375\n",
      "2017-11-05T17:38:06.714126: step 1476, loss 0.598637, acc 0.75\n",
      "2017-11-05T17:38:10.772338: step 1477, loss 0.0964137, acc 0.96875\n",
      "2017-11-05T17:38:14.729444: step 1478, loss 0.361135, acc 0.875\n",
      "2017-11-05T17:38:18.694823: step 1479, loss 0.112008, acc 0.96875\n",
      "2017-11-05T17:38:22.854729: step 1480, loss 0.199132, acc 0.875\n",
      "2017-11-05T17:38:27.077159: step 1481, loss 0.0791681, acc 0.9375\n",
      "2017-11-05T17:38:31.128332: step 1482, loss 0.329622, acc 0.875\n",
      "2017-11-05T17:38:35.281940: step 1483, loss 0.122354, acc 0.96875\n",
      "2017-11-05T17:38:39.319814: step 1484, loss 0.0625799, acc 0.9375\n",
      "2017-11-05T17:38:43.397374: step 1485, loss 0.522329, acc 0.875\n",
      "2017-11-05T17:38:47.390060: step 1486, loss 0.301404, acc 0.8125\n",
      "2017-11-05T17:38:51.429503: step 1487, loss 0.138648, acc 0.9375\n",
      "2017-11-05T17:38:55.410828: step 1488, loss 0.148608, acc 0.90625\n",
      "2017-11-05T17:38:59.361838: step 1489, loss 0.114194, acc 0.96875\n",
      "2017-11-05T17:39:03.290897: step 1490, loss 0.0957126, acc 0.96875\n",
      "2017-11-05T17:39:07.276824: step 1491, loss 0.0721716, acc 0.96875\n",
      "2017-11-05T17:39:11.281068: step 1492, loss 0.190003, acc 0.875\n",
      "2017-11-05T17:39:15.273415: step 1493, loss 0.18432, acc 0.90625\n",
      "2017-11-05T17:39:19.274842: step 1494, loss 0.169254, acc 0.9375\n",
      "2017-11-05T17:39:23.347384: step 1495, loss 0.122334, acc 0.9375\n",
      "2017-11-05T17:39:27.370459: step 1496, loss 0.247627, acc 0.875\n",
      "2017-11-05T17:39:31.390059: step 1497, loss 0.0831721, acc 0.9375\n",
      "2017-11-05T17:39:35.398269: step 1498, loss 0.178116, acc 0.90625\n",
      "2017-11-05T17:39:39.389187: step 1499, loss 0.214686, acc 0.90625\n",
      "2017-11-05T17:39:43.376210: step 1500, loss 0.374456, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:39:45.936968: step 1500, loss 0.877137, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-05T17:39:51.697771: step 1501, loss 0.069613, acc 0.9375\n",
      "2017-11-05T17:39:55.694293: step 1502, loss 0.330998, acc 0.84375\n",
      "2017-11-05T17:39:59.657724: step 1503, loss 0.167718, acc 0.90625\n",
      "2017-11-05T17:40:03.830119: step 1504, loss 0.65175, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T17:40:07.775627: step 1505, loss 0.234298, acc 0.90625\n",
      "2017-11-05T17:40:11.809548: step 1506, loss 0.131283, acc 0.9375\n",
      "2017-11-05T17:40:15.801789: step 1507, loss 0.256359, acc 0.875\n",
      "2017-11-05T17:40:19.792863: step 1508, loss 0.180633, acc 0.9375\n",
      "2017-11-05T17:40:23.840082: step 1509, loss 0.1061, acc 0.96875\n",
      "2017-11-05T17:40:27.816414: step 1510, loss 0.0633309, acc 1\n",
      "2017-11-05T17:40:31.909067: step 1511, loss 0.0367986, acc 1\n",
      "2017-11-05T17:40:34.608767: step 1512, loss 0.389816, acc 0.85\n",
      "2017-11-05T17:40:38.665434: step 1513, loss 0.177116, acc 0.9375\n",
      "2017-11-05T17:40:42.701914: step 1514, loss 0.155016, acc 0.9375\n",
      "2017-11-05T17:40:46.689275: step 1515, loss 0.295177, acc 0.875\n",
      "2017-11-05T17:40:50.674173: step 1516, loss 0.366904, acc 0.875\n",
      "2017-11-05T17:40:54.622157: step 1517, loss 0.243713, acc 0.90625\n",
      "2017-11-05T17:40:58.610947: step 1518, loss 0.294934, acc 0.90625\n",
      "2017-11-05T17:41:02.558496: step 1519, loss 0.166616, acc 0.96875\n",
      "2017-11-05T17:41:06.600400: step 1520, loss 0.0784114, acc 0.9375\n",
      "2017-11-05T17:41:11.036886: step 1521, loss 0.168753, acc 0.90625\n",
      "2017-11-05T17:41:15.990947: step 1522, loss 0.143613, acc 0.96875\n",
      "2017-11-05T17:41:19.944884: step 1523, loss 0.345494, acc 0.84375\n",
      "2017-11-05T17:41:24.024278: step 1524, loss 0.172703, acc 0.9375\n",
      "2017-11-05T17:41:27.958718: step 1525, loss 0.303454, acc 0.875\n",
      "2017-11-05T17:41:31.921357: step 1526, loss 0.242804, acc 0.875\n",
      "2017-11-05T17:41:35.881266: step 1527, loss 0.231043, acc 0.875\n",
      "2017-11-05T17:41:39.903222: step 1528, loss 0.280365, acc 0.875\n",
      "2017-11-05T17:41:43.819367: step 1529, loss 0.0647855, acc 1\n",
      "2017-11-05T17:41:47.795426: step 1530, loss 0.0809454, acc 0.96875\n",
      "2017-11-05T17:41:51.776850: step 1531, loss 0.0806684, acc 0.9375\n",
      "2017-11-05T17:41:55.796695: step 1532, loss 0.0906752, acc 0.96875\n",
      "2017-11-05T17:41:59.883462: step 1533, loss 0.0941015, acc 0.9375\n",
      "2017-11-05T17:42:03.799519: step 1534, loss 0.111635, acc 0.96875\n",
      "2017-11-05T17:42:07.802773: step 1535, loss 0.216837, acc 0.90625\n",
      "2017-11-05T17:42:11.744969: step 1536, loss 0.247067, acc 0.875\n",
      "2017-11-05T17:42:15.711528: step 1537, loss 0.223496, acc 0.9375\n",
      "2017-11-05T17:42:19.673083: step 1538, loss 0.17998, acc 0.875\n",
      "2017-11-05T17:42:23.630976: step 1539, loss 0.270106, acc 0.875\n",
      "2017-11-05T17:42:27.608984: step 1540, loss 0.204031, acc 0.875\n",
      "2017-11-05T17:42:31.602477: step 1541, loss 0.128009, acc 0.9375\n",
      "2017-11-05T17:42:35.799581: step 1542, loss 0.251855, acc 0.875\n",
      "2017-11-05T17:42:39.762478: step 1543, loss 0.163473, acc 0.9375\n",
      "2017-11-05T17:42:43.678580: step 1544, loss 0.304193, acc 0.90625\n",
      "2017-11-05T17:42:47.656396: step 1545, loss 0.289906, acc 0.875\n",
      "2017-11-05T17:42:51.638951: step 1546, loss 0.0957338, acc 0.9375\n",
      "2017-11-05T17:42:55.562957: step 1547, loss 0.0601205, acc 0.96875\n",
      "2017-11-05T17:42:58.051362: step 1548, loss 0.0410884, acc 0.95\n",
      "2017-11-05T17:43:01.994405: step 1549, loss 0.385445, acc 0.90625\n",
      "2017-11-05T17:43:05.906856: step 1550, loss 0.752296, acc 0.84375\n",
      "2017-11-05T17:43:09.854653: step 1551, loss 0.217913, acc 0.9375\n",
      "2017-11-05T17:43:13.909459: step 1552, loss 0.0825149, acc 0.9375\n",
      "2017-11-05T17:43:17.905108: step 1553, loss 0.424499, acc 0.84375\n",
      "2017-11-05T17:43:21.954798: step 1554, loss 0.171065, acc 0.90625\n",
      "2017-11-05T17:43:26.213634: step 1555, loss 0.0952025, acc 0.96875\n",
      "2017-11-05T17:43:30.197628: step 1556, loss 0.113344, acc 0.90625\n",
      "2017-11-05T17:43:34.177096: step 1557, loss 0.274593, acc 0.9375\n",
      "2017-11-05T17:43:38.156311: step 1558, loss 0.621483, acc 0.78125\n",
      "2017-11-05T17:43:42.121068: step 1559, loss 0.290683, acc 0.875\n",
      "2017-11-05T17:43:46.071803: step 1560, loss 0.0990128, acc 0.96875\n",
      "2017-11-05T17:43:50.008607: step 1561, loss 0.27507, acc 0.875\n",
      "2017-11-05T17:43:53.928649: step 1562, loss 0.23729, acc 0.90625\n",
      "2017-11-05T17:43:57.917977: step 1563, loss 0.0349597, acc 1\n",
      "2017-11-05T17:44:01.852521: step 1564, loss 0.153542, acc 0.9375\n",
      "2017-11-05T17:44:05.908855: step 1565, loss 0.0755612, acc 0.96875\n",
      "2017-11-05T17:44:09.929281: step 1566, loss 0.137135, acc 0.96875\n",
      "2017-11-05T17:44:13.937149: step 1567, loss 0.320877, acc 0.84375\n",
      "2017-11-05T17:44:17.923388: step 1568, loss 0.221483, acc 0.90625\n",
      "2017-11-05T17:44:21.957464: step 1569, loss 0.37581, acc 0.875\n",
      "2017-11-05T17:44:25.992217: step 1570, loss 0.0707604, acc 0.96875\n",
      "2017-11-05T17:44:30.038778: step 1571, loss 0.116653, acc 0.96875\n",
      "2017-11-05T17:44:34.146617: step 1572, loss 0.0946503, acc 0.96875\n",
      "2017-11-05T17:44:38.232086: step 1573, loss 0.334972, acc 0.875\n",
      "2017-11-05T17:44:42.253358: step 1574, loss 0.129861, acc 0.96875\n",
      "2017-11-05T17:44:46.152558: step 1575, loss 0.156063, acc 0.90625\n",
      "2017-11-05T17:44:50.110869: step 1576, loss 0.229123, acc 0.90625\n",
      "2017-11-05T17:44:54.085199: step 1577, loss 0.0567865, acc 1\n",
      "2017-11-05T17:44:58.117740: step 1578, loss 0.167138, acc 0.875\n",
      "2017-11-05T17:45:02.058864: step 1579, loss 0.27192, acc 0.875\n",
      "2017-11-05T17:45:06.034985: step 1580, loss 0.0788714, acc 1\n",
      "2017-11-05T17:45:10.042643: step 1581, loss 0.166875, acc 0.9375\n",
      "2017-11-05T17:45:14.010196: step 1582, loss 0.202661, acc 0.90625\n",
      "2017-11-05T17:45:18.013701: step 1583, loss 0.0307357, acc 1\n",
      "2017-11-05T17:45:20.581265: step 1584, loss 0.133054, acc 0.95\n",
      "2017-11-05T17:45:24.650204: step 1585, loss 0.327566, acc 0.84375\n",
      "2017-11-05T17:45:28.617264: step 1586, loss 0.261135, acc 0.9375\n",
      "2017-11-05T17:45:32.597185: step 1587, loss 0.144578, acc 0.90625\n",
      "2017-11-05T17:45:36.632568: step 1588, loss 0.182848, acc 0.90625\n",
      "2017-11-05T17:45:40.607480: step 1589, loss 0.100295, acc 0.9375\n",
      "2017-11-05T17:45:44.533602: step 1590, loss 0.12086, acc 0.9375\n",
      "2017-11-05T17:45:48.530823: step 1591, loss 0.239862, acc 0.84375\n",
      "2017-11-05T17:45:52.532663: step 1592, loss 0.260107, acc 0.8125\n",
      "2017-11-05T17:45:56.491701: step 1593, loss 0.254961, acc 0.875\n",
      "2017-11-05T17:46:00.510902: step 1594, loss 0.164975, acc 0.9375\n",
      "2017-11-05T17:46:04.548737: step 1595, loss 0.115601, acc 0.9375\n",
      "2017-11-05T17:46:08.606346: step 1596, loss 0.179363, acc 0.90625\n",
      "2017-11-05T17:46:12.731435: step 1597, loss 0.168584, acc 0.90625\n",
      "2017-11-05T17:46:16.692006: step 1598, loss 0.109707, acc 0.9375\n",
      "2017-11-05T17:46:20.740611: step 1599, loss 0.229712, acc 0.90625\n",
      "2017-11-05T17:46:24.693406: step 1600, loss 0.194875, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:46:27.314706: step 1600, loss 0.846624, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-05T17:46:32.990430: step 1601, loss 0.234106, acc 0.90625\n",
      "2017-11-05T17:46:37.285133: step 1602, loss 0.203824, acc 0.875\n",
      "2017-11-05T17:46:41.467761: step 1603, loss 0.129388, acc 0.9375\n",
      "2017-11-05T17:46:45.457767: step 1604, loss 0.232078, acc 0.84375\n",
      "2017-11-05T17:46:49.523137: step 1605, loss 0.0939929, acc 0.9375\n",
      "2017-11-05T17:46:53.622949: step 1606, loss 0.328709, acc 0.875\n",
      "2017-11-05T17:46:57.645899: step 1607, loss 0.345876, acc 0.8125\n",
      "2017-11-05T17:47:01.701132: step 1608, loss 0.0796597, acc 0.96875\n",
      "2017-11-05T17:47:05.694377: step 1609, loss 0.211955, acc 0.9375\n",
      "2017-11-05T17:47:09.785086: step 1610, loss 0.0731718, acc 0.96875\n",
      "2017-11-05T17:47:13.888764: step 1611, loss 0.182508, acc 0.90625\n",
      "2017-11-05T17:47:17.913574: step 1612, loss 0.010856, acc 1\n",
      "2017-11-05T17:47:22.005913: step 1613, loss 0.186072, acc 0.9375\n",
      "2017-11-05T17:47:26.043867: step 1614, loss 0.124841, acc 0.9375\n",
      "2017-11-05T17:47:30.097795: step 1615, loss 0.183891, acc 0.90625\n",
      "2017-11-05T17:47:34.197329: step 1616, loss 0.0986909, acc 0.9375\n",
      "2017-11-05T17:47:38.412761: step 1617, loss 0.134987, acc 0.90625\n",
      "2017-11-05T17:47:42.406496: step 1618, loss 0.293723, acc 0.84375\n",
      "2017-11-05T17:47:46.492058: step 1619, loss 0.207042, acc 0.875\n",
      "2017-11-05T17:47:49.157327: step 1620, loss 0.163806, acc 0.95\n",
      "2017-11-05T17:47:53.285414: step 1621, loss 0.193935, acc 0.875\n",
      "2017-11-05T17:47:57.332427: step 1622, loss 0.216936, acc 0.90625\n",
      "2017-11-05T17:48:01.378090: step 1623, loss 0.324501, acc 0.84375\n",
      "2017-11-05T17:48:05.404693: step 1624, loss 0.0881323, acc 0.9375\n",
      "2017-11-05T17:48:09.447768: step 1625, loss 0.121369, acc 0.96875\n",
      "2017-11-05T17:48:13.521346: step 1626, loss 0.258685, acc 0.9375\n",
      "2017-11-05T17:48:17.590789: step 1627, loss 0.51814, acc 0.78125\n",
      "2017-11-05T17:48:21.619876: step 1628, loss 0.272464, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T17:48:25.856701: step 1629, loss 0.125808, acc 0.96875\n",
      "2017-11-05T17:48:29.921755: step 1630, loss 0.463528, acc 0.75\n",
      "2017-11-05T17:48:34.071938: step 1631, loss 0.316859, acc 0.84375\n",
      "2017-11-05T17:48:38.210555: step 1632, loss 0.135663, acc 0.90625\n",
      "2017-11-05T17:48:42.535211: step 1633, loss 0.112687, acc 0.90625\n",
      "2017-11-05T17:48:46.653644: step 1634, loss 0.275015, acc 0.90625\n",
      "2017-11-05T17:48:50.805719: step 1635, loss 0.207707, acc 0.875\n",
      "2017-11-05T17:48:54.893110: step 1636, loss 0.121009, acc 0.9375\n",
      "2017-11-05T17:48:59.071656: step 1637, loss 0.105704, acc 0.96875\n",
      "2017-11-05T17:49:03.197471: step 1638, loss 0.0923778, acc 0.96875\n",
      "2017-11-05T17:49:07.262373: step 1639, loss 0.0353525, acc 1\n",
      "2017-11-05T17:49:11.386326: step 1640, loss 0.132539, acc 0.9375\n",
      "2017-11-05T17:49:15.485806: step 1641, loss 0.266218, acc 0.90625\n",
      "2017-11-05T17:49:19.710274: step 1642, loss 0.275413, acc 0.875\n",
      "2017-11-05T17:49:23.774856: step 1643, loss 0.254669, acc 0.90625\n",
      "2017-11-05T17:49:27.929757: step 1644, loss 0.118442, acc 0.96875\n",
      "2017-11-05T17:49:31.965890: step 1645, loss 0.0928148, acc 0.96875\n",
      "2017-11-05T17:49:36.184667: step 1646, loss 0.105036, acc 0.9375\n",
      "2017-11-05T17:49:40.301147: step 1647, loss 0.143703, acc 0.9375\n",
      "2017-11-05T17:49:44.513381: step 1648, loss 0.344442, acc 0.875\n",
      "2017-11-05T17:49:48.587427: step 1649, loss 0.237562, acc 0.90625\n",
      "2017-11-05T17:49:52.687056: step 1650, loss 0.187881, acc 0.90625\n",
      "2017-11-05T17:49:56.656246: step 1651, loss 0.0834192, acc 0.9375\n",
      "2017-11-05T17:50:00.802611: step 1652, loss 0.149912, acc 0.96875\n",
      "2017-11-05T17:50:04.952388: step 1653, loss 0.380072, acc 0.84375\n",
      "2017-11-05T17:50:08.994574: step 1654, loss 0.424805, acc 0.875\n",
      "2017-11-05T17:50:12.997926: step 1655, loss 0.264476, acc 0.875\n",
      "2017-11-05T17:50:15.523002: step 1656, loss 0.116005, acc 0.95\n",
      "2017-11-05T17:50:19.557209: step 1657, loss 0.127758, acc 0.9375\n",
      "2017-11-05T17:50:23.613408: step 1658, loss 0.28044, acc 0.875\n",
      "2017-11-05T17:50:27.602472: step 1659, loss 0.223924, acc 0.875\n",
      "2017-11-05T17:50:31.578942: step 1660, loss 0.104433, acc 0.96875\n",
      "2017-11-05T17:50:35.718813: step 1661, loss 0.199877, acc 0.90625\n",
      "2017-11-05T17:50:39.732169: step 1662, loss 0.102065, acc 1\n",
      "2017-11-05T17:50:43.718230: step 1663, loss 0.279459, acc 0.875\n",
      "2017-11-05T17:50:47.727273: step 1664, loss 0.143373, acc 0.9375\n",
      "2017-11-05T17:50:51.681011: step 1665, loss 0.193587, acc 0.90625\n",
      "2017-11-05T17:50:55.660667: step 1666, loss 0.0949847, acc 0.96875\n",
      "2017-11-05T17:50:59.653800: step 1667, loss 0.0176461, acc 1\n",
      "2017-11-05T17:51:03.588969: step 1668, loss 0.341478, acc 0.875\n",
      "2017-11-05T17:51:07.621586: step 1669, loss 0.146566, acc 0.90625\n",
      "2017-11-05T17:51:11.602318: step 1670, loss 0.145974, acc 0.90625\n",
      "2017-11-05T17:51:15.588171: step 1671, loss 0.0374509, acc 0.96875\n",
      "2017-11-05T17:51:19.543110: step 1672, loss 0.253767, acc 0.84375\n",
      "2017-11-05T17:51:23.549246: step 1673, loss 0.125868, acc 0.96875\n",
      "2017-11-05T17:51:27.508622: step 1674, loss 0.327075, acc 0.8125\n",
      "2017-11-05T17:51:31.582629: step 1675, loss 0.112491, acc 0.96875\n",
      "2017-11-05T17:51:35.534048: step 1676, loss 0.127781, acc 0.9375\n",
      "2017-11-05T17:51:39.535921: step 1677, loss 0.180423, acc 0.875\n",
      "2017-11-05T17:51:43.507022: step 1678, loss 0.168921, acc 0.90625\n",
      "2017-11-05T17:51:47.484173: step 1679, loss 0.108568, acc 0.9375\n",
      "2017-11-05T17:51:51.484756: step 1680, loss 0.348616, acc 0.84375\n",
      "2017-11-05T17:51:55.492586: step 1681, loss 0.0216578, acc 1\n",
      "2017-11-05T17:51:59.456515: step 1682, loss 0.264895, acc 0.84375\n",
      "2017-11-05T17:52:03.495228: step 1683, loss 0.215959, acc 0.875\n",
      "2017-11-05T17:52:07.534590: step 1684, loss 0.0951559, acc 0.96875\n",
      "2017-11-05T17:52:11.574802: step 1685, loss 0.218461, acc 0.875\n",
      "2017-11-05T17:52:15.577697: step 1686, loss 0.163538, acc 0.90625\n",
      "2017-11-05T17:52:19.495367: step 1687, loss 0.306116, acc 0.8125\n",
      "2017-11-05T17:52:23.503888: step 1688, loss 0.354931, acc 0.8125\n",
      "2017-11-05T17:52:27.445280: step 1689, loss 0.142547, acc 0.9375\n",
      "2017-11-05T17:52:31.391772: step 1690, loss 0.349724, acc 0.875\n",
      "2017-11-05T17:52:35.553608: step 1691, loss 0.27219, acc 0.90625\n",
      "2017-11-05T17:52:38.083721: step 1692, loss 0.338823, acc 0.9\n",
      "2017-11-05T17:52:42.065541: step 1693, loss 0.188337, acc 0.875\n",
      "2017-11-05T17:52:46.108082: step 1694, loss 0.0727427, acc 0.96875\n",
      "2017-11-05T17:52:50.042164: step 1695, loss 0.150487, acc 0.96875\n",
      "2017-11-05T17:52:54.040953: step 1696, loss 0.285362, acc 0.8125\n",
      "2017-11-05T17:52:58.016302: step 1697, loss 0.0349961, acc 1\n",
      "2017-11-05T17:53:02.019001: step 1698, loss 0.0362001, acc 1\n",
      "2017-11-05T17:53:06.176812: step 1699, loss 0.157858, acc 0.875\n",
      "2017-11-05T17:53:10.202875: step 1700, loss 0.118864, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:53:12.817543: step 1700, loss 0.936513, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-05T17:53:18.353510: step 1701, loss 0.284632, acc 0.9375\n",
      "2017-11-05T17:53:22.359082: step 1702, loss 0.194209, acc 0.875\n",
      "2017-11-05T17:53:26.594679: step 1703, loss 0.20907, acc 0.90625\n",
      "2017-11-05T17:53:30.915276: step 1704, loss 0.201529, acc 0.90625\n",
      "2017-11-05T17:53:34.894210: step 1705, loss 0.144663, acc 0.9375\n",
      "2017-11-05T17:53:38.928379: step 1706, loss 0.199131, acc 0.90625\n",
      "2017-11-05T17:53:42.964718: step 1707, loss 0.118347, acc 0.9375\n",
      "2017-11-05T17:53:46.996148: step 1708, loss 0.0819926, acc 1\n",
      "2017-11-05T17:53:51.144411: step 1709, loss 0.235542, acc 0.875\n",
      "2017-11-05T17:53:55.170256: step 1710, loss 0.214721, acc 0.90625\n",
      "2017-11-05T17:53:59.200610: step 1711, loss 0.0437259, acc 1\n",
      "2017-11-05T17:54:03.246712: step 1712, loss 0.144493, acc 0.90625\n",
      "2017-11-05T17:54:07.269041: step 1713, loss 0.154967, acc 0.90625\n",
      "2017-11-05T17:54:11.295999: step 1714, loss 0.17157, acc 0.96875\n",
      "2017-11-05T17:54:15.283597: step 1715, loss 0.24289, acc 0.84375\n",
      "2017-11-05T17:54:19.369481: step 1716, loss 0.0983927, acc 0.9375\n",
      "2017-11-05T17:54:23.414144: step 1717, loss 0.279059, acc 0.875\n",
      "2017-11-05T17:54:27.421389: step 1718, loss 0.111062, acc 0.9375\n",
      "2017-11-05T17:54:31.502508: step 1719, loss 0.237608, acc 0.875\n",
      "2017-11-05T17:54:35.631951: step 1720, loss 0.173488, acc 0.9375\n",
      "2017-11-05T17:54:39.666159: step 1721, loss 0.186032, acc 0.90625\n",
      "2017-11-05T17:54:43.714135: step 1722, loss 0.357499, acc 0.875\n",
      "2017-11-05T17:54:47.880159: step 1723, loss 0.311432, acc 0.90625\n",
      "2017-11-05T17:54:51.857797: step 1724, loss 0.0641061, acc 0.96875\n",
      "2017-11-05T17:54:55.860160: step 1725, loss 0.267416, acc 0.875\n",
      "2017-11-05T17:54:59.882442: step 1726, loss 0.244371, acc 0.875\n",
      "2017-11-05T17:55:03.913184: step 1727, loss 0.310077, acc 0.84375\n",
      "2017-11-05T17:55:06.491738: step 1728, loss 0.313959, acc 0.8\n",
      "2017-11-05T17:55:10.528236: step 1729, loss 0.214933, acc 0.90625\n",
      "2017-11-05T17:55:14.678486: step 1730, loss 0.0830643, acc 0.96875\n",
      "2017-11-05T17:55:18.689136: step 1731, loss 0.162859, acc 0.9375\n",
      "2017-11-05T17:55:22.703020: step 1732, loss 0.237689, acc 0.90625\n",
      "2017-11-05T17:55:26.724626: step 1733, loss 0.168241, acc 0.9375\n",
      "2017-11-05T17:55:30.715283: step 1734, loss 0.0317487, acc 1\n",
      "2017-11-05T17:55:34.679117: step 1735, loss 0.189492, acc 0.875\n",
      "2017-11-05T17:55:38.686265: step 1736, loss 0.0542455, acc 0.96875\n",
      "2017-11-05T17:55:42.654027: step 1737, loss 0.198126, acc 0.84375\n",
      "2017-11-05T17:55:46.713375: step 1738, loss 0.424757, acc 0.875\n",
      "2017-11-05T17:55:50.741600: step 1739, loss 0.126232, acc 0.9375\n",
      "2017-11-05T17:55:54.814104: step 1740, loss 0.296544, acc 0.90625\n",
      "2017-11-05T17:55:58.774071: step 1741, loss 0.323674, acc 0.8125\n",
      "2017-11-05T17:56:02.765232: step 1742, loss 0.150878, acc 0.90625\n",
      "2017-11-05T17:56:06.822546: step 1743, loss 0.199642, acc 0.875\n",
      "2017-11-05T17:56:10.896998: step 1744, loss 0.192278, acc 0.875\n",
      "2017-11-05T17:56:14.921780: step 1745, loss 0.206861, acc 0.9375\n",
      "2017-11-05T17:56:18.973980: step 1746, loss 0.141621, acc 0.9375\n",
      "2017-11-05T17:56:22.981505: step 1747, loss 0.14084, acc 0.90625\n",
      "2017-11-05T17:56:26.961278: step 1748, loss 0.345044, acc 0.90625\n",
      "2017-11-05T17:56:30.920754: step 1749, loss 0.0951115, acc 0.96875\n",
      "2017-11-05T17:56:35.089510: step 1750, loss 0.205987, acc 0.90625\n",
      "2017-11-05T17:56:39.098647: step 1751, loss 0.174753, acc 0.90625\n",
      "2017-11-05T17:56:43.180640: step 1752, loss 0.369749, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T17:56:47.252640: step 1753, loss 0.357353, acc 0.90625\n",
      "2017-11-05T17:56:51.232863: step 1754, loss 0.290666, acc 0.875\n",
      "2017-11-05T17:56:55.236856: step 1755, loss 0.185849, acc 0.90625\n",
      "2017-11-05T17:56:59.238671: step 1756, loss 0.172047, acc 0.90625\n",
      "2017-11-05T17:57:03.285268: step 1757, loss 0.268173, acc 0.90625\n",
      "2017-11-05T17:57:07.279607: step 1758, loss 0.086978, acc 0.9375\n",
      "2017-11-05T17:57:11.283929: step 1759, loss 0.236169, acc 0.875\n",
      "2017-11-05T17:57:15.304377: step 1760, loss 0.477475, acc 0.875\n",
      "2017-11-05T17:57:19.328011: step 1761, loss 0.334912, acc 0.8125\n",
      "2017-11-05T17:57:23.352564: step 1762, loss 0.0775176, acc 1\n",
      "2017-11-05T17:57:27.410264: step 1763, loss 0.111181, acc 0.9375\n",
      "2017-11-05T17:57:30.008137: step 1764, loss 0.139104, acc 0.95\n",
      "2017-11-05T17:57:34.048735: step 1765, loss 0.129075, acc 0.9375\n",
      "2017-11-05T17:57:38.081966: step 1766, loss 0.49898, acc 0.78125\n",
      "2017-11-05T17:57:42.048066: step 1767, loss 0.108116, acc 0.9375\n",
      "2017-11-05T17:57:46.057029: step 1768, loss 0.189823, acc 0.9375\n",
      "2017-11-05T17:57:50.210072: step 1769, loss 0.0796526, acc 0.9375\n",
      "2017-11-05T17:57:54.264462: step 1770, loss 0.232614, acc 0.90625\n",
      "2017-11-05T17:57:58.301665: step 1771, loss 0.419723, acc 0.84375\n",
      "2017-11-05T17:58:02.404044: step 1772, loss 0.169486, acc 0.90625\n",
      "2017-11-05T17:58:06.444472: step 1773, loss 0.154784, acc 0.90625\n",
      "2017-11-05T17:58:10.423696: step 1774, loss 0.13568, acc 0.96875\n",
      "2017-11-05T17:58:14.473916: step 1775, loss 0.0888936, acc 0.9375\n",
      "2017-11-05T17:58:18.554882: step 1776, loss 0.100847, acc 0.96875\n",
      "2017-11-05T17:58:22.620146: step 1777, loss 0.205695, acc 0.90625\n",
      "2017-11-05T17:58:26.663496: step 1778, loss 0.0627046, acc 0.96875\n",
      "2017-11-05T17:58:30.684013: step 1779, loss 0.196856, acc 0.875\n",
      "2017-11-05T17:58:34.921405: step 1780, loss 0.26107, acc 0.84375\n",
      "2017-11-05T17:58:38.968367: step 1781, loss 0.210022, acc 0.90625\n",
      "2017-11-05T17:58:42.978473: step 1782, loss 0.124573, acc 0.96875\n",
      "2017-11-05T17:58:47.012358: step 1783, loss 0.353198, acc 0.90625\n",
      "2017-11-05T17:58:51.044824: step 1784, loss 0.193033, acc 0.90625\n",
      "2017-11-05T17:58:55.111624: step 1785, loss 0.0847847, acc 1\n",
      "2017-11-05T17:58:59.013744: step 1786, loss 0.257728, acc 0.875\n",
      "2017-11-05T17:59:02.967053: step 1787, loss 0.0430849, acc 1\n",
      "2017-11-05T17:59:06.891084: step 1788, loss 0.220943, acc 0.9375\n",
      "2017-11-05T17:59:10.848379: step 1789, loss 0.14767, acc 0.9375\n",
      "2017-11-05T17:59:14.800506: step 1790, loss 0.164529, acc 0.875\n",
      "2017-11-05T17:59:18.762417: step 1791, loss 0.0752642, acc 0.9375\n",
      "2017-11-05T17:59:22.904462: step 1792, loss 0.0359, acc 1\n",
      "2017-11-05T17:59:27.047221: step 1793, loss 0.153386, acc 0.9375\n",
      "2017-11-05T17:59:30.922179: step 1794, loss 0.0794522, acc 0.9375\n",
      "2017-11-05T17:59:34.892098: step 1795, loss 0.315945, acc 0.875\n",
      "2017-11-05T17:59:38.795233: step 1796, loss 0.203032, acc 0.9375\n",
      "2017-11-05T17:59:42.770685: step 1797, loss 0.260379, acc 0.90625\n",
      "2017-11-05T17:59:46.747668: step 1798, loss 0.29922, acc 0.84375\n",
      "2017-11-05T17:59:50.730611: step 1799, loss 0.074618, acc 0.9375\n",
      "2017-11-05T17:59:53.245761: step 1800, loss 0.10636, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T17:59:55.749485: step 1800, loss 0.956887, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-05T18:00:01.511944: step 1801, loss 0.265985, acc 0.875\n",
      "2017-11-05T18:00:05.458894: step 1802, loss 0.261584, acc 0.90625\n",
      "2017-11-05T18:00:09.389607: step 1803, loss 0.0791415, acc 0.9375\n",
      "2017-11-05T18:00:13.304937: step 1804, loss 0.17119, acc 0.90625\n",
      "2017-11-05T18:00:17.282212: step 1805, loss 0.0565079, acc 0.96875\n",
      "2017-11-05T18:00:21.187566: step 1806, loss 0.24733, acc 0.84375\n",
      "2017-11-05T18:00:25.146255: step 1807, loss 0.194301, acc 0.90625\n",
      "2017-11-05T18:00:29.124897: step 1808, loss 0.18737, acc 0.9375\n",
      "2017-11-05T18:00:33.167252: step 1809, loss 0.281055, acc 0.71875\n",
      "2017-11-05T18:00:37.223722: step 1810, loss 0.198017, acc 0.90625\n",
      "2017-11-05T18:00:41.274740: step 1811, loss 0.145498, acc 0.9375\n",
      "2017-11-05T18:00:45.282589: step 1812, loss 0.193406, acc 0.90625\n",
      "2017-11-05T18:00:49.259297: step 1813, loss 0.215814, acc 0.84375\n",
      "2017-11-05T18:00:53.278610: step 1814, loss 0.0658844, acc 0.9375\n",
      "2017-11-05T18:00:57.466812: step 1815, loss 0.219359, acc 0.9375\n",
      "2017-11-05T18:01:01.443870: step 1816, loss 0.0755576, acc 0.96875\n",
      "2017-11-05T18:01:05.393008: step 1817, loss 0.200899, acc 0.875\n",
      "2017-11-05T18:01:09.436046: step 1818, loss 0.334118, acc 0.875\n",
      "2017-11-05T18:01:13.416664: step 1819, loss 0.0415754, acc 1\n",
      "2017-11-05T18:01:17.367083: step 1820, loss 0.228587, acc 0.875\n",
      "2017-11-05T18:01:21.390778: step 1821, loss 0.390655, acc 0.78125\n",
      "2017-11-05T18:01:25.366286: step 1822, loss 0.147347, acc 0.90625\n",
      "2017-11-05T18:01:29.358488: step 1823, loss 0.290554, acc 0.84375\n",
      "2017-11-05T18:01:33.288096: step 1824, loss 0.0127265, acc 1\n",
      "2017-11-05T18:01:37.303277: step 1825, loss 0.264695, acc 0.84375\n",
      "2017-11-05T18:01:41.364957: step 1826, loss 0.181887, acc 0.90625\n",
      "2017-11-05T18:01:45.299271: step 1827, loss 0.303387, acc 0.875\n",
      "2017-11-05T18:01:49.336805: step 1828, loss 0.308408, acc 0.90625\n",
      "2017-11-05T18:01:53.327938: step 1829, loss 0.247591, acc 0.90625\n",
      "2017-11-05T18:01:57.276216: step 1830, loss 0.230116, acc 0.875\n",
      "2017-11-05T18:02:01.333309: step 1831, loss 0.058101, acc 0.96875\n",
      "2017-11-05T18:02:05.271706: step 1832, loss 0.330965, acc 0.8125\n",
      "2017-11-05T18:02:09.333473: step 1833, loss 0.164901, acc 0.90625\n",
      "2017-11-05T18:02:13.294459: step 1834, loss 0.0382653, acc 0.96875\n",
      "2017-11-05T18:02:17.322390: step 1835, loss 0.0332667, acc 1\n",
      "2017-11-05T18:02:19.873579: step 1836, loss 0.296498, acc 0.9\n",
      "2017-11-05T18:02:23.856939: step 1837, loss 0.148633, acc 0.9375\n",
      "2017-11-05T18:02:27.835272: step 1838, loss 0.124381, acc 0.90625\n",
      "2017-11-05T18:02:31.850312: step 1839, loss 0.195757, acc 0.9375\n",
      "2017-11-05T18:02:35.923483: step 1840, loss 0.0941016, acc 0.9375\n",
      "2017-11-05T18:02:40.019008: step 1841, loss 0.225356, acc 0.84375\n",
      "2017-11-05T18:02:43.989900: step 1842, loss 0.297897, acc 0.78125\n",
      "2017-11-05T18:02:48.054030: step 1843, loss 0.10786, acc 0.96875\n",
      "2017-11-05T18:02:52.017259: step 1844, loss 0.151131, acc 0.9375\n",
      "2017-11-05T18:02:56.026191: step 1845, loss 0.32078, acc 0.875\n",
      "2017-11-05T18:02:59.983746: step 1846, loss 0.114376, acc 0.9375\n",
      "2017-11-05T18:03:04.087757: step 1847, loss 0.209981, acc 0.90625\n",
      "2017-11-05T18:03:08.133032: step 1848, loss 0.25906, acc 0.875\n",
      "2017-11-05T18:03:12.134314: step 1849, loss 0.374446, acc 0.875\n",
      "2017-11-05T18:03:16.237424: step 1850, loss 0.222658, acc 0.9375\n",
      "2017-11-05T18:03:20.292490: step 1851, loss 0.298206, acc 0.8125\n",
      "2017-11-05T18:03:24.572730: step 1852, loss 0.0972541, acc 0.96875\n",
      "2017-11-05T18:03:28.623897: step 1853, loss 0.178636, acc 0.90625\n",
      "2017-11-05T18:03:32.671788: step 1854, loss 0.126669, acc 0.96875\n",
      "2017-11-05T18:03:36.658480: step 1855, loss 0.124825, acc 0.9375\n",
      "2017-11-05T18:03:40.648674: step 1856, loss 0.348386, acc 0.78125\n",
      "2017-11-05T18:03:44.717115: step 1857, loss 0.052165, acc 0.96875\n",
      "2017-11-05T18:03:48.817220: step 1858, loss 0.222042, acc 0.875\n",
      "2017-11-05T18:03:52.807417: step 1859, loss 0.195521, acc 0.9375\n",
      "2017-11-05T18:03:56.889974: step 1860, loss 0.14217, acc 0.9375\n",
      "2017-11-05T18:04:00.837028: step 1861, loss 0.133939, acc 0.96875\n",
      "2017-11-05T18:04:04.908758: step 1862, loss 0.229987, acc 0.84375\n",
      "2017-11-05T18:04:08.918553: step 1863, loss 0.242348, acc 0.90625\n",
      "2017-11-05T18:04:12.954942: step 1864, loss 0.111385, acc 0.96875\n",
      "2017-11-05T18:04:16.985033: step 1865, loss 0.177742, acc 0.90625\n",
      "2017-11-05T18:04:20.941494: step 1866, loss 0.157567, acc 0.9375\n",
      "2017-11-05T18:04:24.995004: step 1867, loss 0.150983, acc 0.9375\n",
      "2017-11-05T18:04:28.989084: step 1868, loss 0.145589, acc 0.9375\n",
      "2017-11-05T18:04:33.027087: step 1869, loss 0.227695, acc 0.875\n",
      "2017-11-05T18:04:37.138999: step 1870, loss 0.190698, acc 0.875\n",
      "2017-11-05T18:04:41.216985: step 1871, loss 0.226157, acc 0.84375\n",
      "2017-11-05T18:04:43.822248: step 1872, loss 0.154251, acc 0.85\n",
      "2017-11-05T18:04:47.916685: step 1873, loss 0.134437, acc 0.9375\n",
      "2017-11-05T18:04:51.961878: step 1874, loss 0.102201, acc 0.96875\n",
      "2017-11-05T18:04:56.010796: step 1875, loss 0.22516, acc 0.9375\n",
      "2017-11-05T18:05:00.099112: step 1876, loss 0.160291, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T18:05:04.087011: step 1877, loss 0.346896, acc 0.875\n",
      "2017-11-05T18:05:08.162859: step 1878, loss 0.0245879, acc 1\n",
      "2017-11-05T18:05:12.234277: step 1879, loss 0.12102, acc 0.96875\n",
      "2017-11-05T18:05:16.257382: step 1880, loss 0.15473, acc 0.96875\n",
      "2017-11-05T18:05:20.301375: step 1881, loss 0.0683029, acc 0.9375\n",
      "2017-11-05T18:05:24.328920: step 1882, loss 0.192312, acc 0.90625\n",
      "2017-11-05T18:05:28.374188: step 1883, loss 0.060457, acc 1\n",
      "2017-11-05T18:05:32.389689: step 1884, loss 0.434493, acc 0.75\n",
      "2017-11-05T18:05:36.367356: step 1885, loss 0.259481, acc 0.90625\n",
      "2017-11-05T18:05:40.382773: step 1886, loss 0.0658336, acc 0.96875\n",
      "2017-11-05T18:05:44.432684: step 1887, loss 0.00789816, acc 1\n",
      "2017-11-05T18:05:48.522446: step 1888, loss 0.156893, acc 0.9375\n",
      "2017-11-05T18:05:52.563686: step 1889, loss 0.363375, acc 0.90625\n",
      "2017-11-05T18:05:56.573183: step 1890, loss 0.244312, acc 0.875\n",
      "2017-11-05T18:06:00.672151: step 1891, loss 0.311105, acc 0.875\n",
      "2017-11-05T18:06:04.681262: step 1892, loss 0.284368, acc 0.90625\n",
      "2017-11-05T18:06:08.801340: step 1893, loss 0.311978, acc 0.90625\n",
      "2017-11-05T18:06:12.837088: step 1894, loss 0.465143, acc 0.84375\n",
      "2017-11-05T18:06:16.893814: step 1895, loss 0.238711, acc 0.90625\n",
      "2017-11-05T18:06:20.933376: step 1896, loss 0.312474, acc 0.875\n",
      "2017-11-05T18:06:25.017213: step 1897, loss 0.242334, acc 0.90625\n",
      "2017-11-05T18:06:29.096833: step 1898, loss 0.154807, acc 0.90625\n",
      "2017-11-05T18:06:33.275759: step 1899, loss 0.38227, acc 0.8125\n",
      "2017-11-05T18:06:37.342026: step 1900, loss 0.210654, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T18:06:40.058162: step 1900, loss 0.728383, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-05T18:06:45.812885: step 1901, loss 0.0535461, acc 1\n",
      "2017-11-05T18:06:49.791773: step 1902, loss 0.530206, acc 0.8125\n",
      "2017-11-05T18:06:53.848455: step 1903, loss 0.170238, acc 0.90625\n",
      "2017-11-05T18:06:57.933505: step 1904, loss 0.0881531, acc 0.96875\n",
      "2017-11-05T18:07:01.979031: step 1905, loss 0.147123, acc 0.9375\n",
      "2017-11-05T18:07:06.004660: step 1906, loss 0.141809, acc 0.9375\n",
      "2017-11-05T18:07:10.051819: step 1907, loss 0.310029, acc 0.84375\n",
      "2017-11-05T18:07:12.665522: step 1908, loss 0.00531074, acc 1\n",
      "2017-11-05T18:07:16.706745: step 1909, loss 0.168242, acc 0.9375\n",
      "2017-11-05T18:07:20.920113: step 1910, loss 0.225516, acc 0.9375\n",
      "2017-11-05T18:07:24.944657: step 1911, loss 0.166165, acc 0.90625\n",
      "2017-11-05T18:07:29.029552: step 1912, loss 0.14841, acc 0.90625\n",
      "2017-11-05T18:07:33.102650: step 1913, loss 0.425059, acc 0.84375\n",
      "2017-11-05T18:07:37.159476: step 1914, loss 0.248467, acc 0.875\n",
      "2017-11-05T18:07:41.199067: step 1915, loss 0.234066, acc 0.875\n",
      "2017-11-05T18:07:45.246290: step 1916, loss 0.100153, acc 0.9375\n",
      "2017-11-05T18:07:49.327013: step 1917, loss 0.216831, acc 0.90625\n",
      "2017-11-05T18:07:53.405860: step 1918, loss 0.343166, acc 0.84375\n",
      "2017-11-05T18:07:57.438631: step 1919, loss 0.145245, acc 0.9375\n",
      "2017-11-05T18:08:01.515550: step 1920, loss 0.233139, acc 0.875\n",
      "2017-11-05T18:08:05.526706: step 1921, loss 0.147881, acc 0.9375\n",
      "2017-11-05T18:08:09.573817: step 1922, loss 0.148371, acc 0.90625\n",
      "2017-11-05T18:08:13.560342: step 1923, loss 0.202971, acc 0.84375\n",
      "2017-11-05T18:08:17.623024: step 1924, loss 0.167435, acc 0.90625\n",
      "2017-11-05T18:08:21.719020: step 1925, loss 0.0902557, acc 0.96875\n",
      "2017-11-05T18:08:25.954610: step 1926, loss 0.2142, acc 0.90625\n",
      "2017-11-05T18:08:30.020650: step 1927, loss 0.382236, acc 0.90625\n",
      "2017-11-05T18:08:34.256460: step 1928, loss 0.0423497, acc 0.96875\n",
      "2017-11-05T18:08:38.277042: step 1929, loss 0.230501, acc 0.90625\n",
      "2017-11-05T18:08:42.319308: step 1930, loss 0.254944, acc 0.90625\n",
      "2017-11-05T18:08:46.398138: step 1931, loss 0.182195, acc 0.90625\n",
      "2017-11-05T18:08:50.591893: step 1932, loss 0.219957, acc 0.9375\n",
      "2017-11-05T18:08:54.710931: step 1933, loss 0.198116, acc 0.875\n",
      "2017-11-05T18:08:58.760966: step 1934, loss 0.306889, acc 0.90625\n",
      "2017-11-05T18:09:02.904281: step 1935, loss 0.478141, acc 0.78125\n",
      "2017-11-05T18:09:07.005709: step 1936, loss 0.288704, acc 0.8125\n",
      "2017-11-05T18:09:11.111309: step 1937, loss 0.125799, acc 0.96875\n",
      "2017-11-05T18:09:15.222912: step 1938, loss 0.250427, acc 0.90625\n",
      "2017-11-05T18:09:19.276304: step 1939, loss 0.432159, acc 0.84375\n",
      "2017-11-05T18:09:23.323536: step 1940, loss 0.221814, acc 0.90625\n",
      "2017-11-05T18:09:27.423426: step 1941, loss 0.224093, acc 0.90625\n",
      "2017-11-05T18:09:31.460050: step 1942, loss 0.135416, acc 0.9375\n",
      "2017-11-05T18:09:35.530798: step 1943, loss 0.133396, acc 0.90625\n",
      "2017-11-05T18:09:38.142936: step 1944, loss 0.155385, acc 0.9\n",
      "2017-11-05T18:09:42.219819: step 1945, loss 0.153787, acc 0.90625\n",
      "2017-11-05T18:09:46.321596: step 1946, loss 0.141156, acc 0.90625\n",
      "2017-11-05T18:09:50.365441: step 1947, loss 0.0447931, acc 1\n",
      "2017-11-05T18:09:54.484211: step 1948, loss 0.125221, acc 0.9375\n",
      "2017-11-05T18:09:58.599206: step 1949, loss 0.170693, acc 0.9375\n",
      "2017-11-05T18:10:02.984909: step 1950, loss 0.189446, acc 0.9375\n",
      "2017-11-05T18:10:07.073142: step 1951, loss 0.146603, acc 0.9375\n",
      "2017-11-05T18:10:11.126210: step 1952, loss 0.291613, acc 0.84375\n",
      "2017-11-05T18:10:15.189345: step 1953, loss 0.151867, acc 0.9375\n",
      "2017-11-05T18:10:19.334584: step 1954, loss 0.0668303, acc 0.96875\n",
      "2017-11-05T18:10:23.402515: step 1955, loss 0.0945304, acc 0.96875\n",
      "2017-11-05T18:10:27.456735: step 1956, loss 0.304819, acc 0.84375\n",
      "2017-11-05T18:10:31.489614: step 1957, loss 0.169234, acc 0.9375\n",
      "2017-11-05T18:10:35.730757: step 1958, loss 0.137679, acc 0.9375\n",
      "2017-11-05T18:10:39.723622: step 1959, loss 0.261871, acc 0.90625\n",
      "2017-11-05T18:10:43.755073: step 1960, loss 0.186404, acc 0.90625\n",
      "2017-11-05T18:10:47.851657: step 1961, loss 0.072115, acc 0.96875\n",
      "2017-11-05T18:10:51.909348: step 1962, loss 0.489169, acc 0.75\n",
      "2017-11-05T18:10:55.923246: step 1963, loss 0.315993, acc 0.90625\n",
      "2017-11-05T18:10:59.955463: step 1964, loss 0.227921, acc 0.875\n",
      "2017-11-05T18:11:03.937402: step 1965, loss 0.0836883, acc 0.9375\n",
      "2017-11-05T18:11:08.090902: step 1966, loss 0.270554, acc 0.875\n",
      "2017-11-05T18:11:12.097879: step 1967, loss 0.206313, acc 0.90625\n",
      "2017-11-05T18:11:16.139708: step 1968, loss 0.210644, acc 0.90625\n",
      "2017-11-05T18:11:20.264336: step 1969, loss 0.145184, acc 0.9375\n",
      "2017-11-05T18:11:24.335264: step 1970, loss 0.100724, acc 0.96875\n",
      "2017-11-05T18:11:28.397136: step 1971, loss 0.229371, acc 0.875\n",
      "2017-11-05T18:11:32.476148: step 1972, loss 0.0464259, acc 1\n",
      "2017-11-05T18:11:36.607440: step 1973, loss 0.21812, acc 0.875\n",
      "2017-11-05T18:11:40.695391: step 1974, loss 0.0902406, acc 0.96875\n",
      "2017-11-05T18:11:44.868740: step 1975, loss 0.218379, acc 0.90625\n",
      "2017-11-05T18:11:48.806485: step 1976, loss 0.562524, acc 0.78125\n",
      "2017-11-05T18:11:52.800465: step 1977, loss 0.19917, acc 0.96875\n",
      "2017-11-05T18:11:56.795212: step 1978, loss 0.055551, acc 0.96875\n",
      "2017-11-05T18:12:00.785016: step 1979, loss 0.174654, acc 0.90625\n",
      "2017-11-05T18:12:03.321157: step 1980, loss 0.0076761, acc 1\n",
      "2017-11-05T18:12:07.411704: step 1981, loss 0.177104, acc 0.9375\n",
      "2017-11-05T18:12:11.392300: step 1982, loss 0.104747, acc 0.9375\n",
      "2017-11-05T18:12:15.386251: step 1983, loss 0.133656, acc 0.96875\n",
      "2017-11-05T18:12:19.384662: step 1984, loss 0.342804, acc 0.875\n",
      "2017-11-05T18:12:23.390874: step 1985, loss 0.178582, acc 0.96875\n",
      "2017-11-05T18:12:27.402699: step 1986, loss 0.16898, acc 0.9375\n",
      "2017-11-05T18:12:31.350281: step 1987, loss 0.328947, acc 0.84375\n",
      "2017-11-05T18:12:35.455557: step 1988, loss 0.30956, acc 0.84375\n",
      "2017-11-05T18:12:39.408775: step 1989, loss 0.283064, acc 0.8125\n",
      "2017-11-05T18:12:43.346839: step 1990, loss 0.292137, acc 0.90625\n",
      "2017-11-05T18:12:47.345796: step 1991, loss 0.190365, acc 0.9375\n",
      "2017-11-05T18:12:51.289094: step 1992, loss 0.497858, acc 0.8125\n",
      "2017-11-05T18:12:55.233072: step 1993, loss 0.332646, acc 0.9375\n",
      "2017-11-05T18:12:59.178861: step 1994, loss 0.0417051, acc 1\n",
      "2017-11-05T18:13:03.085445: step 1995, loss 0.10493, acc 0.96875\n",
      "2017-11-05T18:13:07.243836: step 1996, loss 0.188256, acc 0.9375\n",
      "2017-11-05T18:13:11.236640: step 1997, loss 0.19089, acc 0.90625\n",
      "2017-11-05T18:13:15.151176: step 1998, loss 0.303449, acc 0.90625\n",
      "2017-11-05T18:13:19.267717: step 1999, loss 0.148808, acc 0.96875\n",
      "2017-11-05T18:13:23.300861: step 2000, loss 0.211666, acc 0.90625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T18:13:26.101993: step 2000, loss 1.20246, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-05T18:13:31.484781: step 2001, loss 0.577754, acc 0.875\n",
      "2017-11-05T18:13:35.478834: step 2002, loss 0.172542, acc 0.96875\n",
      "2017-11-05T18:13:39.447634: step 2003, loss 0.266754, acc 0.9375\n",
      "2017-11-05T18:13:43.388173: step 2004, loss 0.09739, acc 0.96875\n",
      "2017-11-05T18:13:47.344769: step 2005, loss 0.250074, acc 0.875\n",
      "2017-11-05T18:13:51.357685: step 2006, loss 0.112104, acc 0.96875\n",
      "2017-11-05T18:13:55.272149: step 2007, loss 0.114461, acc 0.9375\n",
      "2017-11-05T18:13:59.242868: step 2008, loss 0.48321, acc 0.8125\n",
      "2017-11-05T18:14:03.183713: step 2009, loss 0.338337, acc 0.875\n",
      "2017-11-05T18:14:07.130930: step 2010, loss 0.11285, acc 0.9375\n",
      "2017-11-05T18:14:11.127500: step 2011, loss 0.342162, acc 0.90625\n",
      "2017-11-05T18:14:15.024868: step 2012, loss 0.24349, acc 0.90625\n",
      "2017-11-05T18:14:19.014697: step 2013, loss 0.397318, acc 0.84375\n",
      "2017-11-05T18:14:22.915068: step 2014, loss 0.406219, acc 0.78125\n",
      "2017-11-05T18:14:26.874525: step 2015, loss 0.115451, acc 0.9375\n",
      "2017-11-05T18:14:29.432847: step 2016, loss 0.187374, acc 0.85\n",
      "2017-11-05T18:14:33.462567: step 2017, loss 0.0847257, acc 0.96875\n",
      "2017-11-05T18:14:37.562684: step 2018, loss 0.200346, acc 0.90625\n",
      "2017-11-05T18:14:41.557750: step 2019, loss 0.0977293, acc 0.96875\n",
      "2017-11-05T18:14:45.563832: step 2020, loss 0.0699931, acc 0.96875\n",
      "2017-11-05T18:14:49.494779: step 2021, loss 0.112814, acc 0.9375\n",
      "2017-11-05T18:14:53.519533: step 2022, loss 0.201451, acc 0.90625\n",
      "2017-11-05T18:14:57.480050: step 2023, loss 0.171225, acc 0.96875\n",
      "2017-11-05T18:15:01.451704: step 2024, loss 0.414659, acc 0.875\n",
      "2017-11-05T18:15:05.452442: step 2025, loss 0.0676261, acc 0.96875\n",
      "2017-11-05T18:15:09.476010: step 2026, loss 0.210123, acc 0.9375\n",
      "2017-11-05T18:15:13.501191: step 2027, loss 0.237586, acc 0.875\n",
      "2017-11-05T18:15:17.484397: step 2028, loss 0.224978, acc 0.90625\n",
      "2017-11-05T18:15:21.522494: step 2029, loss 0.225316, acc 0.9375\n",
      "2017-11-05T18:15:25.537585: step 2030, loss 0.125578, acc 0.90625\n",
      "2017-11-05T18:15:29.495790: step 2031, loss 0.194302, acc 0.90625\n",
      "2017-11-05T18:15:33.449520: step 2032, loss 0.246247, acc 0.90625\n",
      "2017-11-05T18:15:37.370748: step 2033, loss 0.0785112, acc 1\n",
      "2017-11-05T18:15:41.348950: step 2034, loss 0.350849, acc 0.84375\n",
      "2017-11-05T18:15:45.290171: step 2035, loss 0.115919, acc 0.96875\n",
      "2017-11-05T18:15:49.248503: step 2036, loss 0.202568, acc 0.875\n",
      "2017-11-05T18:15:53.261394: step 2037, loss 0.176153, acc 0.9375\n",
      "2017-11-05T18:15:57.222033: step 2038, loss 0.243215, acc 0.90625\n",
      "2017-11-05T18:16:01.142329: step 2039, loss 0.262383, acc 0.84375\n",
      "2017-11-05T18:16:05.143943: step 2040, loss 0.130487, acc 0.9375\n",
      "2017-11-05T18:16:09.220054: step 2041, loss 0.228035, acc 0.875\n",
      "2017-11-05T18:16:13.259864: step 2042, loss 0.0946929, acc 0.9375\n",
      "2017-11-05T18:16:17.267219: step 2043, loss 0.209687, acc 0.90625\n",
      "2017-11-05T18:16:21.249723: step 2044, loss 0.336736, acc 0.8125\n",
      "2017-11-05T18:16:25.173796: step 2045, loss 0.251205, acc 0.875\n",
      "2017-11-05T18:16:29.175227: step 2046, loss 0.0786139, acc 0.96875\n",
      "2017-11-05T18:16:33.190831: step 2047, loss 0.273379, acc 0.875\n",
      "2017-11-05T18:16:37.467801: step 2048, loss 0.28815, acc 0.84375\n",
      "2017-11-05T18:16:41.508029: step 2049, loss 0.134711, acc 0.9375\n",
      "2017-11-05T18:16:45.444264: step 2050, loss 0.147265, acc 0.90625\n",
      "2017-11-05T18:16:49.447129: step 2051, loss 0.243756, acc 0.875\n",
      "2017-11-05T18:16:51.968698: step 2052, loss 0.0656054, acc 0.95\n",
      "2017-11-05T18:16:55.944186: step 2053, loss 0.201492, acc 0.9375\n",
      "2017-11-05T18:16:59.891673: step 2054, loss 0.0296105, acc 1\n",
      "2017-11-05T18:17:03.851424: step 2055, loss 0.287591, acc 0.90625\n",
      "2017-11-05T18:17:07.811470: step 2056, loss 0.253603, acc 0.875\n",
      "2017-11-05T18:17:11.897696: step 2057, loss 0.147358, acc 0.90625\n",
      "2017-11-05T18:17:15.867116: step 2058, loss 0.0745233, acc 0.96875\n",
      "2017-11-05T18:17:19.805247: step 2059, loss 0.307109, acc 0.84375\n",
      "2017-11-05T18:17:23.835143: step 2060, loss 0.121247, acc 0.9375\n",
      "2017-11-05T18:17:27.804359: step 2061, loss 0.167612, acc 0.875\n",
      "2017-11-05T18:17:31.796966: step 2062, loss 0.0501377, acc 0.96875\n",
      "2017-11-05T18:17:35.772424: step 2063, loss 0.336184, acc 0.84375\n",
      "2017-11-05T18:17:39.736092: step 2064, loss 0.0812251, acc 0.9375\n",
      "2017-11-05T18:17:43.682431: step 2065, loss 0.269678, acc 0.90625\n",
      "2017-11-05T18:17:47.646993: step 2066, loss 0.149679, acc 0.9375\n",
      "2017-11-05T18:17:51.679134: step 2067, loss 0.140473, acc 0.9375\n",
      "2017-11-05T18:17:55.645315: step 2068, loss 0.270972, acc 0.90625\n",
      "2017-11-05T18:17:59.622403: step 2069, loss 0.146935, acc 0.9375\n",
      "2017-11-05T18:18:03.629381: step 2070, loss 0.263242, acc 0.84375\n",
      "2017-11-05T18:18:07.588190: step 2071, loss 0.0746413, acc 0.96875\n",
      "2017-11-05T18:18:11.572955: step 2072, loss 0.0675952, acc 0.96875\n",
      "2017-11-05T18:18:15.603564: step 2073, loss 0.301482, acc 0.90625\n",
      "2017-11-05T18:18:19.588503: step 2074, loss 0.283402, acc 0.9375\n",
      "2017-11-05T18:18:23.582848: step 2075, loss 0.223016, acc 0.84375\n",
      "2017-11-05T18:18:27.593776: step 2076, loss 0.0863647, acc 0.96875\n",
      "2017-11-05T18:18:31.579383: step 2077, loss 0.0907593, acc 0.96875\n",
      "2017-11-05T18:18:35.741127: step 2078, loss 0.167429, acc 0.90625\n",
      "2017-11-05T18:18:39.819748: step 2079, loss 0.109233, acc 0.9375\n",
      "2017-11-05T18:18:43.904042: step 2080, loss 0.233373, acc 0.875\n",
      "2017-11-05T18:18:47.948651: step 2081, loss 0.197336, acc 0.9375\n",
      "2017-11-05T18:18:52.002172: step 2082, loss 0.208573, acc 0.875\n",
      "2017-11-05T18:18:56.086427: step 2083, loss 0.357977, acc 0.84375\n",
      "2017-11-05T18:19:00.300103: step 2084, loss 0.192147, acc 0.9375\n",
      "2017-11-05T18:19:04.382744: step 2085, loss 0.197201, acc 0.9375\n",
      "2017-11-05T18:19:08.500335: step 2086, loss 0.202251, acc 0.90625\n",
      "2017-11-05T18:19:12.585201: step 2087, loss 0.0559689, acc 1\n",
      "2017-11-05T18:19:15.293710: step 2088, loss 0.117833, acc 0.95\n",
      "2017-11-05T18:19:19.463330: step 2089, loss 0.111147, acc 0.96875\n",
      "2017-11-05T18:19:23.622387: step 2090, loss 0.116415, acc 0.9375\n",
      "2017-11-05T18:19:28.201610: step 2091, loss 0.302978, acc 0.875\n",
      "2017-11-05T18:19:32.226148: step 2092, loss 0.149384, acc 0.90625\n",
      "2017-11-05T18:19:36.379495: step 2093, loss 0.078347, acc 0.96875\n",
      "2017-11-05T18:19:40.366451: step 2094, loss 0.147975, acc 0.9375\n",
      "2017-11-05T18:19:44.345180: step 2095, loss 0.217721, acc 0.875\n",
      "2017-11-05T18:19:48.271068: step 2096, loss 0.0232184, acc 1\n",
      "2017-11-05T18:19:52.286658: step 2097, loss 0.130045, acc 0.9375\n",
      "2017-11-05T18:19:56.244125: step 2098, loss 0.220481, acc 0.90625\n",
      "2017-11-05T18:20:00.379227: step 2099, loss 0.0342017, acc 1\n",
      "2017-11-05T18:20:04.506040: step 2100, loss 0.327129, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T18:20:07.065914: step 2100, loss 0.954465, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-05T18:20:12.636494: step 2101, loss 0.0681365, acc 0.96875\n",
      "2017-11-05T18:20:16.647858: step 2102, loss 0.440886, acc 0.78125\n",
      "2017-11-05T18:20:20.674718: step 2103, loss 0.132729, acc 0.9375\n",
      "2017-11-05T18:20:24.714076: step 2104, loss 0.0877246, acc 0.96875\n",
      "2017-11-05T18:20:28.707593: step 2105, loss 0.146702, acc 0.9375\n",
      "2017-11-05T18:20:32.747464: step 2106, loss 0.129047, acc 0.9375\n",
      "2017-11-05T18:20:36.816929: step 2107, loss 0.09403, acc 0.96875\n",
      "2017-11-05T18:20:40.797474: step 2108, loss 0.227944, acc 0.90625\n",
      "2017-11-05T18:20:44.790221: step 2109, loss 0.407523, acc 0.84375\n",
      "2017-11-05T18:20:48.844542: step 2110, loss 0.163017, acc 0.90625\n",
      "2017-11-05T18:20:52.807209: step 2111, loss 0.285488, acc 0.875\n",
      "2017-11-05T18:20:56.767789: step 2112, loss 0.303713, acc 0.875\n",
      "2017-11-05T18:21:00.743665: step 2113, loss 0.15596, acc 0.90625\n",
      "2017-11-05T18:21:04.738566: step 2114, loss 0.166365, acc 0.9375\n",
      "2017-11-05T18:21:08.711353: step 2115, loss 0.331538, acc 0.90625\n",
      "2017-11-05T18:21:12.690138: step 2116, loss 0.228868, acc 0.90625\n",
      "2017-11-05T18:21:16.649468: step 2117, loss 0.0834949, acc 0.96875\n",
      "2017-11-05T18:21:20.665530: step 2118, loss 0.0679684, acc 0.9375\n",
      "2017-11-05T18:21:24.632832: step 2119, loss 0.120535, acc 0.9375\n",
      "2017-11-05T18:21:28.607890: step 2120, loss 0.195299, acc 0.9375\n",
      "2017-11-05T18:21:32.536693: step 2121, loss 0.157068, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T18:21:36.528130: step 2122, loss 0.308762, acc 0.90625\n",
      "2017-11-05T18:21:40.479923: step 2123, loss 0.255055, acc 0.84375\n",
      "2017-11-05T18:21:42.954010: step 2124, loss 0.320376, acc 0.8\n",
      "2017-11-05T18:21:46.899668: step 2125, loss 0.261266, acc 0.90625\n",
      "2017-11-05T18:21:50.845665: step 2126, loss 0.174794, acc 0.90625\n",
      "2017-11-05T18:21:54.848168: step 2127, loss 0.192192, acc 0.84375\n",
      "2017-11-05T18:21:58.849796: step 2128, loss 0.381166, acc 0.84375\n",
      "2017-11-05T18:22:02.829270: step 2129, loss 0.199137, acc 0.90625\n",
      "2017-11-05T18:22:06.774302: step 2130, loss 0.320485, acc 0.90625\n",
      "2017-11-05T18:22:10.837541: step 2131, loss 0.178357, acc 0.9375\n",
      "2017-11-05T18:22:14.840817: step 2132, loss 0.206071, acc 0.90625\n",
      "2017-11-05T18:22:18.813553: step 2133, loss 0.0932517, acc 0.96875\n",
      "2017-11-05T18:22:22.742062: step 2134, loss 0.124723, acc 0.9375\n",
      "2017-11-05T18:22:26.754681: step 2135, loss 0.439148, acc 0.875\n",
      "2017-11-05T18:22:30.735058: step 2136, loss 0.119352, acc 0.96875\n",
      "2017-11-05T18:22:34.896028: step 2137, loss 0.17069, acc 0.9375\n",
      "2017-11-05T18:22:38.858655: step 2138, loss 0.0603461, acc 0.96875\n",
      "2017-11-05T18:22:42.917846: step 2139, loss 0.204478, acc 0.9375\n",
      "2017-11-05T18:22:46.893668: step 2140, loss 0.0441539, acc 1\n",
      "2017-11-05T18:22:50.824769: step 2141, loss 0.3177, acc 0.84375\n",
      "2017-11-05T18:22:54.774414: step 2142, loss 0.287336, acc 0.875\n",
      "2017-11-05T18:22:58.743977: step 2143, loss 0.201472, acc 0.90625\n",
      "2017-11-05T18:23:02.741381: step 2144, loss 0.259406, acc 0.90625\n",
      "2017-11-05T18:23:06.684114: step 2145, loss 0.142154, acc 0.90625\n",
      "2017-11-05T18:23:10.654449: step 2146, loss 0.236005, acc 0.875\n",
      "2017-11-05T18:23:14.684841: step 2147, loss 0.286314, acc 0.875\n",
      "2017-11-05T18:23:18.626344: step 2148, loss 0.484467, acc 0.78125\n",
      "2017-11-05T18:23:22.693059: step 2149, loss 0.211231, acc 0.90625\n",
      "2017-11-05T18:23:26.722937: step 2150, loss 0.221352, acc 0.90625\n",
      "2017-11-05T18:23:30.641508: step 2151, loss 0.21445, acc 0.875\n",
      "2017-11-05T18:23:34.648922: step 2152, loss 0.0652327, acc 0.96875\n",
      "2017-11-05T18:23:38.608686: step 2153, loss 0.0419104, acc 1\n",
      "2017-11-05T18:23:42.623399: step 2154, loss 0.376764, acc 0.84375\n",
      "2017-11-05T18:23:46.634865: step 2155, loss 0.117801, acc 0.9375\n",
      "2017-11-05T18:23:50.573211: step 2156, loss 0.187898, acc 0.9375\n",
      "2017-11-05T18:23:54.605931: step 2157, loss 0.14985, acc 0.96875\n",
      "2017-11-05T18:23:58.537663: step 2158, loss 0.164901, acc 0.9375\n",
      "2017-11-05T18:24:02.555197: step 2159, loss 0.0592188, acc 0.96875\n",
      "2017-11-05T18:24:05.058717: step 2160, loss 0.596272, acc 0.75\n",
      "2017-11-05T18:24:09.026620: step 2161, loss 0.21758, acc 0.875\n",
      "2017-11-05T18:24:13.012785: step 2162, loss 0.326569, acc 0.90625\n",
      "2017-11-05T18:24:16.994005: step 2163, loss 0.191882, acc 0.90625\n",
      "2017-11-05T18:24:20.971378: step 2164, loss 0.18978, acc 0.9375\n",
      "2017-11-05T18:24:25.115147: step 2165, loss 0.0723499, acc 0.96875\n",
      "2017-11-05T18:24:29.276485: step 2166, loss 0.28293, acc 0.84375\n",
      "2017-11-05T18:24:33.340519: step 2167, loss 0.163925, acc 0.9375\n",
      "2017-11-05T18:24:37.437683: step 2168, loss 0.231252, acc 0.9375\n",
      "2017-11-05T18:24:41.471494: step 2169, loss 0.22896, acc 0.875\n",
      "2017-11-05T18:24:45.463118: step 2170, loss 0.200135, acc 0.875\n",
      "2017-11-05T18:24:49.425947: step 2171, loss 0.237172, acc 0.84375\n",
      "2017-11-05T18:24:53.446357: step 2172, loss 0.218988, acc 0.9375\n",
      "2017-11-05T18:24:57.397700: step 2173, loss 0.273402, acc 0.875\n",
      "2017-11-05T18:25:01.419789: step 2174, loss 0.173205, acc 0.875\n",
      "2017-11-05T18:25:05.363895: step 2175, loss 0.027508, acc 1\n",
      "2017-11-05T18:25:09.379920: step 2176, loss 0.313584, acc 0.8125\n",
      "2017-11-05T18:25:13.348222: step 2177, loss 0.241984, acc 0.875\n",
      "2017-11-05T18:25:17.403469: step 2178, loss 0.099193, acc 0.96875\n",
      "2017-11-05T18:25:21.444320: step 2179, loss 0.0591475, acc 0.96875\n",
      "2017-11-05T18:25:25.532674: step 2180, loss 0.0672939, acc 0.96875\n",
      "2017-11-05T18:25:29.556075: step 2181, loss 0.114356, acc 0.96875\n",
      "2017-11-05T18:25:33.631052: step 2182, loss 0.111459, acc 0.9375\n",
      "2017-11-05T18:25:37.685692: step 2183, loss 0.0462489, acc 1\n",
      "2017-11-05T18:25:41.646462: step 2184, loss 0.346059, acc 0.875\n",
      "2017-11-05T18:25:45.620621: step 2185, loss 0.169779, acc 0.90625\n",
      "2017-11-05T18:25:49.619520: step 2186, loss 0.327003, acc 0.8125\n",
      "2017-11-05T18:25:53.639163: step 2187, loss 0.147373, acc 0.90625\n",
      "2017-11-05T18:25:57.654407: step 2188, loss 0.0848459, acc 0.96875\n",
      "2017-11-05T18:26:01.687414: step 2189, loss 0.105094, acc 0.96875\n",
      "2017-11-05T18:26:05.753877: step 2190, loss 0.157457, acc 0.9375\n",
      "2017-11-05T18:26:09.789732: step 2191, loss 0.245005, acc 0.90625\n",
      "2017-11-05T18:26:13.844384: step 2192, loss 0.0787569, acc 0.96875\n",
      "2017-11-05T18:26:17.873130: step 2193, loss 0.246139, acc 0.90625\n",
      "2017-11-05T18:26:21.892603: step 2194, loss 0.28071, acc 0.84375\n",
      "2017-11-05T18:26:25.942369: step 2195, loss 0.107278, acc 0.96875\n",
      "2017-11-05T18:26:28.576970: step 2196, loss 0.693244, acc 0.75\n",
      "2017-11-05T18:26:32.594950: step 2197, loss 0.18329, acc 0.9375\n",
      "2017-11-05T18:26:36.700809: step 2198, loss 0.225223, acc 0.90625\n",
      "2017-11-05T18:26:40.684737: step 2199, loss 0.196104, acc 0.90625\n",
      "2017-11-05T18:26:44.694332: step 2200, loss 0.237471, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T18:26:47.196278: step 2200, loss 0.721683, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-05T18:26:52.377373: step 2201, loss 0.294397, acc 0.90625\n",
      "2017-11-05T18:26:56.324223: step 2202, loss 0.0999512, acc 0.96875\n",
      "2017-11-05T18:27:00.351133: step 2203, loss 0.114945, acc 0.9375\n",
      "2017-11-05T18:27:04.321071: step 2204, loss 0.27393, acc 0.90625\n",
      "2017-11-05T18:27:08.308436: step 2205, loss 0.271306, acc 0.90625\n",
      "2017-11-05T18:27:12.305902: step 2206, loss 0.280934, acc 0.84375\n",
      "2017-11-05T18:27:16.305814: step 2207, loss 0.339546, acc 0.90625\n",
      "2017-11-05T18:27:20.316122: step 2208, loss 0.167594, acc 0.90625\n",
      "2017-11-05T18:27:24.303789: step 2209, loss 0.235537, acc 0.875\n",
      "2017-11-05T18:27:28.295353: step 2210, loss 0.136318, acc 0.96875\n",
      "2017-11-05T18:27:32.326156: step 2211, loss 0.0593155, acc 0.96875\n",
      "2017-11-05T18:27:36.340743: step 2212, loss 0.0952989, acc 0.90625\n",
      "2017-11-05T18:27:40.347243: step 2213, loss 0.234855, acc 0.90625\n",
      "2017-11-05T18:27:44.375111: step 2214, loss 0.304226, acc 0.875\n",
      "2017-11-05T18:27:48.427240: step 2215, loss 0.122402, acc 0.90625\n",
      "2017-11-05T18:27:52.355905: step 2216, loss 0.243853, acc 0.90625\n",
      "2017-11-05T18:27:56.322111: step 2217, loss 0.267897, acc 0.875\n",
      "2017-11-05T18:28:00.324019: step 2218, loss 0.238523, acc 0.875\n",
      "2017-11-05T18:28:04.231974: step 2219, loss 0.152018, acc 0.90625\n",
      "2017-11-05T18:28:08.290848: step 2220, loss 0.274912, acc 0.875\n",
      "2017-11-05T18:28:12.293777: step 2221, loss 0.396249, acc 0.875\n",
      "2017-11-05T18:28:16.320299: step 2222, loss 0.0893302, acc 0.96875\n",
      "2017-11-05T18:28:20.337738: step 2223, loss 0.321598, acc 0.875\n",
      "2017-11-05T18:28:24.484711: step 2224, loss 0.1455, acc 0.90625\n",
      "2017-11-05T18:28:28.523307: step 2225, loss 0.115644, acc 0.96875\n",
      "2017-11-05T18:28:32.453641: step 2226, loss 0.201148, acc 0.875\n",
      "2017-11-05T18:28:36.573204: step 2227, loss 0.144742, acc 0.90625\n",
      "2017-11-05T18:28:40.576813: step 2228, loss 0.422509, acc 0.78125\n",
      "2017-11-05T18:28:44.575146: step 2229, loss 0.323352, acc 0.875\n",
      "2017-11-05T18:28:48.533824: step 2230, loss 0.0270436, acc 1\n",
      "2017-11-05T18:28:52.561947: step 2231, loss 0.111109, acc 0.9375\n",
      "2017-11-05T18:28:55.105645: step 2232, loss 0.332086, acc 0.85\n",
      "2017-11-05T18:28:59.136663: step 2233, loss 0.214553, acc 0.9375\n",
      "2017-11-05T18:29:03.128968: step 2234, loss 0.134536, acc 0.9375\n",
      "2017-11-05T18:29:07.155788: step 2235, loss 0.100086, acc 0.96875\n",
      "2017-11-05T18:29:11.070990: step 2236, loss 0.0224579, acc 1\n",
      "2017-11-05T18:29:15.111880: step 2237, loss 0.0703257, acc 0.96875\n",
      "2017-11-05T18:29:19.244405: step 2238, loss 0.241641, acc 0.875\n",
      "2017-11-05T18:29:23.229277: step 2239, loss 0.144058, acc 0.9375\n",
      "2017-11-05T18:29:27.170716: step 2240, loss 0.184162, acc 0.9375\n",
      "2017-11-05T18:29:31.212784: step 2241, loss 0.0251164, acc 1\n",
      "2017-11-05T18:29:35.168683: step 2242, loss 0.267358, acc 0.84375\n",
      "2017-11-05T18:29:39.125105: step 2243, loss 0.31026, acc 0.875\n",
      "2017-11-05T18:29:43.104903: step 2244, loss 0.193896, acc 0.90625\n",
      "2017-11-05T18:29:47.031634: step 2245, loss 0.090724, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T18:29:51.074899: step 2246, loss 0.179339, acc 0.9375\n",
      "2017-11-05T18:29:55.070819: step 2247, loss 0.183526, acc 0.90625\n",
      "2017-11-05T18:29:59.035868: step 2248, loss 0.0414414, acc 1\n",
      "2017-11-05T18:30:03.268519: step 2249, loss 0.403385, acc 0.84375\n",
      "2017-11-05T18:30:07.276647: step 2250, loss 0.130218, acc 0.9375\n",
      "2017-11-05T18:30:11.358804: step 2251, loss 0.21357, acc 0.90625\n",
      "2017-11-05T18:30:15.316020: step 2252, loss 0.286363, acc 0.875\n",
      "2017-11-05T18:30:19.303703: step 2253, loss 0.299677, acc 0.8125\n",
      "2017-11-05T18:30:23.274749: step 2254, loss 0.447161, acc 0.90625\n",
      "2017-11-05T18:30:27.293560: step 2255, loss 0.0968234, acc 0.96875\n",
      "2017-11-05T18:30:31.324912: step 2256, loss 0.128114, acc 0.9375\n",
      "2017-11-05T18:30:35.509566: step 2257, loss 0.298707, acc 0.875\n",
      "2017-11-05T18:30:39.498242: step 2258, loss 0.178155, acc 0.90625\n",
      "2017-11-05T18:30:43.575206: step 2259, loss 0.134467, acc 0.96875\n",
      "2017-11-05T18:30:47.607766: step 2260, loss 0.197734, acc 0.90625\n",
      "2017-11-05T18:30:51.609746: step 2261, loss 0.211051, acc 0.875\n",
      "2017-11-05T18:30:55.638656: step 2262, loss 0.197942, acc 0.90625\n",
      "2017-11-05T18:30:59.607015: step 2263, loss 0.230005, acc 0.90625\n",
      "2017-11-05T18:31:03.659804: step 2264, loss 0.259295, acc 0.8125\n",
      "2017-11-05T18:31:07.668327: step 2265, loss 0.0961537, acc 0.9375\n",
      "2017-11-05T18:31:11.808262: step 2266, loss 0.14539, acc 0.90625\n",
      "2017-11-05T18:31:15.892338: step 2267, loss 0.161181, acc 0.9375\n",
      "2017-11-05T18:31:18.491872: step 2268, loss 0.467497, acc 0.85\n",
      "2017-11-05T18:31:22.456041: step 2269, loss 0.0822034, acc 0.9375\n",
      "2017-11-05T18:31:26.521454: step 2270, loss 0.0830336, acc 0.96875\n",
      "2017-11-05T18:31:30.584096: step 2271, loss 0.241911, acc 0.84375\n",
      "2017-11-05T18:31:34.659172: step 2272, loss 0.389984, acc 0.78125\n",
      "2017-11-05T18:31:38.720293: step 2273, loss 0.130627, acc 0.9375\n",
      "2017-11-05T18:31:42.756271: step 2274, loss 0.156758, acc 0.875\n",
      "2017-11-05T18:31:46.703294: step 2275, loss 0.224303, acc 0.90625\n",
      "2017-11-05T18:31:50.775050: step 2276, loss 0.07854, acc 0.96875\n",
      "2017-11-05T18:31:54.755818: step 2277, loss 0.0203077, acc 1\n",
      "2017-11-05T18:31:58.840830: step 2278, loss 0.271885, acc 0.875\n",
      "2017-11-05T18:32:02.837200: step 2279, loss 0.240605, acc 0.875\n",
      "2017-11-05T18:32:06.774681: step 2280, loss 0.0876979, acc 0.9375\n",
      "2017-11-05T18:32:10.771630: step 2281, loss 0.0760199, acc 0.96875\n",
      "2017-11-05T18:32:14.821576: step 2282, loss 0.226282, acc 0.875\n",
      "2017-11-05T18:32:18.736214: step 2283, loss 0.183142, acc 0.9375\n",
      "2017-11-05T18:32:22.720221: step 2284, loss 0.104287, acc 0.9375\n",
      "2017-11-05T18:32:26.694045: step 2285, loss 0.119276, acc 0.90625\n",
      "2017-11-05T18:32:30.675131: step 2286, loss 0.199972, acc 0.875\n",
      "2017-11-05T18:32:34.859083: step 2287, loss 0.212609, acc 0.875\n",
      "2017-11-05T18:32:39.015123: step 2288, loss 0.307987, acc 0.8125\n",
      "2017-11-05T18:32:42.971405: step 2289, loss 0.275489, acc 0.9375\n",
      "2017-11-05T18:32:46.978345: step 2290, loss 0.254146, acc 0.875\n",
      "2017-11-05T18:32:51.029886: step 2291, loss 0.287385, acc 0.84375\n",
      "2017-11-05T18:32:55.116719: step 2292, loss 0.161389, acc 0.9375\n",
      "2017-11-05T18:32:59.137019: step 2293, loss 0.134333, acc 0.9375\n",
      "2017-11-05T18:33:03.182852: step 2294, loss 0.426353, acc 0.875\n",
      "2017-11-05T18:33:07.124433: step 2295, loss 0.225285, acc 0.875\n",
      "2017-11-05T18:33:11.096109: step 2296, loss 0.0652018, acc 0.9375\n",
      "2017-11-05T18:33:15.084040: step 2297, loss 0.308028, acc 0.875\n",
      "2017-11-05T18:33:19.093643: step 2298, loss 0.146224, acc 0.9375\n",
      "2017-11-05T18:33:23.402281: step 2299, loss 0.163, acc 0.875\n",
      "2017-11-05T18:33:27.585285: step 2300, loss 0.242191, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T18:33:30.181038: step 2300, loss 0.833822, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-05T18:33:35.409763: step 2301, loss 0.0735496, acc 0.96875\n",
      "2017-11-05T18:33:39.400098: step 2302, loss 0.100562, acc 0.9375\n",
      "2017-11-05T18:33:43.308330: step 2303, loss 0.22895, acc 0.90625\n",
      "2017-11-05T18:33:45.940055: step 2304, loss 0.127961, acc 0.9\n",
      "2017-11-05T18:33:50.006898: step 2305, loss 0.0307108, acc 1\n",
      "2017-11-05T18:33:54.043629: step 2306, loss 0.147545, acc 0.90625\n",
      "2017-11-05T18:33:58.068174: step 2307, loss 0.132838, acc 0.90625\n",
      "2017-11-05T18:34:02.056431: step 2308, loss 0.0499655, acc 1\n",
      "2017-11-05T18:34:06.079958: step 2309, loss 0.1923, acc 0.90625\n",
      "2017-11-05T18:34:10.090036: step 2310, loss 0.198068, acc 0.9375\n",
      "2017-11-05T18:34:14.131352: step 2311, loss 0.338134, acc 0.875\n",
      "2017-11-05T18:34:18.140262: step 2312, loss 0.334438, acc 0.90625\n",
      "2017-11-05T18:34:22.136824: step 2313, loss 0.298134, acc 0.84375\n",
      "2017-11-05T18:34:26.159119: step 2314, loss 0.0672137, acc 0.9375\n",
      "2017-11-05T18:34:30.131170: step 2315, loss 0.190544, acc 0.9375\n",
      "2017-11-05T18:34:34.316467: step 2316, loss 0.0364094, acc 1\n",
      "2017-11-05T18:34:38.391830: step 2317, loss 0.209047, acc 0.875\n",
      "2017-11-05T18:34:42.321864: step 2318, loss 0.167964, acc 0.9375\n",
      "2017-11-05T18:34:46.326067: step 2319, loss 0.144393, acc 0.90625\n",
      "2017-11-05T18:34:50.349853: step 2320, loss 0.110391, acc 0.96875\n",
      "2017-11-05T18:34:54.419702: step 2321, loss 0.30496, acc 0.90625\n",
      "2017-11-05T18:34:58.412002: step 2322, loss 0.217725, acc 0.875\n",
      "2017-11-05T18:35:02.418659: step 2323, loss 0.0816031, acc 0.9375\n",
      "2017-11-05T18:35:06.421522: step 2324, loss 0.32884, acc 0.84375\n",
      "2017-11-05T18:35:10.426963: step 2325, loss 0.201296, acc 0.875\n",
      "2017-11-05T18:35:14.475177: step 2326, loss 0.226839, acc 0.90625\n",
      "2017-11-05T18:35:18.564871: step 2327, loss 0.239237, acc 0.875\n",
      "2017-11-05T18:35:22.592522: step 2328, loss 0.16041, acc 0.9375\n",
      "2017-11-05T18:35:26.610004: step 2329, loss 0.0327247, acc 1\n",
      "2017-11-05T18:35:30.731396: step 2330, loss 0.0708587, acc 0.96875\n",
      "2017-11-05T18:35:34.759614: step 2331, loss 0.203875, acc 0.90625\n",
      "2017-11-05T18:35:38.862332: step 2332, loss 0.342713, acc 0.84375\n",
      "2017-11-05T18:35:42.963000: step 2333, loss 0.289307, acc 0.90625\n",
      "2017-11-05T18:35:47.010946: step 2334, loss 0.385029, acc 0.84375\n",
      "2017-11-05T18:35:51.077763: step 2335, loss 0.172495, acc 0.90625\n",
      "2017-11-05T18:35:55.150174: step 2336, loss 0.431265, acc 0.875\n",
      "2017-11-05T18:35:59.233185: step 2337, loss 0.478112, acc 0.84375\n",
      "2017-11-05T18:36:03.306661: step 2338, loss 0.172284, acc 0.9375\n",
      "2017-11-05T18:36:07.395073: step 2339, loss 0.385441, acc 0.8125\n",
      "2017-11-05T18:36:09.988653: step 2340, loss 0.194817, acc 0.9\n",
      "2017-11-05T18:36:13.995954: step 2341, loss 0.136835, acc 0.9375\n",
      "2017-11-05T18:36:18.032147: step 2342, loss 0.161407, acc 0.875\n",
      "2017-11-05T18:36:22.056493: step 2343, loss 0.111939, acc 0.90625\n",
      "2017-11-05T18:36:26.086561: step 2344, loss 0.197144, acc 0.9375\n",
      "2017-11-05T18:36:30.075228: step 2345, loss 0.178335, acc 0.90625\n",
      "2017-11-05T18:36:34.259153: step 2346, loss 0.234485, acc 0.875\n",
      "2017-11-05T18:36:38.360035: step 2347, loss 0.250257, acc 0.875\n",
      "2017-11-05T18:36:42.450893: step 2348, loss 0.283491, acc 0.84375\n",
      "2017-11-05T18:36:46.509331: step 2349, loss 0.016127, acc 1\n",
      "2017-11-05T18:36:50.539805: step 2350, loss 0.21233, acc 0.90625\n",
      "2017-11-05T18:36:54.621303: step 2351, loss 0.0867186, acc 0.96875\n",
      "2017-11-05T18:36:58.641434: step 2352, loss 0.0873073, acc 0.9375\n",
      "2017-11-05T18:37:02.640961: step 2353, loss 0.123981, acc 0.90625\n",
      "2017-11-05T18:37:06.649697: step 2354, loss 0.142167, acc 0.90625\n",
      "2017-11-05T18:37:10.809626: step 2355, loss 0.127744, acc 0.96875\n",
      "2017-11-05T18:37:14.811854: step 2356, loss 0.293207, acc 0.84375\n",
      "2017-11-05T18:37:18.872650: step 2357, loss 0.18173, acc 0.96875\n",
      "2017-11-05T18:37:22.910472: step 2358, loss 0.252886, acc 0.90625\n",
      "2017-11-05T18:37:26.933476: step 2359, loss 0.223397, acc 0.90625\n",
      "2017-11-05T18:37:30.879620: step 2360, loss 0.123296, acc 0.9375\n",
      "2017-11-05T18:37:34.919974: step 2361, loss 0.453616, acc 0.84375\n",
      "2017-11-05T18:37:38.984244: step 2362, loss 0.162205, acc 0.9375\n",
      "2017-11-05T18:37:43.036318: step 2363, loss 0.226543, acc 0.90625\n",
      "2017-11-05T18:37:47.007845: step 2364, loss 0.180312, acc 0.90625\n",
      "2017-11-05T18:37:51.056129: step 2365, loss 0.231286, acc 0.90625\n",
      "2017-11-05T18:37:55.109720: step 2366, loss 0.203175, acc 0.90625\n",
      "2017-11-05T18:37:59.088695: step 2367, loss 0.357473, acc 0.84375\n",
      "2017-11-05T18:38:03.136150: step 2368, loss 0.176895, acc 0.90625\n",
      "2017-11-05T18:38:07.118684: step 2369, loss 0.143166, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T18:38:11.183815: step 2370, loss 0.122168, acc 0.9375\n",
      "2017-11-05T18:38:15.121321: step 2371, loss 0.0997852, acc 0.96875\n",
      "2017-11-05T18:38:19.190824: step 2372, loss 0.0945978, acc 0.96875\n",
      "2017-11-05T18:38:23.420820: step 2373, loss 0.130906, acc 0.96875\n",
      "2017-11-05T18:38:27.669251: step 2374, loss 0.128607, acc 0.9375\n",
      "2017-11-05T18:38:31.650363: step 2375, loss 0.292422, acc 0.9375\n",
      "2017-11-05T18:38:34.389759: step 2376, loss 0.540922, acc 0.75\n",
      "2017-11-05T18:38:38.470337: step 2377, loss 0.307786, acc 0.84375\n",
      "2017-11-05T18:38:42.454017: step 2378, loss 0.194036, acc 0.875\n",
      "2017-11-05T18:38:46.512624: step 2379, loss 0.203037, acc 0.90625\n",
      "2017-11-05T18:38:50.548537: step 2380, loss 0.144183, acc 0.96875\n",
      "2017-11-05T18:38:54.589599: step 2381, loss 0.123444, acc 0.90625\n",
      "2017-11-05T18:38:58.584300: step 2382, loss 0.280456, acc 0.8125\n",
      "2017-11-05T18:39:02.583400: step 2383, loss 0.138352, acc 0.9375\n",
      "2017-11-05T18:39:06.571394: step 2384, loss 0.339992, acc 0.90625\n",
      "2017-11-05T18:39:10.602771: step 2385, loss 0.172287, acc 0.9375\n",
      "2017-11-05T18:39:14.590816: step 2386, loss 0.0873105, acc 0.9375\n",
      "2017-11-05T18:39:18.710577: step 2387, loss 0.170975, acc 0.9375\n",
      "2017-11-05T18:39:22.689413: step 2388, loss 0.224305, acc 0.875\n",
      "2017-11-05T18:39:26.785610: step 2389, loss 0.14608, acc 0.9375\n",
      "2017-11-05T18:39:30.785011: step 2390, loss 0.235235, acc 0.90625\n",
      "2017-11-05T18:39:34.759740: step 2391, loss 0.217032, acc 0.90625\n",
      "2017-11-05T18:39:38.740685: step 2392, loss 0.146412, acc 0.9375\n",
      "2017-11-05T18:39:42.806786: step 2393, loss 0.141101, acc 0.9375\n",
      "2017-11-05T18:39:46.883923: step 2394, loss 0.0945402, acc 0.9375\n",
      "2017-11-05T18:39:50.974703: step 2395, loss 0.134898, acc 0.9375\n",
      "2017-11-05T18:39:55.031007: step 2396, loss 0.192698, acc 0.875\n",
      "2017-11-05T18:39:59.067152: step 2397, loss 0.0951463, acc 0.9375\n",
      "2017-11-05T18:40:03.303476: step 2398, loss 0.193124, acc 0.9375\n",
      "2017-11-05T18:40:07.319544: step 2399, loss 0.0758767, acc 0.96875\n",
      "2017-11-05T18:40:11.339309: step 2400, loss 0.148808, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T18:40:13.912373: step 2400, loss 0.807118, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-05T18:40:19.188847: step 2401, loss 0.290895, acc 0.90625\n",
      "2017-11-05T18:40:23.206222: step 2402, loss 0.269525, acc 0.90625\n",
      "2017-11-05T18:40:27.278395: step 2403, loss 0.0812115, acc 0.96875\n",
      "2017-11-05T18:40:31.280002: step 2404, loss 0.0408244, acc 1\n",
      "2017-11-05T18:40:35.455891: step 2405, loss 0.536188, acc 0.8125\n",
      "2017-11-05T18:40:39.485040: step 2406, loss 0.170132, acc 0.90625\n",
      "2017-11-05T18:40:43.490585: step 2407, loss 0.162969, acc 0.90625\n",
      "2017-11-05T18:40:47.560277: step 2408, loss 0.254395, acc 0.84375\n",
      "2017-11-05T18:40:51.579081: step 2409, loss 0.386829, acc 0.84375\n",
      "2017-11-05T18:40:55.615960: step 2410, loss 0.405765, acc 0.875\n",
      "2017-11-05T18:40:59.623637: step 2411, loss 0.06778, acc 0.96875\n",
      "2017-11-05T18:41:02.188094: step 2412, loss 0.0656027, acc 1\n",
      "2017-11-05T18:41:06.189618: step 2413, loss 0.155098, acc 0.875\n",
      "2017-11-05T18:41:10.472520: step 2414, loss 0.0350314, acc 1\n",
      "2017-11-05T18:41:15.574290: step 2415, loss 0.194681, acc 0.875\n",
      "2017-11-05T18:41:19.647460: step 2416, loss 0.192163, acc 0.9375\n",
      "2017-11-05T18:41:23.748951: step 2417, loss 0.185053, acc 0.9375\n",
      "2017-11-05T18:41:27.721521: step 2418, loss 0.119502, acc 0.9375\n",
      "2017-11-05T18:41:31.707438: step 2419, loss 0.287411, acc 0.84375\n",
      "2017-11-05T18:41:35.689314: step 2420, loss 0.107523, acc 0.9375\n",
      "2017-11-05T18:41:39.709846: step 2421, loss 0.198024, acc 0.9375\n",
      "2017-11-05T18:41:43.769105: step 2422, loss 0.0852015, acc 0.9375\n",
      "2017-11-05T18:41:47.810123: step 2423, loss 0.160383, acc 0.9375\n",
      "2017-11-05T18:41:51.803472: step 2424, loss 0.145943, acc 0.9375\n",
      "2017-11-05T18:41:55.762376: step 2425, loss 0.174706, acc 0.9375\n",
      "2017-11-05T18:41:59.806368: step 2426, loss 0.280781, acc 0.8125\n",
      "2017-11-05T18:42:03.862178: step 2427, loss 0.212815, acc 0.9375\n",
      "2017-11-05T18:42:07.881366: step 2428, loss 0.156197, acc 0.90625\n",
      "2017-11-05T18:42:12.017519: step 2429, loss 0.174754, acc 0.96875\n",
      "2017-11-05T18:42:15.983903: step 2430, loss 0.226754, acc 0.84375\n",
      "2017-11-05T18:42:20.039731: step 2431, loss 0.21649, acc 0.90625\n",
      "2017-11-05T18:42:24.081080: step 2432, loss 0.207556, acc 0.84375\n",
      "2017-11-05T18:42:28.109092: step 2433, loss 0.243321, acc 0.9375\n",
      "2017-11-05T18:42:32.131124: step 2434, loss 0.0998018, acc 0.9375\n",
      "2017-11-05T18:42:36.299934: step 2435, loss 0.0874652, acc 0.96875\n",
      "2017-11-05T18:42:40.358970: step 2436, loss 0.139027, acc 0.9375\n",
      "2017-11-05T18:42:44.372276: step 2437, loss 0.289363, acc 0.84375\n",
      "2017-11-05T18:42:48.376376: step 2438, loss 0.115277, acc 0.9375\n",
      "2017-11-05T18:42:52.411831: step 2439, loss 0.14079, acc 0.90625\n",
      "2017-11-05T18:42:56.490913: step 2440, loss 0.225062, acc 0.84375\n",
      "2017-11-05T18:43:00.543491: step 2441, loss 0.194916, acc 0.90625\n",
      "2017-11-05T18:43:04.556006: step 2442, loss 0.0851758, acc 0.96875\n",
      "2017-11-05T18:43:08.573644: step 2443, loss 0.0816383, acc 0.96875\n",
      "2017-11-05T18:43:12.557958: step 2444, loss 0.207344, acc 0.84375\n",
      "2017-11-05T18:43:16.596849: step 2445, loss 0.283687, acc 0.84375\n",
      "2017-11-05T18:43:20.671451: step 2446, loss 0.279392, acc 0.875\n",
      "2017-11-05T18:43:24.941995: step 2447, loss 0.238421, acc 0.84375\n",
      "2017-11-05T18:43:27.640949: step 2448, loss 0.040739, acc 1\n",
      "2017-11-05T18:43:31.668563: step 2449, loss 0.0567977, acc 0.96875\n",
      "2017-11-05T18:43:35.690329: step 2450, loss 0.0969106, acc 0.9375\n",
      "2017-11-05T18:43:39.695070: step 2451, loss 0.167055, acc 0.875\n",
      "2017-11-05T18:43:43.656877: step 2452, loss 0.157478, acc 0.90625\n",
      "2017-11-05T18:43:47.715666: step 2453, loss 0.110037, acc 0.90625\n",
      "2017-11-05T18:43:51.775304: step 2454, loss 0.158709, acc 0.90625\n",
      "2017-11-05T18:43:55.724292: step 2455, loss 0.0957224, acc 0.96875\n",
      "2017-11-05T18:43:59.764638: step 2456, loss 0.130738, acc 0.9375\n",
      "2017-11-05T18:44:03.803802: step 2457, loss 0.0118298, acc 1\n",
      "2017-11-05T18:44:07.799862: step 2458, loss 0.325189, acc 0.84375\n",
      "2017-11-05T18:44:11.826982: step 2459, loss 0.355237, acc 0.84375\n",
      "2017-11-05T18:44:15.834838: step 2460, loss 0.0990514, acc 0.96875\n",
      "2017-11-05T18:44:19.868762: step 2461, loss 0.285586, acc 0.875\n",
      "2017-11-05T18:44:23.933076: step 2462, loss 0.0689514, acc 1\n",
      "2017-11-05T18:44:27.939582: step 2463, loss 0.0539605, acc 1\n",
      "2017-11-05T18:44:31.980140: step 2464, loss 0.154666, acc 0.90625\n",
      "2017-11-05T18:44:36.203324: step 2465, loss 0.303889, acc 0.875\n",
      "2017-11-05T18:44:40.246756: step 2466, loss 0.130039, acc 0.90625\n",
      "2017-11-05T18:44:44.275165: step 2467, loss 0.241951, acc 0.9375\n",
      "2017-11-05T18:44:48.364409: step 2468, loss 0.0610426, acc 1\n",
      "2017-11-05T18:44:52.418697: step 2469, loss 0.218503, acc 0.90625\n",
      "2017-11-05T18:44:56.405842: step 2470, loss 0.196128, acc 0.96875\n",
      "2017-11-05T18:45:00.549089: step 2471, loss 0.150068, acc 0.9375\n",
      "2017-11-05T18:45:04.591113: step 2472, loss 0.247092, acc 0.84375\n",
      "2017-11-05T18:45:08.593334: step 2473, loss 0.146611, acc 0.9375\n",
      "2017-11-05T18:45:12.662704: step 2474, loss 0.135683, acc 0.9375\n",
      "2017-11-05T18:45:16.687476: step 2475, loss 0.0912463, acc 0.96875\n",
      "2017-11-05T18:45:20.768708: step 2476, loss 0.102961, acc 0.9375\n",
      "2017-11-05T18:45:24.802889: step 2477, loss 0.0358494, acc 0.96875\n",
      "2017-11-05T18:45:28.809009: step 2478, loss 0.398839, acc 0.84375\n",
      "2017-11-05T18:45:32.837295: step 2479, loss 0.28517, acc 0.90625\n",
      "2017-11-05T18:45:36.903582: step 2480, loss 0.139656, acc 0.96875\n",
      "2017-11-05T18:45:40.865494: step 2481, loss 0.239014, acc 0.84375\n",
      "2017-11-05T18:45:44.871881: step 2482, loss 0.237836, acc 0.875\n",
      "2017-11-05T18:45:48.954996: step 2483, loss 0.246827, acc 0.875\n",
      "2017-11-05T18:45:51.585141: step 2484, loss 0.211692, acc 0.9\n",
      "2017-11-05T18:45:55.688242: step 2485, loss 0.164928, acc 0.90625\n",
      "2017-11-05T18:45:59.656073: step 2486, loss 0.248251, acc 0.84375\n",
      "2017-11-05T18:46:03.787146: step 2487, loss 0.0618403, acc 1\n",
      "2017-11-05T18:46:07.867108: step 2488, loss 0.339287, acc 0.84375\n",
      "2017-11-05T18:46:11.884692: step 2489, loss 0.306043, acc 0.84375\n",
      "2017-11-05T18:46:15.922135: step 2490, loss 0.203015, acc 0.875\n",
      "2017-11-05T18:46:19.985172: step 2491, loss 0.161959, acc 0.90625\n",
      "2017-11-05T18:46:24.056574: step 2492, loss 0.323548, acc 0.84375\n",
      "2017-11-05T18:46:28.123869: step 2493, loss 0.126426, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T18:46:32.149093: step 2494, loss 0.165149, acc 0.90625\n",
      "2017-11-05T18:46:36.489525: step 2495, loss 0.0392434, acc 1\n",
      "2017-11-05T18:46:40.600220: step 2496, loss 0.0732315, acc 0.9375\n",
      "2017-11-05T18:46:44.675091: step 2497, loss 0.148995, acc 0.875\n",
      "2017-11-05T18:46:48.659361: step 2498, loss 0.251723, acc 0.875\n",
      "2017-11-05T18:46:52.663446: step 2499, loss 0.338526, acc 0.84375\n",
      "2017-11-05T18:46:56.718780: step 2500, loss 0.180557, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T18:46:59.255467: step 2500, loss 0.885378, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-05T18:47:04.624984: step 2501, loss 0.0936474, acc 0.90625\n",
      "2017-11-05T18:47:08.600635: step 2502, loss 0.316825, acc 0.875\n",
      "2017-11-05T18:47:12.594492: step 2503, loss 0.125994, acc 0.96875\n",
      "2017-11-05T18:47:16.618361: step 2504, loss 0.0617491, acc 0.9375\n",
      "2017-11-05T18:47:20.646518: step 2505, loss 0.124234, acc 0.9375\n",
      "2017-11-05T18:47:24.716162: step 2506, loss 0.224996, acc 0.875\n",
      "2017-11-05T18:47:28.714912: step 2507, loss 0.264454, acc 0.9375\n",
      "2017-11-05T18:47:32.716635: step 2508, loss 0.403444, acc 0.8125\n",
      "2017-11-05T18:47:36.806684: step 2509, loss 0.152391, acc 0.90625\n",
      "2017-11-05T18:47:40.846772: step 2510, loss 0.0397909, acc 0.96875\n",
      "2017-11-05T18:47:44.823159: step 2511, loss 0.0725981, acc 0.96875\n",
      "2017-11-05T18:47:48.865648: step 2512, loss 0.271451, acc 0.875\n",
      "2017-11-05T18:47:52.907753: step 2513, loss 0.0720912, acc 0.9375\n",
      "2017-11-05T18:47:56.996140: step 2514, loss 0.103864, acc 0.96875\n",
      "2017-11-05T18:48:01.017017: step 2515, loss 0.120048, acc 0.9375\n",
      "2017-11-05T18:48:05.083368: step 2516, loss 0.195474, acc 0.875\n",
      "2017-11-05T18:48:09.118712: step 2517, loss 0.0895276, acc 0.96875\n",
      "2017-11-05T18:48:13.139401: step 2518, loss 0.107555, acc 0.9375\n",
      "2017-11-05T18:48:17.180782: step 2519, loss 0.197631, acc 0.875\n",
      "2017-11-05T18:48:19.848197: step 2520, loss 0.288864, acc 0.85\n",
      "2017-11-05T18:48:23.940423: step 2521, loss 0.142658, acc 0.9375\n",
      "2017-11-05T18:48:28.030247: step 2522, loss 0.156845, acc 0.9375\n",
      "2017-11-05T18:48:32.005306: step 2523, loss 0.204603, acc 0.875\n",
      "2017-11-05T18:48:36.189318: step 2524, loss 0.152177, acc 0.90625\n",
      "2017-11-05T18:48:40.353407: step 2525, loss 0.177535, acc 0.9375\n",
      "2017-11-05T18:48:44.624019: step 2526, loss 0.0225915, acc 1\n",
      "2017-11-05T18:48:48.708409: step 2527, loss 0.182408, acc 0.875\n",
      "2017-11-05T18:48:52.812652: step 2528, loss 0.237316, acc 0.875\n",
      "2017-11-05T18:48:56.989481: step 2529, loss 0.11044, acc 0.9375\n",
      "2017-11-05T18:49:01.178246: step 2530, loss 0.200659, acc 0.875\n",
      "2017-11-05T18:49:05.320040: step 2531, loss 0.205455, acc 0.875\n",
      "2017-11-05T18:49:09.575570: step 2532, loss 0.0379355, acc 1\n",
      "2017-11-05T18:49:13.659018: step 2533, loss 0.0386314, acc 0.96875\n",
      "2017-11-05T18:49:17.875692: step 2534, loss 0.0817792, acc 0.9375\n",
      "2017-11-05T18:49:22.055473: step 2535, loss 0.28437, acc 0.84375\n",
      "2017-11-05T18:49:26.397526: step 2536, loss 0.329535, acc 0.84375\n",
      "2017-11-05T18:49:30.456057: step 2537, loss 0.139912, acc 0.90625\n",
      "2017-11-05T18:49:34.624391: step 2538, loss 0.110553, acc 0.9375\n",
      "2017-11-05T18:49:38.706619: step 2539, loss 0.186936, acc 0.90625\n",
      "2017-11-05T18:49:42.923223: step 2540, loss 0.19513, acc 0.9375\n",
      "2017-11-05T18:49:46.902188: step 2541, loss 0.200014, acc 0.90625\n",
      "2017-11-05T18:49:50.978567: step 2542, loss 0.175575, acc 0.9375\n",
      "2017-11-05T18:49:54.994196: step 2543, loss 0.109202, acc 0.90625\n",
      "2017-11-05T18:49:59.034872: step 2544, loss 0.153359, acc 0.9375\n",
      "2017-11-05T18:50:03.404972: step 2545, loss 0.27382, acc 0.875\n",
      "2017-11-05T18:50:07.397399: step 2546, loss 0.178147, acc 0.9375\n",
      "2017-11-05T18:50:11.456068: step 2547, loss 0.187282, acc 0.875\n",
      "2017-11-05T18:50:15.489712: step 2548, loss 0.171989, acc 0.96875\n",
      "2017-11-05T18:50:19.559866: step 2549, loss 0.264006, acc 0.875\n",
      "2017-11-05T18:50:23.633757: step 2550, loss 0.137368, acc 0.96875\n",
      "2017-11-05T18:50:27.672064: step 2551, loss 0.133054, acc 0.9375\n",
      "2017-11-05T18:50:31.643043: step 2552, loss 0.202457, acc 0.9375\n",
      "2017-11-05T18:50:35.857823: step 2553, loss 0.0764219, acc 0.9375\n",
      "2017-11-05T18:50:39.895100: step 2554, loss 0.184311, acc 0.875\n",
      "2017-11-05T18:50:43.949012: step 2555, loss 0.272987, acc 0.875\n",
      "2017-11-05T18:50:46.524644: step 2556, loss 0.303128, acc 0.9\n",
      "2017-11-05T18:50:50.585212: step 2557, loss 0.14481, acc 0.90625\n",
      "2017-11-05T18:50:54.606115: step 2558, loss 0.176895, acc 0.9375\n",
      "2017-11-05T18:50:58.656873: step 2559, loss 0.0758104, acc 0.9375\n",
      "2017-11-05T18:51:02.688290: step 2560, loss 0.365364, acc 0.84375\n",
      "2017-11-05T18:51:06.718745: step 2561, loss 0.457367, acc 0.8125\n",
      "2017-11-05T18:51:10.769935: step 2562, loss 0.112879, acc 0.90625\n",
      "2017-11-05T18:51:14.804203: step 2563, loss 0.172983, acc 0.90625\n",
      "2017-11-05T18:51:18.848242: step 2564, loss 0.150688, acc 0.9375\n",
      "2017-11-05T18:51:22.883929: step 2565, loss 0.203288, acc 0.9375\n",
      "2017-11-05T18:51:26.932115: step 2566, loss 0.351393, acc 0.8125\n",
      "2017-11-05T18:51:30.959514: step 2567, loss 0.200747, acc 0.90625\n",
      "2017-11-05T18:51:35.035612: step 2568, loss 0.172394, acc 0.90625\n",
      "2017-11-05T18:51:39.075184: step 2569, loss 0.0510879, acc 1\n",
      "2017-11-05T18:51:43.099207: step 2570, loss 0.135226, acc 0.9375\n",
      "2017-11-05T18:51:47.373130: step 2571, loss 0.157103, acc 0.96875\n",
      "2017-11-05T18:51:51.321295: step 2572, loss 0.196209, acc 0.9375\n",
      "2017-11-05T18:51:55.385351: step 2573, loss 0.148222, acc 0.90625\n",
      "2017-11-05T18:51:59.451329: step 2574, loss 0.302507, acc 0.8125\n",
      "2017-11-05T18:52:03.506866: step 2575, loss 0.193753, acc 0.875\n",
      "2017-11-05T18:52:07.552418: step 2576, loss 0.0860867, acc 0.96875\n",
      "2017-11-05T18:52:11.637732: step 2577, loss 0.245903, acc 0.90625\n",
      "2017-11-05T18:52:15.637707: step 2578, loss 0.301206, acc 0.90625\n",
      "2017-11-05T18:52:19.586667: step 2579, loss 0.111591, acc 0.9375\n",
      "2017-11-05T18:52:23.531456: step 2580, loss 0.0419498, acc 1\n",
      "2017-11-05T18:52:27.606380: step 2581, loss 0.0989363, acc 0.96875\n",
      "2017-11-05T18:52:31.618508: step 2582, loss 0.157693, acc 0.9375\n",
      "2017-11-05T18:52:35.803967: step 2583, loss 0.0983987, acc 0.96875\n",
      "2017-11-05T18:52:39.909475: step 2584, loss 0.170583, acc 0.9375\n",
      "2017-11-05T18:52:43.926813: step 2585, loss 0.120144, acc 0.90625\n",
      "2017-11-05T18:52:47.930678: step 2586, loss 0.216866, acc 0.90625\n",
      "2017-11-05T18:52:52.000227: step 2587, loss 0.106278, acc 0.9375\n",
      "2017-11-05T18:52:56.024698: step 2588, loss 0.164771, acc 0.96875\n",
      "2017-11-05T18:52:59.958398: step 2589, loss 0.163452, acc 0.875\n",
      "2017-11-05T18:53:03.941815: step 2590, loss 0.13378, acc 0.9375\n",
      "2017-11-05T18:53:07.972130: step 2591, loss 0.0680988, acc 0.96875\n",
      "2017-11-05T18:53:10.496420: step 2592, loss 0.144199, acc 0.95\n",
      "2017-11-05T18:53:14.506466: step 2593, loss 0.0977331, acc 0.9375\n",
      "2017-11-05T18:53:18.586642: step 2594, loss 0.116038, acc 0.9375\n",
      "2017-11-05T18:53:22.643598: step 2595, loss 0.175943, acc 0.9375\n",
      "2017-11-05T18:53:26.840636: step 2596, loss 0.140038, acc 0.90625\n",
      "2017-11-05T18:53:30.879157: step 2597, loss 0.167853, acc 0.9375\n",
      "2017-11-05T18:53:34.875441: step 2598, loss 0.144224, acc 0.90625\n",
      "2017-11-05T18:53:38.916206: step 2599, loss 0.163247, acc 0.875\n",
      "2017-11-05T18:53:42.970968: step 2600, loss 0.0575744, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T18:53:45.542755: step 2600, loss 0.867206, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-05T18:53:50.751440: step 2601, loss 0.232211, acc 0.84375\n",
      "2017-11-05T18:53:54.719784: step 2602, loss 0.0645827, acc 0.96875\n",
      "2017-11-05T18:53:58.777817: step 2603, loss 0.335538, acc 0.875\n",
      "2017-11-05T18:54:02.855055: step 2604, loss 0.152967, acc 0.9375\n",
      "2017-11-05T18:54:06.882528: step 2605, loss 0.0408087, acc 1\n",
      "2017-11-05T18:54:10.909094: step 2606, loss 0.045284, acc 1\n",
      "2017-11-05T18:54:14.958420: step 2607, loss 0.134233, acc 0.96875\n",
      "2017-11-05T18:54:19.032617: step 2608, loss 0.214976, acc 0.90625\n",
      "2017-11-05T18:54:23.038033: step 2609, loss 0.414528, acc 0.8125\n",
      "2017-11-05T18:54:27.103956: step 2610, loss 0.203035, acc 0.90625\n",
      "2017-11-05T18:54:31.097991: step 2611, loss 0.183755, acc 0.90625\n",
      "2017-11-05T18:54:35.309824: step 2612, loss 0.0885209, acc 0.96875\n",
      "2017-11-05T18:54:39.395250: step 2613, loss 0.107075, acc 0.9375\n",
      "2017-11-05T18:54:43.378239: step 2614, loss 0.197057, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T18:54:47.420057: step 2615, loss 0.240855, acc 0.90625\n",
      "2017-11-05T18:54:51.466888: step 2616, loss 0.0817583, acc 0.96875\n",
      "2017-11-05T18:54:55.422013: step 2617, loss 0.650936, acc 0.84375\n",
      "2017-11-05T18:54:59.417232: step 2618, loss 0.112384, acc 0.90625\n",
      "2017-11-05T18:55:03.456046: step 2619, loss 0.466902, acc 0.84375\n",
      "2017-11-05T18:55:07.494504: step 2620, loss 0.224238, acc 0.9375\n",
      "2017-11-05T18:55:11.525911: step 2621, loss 0.106039, acc 0.9375\n",
      "2017-11-05T18:55:15.659531: step 2622, loss 0.160396, acc 0.90625\n",
      "2017-11-05T18:55:19.724500: step 2623, loss 0.153815, acc 0.90625\n",
      "2017-11-05T18:55:23.803041: step 2624, loss 0.0771512, acc 0.96875\n",
      "2017-11-05T18:55:27.841345: step 2625, loss 0.272965, acc 0.84375\n",
      "2017-11-05T18:55:31.868552: step 2626, loss 0.256294, acc 0.875\n",
      "2017-11-05T18:55:35.957305: step 2627, loss 0.186214, acc 0.875\n",
      "2017-11-05T18:55:38.471852: step 2628, loss 0.165737, acc 0.9\n",
      "2017-11-05T18:55:42.467036: step 2629, loss 0.076749, acc 0.9375\n",
      "2017-11-05T18:55:46.523850: step 2630, loss 0.268484, acc 0.875\n",
      "2017-11-05T18:55:50.626314: step 2631, loss 0.129587, acc 0.9375\n",
      "2017-11-05T18:55:54.681713: step 2632, loss 0.271041, acc 0.875\n",
      "2017-11-05T18:55:58.740398: step 2633, loss 0.136027, acc 0.9375\n",
      "2017-11-05T18:56:02.773943: step 2634, loss 0.14121, acc 0.96875\n",
      "2017-11-05T18:56:06.895577: step 2635, loss 0.0685195, acc 0.96875\n",
      "2017-11-05T18:56:10.932812: step 2636, loss 0.108948, acc 0.9375\n",
      "2017-11-05T18:56:15.012285: step 2637, loss 0.170467, acc 0.9375\n",
      "2017-11-05T18:56:19.134865: step 2638, loss 0.0590451, acc 0.96875\n",
      "2017-11-05T18:56:23.168965: step 2639, loss 0.156087, acc 0.9375\n",
      "2017-11-05T18:56:27.283889: step 2640, loss 0.0644828, acc 0.96875\n",
      "2017-11-05T18:56:31.357455: step 2641, loss 0.191845, acc 0.9375\n",
      "2017-11-05T18:56:35.552311: step 2642, loss 0.10443, acc 0.96875\n",
      "2017-11-05T18:56:39.591915: step 2643, loss 0.177888, acc 0.90625\n",
      "2017-11-05T18:56:43.629933: step 2644, loss 0.156326, acc 0.9375\n",
      "2017-11-05T18:56:47.672410: step 2645, loss 0.510468, acc 0.78125\n",
      "2017-11-05T18:56:51.704472: step 2646, loss 0.140834, acc 0.9375\n",
      "2017-11-05T18:56:55.755093: step 2647, loss 0.188672, acc 0.875\n",
      "2017-11-05T18:56:59.778860: step 2648, loss 0.238519, acc 0.9375\n",
      "2017-11-05T18:57:03.809516: step 2649, loss 0.105192, acc 0.9375\n",
      "2017-11-05T18:57:07.847095: step 2650, loss 0.13426, acc 0.9375\n",
      "2017-11-05T18:57:11.921829: step 2651, loss 0.106108, acc 0.96875\n",
      "2017-11-05T18:57:15.995477: step 2652, loss 0.101566, acc 0.9375\n",
      "2017-11-05T18:57:20.021391: step 2653, loss 0.343268, acc 0.84375\n",
      "2017-11-05T18:57:24.109600: step 2654, loss 0.345326, acc 0.8125\n",
      "2017-11-05T18:57:28.132715: step 2655, loss 0.176735, acc 0.875\n",
      "2017-11-05T18:57:32.155126: step 2656, loss 0.0292428, acc 1\n",
      "2017-11-05T18:57:36.158810: step 2657, loss 0.378877, acc 0.84375\n",
      "2017-11-05T18:57:40.213272: step 2658, loss 0.114045, acc 0.9375\n",
      "2017-11-05T18:57:44.226203: step 2659, loss 0.169748, acc 0.9375\n",
      "2017-11-05T18:57:48.275186: step 2660, loss 0.163883, acc 0.90625\n",
      "2017-11-05T18:57:52.398114: step 2661, loss 0.13323, acc 0.96875\n",
      "2017-11-05T18:57:56.371907: step 2662, loss 0.110812, acc 0.96875\n",
      "2017-11-05T18:58:00.422769: step 2663, loss 0.382012, acc 0.875\n",
      "2017-11-05T18:58:02.959625: step 2664, loss 0.222869, acc 0.9\n",
      "2017-11-05T18:58:06.955568: step 2665, loss 0.116791, acc 0.9375\n",
      "2017-11-05T18:58:11.040724: step 2666, loss 0.132227, acc 0.96875\n",
      "2017-11-05T18:58:15.039913: step 2667, loss 0.0353069, acc 1\n",
      "2017-11-05T18:58:19.109451: step 2668, loss 0.162421, acc 0.9375\n",
      "2017-11-05T18:58:23.274895: step 2669, loss 0.00778489, acc 1\n",
      "2017-11-05T18:58:27.521752: step 2670, loss 0.13631, acc 0.9375\n",
      "2017-11-05T18:58:31.530321: step 2671, loss 0.222734, acc 0.90625\n",
      "2017-11-05T18:58:35.668548: step 2672, loss 0.116511, acc 0.9375\n",
      "2017-11-05T18:58:39.709431: step 2673, loss 0.0652522, acc 0.96875\n",
      "2017-11-05T18:58:43.691399: step 2674, loss 0.24842, acc 0.8125\n",
      "2017-11-05T18:58:47.775009: step 2675, loss 0.123036, acc 0.9375\n",
      "2017-11-05T18:58:51.816829: step 2676, loss 0.269854, acc 0.84375\n",
      "2017-11-05T18:58:55.810020: step 2677, loss 0.093253, acc 0.9375\n",
      "2017-11-05T18:58:59.819447: step 2678, loss 0.150051, acc 0.9375\n",
      "2017-11-05T18:59:03.731529: step 2679, loss 0.0589406, acc 1\n",
      "2017-11-05T18:59:07.772853: step 2680, loss 0.0764595, acc 0.96875\n",
      "2017-11-05T18:59:11.820212: step 2681, loss 0.302445, acc 0.90625\n",
      "2017-11-05T18:59:15.857420: step 2682, loss 0.0854198, acc 1\n",
      "2017-11-05T18:59:19.863863: step 2683, loss 0.234163, acc 0.875\n",
      "2017-11-05T18:59:23.903431: step 2684, loss 0.0820748, acc 0.9375\n",
      "2017-11-05T18:59:27.930073: step 2685, loss 0.219372, acc 0.90625\n",
      "2017-11-05T18:59:31.901958: step 2686, loss 0.294784, acc 0.84375\n",
      "2017-11-05T18:59:35.932386: step 2687, loss 0.0548706, acc 0.96875\n",
      "2017-11-05T18:59:39.956229: step 2688, loss 0.204267, acc 0.9375\n",
      "2017-11-05T18:59:43.953822: step 2689, loss 0.115799, acc 0.9375\n",
      "2017-11-05T18:59:47.891572: step 2690, loss 0.570001, acc 0.84375\n",
      "2017-11-05T18:59:51.872065: step 2691, loss 0.380493, acc 0.8125\n",
      "2017-11-05T18:59:55.853134: step 2692, loss 0.441957, acc 0.8125\n",
      "2017-11-05T18:59:59.828557: step 2693, loss 0.16144, acc 0.90625\n",
      "2017-11-05T19:00:04.156643: step 2694, loss 0.244557, acc 0.875\n",
      "2017-11-05T19:00:08.223459: step 2695, loss 0.156323, acc 0.9375\n",
      "2017-11-05T19:00:12.281366: step 2696, loss 0.140825, acc 0.875\n",
      "2017-11-05T19:00:16.262349: step 2697, loss 0.286762, acc 0.78125\n",
      "2017-11-05T19:00:20.332211: step 2698, loss 0.0704621, acc 0.96875\n",
      "2017-11-05T19:00:24.354956: step 2699, loss 0.121602, acc 0.90625\n",
      "2017-11-05T19:00:26.986199: step 2700, loss 0.0466983, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:00:29.570765: step 2700, loss 0.866401, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-05T19:00:35.058106: step 2701, loss 0.173165, acc 0.90625\n",
      "2017-11-05T19:00:39.215584: step 2702, loss 0.15069, acc 0.90625\n",
      "2017-11-05T19:00:43.217590: step 2703, loss 0.137371, acc 0.9375\n",
      "2017-11-05T19:00:47.256595: step 2704, loss 0.167871, acc 0.875\n",
      "2017-11-05T19:00:51.269938: step 2705, loss 0.142427, acc 0.90625\n",
      "2017-11-05T19:00:55.274434: step 2706, loss 0.127365, acc 0.90625\n",
      "2017-11-05T19:00:59.288470: step 2707, loss 0.207416, acc 0.9375\n",
      "2017-11-05T19:01:03.287849: step 2708, loss 0.119517, acc 0.90625\n",
      "2017-11-05T19:01:07.366212: step 2709, loss 0.0488846, acc 0.96875\n",
      "2017-11-05T19:01:11.410223: step 2710, loss 0.227059, acc 0.9375\n",
      "2017-11-05T19:01:15.434542: step 2711, loss 0.161121, acc 0.9375\n",
      "2017-11-05T19:01:19.465625: step 2712, loss 0.241781, acc 0.90625\n",
      "2017-11-05T19:01:23.525064: step 2713, loss 0.713238, acc 0.75\n",
      "2017-11-05T19:01:27.557139: step 2714, loss 0.0601236, acc 0.96875\n",
      "2017-11-05T19:01:31.559637: step 2715, loss 0.043931, acc 1\n",
      "2017-11-05T19:01:35.665369: step 2716, loss 0.18508, acc 0.9375\n",
      "2017-11-05T19:01:39.760422: step 2717, loss 0.38182, acc 0.8125\n",
      "2017-11-05T19:01:43.710220: step 2718, loss 0.232632, acc 0.90625\n",
      "2017-11-05T19:01:47.716857: step 2719, loss 0.19921, acc 0.90625\n",
      "2017-11-05T19:01:51.721214: step 2720, loss 0.0542314, acc 0.96875\n",
      "2017-11-05T19:01:55.690654: step 2721, loss 0.0557817, acc 0.96875\n",
      "2017-11-05T19:01:59.716941: step 2722, loss 0.266878, acc 0.875\n",
      "2017-11-05T19:02:03.759591: step 2723, loss 0.158792, acc 0.9375\n",
      "2017-11-05T19:02:07.734966: step 2724, loss 0.195955, acc 0.90625\n",
      "2017-11-05T19:02:11.900009: step 2725, loss 0.184501, acc 0.84375\n",
      "2017-11-05T19:02:15.975535: step 2726, loss 0.129911, acc 0.96875\n",
      "2017-11-05T19:02:20.061987: step 2727, loss 0.351637, acc 0.78125\n",
      "2017-11-05T19:02:24.133253: step 2728, loss 0.326994, acc 0.90625\n",
      "2017-11-05T19:02:28.206227: step 2729, loss 0.34123, acc 0.875\n",
      "2017-11-05T19:02:32.209702: step 2730, loss 0.184216, acc 0.9375\n",
      "2017-11-05T19:02:36.372089: step 2731, loss 0.175939, acc 0.9375\n",
      "2017-11-05T19:02:40.332007: step 2732, loss 0.14856, acc 0.90625\n",
      "2017-11-05T19:02:44.364705: step 2733, loss 0.0440906, acc 1\n",
      "2017-11-05T19:02:48.334573: step 2734, loss 0.107209, acc 0.96875\n",
      "2017-11-05T19:02:52.379102: step 2735, loss 0.213202, acc 0.84375\n",
      "2017-11-05T19:02:54.937429: step 2736, loss 0.454799, acc 0.75\n",
      "2017-11-05T19:02:58.975350: step 2737, loss 0.288638, acc 0.90625\n",
      "2017-11-05T19:03:02.956421: step 2738, loss 0.355215, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T19:03:06.989870: step 2739, loss 0.210565, acc 0.84375\n",
      "2017-11-05T19:03:11.022968: step 2740, loss 0.0979947, acc 0.96875\n",
      "2017-11-05T19:03:15.088014: step 2741, loss 0.0947764, acc 0.96875\n",
      "2017-11-05T19:03:19.149882: step 2742, loss 0.281731, acc 0.90625\n",
      "2017-11-05T19:03:23.346566: step 2743, loss 0.0368827, acc 0.96875\n",
      "2017-11-05T19:03:27.513516: step 2744, loss 0.206713, acc 0.90625\n",
      "2017-11-05T19:03:31.556222: step 2745, loss 0.251033, acc 0.90625\n",
      "2017-11-05T19:03:35.563356: step 2746, loss 0.20104, acc 0.9375\n",
      "2017-11-05T19:03:39.559640: step 2747, loss 0.172723, acc 0.9375\n",
      "2017-11-05T19:03:43.628778: step 2748, loss 0.348278, acc 0.875\n",
      "2017-11-05T19:03:47.667545: step 2749, loss 0.20615, acc 0.90625\n",
      "2017-11-05T19:03:51.663119: step 2750, loss 0.201522, acc 0.90625\n",
      "2017-11-05T19:03:55.667219: step 2751, loss 0.278236, acc 0.875\n",
      "2017-11-05T19:03:59.715341: step 2752, loss 0.15686, acc 0.90625\n",
      "2017-11-05T19:04:03.789078: step 2753, loss 0.177464, acc 0.9375\n",
      "2017-11-05T19:04:07.905054: step 2754, loss 0.206865, acc 0.875\n",
      "2017-11-05T19:04:11.944656: step 2755, loss 0.0832215, acc 0.96875\n",
      "2017-11-05T19:04:15.945389: step 2756, loss 0.145844, acc 0.90625\n",
      "2017-11-05T19:04:19.909398: step 2757, loss 0.230757, acc 0.875\n",
      "2017-11-05T19:04:23.938849: step 2758, loss 0.129016, acc 0.96875\n",
      "2017-11-05T19:04:27.959206: step 2759, loss 0.175252, acc 0.90625\n",
      "2017-11-05T19:04:31.948265: step 2760, loss 0.0632647, acc 0.96875\n",
      "2017-11-05T19:04:36.160246: step 2761, loss 0.14219, acc 0.9375\n",
      "2017-11-05T19:04:40.159267: step 2762, loss 0.191415, acc 0.9375\n",
      "2017-11-05T19:04:44.246922: step 2763, loss 0.239824, acc 0.90625\n",
      "2017-11-05T19:04:48.321808: step 2764, loss 0.409416, acc 0.8125\n",
      "2017-11-05T19:04:52.219482: step 2765, loss 0.0843517, acc 0.96875\n",
      "2017-11-05T19:04:56.224322: step 2766, loss 0.379849, acc 0.78125\n",
      "2017-11-05T19:05:00.253546: step 2767, loss 0.159489, acc 0.875\n",
      "2017-11-05T19:05:04.279887: step 2768, loss 0.163764, acc 0.90625\n",
      "2017-11-05T19:05:08.268953: step 2769, loss 0.161902, acc 0.90625\n",
      "2017-11-05T19:05:12.231842: step 2770, loss 0.303931, acc 0.875\n",
      "2017-11-05T19:05:16.235888: step 2771, loss 0.270802, acc 0.90625\n",
      "2017-11-05T19:05:18.792878: step 2772, loss 0.161395, acc 0.95\n",
      "2017-11-05T19:05:22.832088: step 2773, loss 0.197228, acc 0.90625\n",
      "2017-11-05T19:05:26.858516: step 2774, loss 0.081472, acc 0.96875\n",
      "2017-11-05T19:05:30.895221: step 2775, loss 0.0482222, acc 0.96875\n",
      "2017-11-05T19:05:34.931499: step 2776, loss 0.191039, acc 0.9375\n",
      "2017-11-05T19:05:38.947427: step 2777, loss 0.191691, acc 0.90625\n",
      "2017-11-05T19:05:42.940648: step 2778, loss 0.198753, acc 0.875\n",
      "2017-11-05T19:05:47.005608: step 2779, loss 0.15022, acc 0.9375\n",
      "2017-11-05T19:05:51.035282: step 2780, loss 0.285455, acc 0.90625\n",
      "2017-11-05T19:05:55.045961: step 2781, loss 0.263355, acc 0.84375\n",
      "2017-11-05T19:05:59.006292: step 2782, loss 0.214283, acc 0.9375\n",
      "2017-11-05T19:06:03.061396: step 2783, loss 0.0475225, acc 0.96875\n",
      "2017-11-05T19:06:07.009900: step 2784, loss 0.067938, acc 0.96875\n",
      "2017-11-05T19:06:11.005226: step 2785, loss 0.189773, acc 0.90625\n",
      "2017-11-05T19:06:14.972259: step 2786, loss 0.0914694, acc 0.9375\n",
      "2017-11-05T19:06:18.972525: step 2787, loss 0.121677, acc 0.875\n",
      "2017-11-05T19:06:23.107674: step 2788, loss 0.28773, acc 0.875\n",
      "2017-11-05T19:06:27.157455: step 2789, loss 0.301873, acc 0.9375\n",
      "2017-11-05T19:06:31.210811: step 2790, loss 0.211488, acc 0.9375\n",
      "2017-11-05T19:06:35.422138: step 2791, loss 0.0869212, acc 0.96875\n",
      "2017-11-05T19:06:39.472281: step 2792, loss 0.122468, acc 0.90625\n",
      "2017-11-05T19:06:43.531726: step 2793, loss 0.207688, acc 0.90625\n",
      "2017-11-05T19:06:47.487286: step 2794, loss 0.186072, acc 0.90625\n",
      "2017-11-05T19:06:51.517106: step 2795, loss 0.191289, acc 0.9375\n",
      "2017-11-05T19:06:55.546384: step 2796, loss 0.197991, acc 0.875\n",
      "2017-11-05T19:06:59.572823: step 2797, loss 0.188205, acc 0.90625\n",
      "2017-11-05T19:07:03.639983: step 2798, loss 0.12707, acc 0.9375\n",
      "2017-11-05T19:07:07.682482: step 2799, loss 0.178894, acc 0.90625\n",
      "2017-11-05T19:07:11.754282: step 2800, loss 0.197205, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:07:14.274284: step 2800, loss 0.748452, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-05T19:07:19.556139: step 2801, loss 0.234926, acc 0.90625\n",
      "2017-11-05T19:07:23.722281: step 2802, loss 0.271743, acc 0.875\n",
      "2017-11-05T19:07:27.714685: step 2803, loss 0.27336, acc 0.90625\n",
      "2017-11-05T19:07:31.725510: step 2804, loss 0.227433, acc 0.875\n",
      "2017-11-05T19:07:35.764426: step 2805, loss 0.351874, acc 0.8125\n",
      "2017-11-05T19:07:39.737369: step 2806, loss 0.252985, acc 0.90625\n",
      "2017-11-05T19:07:43.836873: step 2807, loss 0.23677, acc 0.90625\n",
      "2017-11-05T19:07:46.504908: step 2808, loss 0.166265, acc 0.9\n",
      "2017-11-05T19:07:50.496042: step 2809, loss 0.320197, acc 0.875\n",
      "2017-11-05T19:07:54.553343: step 2810, loss 0.197935, acc 0.90625\n",
      "2017-11-05T19:07:58.569109: step 2811, loss 0.416496, acc 0.84375\n",
      "2017-11-05T19:08:02.541262: step 2812, loss 0.107886, acc 0.9375\n",
      "2017-11-05T19:08:06.568261: step 2813, loss 0.099502, acc 0.96875\n",
      "2017-11-05T19:08:10.575097: step 2814, loss 0.223031, acc 0.90625\n",
      "2017-11-05T19:08:14.708772: step 2815, loss 0.107573, acc 0.9375\n",
      "2017-11-05T19:08:18.715455: step 2816, loss 0.196367, acc 0.90625\n",
      "2017-11-05T19:08:22.780229: step 2817, loss 0.328085, acc 0.78125\n",
      "2017-11-05T19:08:26.849702: step 2818, loss 0.0561005, acc 1\n",
      "2017-11-05T19:08:30.801492: step 2819, loss 0.163319, acc 0.90625\n",
      "2017-11-05T19:08:35.004940: step 2820, loss 0.133191, acc 0.9375\n",
      "2017-11-05T19:08:39.030584: step 2821, loss 0.166489, acc 0.90625\n",
      "2017-11-05T19:08:43.001719: step 2822, loss 0.170419, acc 0.90625\n",
      "2017-11-05T19:08:47.046180: step 2823, loss 0.187714, acc 0.875\n",
      "2017-11-05T19:08:51.041761: step 2824, loss 0.279207, acc 0.84375\n",
      "2017-11-05T19:08:55.044986: step 2825, loss 0.0671948, acc 0.96875\n",
      "2017-11-05T19:08:59.115119: step 2826, loss 0.330166, acc 0.875\n",
      "2017-11-05T19:09:03.005335: step 2827, loss 0.234721, acc 0.90625\n",
      "2017-11-05T19:09:07.012186: step 2828, loss 0.273003, acc 0.90625\n",
      "2017-11-05T19:09:11.000788: step 2829, loss 0.271457, acc 0.78125\n",
      "2017-11-05T19:09:15.006466: step 2830, loss 0.0361862, acc 1\n",
      "2017-11-05T19:09:19.132855: step 2831, loss 0.304194, acc 0.90625\n",
      "2017-11-05T19:09:23.366436: step 2832, loss 0.0802707, acc 0.96875\n",
      "2017-11-05T19:09:27.638044: step 2833, loss 0.0994579, acc 0.9375\n",
      "2017-11-05T19:09:31.615281: step 2834, loss 0.15181, acc 0.9375\n",
      "2017-11-05T19:09:35.610368: step 2835, loss 0.412284, acc 0.8125\n",
      "2017-11-05T19:09:39.616902: step 2836, loss 0.0915953, acc 0.9375\n",
      "2017-11-05T19:09:43.639926: step 2837, loss 0.113728, acc 0.9375\n",
      "2017-11-05T19:09:47.678604: step 2838, loss 0.0327266, acc 1\n",
      "2017-11-05T19:09:51.710240: step 2839, loss 0.248821, acc 0.9375\n",
      "2017-11-05T19:09:55.700488: step 2840, loss 0.210285, acc 0.90625\n",
      "2017-11-05T19:09:59.737575: step 2841, loss 0.23605, acc 0.9375\n",
      "2017-11-05T19:10:04.022537: step 2842, loss 0.0939824, acc 0.96875\n",
      "2017-11-05T19:10:08.045530: step 2843, loss 0.161466, acc 0.9375\n",
      "2017-11-05T19:10:10.587751: step 2844, loss 0.442562, acc 0.7\n",
      "2017-11-05T19:10:14.723289: step 2845, loss 0.130961, acc 0.90625\n",
      "2017-11-05T19:10:18.744765: step 2846, loss 0.154194, acc 0.90625\n",
      "2017-11-05T19:10:22.759600: step 2847, loss 0.075709, acc 0.9375\n",
      "2017-11-05T19:10:26.827810: step 2848, loss 0.219483, acc 0.84375\n",
      "2017-11-05T19:10:30.811394: step 2849, loss 0.251705, acc 0.8125\n",
      "2017-11-05T19:10:34.988963: step 2850, loss 0.0643413, acc 0.96875\n",
      "2017-11-05T19:10:39.009909: step 2851, loss 0.0804646, acc 0.96875\n",
      "2017-11-05T19:10:42.989708: step 2852, loss 0.0831576, acc 0.96875\n",
      "2017-11-05T19:10:47.041197: step 2853, loss 0.296386, acc 0.90625\n",
      "2017-11-05T19:10:51.055814: step 2854, loss 0.155673, acc 0.9375\n",
      "2017-11-05T19:10:55.075682: step 2855, loss 0.208784, acc 0.90625\n",
      "2017-11-05T19:10:59.103717: step 2856, loss 0.18029, acc 0.90625\n",
      "2017-11-05T19:11:03.125191: step 2857, loss 0.12858, acc 0.90625\n",
      "2017-11-05T19:11:07.225717: step 2858, loss 0.282691, acc 0.875\n",
      "2017-11-05T19:11:11.330424: step 2859, loss 0.360681, acc 0.875\n",
      "2017-11-05T19:11:15.272087: step 2860, loss 0.105836, acc 0.96875\n",
      "2017-11-05T19:11:19.213723: step 2861, loss 0.171312, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T19:11:23.141997: step 2862, loss 0.216641, acc 0.90625\n",
      "2017-11-05T19:11:27.089155: step 2863, loss 0.167396, acc 0.90625\n",
      "2017-11-05T19:11:31.041340: step 2864, loss 0.358555, acc 0.8125\n",
      "2017-11-05T19:11:34.960362: step 2865, loss 0.109958, acc 0.9375\n",
      "2017-11-05T19:11:38.924370: step 2866, loss 0.136148, acc 0.9375\n",
      "2017-11-05T19:11:42.843571: step 2867, loss 0.222825, acc 0.9375\n",
      "2017-11-05T19:11:46.742172: step 2868, loss 0.146691, acc 0.9375\n",
      "2017-11-05T19:11:50.672763: step 2869, loss 0.120963, acc 0.96875\n",
      "2017-11-05T19:11:54.653432: step 2870, loss 0.0794044, acc 0.96875\n",
      "2017-11-05T19:11:58.592493: step 2871, loss 0.234456, acc 0.875\n",
      "2017-11-05T19:12:02.609927: step 2872, loss 0.273942, acc 0.90625\n",
      "2017-11-05T19:12:06.506913: step 2873, loss 0.0521466, acc 1\n",
      "2017-11-05T19:12:10.509838: step 2874, loss 0.08724, acc 0.9375\n",
      "2017-11-05T19:12:14.399795: step 2875, loss 0.25754, acc 0.90625\n",
      "2017-11-05T19:12:18.594264: step 2876, loss 0.0102906, acc 1\n",
      "2017-11-05T19:12:22.489446: step 2877, loss 0.203627, acc 0.90625\n",
      "2017-11-05T19:12:26.440938: step 2878, loss 0.230774, acc 0.875\n",
      "2017-11-05T19:12:30.392001: step 2879, loss 0.0656283, acc 0.96875\n",
      "2017-11-05T19:12:33.050655: step 2880, loss 0.116331, acc 0.95\n",
      "2017-11-05T19:12:37.072336: step 2881, loss 0.165766, acc 0.9375\n",
      "2017-11-05T19:12:40.979084: step 2882, loss 0.144127, acc 0.9375\n",
      "2017-11-05T19:12:44.919910: step 2883, loss 0.224749, acc 0.875\n",
      "2017-11-05T19:12:48.858729: step 2884, loss 0.226933, acc 0.90625\n",
      "2017-11-05T19:12:52.780349: step 2885, loss 0.199346, acc 0.9375\n",
      "2017-11-05T19:12:56.766692: step 2886, loss 0.0539704, acc 1\n",
      "2017-11-05T19:13:00.674502: step 2887, loss 0.193866, acc 0.9375\n",
      "2017-11-05T19:13:04.622837: step 2888, loss 0.0813286, acc 0.96875\n",
      "2017-11-05T19:13:08.571283: step 2889, loss 0.160213, acc 0.9375\n",
      "2017-11-05T19:13:12.528581: step 2890, loss 0.126413, acc 0.90625\n",
      "2017-11-05T19:13:16.464279: step 2891, loss 0.166614, acc 0.90625\n",
      "2017-11-05T19:13:20.470309: step 2892, loss 0.485568, acc 0.875\n",
      "2017-11-05T19:13:24.540837: step 2893, loss 0.128784, acc 0.96875\n",
      "2017-11-05T19:13:28.597178: step 2894, loss 0.134864, acc 0.9375\n",
      "2017-11-05T19:13:32.557499: step 2895, loss 0.0554466, acc 0.96875\n",
      "2017-11-05T19:13:36.457125: step 2896, loss 0.103952, acc 0.9375\n",
      "2017-11-05T19:13:40.385446: step 2897, loss 0.0432164, acc 0.96875\n",
      "2017-11-05T19:13:44.302339: step 2898, loss 0.0920743, acc 0.96875\n",
      "2017-11-05T19:13:48.225012: step 2899, loss 0.13942, acc 0.9375\n",
      "2017-11-05T19:13:52.191063: step 2900, loss 0.0334273, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:13:54.717908: step 2900, loss 0.962415, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-05T19:13:59.857349: step 2901, loss 0.129866, acc 0.9375\n",
      "2017-11-05T19:14:03.819277: step 2902, loss 0.24054, acc 0.84375\n",
      "2017-11-05T19:14:07.752310: step 2903, loss 0.236544, acc 0.90625\n",
      "2017-11-05T19:14:11.738457: step 2904, loss 0.286728, acc 0.875\n",
      "2017-11-05T19:14:15.688303: step 2905, loss 0.0868971, acc 0.96875\n",
      "2017-11-05T19:14:19.760213: step 2906, loss 0.271197, acc 0.90625\n",
      "2017-11-05T19:14:23.660336: step 2907, loss 0.185777, acc 0.875\n",
      "2017-11-05T19:14:27.604570: step 2908, loss 0.328669, acc 0.84375\n",
      "2017-11-05T19:14:31.537621: step 2909, loss 0.0760696, acc 0.96875\n",
      "2017-11-05T19:14:35.648400: step 2910, loss 0.412376, acc 0.8125\n",
      "2017-11-05T19:14:39.574367: step 2911, loss 0.27081, acc 0.84375\n",
      "2017-11-05T19:14:43.516875: step 2912, loss 0.196515, acc 0.9375\n",
      "2017-11-05T19:14:47.475740: step 2913, loss 0.133128, acc 0.9375\n",
      "2017-11-05T19:14:51.398981: step 2914, loss 0.202291, acc 0.875\n",
      "2017-11-05T19:14:55.341169: step 2915, loss 0.248801, acc 0.90625\n",
      "2017-11-05T19:14:57.861135: step 2916, loss 0.0990118, acc 0.95\n",
      "2017-11-05T19:15:01.810364: step 2917, loss 0.24594, acc 0.84375\n",
      "2017-11-05T19:15:05.742835: step 2918, loss 0.0746341, acc 0.96875\n",
      "2017-11-05T19:15:09.690524: step 2919, loss 0.327523, acc 0.90625\n",
      "2017-11-05T19:15:13.634809: step 2920, loss 0.203408, acc 0.9375\n",
      "2017-11-05T19:15:17.576086: step 2921, loss 0.0937331, acc 0.96875\n",
      "2017-11-05T19:15:21.504076: step 2922, loss 0.0707159, acc 0.96875\n",
      "2017-11-05T19:15:25.472903: step 2923, loss 0.0859426, acc 0.96875\n",
      "2017-11-05T19:15:29.397985: step 2924, loss 0.17796, acc 0.90625\n",
      "2017-11-05T19:15:33.356686: step 2925, loss 0.153531, acc 0.9375\n",
      "2017-11-05T19:15:37.303976: step 2926, loss 0.100393, acc 0.96875\n",
      "2017-11-05T19:15:41.218064: step 2927, loss 0.138044, acc 0.9375\n",
      "2017-11-05T19:15:45.131628: step 2928, loss 0.449544, acc 0.71875\n",
      "2017-11-05T19:15:49.091200: step 2929, loss 0.339586, acc 0.875\n",
      "2017-11-05T19:15:53.052450: step 2930, loss 0.086859, acc 0.9375\n",
      "2017-11-05T19:15:56.960533: step 2931, loss 0.233982, acc 0.875\n",
      "2017-11-05T19:16:00.899352: step 2932, loss 0.171856, acc 0.90625\n",
      "2017-11-05T19:16:04.872851: step 2933, loss 0.317941, acc 0.84375\n",
      "2017-11-05T19:16:08.974654: step 2934, loss 0.271495, acc 0.8125\n",
      "2017-11-05T19:16:13.041368: step 2935, loss 0.225013, acc 0.84375\n",
      "2017-11-05T19:16:17.004048: step 2936, loss 0.111167, acc 0.90625\n",
      "2017-11-05T19:16:20.979919: step 2937, loss 0.00963368, acc 1\n",
      "2017-11-05T19:16:24.915471: step 2938, loss 0.076516, acc 0.96875\n",
      "2017-11-05T19:16:28.868899: step 2939, loss 0.569272, acc 0.8125\n",
      "2017-11-05T19:16:32.907713: step 2940, loss 0.615535, acc 0.875\n",
      "2017-11-05T19:16:37.197767: step 2941, loss 0.376102, acc 0.90625\n",
      "2017-11-05T19:16:41.241136: step 2942, loss 0.33539, acc 0.90625\n",
      "2017-11-05T19:16:45.203048: step 2943, loss 0.342448, acc 0.875\n",
      "2017-11-05T19:16:49.197077: step 2944, loss 0.13989, acc 0.90625\n",
      "2017-11-05T19:16:53.156713: step 2945, loss 0.142937, acc 0.90625\n",
      "2017-11-05T19:16:57.163298: step 2946, loss 0.205056, acc 0.90625\n",
      "2017-11-05T19:17:01.103225: step 2947, loss 0.194414, acc 0.9375\n",
      "2017-11-05T19:17:05.056927: step 2948, loss 0.186977, acc 0.9375\n",
      "2017-11-05T19:17:09.063254: step 2949, loss 0.197475, acc 0.875\n",
      "2017-11-05T19:17:13.091102: step 2950, loss 0.14765, acc 0.9375\n",
      "2017-11-05T19:17:17.088196: step 2951, loss 0.184431, acc 0.90625\n",
      "2017-11-05T19:17:19.592344: step 2952, loss 0.300799, acc 0.8\n",
      "2017-11-05T19:17:23.575339: step 2953, loss 0.0976916, acc 0.9375\n",
      "2017-11-05T19:17:27.541712: step 2954, loss 0.110778, acc 0.96875\n",
      "2017-11-05T19:17:31.539419: step 2955, loss 0.0956485, acc 0.90625\n",
      "2017-11-05T19:17:35.557180: step 2956, loss 0.0769466, acc 0.96875\n",
      "2017-11-05T19:17:39.497381: step 2957, loss 0.103524, acc 0.96875\n",
      "2017-11-05T19:17:43.509723: step 2958, loss 0.0690765, acc 0.96875\n",
      "2017-11-05T19:17:47.407270: step 2959, loss 0.190107, acc 0.875\n",
      "2017-11-05T19:17:51.371223: step 2960, loss 0.118038, acc 0.96875\n",
      "2017-11-05T19:17:55.356730: step 2961, loss 0.135464, acc 0.9375\n",
      "2017-11-05T19:17:59.291464: step 2962, loss 0.135168, acc 0.90625\n",
      "2017-11-05T19:18:03.226417: step 2963, loss 0.314198, acc 0.875\n",
      "2017-11-05T19:18:07.135701: step 2964, loss 0.22166, acc 0.84375\n",
      "2017-11-05T19:18:11.160486: step 2965, loss 0.0874646, acc 0.96875\n",
      "2017-11-05T19:18:15.116736: step 2966, loss 0.10804, acc 0.9375\n",
      "2017-11-05T19:18:19.035848: step 2967, loss 0.208282, acc 0.90625\n",
      "2017-11-05T19:18:23.094403: step 2968, loss 0.155215, acc 0.9375\n",
      "2017-11-05T19:18:27.050897: step 2969, loss 0.20652, acc 0.90625\n",
      "2017-11-05T19:18:31.018017: step 2970, loss 0.26956, acc 0.90625\n",
      "2017-11-05T19:18:35.121020: step 2971, loss 0.220219, acc 0.90625\n",
      "2017-11-05T19:18:39.209116: step 2972, loss 0.0707408, acc 0.96875\n",
      "2017-11-05T19:18:43.234519: step 2973, loss 0.138623, acc 0.90625\n",
      "2017-11-05T19:18:47.369535: step 2974, loss 0.114695, acc 0.96875\n",
      "2017-11-05T19:18:51.441099: step 2975, loss 0.175209, acc 0.875\n",
      "2017-11-05T19:18:55.565690: step 2976, loss 0.0910686, acc 0.96875\n",
      "2017-11-05T19:18:59.663287: step 2977, loss 0.351649, acc 0.875\n",
      "2017-11-05T19:19:03.691025: step 2978, loss 0.0738908, acc 0.96875\n",
      "2017-11-05T19:19:07.840539: step 2979, loss 0.349119, acc 0.84375\n",
      "2017-11-05T19:19:11.888825: step 2980, loss 0.297965, acc 0.78125\n",
      "2017-11-05T19:19:16.022639: step 2981, loss 0.217537, acc 0.875\n",
      "2017-11-05T19:19:20.079655: step 2982, loss 0.299714, acc 0.875\n",
      "2017-11-05T19:19:24.349190: step 2983, loss 0.323448, acc 0.75\n",
      "2017-11-05T19:19:28.690174: step 2984, loss 0.211162, acc 0.90625\n",
      "2017-11-05T19:19:32.823441: step 2985, loss 0.362285, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T19:19:37.102559: step 2986, loss 0.203351, acc 0.9375\n",
      "2017-11-05T19:19:41.084565: step 2987, loss 0.294746, acc 0.78125\n",
      "2017-11-05T19:19:43.606871: step 2988, loss 0.0175981, acc 1\n",
      "2017-11-05T19:19:47.529735: step 2989, loss 0.130712, acc 0.90625\n",
      "2017-11-05T19:19:51.461454: step 2990, loss 0.295844, acc 0.84375\n",
      "2017-11-05T19:19:55.459123: step 2991, loss 0.284292, acc 0.90625\n",
      "2017-11-05T19:19:59.391201: step 2992, loss 0.0911337, acc 0.96875\n",
      "2017-11-05T19:20:03.569973: step 2993, loss 0.101846, acc 0.9375\n",
      "2017-11-05T19:20:07.550475: step 2994, loss 0.181377, acc 0.9375\n",
      "2017-11-05T19:20:11.513097: step 2995, loss 0.0863168, acc 0.96875\n",
      "2017-11-05T19:20:15.441057: step 2996, loss 0.194522, acc 0.9375\n",
      "2017-11-05T19:20:19.399481: step 2997, loss 0.239776, acc 0.90625\n",
      "2017-11-05T19:20:23.373442: step 2998, loss 0.130415, acc 0.9375\n",
      "2017-11-05T19:20:27.289963: step 2999, loss 0.17298, acc 0.875\n",
      "2017-11-05T19:20:31.194304: step 3000, loss 0.286162, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:20:33.806661: step 3000, loss 0.859046, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-05T19:20:39.354701: step 3001, loss 0.0562695, acc 0.96875\n",
      "2017-11-05T19:20:43.336289: step 3002, loss 0.0685393, acc 0.96875\n",
      "2017-11-05T19:20:47.337013: step 3003, loss 0.286428, acc 0.8125\n",
      "2017-11-05T19:20:51.297102: step 3004, loss 0.0448163, acc 1\n",
      "2017-11-05T19:20:55.254728: step 3005, loss 0.280778, acc 0.8125\n",
      "2017-11-05T19:20:59.207254: step 3006, loss 0.351864, acc 0.84375\n",
      "2017-11-05T19:21:03.167871: step 3007, loss 0.188782, acc 0.90625\n",
      "2017-11-05T19:21:07.088950: step 3008, loss 0.281599, acc 0.875\n",
      "2017-11-05T19:21:11.107118: step 3009, loss 0.156149, acc 0.9375\n",
      "2017-11-05T19:21:15.034429: step 3010, loss 0.216531, acc 0.9375\n",
      "2017-11-05T19:21:19.017822: step 3011, loss 0.0767831, acc 0.96875\n",
      "2017-11-05T19:21:22.976025: step 3012, loss 0.169746, acc 0.90625\n",
      "2017-11-05T19:21:26.872455: step 3013, loss 0.185796, acc 0.90625\n",
      "2017-11-05T19:21:30.821058: step 3014, loss 0.340676, acc 0.84375\n",
      "2017-11-05T19:21:34.819489: step 3015, loss 0.162358, acc 0.90625\n",
      "2017-11-05T19:21:38.797077: step 3016, loss 0.279943, acc 0.875\n",
      "2017-11-05T19:21:42.807612: step 3017, loss 0.195379, acc 0.84375\n",
      "2017-11-05T19:21:46.759963: step 3018, loss 0.25368, acc 0.84375\n",
      "2017-11-05T19:21:50.735431: step 3019, loss 0.0350001, acc 1\n",
      "2017-11-05T19:21:54.662496: step 3020, loss 0.114872, acc 0.9375\n",
      "2017-11-05T19:21:58.660294: step 3021, loss 0.0564468, acc 1\n",
      "2017-11-05T19:22:02.634743: step 3022, loss 0.139894, acc 0.9375\n",
      "2017-11-05T19:22:06.599877: step 3023, loss 0.151057, acc 0.90625\n",
      "2017-11-05T19:22:09.128089: step 3024, loss 0.189922, acc 0.9\n",
      "2017-11-05T19:22:13.084543: step 3025, loss 0.152998, acc 0.90625\n",
      "2017-11-05T19:22:17.102052: step 3026, loss 0.0237734, acc 1\n",
      "2017-11-05T19:22:21.091709: step 3027, loss 0.160145, acc 0.96875\n",
      "2017-11-05T19:22:25.170596: step 3028, loss 0.166155, acc 0.90625\n",
      "2017-11-05T19:22:29.157411: step 3029, loss 0.36902, acc 0.875\n",
      "2017-11-05T19:22:33.272574: step 3030, loss 0.149816, acc 0.90625\n",
      "2017-11-05T19:22:37.343907: step 3031, loss 0.291189, acc 0.90625\n",
      "2017-11-05T19:22:41.417539: step 3032, loss 0.11488, acc 0.9375\n",
      "2017-11-05T19:22:45.356212: step 3033, loss 0.152201, acc 0.9375\n",
      "2017-11-05T19:22:49.266600: step 3034, loss 0.044408, acc 1\n",
      "2017-11-05T19:22:53.222369: step 3035, loss 0.171255, acc 0.90625\n",
      "2017-11-05T19:22:57.193489: step 3036, loss 0.127705, acc 0.96875\n",
      "2017-11-05T19:23:01.194453: step 3037, loss 0.0720538, acc 0.96875\n",
      "2017-11-05T19:23:05.159716: step 3038, loss 0.232295, acc 0.84375\n",
      "2017-11-05T19:23:09.146570: step 3039, loss 0.165682, acc 0.9375\n",
      "2017-11-05T19:23:13.128000: step 3040, loss 0.16309, acc 0.90625\n",
      "2017-11-05T19:23:17.106629: step 3041, loss 0.162205, acc 0.9375\n",
      "2017-11-05T19:23:21.102226: step 3042, loss 0.0667503, acc 0.9375\n",
      "2017-11-05T19:23:25.157183: step 3043, loss 0.125709, acc 0.9375\n",
      "2017-11-05T19:23:29.201265: step 3044, loss 0.0709043, acc 0.96875\n",
      "2017-11-05T19:23:33.160172: step 3045, loss 0.168629, acc 0.9375\n",
      "2017-11-05T19:23:37.120468: step 3046, loss 0.209512, acc 0.875\n",
      "2017-11-05T19:23:41.121377: step 3047, loss 0.157149, acc 0.90625\n",
      "2017-11-05T19:23:45.160468: step 3048, loss 0.10303, acc 0.9375\n",
      "2017-11-05T19:23:49.119545: step 3049, loss 0.326814, acc 0.875\n",
      "2017-11-05T19:23:53.113959: step 3050, loss 0.216074, acc 0.875\n",
      "2017-11-05T19:23:57.057810: step 3051, loss 0.249887, acc 0.84375\n",
      "2017-11-05T19:24:01.016703: step 3052, loss 0.154726, acc 0.9375\n",
      "2017-11-05T19:24:05.025895: step 3053, loss 0.262947, acc 0.875\n",
      "2017-11-05T19:24:09.014298: step 3054, loss 0.204605, acc 0.875\n",
      "2017-11-05T19:24:13.013170: step 3055, loss 0.111249, acc 0.9375\n",
      "2017-11-05T19:24:16.993362: step 3056, loss 0.116435, acc 0.9375\n",
      "2017-11-05T19:24:20.971515: step 3057, loss 0.0554916, acc 0.96875\n",
      "2017-11-05T19:24:24.912307: step 3058, loss 0.486718, acc 0.71875\n",
      "2017-11-05T19:24:28.929945: step 3059, loss 0.111056, acc 0.9375\n",
      "2017-11-05T19:24:31.442246: step 3060, loss 0.361429, acc 0.8\n",
      "2017-11-05T19:24:35.608819: step 3061, loss 0.0999649, acc 0.96875\n",
      "2017-11-05T19:24:39.607820: step 3062, loss 0.225177, acc 0.9375\n",
      "2017-11-05T19:24:43.604183: step 3063, loss 0.0902843, acc 0.96875\n",
      "2017-11-05T19:24:47.599485: step 3064, loss 0.298886, acc 0.90625\n",
      "2017-11-05T19:24:51.547850: step 3065, loss 0.164119, acc 0.875\n",
      "2017-11-05T19:24:55.488448: step 3066, loss 0.1007, acc 0.9375\n",
      "2017-11-05T19:24:59.525195: step 3067, loss 0.278173, acc 0.84375\n",
      "2017-11-05T19:25:03.491470: step 3068, loss 0.173573, acc 0.90625\n",
      "2017-11-05T19:25:07.475774: step 3069, loss 0.0601431, acc 0.96875\n",
      "2017-11-05T19:25:11.462507: step 3070, loss 0.197101, acc 0.9375\n",
      "2017-11-05T19:25:15.375463: step 3071, loss 0.227647, acc 0.875\n",
      "2017-11-05T19:25:19.359912: step 3072, loss 0.183852, acc 0.84375\n",
      "2017-11-05T19:25:23.317702: step 3073, loss 0.111889, acc 0.96875\n",
      "2017-11-05T19:25:27.260519: step 3074, loss 0.134972, acc 0.9375\n",
      "2017-11-05T19:25:31.220566: step 3075, loss 0.145375, acc 0.90625\n",
      "2017-11-05T19:25:35.228676: step 3076, loss 0.296969, acc 0.84375\n",
      "2017-11-05T19:25:39.207397: step 3077, loss 0.175002, acc 0.96875\n",
      "2017-11-05T19:25:43.182450: step 3078, loss 0.0916919, acc 0.96875\n",
      "2017-11-05T19:25:47.175866: step 3079, loss 0.0815549, acc 0.9375\n",
      "2017-11-05T19:25:51.122362: step 3080, loss 0.196486, acc 0.9375\n",
      "2017-11-05T19:25:55.128649: step 3081, loss 0.204522, acc 0.90625\n",
      "2017-11-05T19:25:59.087980: step 3082, loss 0.374006, acc 0.9375\n",
      "2017-11-05T19:26:03.105503: step 3083, loss 0.168567, acc 0.9375\n",
      "2017-11-05T19:26:07.055570: step 3084, loss 0.0644952, acc 0.9375\n",
      "2017-11-05T19:26:11.056528: step 3085, loss 0.177093, acc 0.9375\n",
      "2017-11-05T19:26:15.031187: step 3086, loss 0.129397, acc 0.90625\n",
      "2017-11-05T19:26:19.107163: step 3087, loss 0.299561, acc 0.84375\n",
      "2017-11-05T19:26:23.146161: step 3088, loss 0.0709582, acc 0.96875\n",
      "2017-11-05T19:26:27.098236: step 3089, loss 0.298757, acc 0.84375\n",
      "2017-11-05T19:26:31.103017: step 3090, loss 0.141629, acc 0.9375\n",
      "2017-11-05T19:26:35.191350: step 3091, loss 0.186793, acc 0.90625\n",
      "2017-11-05T19:26:39.189213: step 3092, loss 0.143024, acc 0.9375\n",
      "2017-11-05T19:26:43.214068: step 3093, loss 0.250255, acc 0.875\n",
      "2017-11-05T19:26:47.217920: step 3094, loss 0.159477, acc 0.9375\n",
      "2017-11-05T19:26:51.165446: step 3095, loss 0.123915, acc 0.96875\n",
      "2017-11-05T19:26:53.772776: step 3096, loss 0.141948, acc 0.9\n",
      "2017-11-05T19:26:57.756944: step 3097, loss 0.115072, acc 0.9375\n",
      "2017-11-05T19:27:01.737241: step 3098, loss 0.140197, acc 0.90625\n",
      "2017-11-05T19:27:05.785910: step 3099, loss 0.0325827, acc 1\n",
      "2017-11-05T19:27:09.982307: step 3100, loss 0.142553, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:27:12.540355: step 3100, loss 1.07402, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-05T19:27:18.147955: step 3101, loss 0.18526, acc 0.90625\n",
      "2017-11-05T19:27:22.133762: step 3102, loss 0.258478, acc 0.90625\n",
      "2017-11-05T19:27:26.096692: step 3103, loss 0.355457, acc 0.875\n",
      "2017-11-05T19:27:30.109972: step 3104, loss 0.226577, acc 0.9375\n",
      "2017-11-05T19:27:34.069048: step 3105, loss 0.0699881, acc 0.96875\n",
      "2017-11-05T19:27:37.989791: step 3106, loss 0.179653, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T19:27:41.986241: step 3107, loss 0.243626, acc 0.90625\n",
      "2017-11-05T19:27:45.972640: step 3108, loss 0.166564, acc 0.875\n",
      "2017-11-05T19:27:49.971983: step 3109, loss 0.0260249, acc 1\n",
      "2017-11-05T19:27:53.941128: step 3110, loss 0.22389, acc 0.90625\n",
      "2017-11-05T19:27:57.885258: step 3111, loss 0.181674, acc 0.96875\n",
      "2017-11-05T19:28:01.863454: step 3112, loss 0.358075, acc 0.875\n",
      "2017-11-05T19:28:05.890963: step 3113, loss 0.24766, acc 0.875\n",
      "2017-11-05T19:28:10.130835: step 3114, loss 0.18346, acc 0.90625\n",
      "2017-11-05T19:28:14.130875: step 3115, loss 0.203737, acc 0.90625\n",
      "2017-11-05T19:28:18.165664: step 3116, loss 0.0720176, acc 0.96875\n",
      "2017-11-05T19:28:22.379075: step 3117, loss 0.199545, acc 0.9375\n",
      "2017-11-05T19:28:26.550854: step 3118, loss 0.0959748, acc 0.96875\n",
      "2017-11-05T19:28:30.559999: step 3119, loss 0.0468945, acc 0.96875\n",
      "2017-11-05T19:28:34.727239: step 3120, loss 0.347498, acc 0.875\n",
      "2017-11-05T19:28:38.741497: step 3121, loss 0.20512, acc 0.90625\n",
      "2017-11-05T19:28:42.700441: step 3122, loss 0.144159, acc 0.9375\n",
      "2017-11-05T19:28:46.681525: step 3123, loss 0.199552, acc 0.9375\n",
      "2017-11-05T19:28:50.687491: step 3124, loss 0.0242568, acc 1\n",
      "2017-11-05T19:28:54.699357: step 3125, loss 0.121703, acc 0.90625\n",
      "2017-11-05T19:28:58.706878: step 3126, loss 0.138124, acc 0.9375\n",
      "2017-11-05T19:29:02.651747: step 3127, loss 0.194865, acc 0.90625\n",
      "2017-11-05T19:29:06.634988: step 3128, loss 0.178055, acc 0.875\n",
      "2017-11-05T19:29:10.643992: step 3129, loss 0.219328, acc 0.90625\n",
      "2017-11-05T19:29:14.636186: step 3130, loss 0.164487, acc 0.9375\n",
      "2017-11-05T19:29:18.641333: step 3131, loss 0.207102, acc 0.875\n",
      "2017-11-05T19:29:21.107412: step 3132, loss 0.274672, acc 0.9\n",
      "2017-11-05T19:29:25.129149: step 3133, loss 0.270355, acc 0.875\n",
      "2017-11-05T19:29:29.089942: step 3134, loss 0.198766, acc 0.875\n",
      "2017-11-05T19:29:33.098751: step 3135, loss 0.0927513, acc 0.9375\n",
      "2017-11-05T19:29:37.032084: step 3136, loss 0.125172, acc 0.9375\n",
      "2017-11-05T19:29:41.010352: step 3137, loss 0.28292, acc 0.78125\n",
      "2017-11-05T19:29:44.985498: step 3138, loss 0.153231, acc 0.90625\n",
      "2017-11-05T19:29:48.983088: step 3139, loss 0.30207, acc 0.84375\n",
      "2017-11-05T19:29:53.021859: step 3140, loss 0.207371, acc 0.90625\n",
      "2017-11-05T19:29:56.964595: step 3141, loss 0.165575, acc 0.90625\n",
      "2017-11-05T19:30:01.166893: step 3142, loss 0.0495915, acc 0.96875\n",
      "2017-11-05T19:30:05.191108: step 3143, loss 0.114672, acc 0.90625\n",
      "2017-11-05T19:30:09.203568: step 3144, loss 0.11389, acc 0.9375\n",
      "2017-11-05T19:30:13.165277: step 3145, loss 0.297828, acc 0.875\n",
      "2017-11-05T19:30:17.169232: step 3146, loss 0.157725, acc 0.875\n",
      "2017-11-05T19:30:21.166959: step 3147, loss 0.405585, acc 0.875\n",
      "2017-11-05T19:30:25.191752: step 3148, loss 0.127708, acc 0.9375\n",
      "2017-11-05T19:30:29.100428: step 3149, loss 0.198191, acc 0.9375\n",
      "2017-11-05T19:30:33.101390: step 3150, loss 0.0955571, acc 0.96875\n",
      "2017-11-05T19:30:37.195724: step 3151, loss 0.152446, acc 0.96875\n",
      "2017-11-05T19:30:41.241919: step 3152, loss 0.0703439, acc 0.9375\n",
      "2017-11-05T19:30:45.129755: step 3153, loss 0.191334, acc 0.90625\n",
      "2017-11-05T19:30:49.124208: step 3154, loss 0.104431, acc 0.9375\n",
      "2017-11-05T19:30:53.196118: step 3155, loss 0.128464, acc 0.90625\n",
      "2017-11-05T19:30:57.133408: step 3156, loss 0.252335, acc 0.875\n",
      "2017-11-05T19:31:01.160564: step 3157, loss 0.284137, acc 0.90625\n",
      "2017-11-05T19:31:05.177956: step 3158, loss 0.159863, acc 0.90625\n",
      "2017-11-05T19:31:09.210124: step 3159, loss 0.234177, acc 0.875\n",
      "2017-11-05T19:31:13.176906: step 3160, loss 0.18958, acc 0.875\n",
      "2017-11-05T19:31:17.210714: step 3161, loss 0.139049, acc 0.9375\n",
      "2017-11-05T19:31:21.189728: step 3162, loss 0.128833, acc 0.9375\n",
      "2017-11-05T19:31:25.175589: step 3163, loss 0.14162, acc 0.96875\n",
      "2017-11-05T19:31:29.137280: step 3164, loss 0.238796, acc 0.875\n",
      "2017-11-05T19:31:33.104611: step 3165, loss 0.191525, acc 0.9375\n",
      "2017-11-05T19:31:37.119377: step 3166, loss 0.309197, acc 0.875\n",
      "2017-11-05T19:31:41.111175: step 3167, loss 0.232976, acc 0.9375\n",
      "2017-11-05T19:31:43.661213: step 3168, loss 0.117011, acc 0.95\n",
      "2017-11-05T19:31:47.603020: step 3169, loss 0.167962, acc 0.96875\n",
      "2017-11-05T19:31:51.653648: step 3170, loss 0.165726, acc 0.90625\n",
      "2017-11-05T19:31:55.692491: step 3171, loss 0.0625074, acc 0.96875\n",
      "2017-11-05T19:31:59.716847: step 3172, loss 0.288872, acc 0.90625\n",
      "2017-11-05T19:32:03.684339: step 3173, loss 0.316502, acc 0.875\n",
      "2017-11-05T19:32:07.659662: step 3174, loss 0.155025, acc 0.90625\n",
      "2017-11-05T19:32:11.675888: step 3175, loss 0.327886, acc 0.90625\n",
      "2017-11-05T19:32:15.653214: step 3176, loss 0.224917, acc 0.875\n",
      "2017-11-05T19:32:19.655002: step 3177, loss 0.1671, acc 0.9375\n",
      "2017-11-05T19:32:23.677076: step 3178, loss 0.241399, acc 0.8125\n",
      "2017-11-05T19:32:27.725760: step 3179, loss 0.171062, acc 0.9375\n",
      "2017-11-05T19:32:31.649050: step 3180, loss 0.366861, acc 0.84375\n",
      "2017-11-05T19:32:35.754003: step 3181, loss 0.327775, acc 0.84375\n",
      "2017-11-05T19:32:39.747333: step 3182, loss 0.331941, acc 0.875\n",
      "2017-11-05T19:32:43.756792: step 3183, loss 0.113982, acc 0.96875\n",
      "2017-11-05T19:32:47.744458: step 3184, loss 0.127203, acc 0.9375\n",
      "2017-11-05T19:32:51.740545: step 3185, loss 0.480718, acc 0.8125\n",
      "2017-11-05T19:32:55.750906: step 3186, loss 0.25213, acc 0.875\n",
      "2017-11-05T19:32:59.734789: step 3187, loss 0.192753, acc 0.875\n",
      "2017-11-05T19:33:03.738079: step 3188, loss 0.033516, acc 0.96875\n",
      "2017-11-05T19:33:07.713021: step 3189, loss 0.030089, acc 0.96875\n",
      "2017-11-05T19:33:11.707424: step 3190, loss 0.0947466, acc 0.96875\n",
      "2017-11-05T19:33:15.733830: step 3191, loss 0.467174, acc 0.90625\n",
      "2017-11-05T19:33:19.723152: step 3192, loss 0.251139, acc 0.9375\n",
      "2017-11-05T19:33:23.831819: step 3193, loss 0.314087, acc 0.90625\n",
      "2017-11-05T19:33:28.075846: step 3194, loss 0.271439, acc 0.90625\n",
      "2017-11-05T19:33:32.220223: step 3195, loss 0.623214, acc 0.875\n",
      "2017-11-05T19:33:36.188179: step 3196, loss 0.378925, acc 0.90625\n",
      "2017-11-05T19:33:40.194834: step 3197, loss 0.0973939, acc 0.9375\n",
      "2017-11-05T19:33:44.206360: step 3198, loss 0.00762576, acc 1\n",
      "2017-11-05T19:33:48.187468: step 3199, loss 0.165912, acc 0.90625\n",
      "2017-11-05T19:33:52.258521: step 3200, loss 0.295411, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:33:54.807138: step 3200, loss 0.889697, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-05T19:34:00.189142: step 3201, loss 0.25117, acc 0.875\n",
      "2017-11-05T19:34:04.187414: step 3202, loss 0.347935, acc 0.84375\n",
      "2017-11-05T19:34:08.145647: step 3203, loss 0.143617, acc 0.875\n",
      "2017-11-05T19:34:10.699970: step 3204, loss 0.327313, acc 0.8\n",
      "2017-11-05T19:34:14.709815: step 3205, loss 0.230923, acc 0.84375\n",
      "2017-11-05T19:34:18.727790: step 3206, loss 0.216872, acc 0.9375\n",
      "2017-11-05T19:34:22.711187: step 3207, loss 0.0273576, acc 1\n",
      "2017-11-05T19:34:26.760189: step 3208, loss 0.258849, acc 0.90625\n",
      "2017-11-05T19:34:30.723008: step 3209, loss 0.105361, acc 0.9375\n",
      "2017-11-05T19:34:34.902288: step 3210, loss 0.109809, acc 0.90625\n",
      "2017-11-05T19:34:38.891863: step 3211, loss 0.340318, acc 0.90625\n",
      "2017-11-05T19:34:42.850672: step 3212, loss 0.0959994, acc 0.9375\n",
      "2017-11-05T19:34:46.851089: step 3213, loss 0.208586, acc 0.90625\n",
      "2017-11-05T19:34:50.821462: step 3214, loss 0.242048, acc 0.90625\n",
      "2017-11-05T19:34:54.813685: step 3215, loss 0.219504, acc 0.90625\n",
      "2017-11-05T19:34:58.809901: step 3216, loss 0.159104, acc 0.90625\n",
      "2017-11-05T19:35:02.853234: step 3217, loss 0.217543, acc 0.84375\n",
      "2017-11-05T19:35:06.781802: step 3218, loss 0.134048, acc 0.9375\n",
      "2017-11-05T19:35:10.789289: step 3219, loss 0.0788509, acc 0.96875\n",
      "2017-11-05T19:35:14.731546: step 3220, loss 0.140828, acc 0.9375\n",
      "2017-11-05T19:35:18.726898: step 3221, loss 0.13336, acc 0.90625\n",
      "2017-11-05T19:35:22.754906: step 3222, loss 0.130616, acc 0.9375\n",
      "2017-11-05T19:35:26.732763: step 3223, loss 0.0838425, acc 0.96875\n",
      "2017-11-05T19:35:30.785766: step 3224, loss 0.167428, acc 0.9375\n",
      "2017-11-05T19:35:34.757586: step 3225, loss 0.118801, acc 0.96875\n",
      "2017-11-05T19:35:38.705482: step 3226, loss 0.0526057, acc 1\n",
      "2017-11-05T19:35:42.661705: step 3227, loss 0.0533755, acc 0.96875\n",
      "2017-11-05T19:35:46.660652: step 3228, loss 0.16766, acc 0.9375\n",
      "2017-11-05T19:35:50.687085: step 3229, loss 0.165925, acc 0.9375\n",
      "2017-11-05T19:35:54.714604: step 3230, loss 0.286097, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T19:35:58.756285: step 3231, loss 0.150727, acc 0.9375\n",
      "2017-11-05T19:36:02.744930: step 3232, loss 0.17311, acc 0.9375\n",
      "2017-11-05T19:36:06.686144: step 3233, loss 0.319366, acc 0.84375\n",
      "2017-11-05T19:36:10.663287: step 3234, loss 0.225369, acc 0.90625\n",
      "2017-11-05T19:36:14.660535: step 3235, loss 0.099863, acc 0.96875\n",
      "2017-11-05T19:36:18.656804: step 3236, loss 0.268954, acc 0.84375\n",
      "2017-11-05T19:36:22.704064: step 3237, loss 0.373404, acc 0.8125\n",
      "2017-11-05T19:36:26.632565: step 3238, loss 0.151578, acc 0.9375\n",
      "2017-11-05T19:36:30.572351: step 3239, loss 0.279655, acc 0.875\n",
      "2017-11-05T19:36:33.277203: step 3240, loss 0.376899, acc 0.9\n",
      "2017-11-05T19:36:37.342194: step 3241, loss 0.226838, acc 0.90625\n",
      "2017-11-05T19:36:41.318936: step 3242, loss 0.396025, acc 0.8125\n",
      "2017-11-05T19:36:45.281177: step 3243, loss 0.179801, acc 0.90625\n",
      "2017-11-05T19:36:49.294963: step 3244, loss 0.0549133, acc 0.96875\n",
      "2017-11-05T19:36:53.275949: step 3245, loss 0.168102, acc 0.90625\n",
      "2017-11-05T19:36:57.243138: step 3246, loss 0.153148, acc 0.9375\n",
      "2017-11-05T19:37:01.209312: step 3247, loss 0.0333046, acc 1\n",
      "2017-11-05T19:37:05.207226: step 3248, loss 0.0503957, acc 0.96875\n",
      "2017-11-05T19:37:09.241202: step 3249, loss 0.150966, acc 0.90625\n",
      "2017-11-05T19:37:13.216858: step 3250, loss 0.220287, acc 0.875\n",
      "2017-11-05T19:37:17.266237: step 3251, loss 0.255422, acc 0.875\n",
      "2017-11-05T19:37:21.307080: step 3252, loss 0.420457, acc 0.84375\n",
      "2017-11-05T19:37:25.427733: step 3253, loss 0.046913, acc 1\n",
      "2017-11-05T19:37:29.497864: step 3254, loss 0.117491, acc 0.9375\n",
      "2017-11-05T19:37:33.440511: step 3255, loss 0.174395, acc 0.90625\n",
      "2017-11-05T19:37:37.561160: step 3256, loss 0.251449, acc 0.875\n",
      "2017-11-05T19:37:41.533693: step 3257, loss 0.244938, acc 0.90625\n",
      "2017-11-05T19:37:45.535290: step 3258, loss 0.120072, acc 0.9375\n",
      "2017-11-05T19:37:49.522483: step 3259, loss 0.229777, acc 0.875\n",
      "2017-11-05T19:37:53.577926: step 3260, loss 0.159682, acc 0.9375\n",
      "2017-11-05T19:37:57.543048: step 3261, loss 0.254566, acc 0.90625\n",
      "2017-11-05T19:38:01.620053: step 3262, loss 0.393175, acc 0.84375\n",
      "2017-11-05T19:38:05.639223: step 3263, loss 0.114863, acc 0.90625\n",
      "2017-11-05T19:38:09.707216: step 3264, loss 0.219115, acc 0.9375\n",
      "2017-11-05T19:38:13.728670: step 3265, loss 0.17426, acc 0.90625\n",
      "2017-11-05T19:38:17.806963: step 3266, loss 0.259738, acc 0.84375\n",
      "2017-11-05T19:38:21.892769: step 3267, loss 0.125437, acc 0.96875\n",
      "2017-11-05T19:38:26.080235: step 3268, loss 0.304177, acc 0.8125\n",
      "2017-11-05T19:38:30.049141: step 3269, loss 0.136305, acc 0.90625\n",
      "2017-11-05T19:38:34.141674: step 3270, loss 0.228578, acc 0.90625\n",
      "2017-11-05T19:38:38.148228: step 3271, loss 0.108762, acc 0.9375\n",
      "2017-11-05T19:38:42.141471: step 3272, loss 0.252711, acc 0.90625\n",
      "2017-11-05T19:38:46.108552: step 3273, loss 0.163771, acc 0.90625\n",
      "2017-11-05T19:38:50.117944: step 3274, loss 0.149751, acc 0.9375\n",
      "2017-11-05T19:38:54.090138: step 3275, loss 0.185051, acc 0.9375\n",
      "2017-11-05T19:38:56.708782: step 3276, loss 0.151213, acc 0.9\n",
      "2017-11-05T19:39:00.692825: step 3277, loss 0.141268, acc 0.9375\n",
      "2017-11-05T19:39:04.687119: step 3278, loss 0.372104, acc 0.90625\n",
      "2017-11-05T19:39:08.676955: step 3279, loss 0.0349279, acc 1\n",
      "2017-11-05T19:39:12.627662: step 3280, loss 0.429664, acc 0.84375\n",
      "2017-11-05T19:39:16.636858: step 3281, loss 0.308528, acc 0.90625\n",
      "2017-11-05T19:39:20.626085: step 3282, loss 0.269036, acc 0.90625\n",
      "2017-11-05T19:39:24.627231: step 3283, loss 0.154204, acc 0.875\n",
      "2017-11-05T19:39:28.636535: step 3284, loss 0.23198, acc 0.9375\n",
      "2017-11-05T19:39:32.604038: step 3285, loss 0.165343, acc 0.90625\n",
      "2017-11-05T19:39:36.635488: step 3286, loss 0.210574, acc 0.875\n",
      "2017-11-05T19:39:40.628997: step 3287, loss 0.201011, acc 0.9375\n",
      "2017-11-05T19:39:44.691230: step 3288, loss 0.340859, acc 0.84375\n",
      "2017-11-05T19:39:48.733237: step 3289, loss 0.149844, acc 0.9375\n",
      "2017-11-05T19:39:52.724551: step 3290, loss 0.178374, acc 0.9375\n",
      "2017-11-05T19:39:56.757321: step 3291, loss 0.215686, acc 0.90625\n",
      "2017-11-05T19:40:01.006999: step 3292, loss 0.0818061, acc 0.9375\n",
      "2017-11-05T19:40:05.119689: step 3293, loss 0.230848, acc 0.90625\n",
      "2017-11-05T19:40:09.118145: step 3294, loss 0.516084, acc 0.71875\n",
      "2017-11-05T19:40:13.156849: step 3295, loss 0.212002, acc 0.90625\n",
      "2017-11-05T19:40:17.164208: step 3296, loss 0.0833065, acc 0.96875\n",
      "2017-11-05T19:40:21.239023: step 3297, loss 0.312324, acc 0.84375\n",
      "2017-11-05T19:40:25.266595: step 3298, loss 0.280574, acc 0.875\n",
      "2017-11-05T19:40:29.304774: step 3299, loss 0.270895, acc 0.875\n",
      "2017-11-05T19:40:33.368378: step 3300, loss 0.259764, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:40:35.987337: step 3300, loss 0.765206, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-05T19:40:41.655530: step 3301, loss 0.1257, acc 0.9375\n",
      "2017-11-05T19:40:45.625520: step 3302, loss 0.44456, acc 0.8125\n",
      "2017-11-05T19:40:49.605023: step 3303, loss 0.172459, acc 0.9375\n",
      "2017-11-05T19:40:53.620170: step 3304, loss 0.164481, acc 0.9375\n",
      "2017-11-05T19:40:57.607543: step 3305, loss 0.217769, acc 0.90625\n",
      "2017-11-05T19:41:01.634669: step 3306, loss 0.236282, acc 0.875\n",
      "2017-11-05T19:41:05.616877: step 3307, loss 0.445437, acc 0.8125\n",
      "2017-11-05T19:41:09.807012: step 3308, loss 0.133576, acc 0.9375\n",
      "2017-11-05T19:41:14.804409: step 3309, loss 0.30371, acc 0.9375\n",
      "2017-11-05T19:41:18.863938: step 3310, loss 0.479519, acc 0.8125\n",
      "2017-11-05T19:41:22.976266: step 3311, loss 0.0809811, acc 0.9375\n",
      "2017-11-05T19:41:25.575933: step 3312, loss 0.0919639, acc 0.95\n",
      "2017-11-05T19:41:29.586899: step 3313, loss 0.101297, acc 0.96875\n",
      "2017-11-05T19:41:33.543268: step 3314, loss 0.269579, acc 0.90625\n",
      "2017-11-05T19:41:37.580212: step 3315, loss 0.0791199, acc 0.96875\n",
      "2017-11-05T19:41:41.570904: step 3316, loss 0.227091, acc 0.875\n",
      "2017-11-05T19:41:45.600959: step 3317, loss 0.104484, acc 0.9375\n",
      "2017-11-05T19:41:49.642230: step 3318, loss 0.170421, acc 0.90625\n",
      "2017-11-05T19:41:53.632372: step 3319, loss 0.0853853, acc 0.96875\n",
      "2017-11-05T19:41:57.688731: step 3320, loss 0.239222, acc 0.84375\n",
      "2017-11-05T19:42:01.610917: step 3321, loss 0.0709943, acc 0.9375\n",
      "2017-11-05T19:42:05.677690: step 3322, loss 0.114169, acc 0.9375\n",
      "2017-11-05T19:42:09.719498: step 3323, loss 0.0344567, acc 1\n",
      "2017-11-05T19:42:13.723993: step 3324, loss 0.165899, acc 0.875\n",
      "2017-11-05T19:42:17.726432: step 3325, loss 0.196402, acc 0.90625\n",
      "2017-11-05T19:42:21.875177: step 3326, loss 0.100391, acc 0.9375\n",
      "2017-11-05T19:42:25.912567: step 3327, loss 0.155399, acc 0.9375\n",
      "2017-11-05T19:42:29.928323: step 3328, loss 0.242027, acc 0.90625\n",
      "2017-11-05T19:42:34.092045: step 3329, loss 0.192554, acc 0.90625\n",
      "2017-11-05T19:42:38.087683: step 3330, loss 0.281121, acc 0.84375\n",
      "2017-11-05T19:42:42.109598: step 3331, loss 0.170729, acc 0.96875\n",
      "2017-11-05T19:42:45.993405: step 3332, loss 0.204795, acc 0.84375\n",
      "2017-11-05T19:42:50.037322: step 3333, loss 0.251966, acc 0.90625\n",
      "2017-11-05T19:42:54.059670: step 3334, loss 0.052742, acc 0.96875\n",
      "2017-11-05T19:42:58.015071: step 3335, loss 0.325074, acc 0.8125\n",
      "2017-11-05T19:43:01.983785: step 3336, loss 0.213453, acc 0.96875\n",
      "2017-11-05T19:43:05.982901: step 3337, loss 0.167209, acc 0.9375\n",
      "2017-11-05T19:43:10.067061: step 3338, loss 0.159604, acc 0.96875\n",
      "2017-11-05T19:43:14.060407: step 3339, loss 0.0259325, acc 1\n",
      "2017-11-05T19:43:18.086916: step 3340, loss 0.210936, acc 0.90625\n",
      "2017-11-05T19:43:22.067147: step 3341, loss 0.497554, acc 0.8125\n",
      "2017-11-05T19:43:26.108937: step 3342, loss 0.264292, acc 0.90625\n",
      "2017-11-05T19:43:30.175940: step 3343, loss 0.301823, acc 0.875\n",
      "2017-11-05T19:43:34.141609: step 3344, loss 0.215372, acc 0.90625\n",
      "2017-11-05T19:43:38.164998: step 3345, loss 0.255076, acc 0.875\n",
      "2017-11-05T19:43:42.091780: step 3346, loss 0.219796, acc 0.875\n",
      "2017-11-05T19:43:46.114944: step 3347, loss 0.214624, acc 0.875\n",
      "2017-11-05T19:43:48.584292: step 3348, loss 0.232613, acc 0.9\n",
      "2017-11-05T19:43:52.607588: step 3349, loss 0.102332, acc 0.9375\n",
      "2017-11-05T19:43:56.638417: step 3350, loss 0.124496, acc 0.9375\n",
      "2017-11-05T19:44:00.705089: step 3351, loss 0.142122, acc 0.9375\n",
      "2017-11-05T19:44:04.696729: step 3352, loss 0.0824208, acc 0.96875\n",
      "2017-11-05T19:44:08.638025: step 3353, loss 0.286285, acc 0.8125\n",
      "2017-11-05T19:44:12.686716: step 3354, loss 0.183572, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T19:44:16.638758: step 3355, loss 0.0979607, acc 0.96875\n",
      "2017-11-05T19:44:20.607436: step 3356, loss 0.0644293, acc 0.96875\n",
      "2017-11-05T19:44:24.791953: step 3357, loss 0.182334, acc 0.90625\n",
      "2017-11-05T19:44:29.146956: step 3358, loss 0.11596, acc 0.9375\n",
      "2017-11-05T19:44:33.389239: step 3359, loss 0.108589, acc 0.96875\n",
      "2017-11-05T19:44:37.495506: step 3360, loss 0.102518, acc 0.9375\n",
      "2017-11-05T19:44:41.462726: step 3361, loss 0.0991022, acc 0.96875\n",
      "2017-11-05T19:44:45.456891: step 3362, loss 0.369395, acc 0.8125\n",
      "2017-11-05T19:44:49.482602: step 3363, loss 0.15739, acc 0.9375\n",
      "2017-11-05T19:44:53.460429: step 3364, loss 0.0529348, acc 0.96875\n",
      "2017-11-05T19:44:57.394830: step 3365, loss 0.0806547, acc 0.96875\n",
      "2017-11-05T19:45:01.388231: step 3366, loss 0.0825129, acc 0.96875\n",
      "2017-11-05T19:45:05.359636: step 3367, loss 0.0781281, acc 0.96875\n",
      "2017-11-05T19:45:09.377461: step 3368, loss 0.113672, acc 0.9375\n",
      "2017-11-05T19:45:13.419159: step 3369, loss 0.0701278, acc 1\n",
      "2017-11-05T19:45:17.505329: step 3370, loss 0.257268, acc 0.875\n",
      "2017-11-05T19:45:21.569536: step 3371, loss 0.143085, acc 0.90625\n",
      "2017-11-05T19:45:25.612449: step 3372, loss 0.11764, acc 0.9375\n",
      "2017-11-05T19:45:29.621128: step 3373, loss 0.167674, acc 0.875\n",
      "2017-11-05T19:45:33.649171: step 3374, loss 0.194892, acc 0.90625\n",
      "2017-11-05T19:45:37.695972: step 3375, loss 0.163073, acc 0.9375\n",
      "2017-11-05T19:45:41.646029: step 3376, loss 0.1215, acc 0.90625\n",
      "2017-11-05T19:45:45.584322: step 3377, loss 0.27373, acc 0.875\n",
      "2017-11-05T19:45:49.580139: step 3378, loss 0.439237, acc 0.78125\n",
      "2017-11-05T19:45:53.552215: step 3379, loss 0.14104, acc 0.90625\n",
      "2017-11-05T19:45:57.510599: step 3380, loss 0.140412, acc 0.9375\n",
      "2017-11-05T19:46:01.536985: step 3381, loss 0.226961, acc 0.90625\n",
      "2017-11-05T19:46:05.465202: step 3382, loss 0.251366, acc 0.84375\n",
      "2017-11-05T19:46:09.473205: step 3383, loss 0.249576, acc 0.875\n",
      "2017-11-05T19:46:12.079133: step 3384, loss 0.306752, acc 0.9\n",
      "2017-11-05T19:46:16.154118: step 3385, loss 0.16225, acc 0.90625\n",
      "2017-11-05T19:46:20.174239: step 3386, loss 0.140229, acc 0.96875\n",
      "2017-11-05T19:46:24.156866: step 3387, loss 0.133149, acc 0.96875\n",
      "2017-11-05T19:46:28.209157: step 3388, loss 0.168965, acc 0.9375\n",
      "2017-11-05T19:46:32.108584: step 3389, loss 0.130218, acc 0.96875\n",
      "2017-11-05T19:46:36.448086: step 3390, loss 0.0498143, acc 0.96875\n",
      "2017-11-05T19:46:40.560490: step 3391, loss 0.435295, acc 0.78125\n",
      "2017-11-05T19:46:44.607528: step 3392, loss 0.270901, acc 0.84375\n",
      "2017-11-05T19:46:48.610875: step 3393, loss 0.143255, acc 0.90625\n",
      "2017-11-05T19:46:52.602422: step 3394, loss 0.089299, acc 0.96875\n",
      "2017-11-05T19:46:56.587135: step 3395, loss 0.113721, acc 0.9375\n",
      "2017-11-05T19:47:00.566257: step 3396, loss 0.176088, acc 0.90625\n",
      "2017-11-05T19:47:04.563809: step 3397, loss 0.228838, acc 0.875\n",
      "2017-11-05T19:47:08.607150: step 3398, loss 0.0787717, acc 0.96875\n",
      "2017-11-05T19:47:12.666108: step 3399, loss 0.169239, acc 0.90625\n",
      "2017-11-05T19:47:16.692381: step 3400, loss 0.226072, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:47:19.201850: step 3400, loss 0.910381, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-05T19:47:24.459614: step 3401, loss 0.225693, acc 0.875\n",
      "2017-11-05T19:47:28.476034: step 3402, loss 0.2273, acc 0.90625\n",
      "2017-11-05T19:47:32.539417: step 3403, loss 0.105754, acc 0.96875\n",
      "2017-11-05T19:47:36.521751: step 3404, loss 0.135765, acc 0.9375\n",
      "2017-11-05T19:47:40.473498: step 3405, loss 0.298198, acc 0.90625\n",
      "2017-11-05T19:47:44.472927: step 3406, loss 0.161389, acc 0.9375\n",
      "2017-11-05T19:47:48.538544: step 3407, loss 0.305149, acc 0.84375\n",
      "2017-11-05T19:47:52.560512: step 3408, loss 0.0975477, acc 0.96875\n",
      "2017-11-05T19:47:56.526036: step 3409, loss 0.124576, acc 0.9375\n",
      "2017-11-05T19:48:00.520165: step 3410, loss 0.210296, acc 0.90625\n",
      "2017-11-05T19:48:04.528931: step 3411, loss 0.049291, acc 0.96875\n",
      "2017-11-05T19:48:08.519039: step 3412, loss 0.221385, acc 0.875\n",
      "2017-11-05T19:48:12.534007: step 3413, loss 0.194307, acc 0.9375\n",
      "2017-11-05T19:48:16.621516: step 3414, loss 0.203349, acc 0.9375\n",
      "2017-11-05T19:48:20.551142: step 3415, loss 0.329718, acc 0.84375\n",
      "2017-11-05T19:48:24.716913: step 3416, loss 0.489246, acc 0.84375\n",
      "2017-11-05T19:48:28.709997: step 3417, loss 0.188785, acc 0.9375\n",
      "2017-11-05T19:48:32.853302: step 3418, loss 0.131635, acc 0.9375\n",
      "2017-11-05T19:48:36.936484: step 3419, loss 0.0782963, acc 0.9375\n",
      "2017-11-05T19:48:39.411101: step 3420, loss 0.388276, acc 0.8\n",
      "2017-11-05T19:48:43.578711: step 3421, loss 0.139045, acc 0.875\n",
      "2017-11-05T19:48:47.626581: step 3422, loss 0.163318, acc 0.875\n",
      "2017-11-05T19:48:51.841465: step 3423, loss 0.212932, acc 0.875\n",
      "2017-11-05T19:48:55.910139: step 3424, loss 0.183733, acc 0.90625\n",
      "2017-11-05T19:49:00.039144: step 3425, loss 0.100078, acc 0.96875\n",
      "2017-11-05T19:49:04.148521: step 3426, loss 0.164926, acc 0.9375\n",
      "2017-11-05T19:49:08.276430: step 3427, loss 0.172957, acc 0.9375\n",
      "2017-11-05T19:49:12.345497: step 3428, loss 0.155073, acc 0.875\n",
      "2017-11-05T19:49:16.494874: step 3429, loss 0.340321, acc 0.875\n",
      "2017-11-05T19:49:20.630229: step 3430, loss 0.144074, acc 0.9375\n",
      "2017-11-05T19:49:24.795538: step 3431, loss 0.0298424, acc 1\n",
      "2017-11-05T19:49:28.910952: step 3432, loss 0.0673072, acc 0.9375\n",
      "2017-11-05T19:49:33.034059: step 3433, loss 0.0472781, acc 0.96875\n",
      "2017-11-05T19:49:37.129029: step 3434, loss 0.0490459, acc 0.96875\n",
      "2017-11-05T19:49:41.260942: step 3435, loss 0.019438, acc 1\n",
      "2017-11-05T19:49:45.229419: step 3436, loss 0.230559, acc 0.8125\n",
      "2017-11-05T19:49:49.232789: step 3437, loss 0.186162, acc 0.90625\n",
      "2017-11-05T19:49:53.289340: step 3438, loss 0.100364, acc 0.96875\n",
      "2017-11-05T19:49:57.204470: step 3439, loss 0.498625, acc 0.8125\n",
      "2017-11-05T19:50:01.501802: step 3440, loss 0.195401, acc 0.875\n",
      "2017-11-05T19:50:05.560518: step 3441, loss 0.35154, acc 0.84375\n",
      "2017-11-05T19:50:09.548175: step 3442, loss 0.0408187, acc 0.96875\n",
      "2017-11-05T19:50:13.485819: step 3443, loss 0.26913, acc 0.8125\n",
      "2017-11-05T19:50:17.543889: step 3444, loss 0.253778, acc 0.875\n",
      "2017-11-05T19:50:21.560571: step 3445, loss 0.37341, acc 0.84375\n",
      "2017-11-05T19:50:25.513452: step 3446, loss 0.148626, acc 0.9375\n",
      "2017-11-05T19:50:29.506923: step 3447, loss 0.257914, acc 0.84375\n",
      "2017-11-05T19:50:33.660704: step 3448, loss 0.201506, acc 0.9375\n",
      "2017-11-05T19:50:37.702581: step 3449, loss 0.448098, acc 0.8125\n",
      "2017-11-05T19:50:41.665467: step 3450, loss 0.224978, acc 0.875\n",
      "2017-11-05T19:50:45.657689: step 3451, loss 0.139569, acc 0.875\n",
      "2017-11-05T19:50:49.713023: step 3452, loss 0.249452, acc 0.90625\n",
      "2017-11-05T19:50:53.700577: step 3453, loss 0.193768, acc 0.875\n",
      "2017-11-05T19:50:57.671115: step 3454, loss 0.0886921, acc 0.9375\n",
      "2017-11-05T19:51:01.660937: step 3455, loss 0.299457, acc 0.78125\n",
      "2017-11-05T19:51:04.186047: step 3456, loss 0.229063, acc 0.85\n",
      "2017-11-05T19:51:08.223646: step 3457, loss 0.116216, acc 0.96875\n",
      "2017-11-05T19:51:12.234352: step 3458, loss 0.11324, acc 0.9375\n",
      "2017-11-05T19:51:16.301695: step 3459, loss 0.227263, acc 0.875\n",
      "2017-11-05T19:51:20.295372: step 3460, loss 0.374161, acc 0.875\n",
      "2017-11-05T19:51:24.365848: step 3461, loss 0.249791, acc 0.90625\n",
      "2017-11-05T19:51:28.388049: step 3462, loss 0.0692706, acc 0.9375\n",
      "2017-11-05T19:51:32.398436: step 3463, loss 0.1571, acc 0.9375\n",
      "2017-11-05T19:51:36.407166: step 3464, loss 0.148769, acc 0.90625\n",
      "2017-11-05T19:51:40.322981: step 3465, loss 0.275803, acc 0.875\n",
      "2017-11-05T19:51:44.350415: step 3466, loss 0.119265, acc 0.96875\n",
      "2017-11-05T19:51:48.339240: step 3467, loss 0.175231, acc 0.875\n",
      "2017-11-05T19:51:52.296229: step 3468, loss 0.0429396, acc 1\n",
      "2017-11-05T19:51:56.326375: step 3469, loss 0.211558, acc 0.9375\n",
      "2017-11-05T19:52:00.286757: step 3470, loss 0.182908, acc 0.90625\n",
      "2017-11-05T19:52:04.342147: step 3471, loss 0.309783, acc 0.84375\n",
      "2017-11-05T19:52:08.341275: step 3472, loss 0.106967, acc 0.96875\n",
      "2017-11-05T19:52:12.437292: step 3473, loss 0.187993, acc 0.90625\n",
      "2017-11-05T19:52:16.433599: step 3474, loss 0.180105, acc 0.90625\n",
      "2017-11-05T19:52:20.441385: step 3475, loss 0.273991, acc 0.90625\n",
      "2017-11-05T19:52:24.512096: step 3476, loss 0.0677657, acc 0.96875\n",
      "2017-11-05T19:52:28.542581: step 3477, loss 0.0932579, acc 0.96875\n",
      "2017-11-05T19:52:32.619772: step 3478, loss 0.391656, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T19:52:36.760155: step 3479, loss 0.165626, acc 0.9375\n",
      "2017-11-05T19:52:40.772864: step 3480, loss 0.196371, acc 0.90625\n",
      "2017-11-05T19:52:44.760060: step 3481, loss 0.199826, acc 0.90625\n",
      "2017-11-05T19:52:48.776363: step 3482, loss 0.140617, acc 0.90625\n",
      "2017-11-05T19:52:52.757426: step 3483, loss 0.218997, acc 0.875\n",
      "2017-11-05T19:52:56.723814: step 3484, loss 0.286795, acc 0.8125\n",
      "2017-11-05T19:53:00.753477: step 3485, loss 0.0915882, acc 0.96875\n",
      "2017-11-05T19:53:04.710396: step 3486, loss 0.220845, acc 0.90625\n",
      "2017-11-05T19:53:08.659186: step 3487, loss 0.163746, acc 0.9375\n",
      "2017-11-05T19:53:12.651707: step 3488, loss 0.162324, acc 0.9375\n",
      "2017-11-05T19:53:16.668580: step 3489, loss 0.0817764, acc 0.96875\n",
      "2017-11-05T19:53:20.707854: step 3490, loss 0.312982, acc 0.84375\n",
      "2017-11-05T19:53:24.857135: step 3491, loss 0.00939383, acc 1\n",
      "2017-11-05T19:53:27.641610: step 3492, loss 0.266387, acc 0.9\n",
      "2017-11-05T19:53:31.786929: step 3493, loss 0.0420035, acc 0.96875\n",
      "2017-11-05T19:53:35.691752: step 3494, loss 0.118784, acc 0.9375\n",
      "2017-11-05T19:53:39.680283: step 3495, loss 0.178034, acc 0.875\n",
      "2017-11-05T19:53:43.690265: step 3496, loss 0.325572, acc 0.84375\n",
      "2017-11-05T19:53:47.732296: step 3497, loss 0.28536, acc 0.875\n",
      "2017-11-05T19:53:51.691921: step 3498, loss 0.165693, acc 0.90625\n",
      "2017-11-05T19:53:55.673494: step 3499, loss 0.359862, acc 0.8125\n",
      "2017-11-05T19:53:59.672750: step 3500, loss 0.120129, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T19:54:02.257886: step 3500, loss 0.804693, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-05T19:54:07.414287: step 3501, loss 0.2692, acc 0.84375\n",
      "2017-11-05T19:54:11.463824: step 3502, loss 0.0329201, acc 1\n",
      "2017-11-05T19:54:15.392619: step 3503, loss 0.134101, acc 0.875\n",
      "2017-11-05T19:54:19.410184: step 3504, loss 0.0248826, acc 1\n",
      "2017-11-05T19:54:23.458238: step 3505, loss 0.282133, acc 0.84375\n",
      "2017-11-05T19:54:27.491803: step 3506, loss 0.125262, acc 0.96875\n",
      "2017-11-05T19:54:31.416825: step 3507, loss 0.284306, acc 0.90625\n",
      "2017-11-05T19:54:35.601358: step 3508, loss 0.118649, acc 0.9375\n",
      "2017-11-05T19:54:39.572322: step 3509, loss 0.186762, acc 0.875\n",
      "2017-11-05T19:54:43.592070: step 3510, loss 0.285422, acc 0.875\n",
      "2017-11-05T19:54:47.520089: step 3511, loss 0.389102, acc 0.90625\n",
      "2017-11-05T19:54:51.541365: step 3512, loss 0.0418174, acc 1\n",
      "2017-11-05T19:54:55.520178: step 3513, loss 0.0807552, acc 0.96875\n",
      "2017-11-05T19:54:59.482839: step 3514, loss 0.12491, acc 0.90625\n",
      "2017-11-05T19:55:03.454756: step 3515, loss 0.328027, acc 0.8125\n",
      "2017-11-05T19:55:07.410698: step 3516, loss 0.055255, acc 0.96875\n",
      "2017-11-05T19:55:11.442988: step 3517, loss 0.22818, acc 0.875\n",
      "2017-11-05T19:55:15.426392: step 3518, loss 0.0935223, acc 0.9375\n",
      "2017-11-05T19:55:19.347073: step 3519, loss 0.301893, acc 0.84375\n",
      "2017-11-05T19:55:23.418336: step 3520, loss 0.117482, acc 0.90625\n",
      "2017-11-05T19:55:27.419773: step 3521, loss 0.210549, acc 0.84375\n",
      "2017-11-05T19:55:31.487106: step 3522, loss 0.0563906, acc 1\n",
      "2017-11-05T19:55:35.500087: step 3523, loss 0.0713026, acc 0.96875\n",
      "2017-11-05T19:55:39.476106: step 3524, loss 0.318051, acc 0.84375\n",
      "2017-11-05T19:55:43.517212: step 3525, loss 0.159417, acc 0.90625\n",
      "2017-11-05T19:55:47.516426: step 3526, loss 0.379107, acc 0.84375\n",
      "2017-11-05T19:55:51.551556: step 3527, loss 0.226496, acc 0.90625\n",
      "2017-11-05T19:55:54.020954: step 3528, loss 0.101287, acc 0.9\n",
      "2017-11-05T19:55:58.034642: step 3529, loss 0.148887, acc 0.90625\n",
      "2017-11-05T19:56:02.022704: step 3530, loss 0.0638181, acc 0.96875\n",
      "2017-11-05T19:56:06.073796: step 3531, loss 0.154249, acc 0.9375\n",
      "2017-11-05T19:56:10.073438: step 3532, loss 0.164477, acc 0.90625\n",
      "2017-11-05T19:56:14.021757: step 3533, loss 0.101091, acc 0.9375\n",
      "2017-11-05T19:56:18.005332: step 3534, loss 0.0444891, acc 0.96875\n",
      "2017-11-05T19:56:22.042360: step 3535, loss 0.206114, acc 0.9375\n",
      "2017-11-05T19:56:26.046985: step 3536, loss 0.188714, acc 0.90625\n",
      "2017-11-05T19:56:30.058656: step 3537, loss 0.316018, acc 0.90625\n",
      "2017-11-05T19:56:34.195177: step 3538, loss 0.173578, acc 0.90625\n",
      "2017-11-05T19:56:38.298996: step 3539, loss 0.328623, acc 0.875\n",
      "2017-11-05T19:56:42.319701: step 3540, loss 0.0443078, acc 0.96875\n",
      "2017-11-05T19:56:46.267070: step 3541, loss 0.116651, acc 0.9375\n",
      "2017-11-05T19:56:50.260710: step 3542, loss 0.163319, acc 0.90625\n",
      "2017-11-05T19:56:54.304602: step 3543, loss 0.199385, acc 0.90625\n",
      "2017-11-05T19:56:58.252040: step 3544, loss 0.133502, acc 0.96875\n",
      "2017-11-05T19:57:02.175820: step 3545, loss 0.149906, acc 0.9375\n",
      "2017-11-05T19:57:06.224611: step 3546, loss 0.189827, acc 0.90625\n",
      "2017-11-05T19:57:10.218033: step 3547, loss 0.114898, acc 0.9375\n",
      "2017-11-05T19:57:14.185631: step 3548, loss 0.157323, acc 0.90625\n",
      "2017-11-05T19:57:18.237104: step 3549, loss 0.21296, acc 0.9375\n",
      "2017-11-05T19:57:22.245282: step 3550, loss 0.10296, acc 0.96875\n",
      "2017-11-05T19:57:26.235364: step 3551, loss 0.236945, acc 0.875\n",
      "2017-11-05T19:57:30.285433: step 3552, loss 0.0662956, acc 0.96875\n",
      "2017-11-05T19:57:34.260851: step 3553, loss 0.371436, acc 0.90625\n",
      "2017-11-05T19:57:38.213581: step 3554, loss 0.0445375, acc 1\n",
      "2017-11-05T19:57:42.201051: step 3555, loss 0.0767035, acc 1\n",
      "2017-11-05T19:57:46.191754: step 3556, loss 0.166393, acc 0.90625\n",
      "2017-11-05T19:57:50.287114: step 3557, loss 0.336789, acc 0.8125\n",
      "2017-11-05T19:57:54.357477: step 3558, loss 0.223797, acc 0.875\n",
      "2017-11-05T19:57:58.320230: step 3559, loss 0.0547366, acc 0.96875\n",
      "2017-11-05T19:58:02.423140: step 3560, loss 0.198149, acc 0.90625\n",
      "2017-11-05T19:58:06.342821: step 3561, loss 0.17973, acc 0.90625\n",
      "2017-11-05T19:58:10.351377: step 3562, loss 0.211459, acc 0.875\n",
      "2017-11-05T19:58:14.323317: step 3563, loss 0.272371, acc 0.84375\n",
      "2017-11-05T19:58:16.858098: step 3564, loss 0.0983094, acc 0.95\n",
      "2017-11-05T19:58:20.827367: step 3565, loss 0.134969, acc 0.90625\n",
      "2017-11-05T19:58:24.947937: step 3566, loss 0.26396, acc 0.875\n",
      "2017-11-05T19:58:29.224939: step 3567, loss 0.0779109, acc 0.96875\n",
      "2017-11-05T19:58:33.490362: step 3568, loss 0.111675, acc 0.9375\n",
      "2017-11-05T19:58:37.595289: step 3569, loss 0.192202, acc 0.9375\n",
      "2017-11-05T19:58:41.560998: step 3570, loss 0.131741, acc 0.9375\n",
      "2017-11-05T19:58:45.489184: step 3571, loss 0.357223, acc 0.84375\n",
      "2017-11-05T19:58:49.422635: step 3572, loss 0.195211, acc 0.90625\n",
      "2017-11-05T19:58:53.400236: step 3573, loss 0.035246, acc 1\n",
      "2017-11-05T19:58:57.329976: step 3574, loss 0.167001, acc 0.90625\n",
      "2017-11-05T19:59:01.282575: step 3575, loss 0.103575, acc 0.9375\n",
      "2017-11-05T19:59:05.218722: step 3576, loss 0.217711, acc 0.875\n",
      "2017-11-05T19:59:09.182476: step 3577, loss 0.275484, acc 0.875\n",
      "2017-11-05T19:59:13.201162: step 3578, loss 0.235033, acc 0.90625\n",
      "2017-11-05T19:59:17.140878: step 3579, loss 0.152635, acc 0.875\n",
      "2017-11-05T19:59:21.061187: step 3580, loss 0.245904, acc 0.90625\n",
      "2017-11-05T19:59:25.037367: step 3581, loss 0.0891525, acc 0.9375\n",
      "2017-11-05T19:59:29.005688: step 3582, loss 0.217472, acc 0.90625\n",
      "2017-11-05T19:59:32.990616: step 3583, loss 0.0492538, acc 1\n",
      "2017-11-05T19:59:36.936063: step 3584, loss 0.190086, acc 0.90625\n",
      "2017-11-05T19:59:40.867026: step 3585, loss 0.0882018, acc 0.9375\n",
      "2017-11-05T19:59:44.803457: step 3586, loss 0.109598, acc 0.9375\n",
      "2017-11-05T19:59:48.729409: step 3587, loss 0.175202, acc 0.90625\n",
      "2017-11-05T19:59:52.707363: step 3588, loss 0.10656, acc 0.9375\n",
      "2017-11-05T19:59:56.645402: step 3589, loss 0.284342, acc 0.875\n",
      "2017-11-05T20:00:00.694869: step 3590, loss 0.222809, acc 0.9375\n",
      "2017-11-05T20:00:04.670911: step 3591, loss 0.156317, acc 0.90625\n",
      "2017-11-05T20:00:08.610395: step 3592, loss 0.2062, acc 0.90625\n",
      "2017-11-05T20:00:12.543413: step 3593, loss 0.143442, acc 0.90625\n",
      "2017-11-05T20:00:16.490555: step 3594, loss 0.237287, acc 0.90625\n",
      "2017-11-05T20:00:20.461003: step 3595, loss 0.207501, acc 0.90625\n",
      "2017-11-05T20:00:24.386692: step 3596, loss 0.181647, acc 0.875\n",
      "2017-11-05T20:00:28.311035: step 3597, loss 0.157999, acc 0.9375\n",
      "2017-11-05T20:00:32.331205: step 3598, loss 0.150174, acc 0.90625\n",
      "2017-11-05T20:00:36.376391: step 3599, loss 0.273318, acc 0.90625\n",
      "2017-11-05T20:00:38.893711: step 3600, loss 0.185898, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:00:41.416035: step 3600, loss 0.949174, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\2\\1509872314\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C9097DD8>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\n",
      "\n",
      "2017-11-05T20:00:51.457646: step 1, loss 0.593064, acc 0.8125\n",
      "2017-11-05T20:00:55.390837: step 2, loss 1.3886, acc 0.8125\n",
      "2017-11-05T20:00:59.328988: step 3, loss 1.07014, acc 0.84375\n",
      "2017-11-05T20:01:03.310717: step 4, loss 0.782752, acc 0.9375\n",
      "2017-11-05T20:01:07.319494: step 5, loss 0.0658503, acc 0.96875\n",
      "2017-11-05T20:01:11.293029: step 6, loss 0.508226, acc 0.96875\n",
      "2017-11-05T20:01:15.272696: step 7, loss 0.812579, acc 0.90625\n",
      "2017-11-05T20:01:19.221793: step 8, loss 2.07526, acc 0.875\n",
      "2017-11-05T20:01:23.165115: step 9, loss 1.03706, acc 0.90625\n",
      "2017-11-05T20:01:27.126340: step 10, loss 2.52537, acc 0.8125\n",
      "2017-11-05T20:01:31.037964: step 11, loss 0.550816, acc 0.96875\n",
      "2017-11-05T20:01:35.025059: step 12, loss 2.99107, acc 0.78125\n",
      "2017-11-05T20:01:38.956211: step 13, loss 1.11165, acc 0.8125\n",
      "2017-11-05T20:01:42.876332: step 14, loss 2.35112, acc 0.75\n",
      "2017-11-05T20:01:46.789906: step 15, loss 1.28522, acc 0.875\n",
      "2017-11-05T20:01:50.767993: step 16, loss 1.08733, acc 0.875\n",
      "2017-11-05T20:01:54.713584: step 17, loss 1.03533, acc 0.71875\n",
      "2017-11-05T20:01:58.657490: step 18, loss 2.34312, acc 0.625\n",
      "2017-11-05T20:02:02.547890: step 19, loss 1.41491, acc 0.6875\n",
      "2017-11-05T20:02:06.457823: step 20, loss 2.89075, acc 0.53125\n",
      "2017-11-05T20:02:10.460720: step 21, loss 1.87449, acc 0.6875\n",
      "2017-11-05T20:02:14.439063: step 22, loss 0.717942, acc 0.875\n",
      "2017-11-05T20:02:18.389028: step 23, loss 0.753143, acc 0.8125\n",
      "2017-11-05T20:02:22.295276: step 24, loss 0.524255, acc 0.8125\n",
      "2017-11-05T20:02:26.237018: step 25, loss 1.04043, acc 0.78125\n",
      "2017-11-05T20:02:30.204671: step 26, loss 2.04176, acc 0.78125\n",
      "2017-11-05T20:02:34.342115: step 27, loss 1.32469, acc 0.75\n",
      "2017-11-05T20:02:38.362779: step 28, loss 1.9186, acc 0.84375\n",
      "2017-11-05T20:02:42.290634: step 29, loss 1.46151, acc 0.90625\n",
      "2017-11-05T20:02:46.238783: step 30, loss 2.03593, acc 0.875\n",
      "2017-11-05T20:02:50.203566: step 31, loss 2.47903, acc 0.84375\n",
      "2017-11-05T20:02:54.193286: step 32, loss 0.867242, acc 0.9375\n",
      "2017-11-05T20:02:58.179177: step 33, loss 0.630786, acc 0.96875\n",
      "2017-11-05T20:03:02.115035: step 34, loss 1.71255, acc 0.9375\n",
      "2017-11-05T20:03:06.060155: step 35, loss 2.38979, acc 0.84375\n",
      "2017-11-05T20:03:08.585425: step 36, loss 0.679051, acc 0.9\n",
      "2017-11-05T20:03:12.523456: step 37, loss 1.03793, acc 0.84375\n",
      "2017-11-05T20:03:16.430523: step 38, loss 0.550445, acc 0.875\n",
      "2017-11-05T20:03:20.396330: step 39, loss 1.7591, acc 0.78125\n",
      "2017-11-05T20:03:24.423258: step 40, loss 0.887579, acc 0.84375\n",
      "2017-11-05T20:03:28.503517: step 41, loss 1.45923, acc 0.84375\n",
      "2017-11-05T20:03:32.615232: step 42, loss 0.873262, acc 0.84375\n",
      "2017-11-05T20:03:36.622341: step 43, loss 0.510564, acc 0.8125\n",
      "2017-11-05T20:03:40.600906: step 44, loss 1.2894, acc 0.78125\n",
      "2017-11-05T20:03:44.513274: step 45, loss 1.70833, acc 0.71875\n",
      "2017-11-05T20:03:48.476483: step 46, loss 0.127964, acc 0.96875\n",
      "2017-11-05T20:03:52.407900: step 47, loss 0.276993, acc 0.875\n",
      "2017-11-05T20:03:56.445963: step 48, loss 0.631651, acc 0.875\n",
      "2017-11-05T20:04:00.383058: step 49, loss 0.732411, acc 0.9375\n",
      "2017-11-05T20:04:04.323342: step 50, loss 0.664531, acc 0.84375\n",
      "2017-11-05T20:04:08.283131: step 51, loss 0.888598, acc 0.84375\n",
      "2017-11-05T20:04:12.264609: step 52, loss 0.737246, acc 0.84375\n",
      "2017-11-05T20:04:16.226394: step 53, loss 0.872293, acc 0.8125\n",
      "2017-11-05T20:04:20.151727: step 54, loss 1.31928, acc 0.8125\n",
      "2017-11-05T20:04:24.107501: step 55, loss 1.65886, acc 0.75\n",
      "2017-11-05T20:04:28.060888: step 56, loss 0.349193, acc 0.90625\n",
      "2017-11-05T20:04:31.978354: step 57, loss 0.734909, acc 0.90625\n",
      "2017-11-05T20:04:36.107126: step 58, loss 0.796163, acc 0.875\n",
      "2017-11-05T20:04:40.084157: step 59, loss 0.293498, acc 0.9375\n",
      "2017-11-05T20:04:44.027263: step 60, loss 1.04606, acc 0.8125\n",
      "2017-11-05T20:04:47.980000: step 61, loss 1.0207, acc 0.8125\n",
      "2017-11-05T20:04:51.947625: step 62, loss 2.10911, acc 0.8125\n",
      "2017-11-05T20:04:55.884363: step 63, loss 1.10931, acc 0.84375\n",
      "2017-11-05T20:04:59.832462: step 64, loss 1.17037, acc 0.8125\n",
      "2017-11-05T20:05:03.792411: step 65, loss 0.440023, acc 0.90625\n",
      "2017-11-05T20:05:07.784782: step 66, loss 0.330257, acc 0.84375\n",
      "2017-11-05T20:05:11.752720: step 67, loss 0.797862, acc 0.84375\n",
      "2017-11-05T20:05:15.720209: step 68, loss 0.556844, acc 0.84375\n",
      "2017-11-05T20:05:19.660262: step 69, loss 2.10317, acc 0.71875\n",
      "2017-11-05T20:05:23.610325: step 70, loss 1.14732, acc 0.8125\n",
      "2017-11-05T20:05:27.592355: step 71, loss 0.966112, acc 0.84375\n",
      "2017-11-05T20:05:30.089926: step 72, loss 1.75693, acc 0.75\n",
      "2017-11-05T20:05:34.023137: step 73, loss 0.206944, acc 0.90625\n",
      "2017-11-05T20:05:37.998813: step 74, loss 1.02127, acc 0.71875\n",
      "2017-11-05T20:05:42.004967: step 75, loss 0.955401, acc 0.8125\n",
      "2017-11-05T20:05:45.929623: step 76, loss 0.478958, acc 0.84375\n",
      "2017-11-05T20:05:49.860859: step 77, loss 0.699583, acc 0.84375\n",
      "2017-11-05T20:05:53.800575: step 78, loss 0.920169, acc 0.875\n",
      "2017-11-05T20:05:57.719367: step 79, loss 0.581587, acc 0.9375\n",
      "2017-11-05T20:06:01.736943: step 80, loss 0.51529, acc 0.75\n",
      "2017-11-05T20:06:05.678529: step 81, loss 0.41993, acc 0.84375\n",
      "2017-11-05T20:06:09.638468: step 82, loss 0.71082, acc 0.84375\n",
      "2017-11-05T20:06:13.557021: step 83, loss 0.742074, acc 0.84375\n",
      "2017-11-05T20:06:17.570129: step 84, loss 1.09838, acc 0.78125\n",
      "2017-11-05T20:06:21.507450: step 85, loss 1.02071, acc 0.84375\n",
      "2017-11-05T20:06:25.475912: step 86, loss 0.150876, acc 0.96875\n",
      "2017-11-05T20:06:29.414147: step 87, loss 0.815495, acc 0.84375\n",
      "2017-11-05T20:06:33.423318: step 88, loss 0.0319939, acc 1\n",
      "2017-11-05T20:06:37.474085: step 89, loss 0.748022, acc 0.90625\n",
      "2017-11-05T20:06:41.473086: step 90, loss 1.233, acc 0.78125\n",
      "2017-11-05T20:06:45.402053: step 91, loss 0.607096, acc 0.90625\n",
      "2017-11-05T20:06:49.341324: step 92, loss 0.701566, acc 0.90625\n",
      "2017-11-05T20:06:53.341970: step 93, loss 0.526032, acc 0.875\n",
      "2017-11-05T20:06:57.340763: step 94, loss 0.978953, acc 0.875\n",
      "2017-11-05T20:07:01.264721: step 95, loss 0.434676, acc 0.96875\n",
      "2017-11-05T20:07:05.211255: step 96, loss 0.759883, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T20:07:09.191134: step 97, loss 0.832066, acc 0.875\n",
      "2017-11-05T20:07:13.105039: step 98, loss 0.787777, acc 0.84375\n",
      "2017-11-05T20:07:17.110866: step 99, loss 0.553336, acc 0.875\n",
      "2017-11-05T20:07:21.014516: step 100, loss 0.946099, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:07:23.525453: step 100, loss 1.33748, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-100\n",
      "\n",
      "2017-11-05T20:07:28.902659: step 101, loss 0.61211, acc 0.8125\n",
      "2017-11-05T20:07:32.833608: step 102, loss 0.567651, acc 0.8125\n",
      "2017-11-05T20:07:36.862587: step 103, loss 0.639326, acc 0.8125\n",
      "2017-11-05T20:07:40.822376: step 104, loss 0.639129, acc 0.875\n",
      "2017-11-05T20:07:44.776577: step 105, loss 0.616428, acc 0.84375\n",
      "2017-11-05T20:07:48.753114: step 106, loss 0.561992, acc 0.9375\n",
      "2017-11-05T20:07:52.702750: step 107, loss 0.396122, acc 0.90625\n",
      "2017-11-05T20:07:55.223581: step 108, loss 2.08811, acc 0.8\n",
      "2017-11-05T20:07:59.172947: step 109, loss 0.369554, acc 0.9375\n",
      "2017-11-05T20:08:03.150124: step 110, loss 1.21324, acc 0.78125\n",
      "2017-11-05T20:08:07.152680: step 111, loss 1.12977, acc 0.8125\n",
      "2017-11-05T20:08:11.119987: step 112, loss 0.590494, acc 0.875\n",
      "2017-11-05T20:08:15.032615: step 113, loss 0.229226, acc 0.90625\n",
      "2017-11-05T20:08:19.020291: step 114, loss 2.00228, acc 0.6875\n",
      "2017-11-05T20:08:23.089519: step 115, loss 0.532827, acc 0.875\n",
      "2017-11-05T20:08:27.172919: step 116, loss 1.10922, acc 0.75\n",
      "2017-11-05T20:08:31.156070: step 117, loss 0.253065, acc 0.9375\n",
      "2017-11-05T20:08:35.225117: step 118, loss 1.25217, acc 0.75\n",
      "2017-11-05T20:08:39.236013: step 119, loss 0.450189, acc 0.84375\n",
      "2017-11-05T20:08:43.239912: step 120, loss 0.343229, acc 0.90625\n",
      "2017-11-05T20:08:47.185791: step 121, loss 0.989161, acc 0.875\n",
      "2017-11-05T20:08:51.181040: step 122, loss 0.615692, acc 0.875\n",
      "2017-11-05T20:08:55.206316: step 123, loss 0.224343, acc 0.9375\n",
      "2017-11-05T20:08:59.179663: step 124, loss 0.369188, acc 0.875\n",
      "2017-11-05T20:09:03.115460: step 125, loss 1.01495, acc 0.84375\n",
      "2017-11-05T20:09:07.089698: step 126, loss 0.688838, acc 0.875\n",
      "2017-11-05T20:09:11.051859: step 127, loss 0.388364, acc 0.90625\n",
      "2017-11-05T20:09:14.986139: step 128, loss 0.41634, acc 0.9375\n",
      "2017-11-05T20:09:18.973429: step 129, loss 0.691945, acc 0.875\n",
      "2017-11-05T20:09:22.918115: step 130, loss 0.371182, acc 0.9375\n",
      "2017-11-05T20:09:26.879235: step 131, loss 0.275062, acc 0.9375\n",
      "2017-11-05T20:09:30.801805: step 132, loss 0.672288, acc 0.90625\n",
      "2017-11-05T20:09:34.754888: step 133, loss 0.700768, acc 0.90625\n",
      "2017-11-05T20:09:38.721193: step 134, loss 0.0773363, acc 0.96875\n",
      "2017-11-05T20:09:42.676210: step 135, loss 0.401674, acc 0.96875\n",
      "2017-11-05T20:09:46.592342: step 136, loss 0.988487, acc 0.8125\n",
      "2017-11-05T20:09:50.576184: step 137, loss 0.375043, acc 0.9375\n",
      "2017-11-05T20:09:54.528854: step 138, loss 0.0821507, acc 0.96875\n",
      "2017-11-05T20:09:58.460783: step 139, loss 0.629634, acc 0.90625\n",
      "2017-11-05T20:10:02.728454: step 140, loss 0.878479, acc 0.84375\n",
      "2017-11-05T20:10:06.705393: step 141, loss 1.11797, acc 0.875\n",
      "2017-11-05T20:10:10.661456: step 142, loss 0.392396, acc 0.875\n",
      "2017-11-05T20:10:14.684936: step 143, loss 0.886515, acc 0.78125\n",
      "2017-11-05T20:10:17.252618: step 144, loss 0.937525, acc 0.85\n",
      "2017-11-05T20:10:21.172952: step 145, loss 0.137533, acc 0.9375\n",
      "2017-11-05T20:10:25.086753: step 146, loss 0.757814, acc 0.8125\n",
      "2017-11-05T20:10:29.035350: step 147, loss 0.283682, acc 0.875\n",
      "2017-11-05T20:10:33.101263: step 148, loss 1.15995, acc 0.75\n",
      "2017-11-05T20:10:37.166532: step 149, loss 1.07747, acc 0.78125\n",
      "2017-11-05T20:10:41.143975: step 150, loss 0.469554, acc 0.875\n",
      "2017-11-05T20:10:45.113473: step 151, loss 0.352727, acc 0.875\n",
      "2017-11-05T20:10:49.130600: step 152, loss 0.700157, acc 0.84375\n",
      "2017-11-05T20:10:53.084021: step 153, loss 0.780853, acc 0.90625\n",
      "2017-11-05T20:10:57.097283: step 154, loss 0.863675, acc 0.875\n",
      "2017-11-05T20:11:01.091387: step 155, loss 0.200023, acc 0.9375\n",
      "2017-11-05T20:11:05.038788: step 156, loss 0.351744, acc 0.9375\n",
      "2017-11-05T20:11:08.986603: step 157, loss 1.02485, acc 0.84375\n",
      "2017-11-05T20:11:12.893034: step 158, loss 0.55122, acc 0.90625\n",
      "2017-11-05T20:11:16.904205: step 159, loss 0.303634, acc 0.9375\n",
      "2017-11-05T20:11:20.851110: step 160, loss 1.1788, acc 0.84375\n",
      "2017-11-05T20:11:24.860367: step 161, loss 1.41684, acc 0.75\n",
      "2017-11-05T20:11:28.807407: step 162, loss 0.7711, acc 0.8125\n",
      "2017-11-05T20:11:32.753847: step 163, loss 0.485747, acc 0.90625\n",
      "2017-11-05T20:11:36.713196: step 164, loss 0.665496, acc 0.8125\n",
      "2017-11-05T20:11:40.692542: step 165, loss 0.270624, acc 0.90625\n",
      "2017-11-05T20:11:44.641622: step 166, loss 0.451601, acc 0.90625\n",
      "2017-11-05T20:11:48.634766: step 167, loss 1.1464, acc 0.84375\n",
      "2017-11-05T20:11:52.540959: step 168, loss 1.03631, acc 0.75\n",
      "2017-11-05T20:11:56.536129: step 169, loss 1.52868, acc 0.78125\n",
      "2017-11-05T20:12:00.482771: step 170, loss 0.467831, acc 0.90625\n",
      "2017-11-05T20:12:04.450040: step 171, loss 1.21767, acc 0.78125\n",
      "2017-11-05T20:12:08.397884: step 172, loss 0.931587, acc 0.71875\n",
      "2017-11-05T20:12:12.432661: step 173, loss 0.462841, acc 0.84375\n",
      "2017-11-05T20:12:16.548786: step 174, loss 0.584807, acc 0.84375\n",
      "2017-11-05T20:12:20.481092: step 175, loss 0.504509, acc 0.875\n",
      "2017-11-05T20:12:24.457568: step 176, loss 0.443268, acc 0.84375\n",
      "2017-11-05T20:12:28.471014: step 177, loss 0.519499, acc 0.875\n",
      "2017-11-05T20:12:32.398245: step 178, loss 0.549353, acc 0.84375\n",
      "2017-11-05T20:12:36.500487: step 179, loss 0.555316, acc 0.90625\n",
      "2017-11-05T20:12:39.001584: step 180, loss 0.177572, acc 0.95\n",
      "2017-11-05T20:12:42.909061: step 181, loss 1.60609, acc 0.875\n",
      "2017-11-05T20:12:46.921011: step 182, loss 0.816545, acc 0.90625\n",
      "2017-11-05T20:12:50.860891: step 183, loss 0.146957, acc 0.9375\n",
      "2017-11-05T20:12:54.808741: step 184, loss 0.0188675, acc 1\n",
      "2017-11-05T20:12:58.864801: step 185, loss 0.855459, acc 0.875\n",
      "2017-11-05T20:13:02.794790: step 186, loss 0.406392, acc 0.90625\n",
      "2017-11-05T20:13:06.738867: step 187, loss 1.39358, acc 0.8125\n",
      "2017-11-05T20:13:10.704149: step 188, loss 0.424917, acc 0.90625\n",
      "2017-11-05T20:13:14.686155: step 189, loss 0.748585, acc 0.9375\n",
      "2017-11-05T20:13:18.722932: step 190, loss 0.518041, acc 0.90625\n",
      "2017-11-05T20:13:22.778984: step 191, loss 0.274458, acc 0.875\n",
      "2017-11-05T20:13:26.994628: step 192, loss 0.487263, acc 0.9375\n",
      "2017-11-05T20:13:30.926776: step 193, loss 0.607288, acc 0.84375\n",
      "2017-11-05T20:13:35.062250: step 194, loss 0.677575, acc 0.875\n",
      "2017-11-05T20:13:39.083443: step 195, loss 0.253446, acc 0.875\n",
      "2017-11-05T20:13:43.053471: step 196, loss 0.116251, acc 0.9375\n",
      "2017-11-05T20:13:47.026250: step 197, loss 0.490189, acc 0.84375\n",
      "2017-11-05T20:13:50.978751: step 198, loss 0.834048, acc 0.8125\n",
      "2017-11-05T20:13:54.975518: step 199, loss 0.0524355, acc 0.96875\n",
      "2017-11-05T20:13:58.989019: step 200, loss 1.07728, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:14:01.555551: step 200, loss 1.47126, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-200\n",
      "\n",
      "2017-11-05T20:14:06.863180: step 201, loss 0.43146, acc 0.90625\n",
      "2017-11-05T20:14:10.873213: step 202, loss 0.581141, acc 0.875\n",
      "2017-11-05T20:14:14.799156: step 203, loss 0.461082, acc 0.78125\n",
      "2017-11-05T20:14:18.875987: step 204, loss 1.24367, acc 0.75\n",
      "2017-11-05T20:14:22.790986: step 205, loss 0.876227, acc 0.84375\n",
      "2017-11-05T20:14:26.830556: step 206, loss 0.704317, acc 0.84375\n",
      "2017-11-05T20:14:30.893160: step 207, loss 0.21465, acc 0.9375\n",
      "2017-11-05T20:14:35.035522: step 208, loss 1.17862, acc 0.78125\n",
      "2017-11-05T20:14:39.111785: step 209, loss 0.405422, acc 0.9375\n",
      "2017-11-05T20:14:43.105101: step 210, loss 0.357585, acc 0.90625\n",
      "2017-11-05T20:14:47.069682: step 211, loss 0.603403, acc 0.84375\n",
      "2017-11-05T20:14:51.108213: step 212, loss 0.83964, acc 0.875\n",
      "2017-11-05T20:14:55.152916: step 213, loss 0.295956, acc 0.9375\n",
      "2017-11-05T20:14:59.251368: step 214, loss 0.454035, acc 0.90625\n",
      "2017-11-05T20:15:03.241579: step 215, loss 0.778868, acc 0.875\n",
      "2017-11-05T20:15:05.886931: step 216, loss 0.00339346, acc 1\n",
      "2017-11-05T20:15:09.899546: step 217, loss 0.41303, acc 0.90625\n",
      "2017-11-05T20:15:13.920128: step 218, loss 0.803793, acc 0.8125\n",
      "2017-11-05T20:15:17.969153: step 219, loss 0.130812, acc 0.9375\n",
      "2017-11-05T20:15:21.941241: step 220, loss 0.478648, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T20:15:25.924683: step 221, loss 0.0577793, acc 0.96875\n",
      "2017-11-05T20:15:29.909424: step 222, loss 0.363485, acc 0.9375\n",
      "2017-11-05T20:15:33.900816: step 223, loss 0.22862, acc 0.90625\n",
      "2017-11-05T20:15:37.940457: step 224, loss 0.880926, acc 0.8125\n",
      "2017-11-05T20:15:41.965514: step 225, loss 0.507453, acc 0.90625\n",
      "2017-11-05T20:15:46.026459: step 226, loss 0.195002, acc 0.9375\n",
      "2017-11-05T20:15:49.954974: step 227, loss 0.869243, acc 0.84375\n",
      "2017-11-05T20:15:54.041392: step 228, loss 0.340406, acc 0.875\n",
      "2017-11-05T20:15:58.075633: step 229, loss 1.15771, acc 0.78125\n",
      "2017-11-05T20:16:02.078979: step 230, loss 0.138312, acc 0.9375\n",
      "2017-11-05T20:16:06.137349: step 231, loss 0.726061, acc 0.84375\n",
      "2017-11-05T20:16:10.273123: step 232, loss 1.06768, acc 0.875\n",
      "2017-11-05T20:16:14.285537: step 233, loss 1.07885, acc 0.75\n",
      "2017-11-05T20:16:18.414586: step 234, loss 0.0293766, acc 0.96875\n",
      "2017-11-05T20:16:22.473623: step 235, loss 0.679544, acc 0.8125\n",
      "2017-11-05T20:16:26.537659: step 236, loss 0.273504, acc 0.90625\n",
      "2017-11-05T20:16:30.570330: step 237, loss 0.560902, acc 0.8125\n",
      "2017-11-05T20:16:34.821537: step 238, loss 0.788941, acc 0.84375\n",
      "2017-11-05T20:16:39.035478: step 239, loss 0.320302, acc 0.9375\n",
      "2017-11-05T20:16:43.134622: step 240, loss 0.178311, acc 0.96875\n",
      "2017-11-05T20:16:47.027885: step 241, loss 0.489167, acc 0.875\n",
      "2017-11-05T20:16:51.029120: step 242, loss 0.0558322, acc 0.96875\n",
      "2017-11-05T20:16:55.060220: step 243, loss 1.15274, acc 0.78125\n",
      "2017-11-05T20:16:59.094630: step 244, loss 0.358265, acc 0.90625\n",
      "2017-11-05T20:17:03.115521: step 245, loss 0.165773, acc 0.96875\n",
      "2017-11-05T20:17:07.109455: step 246, loss 0.904385, acc 0.875\n",
      "2017-11-05T20:17:11.151476: step 247, loss 0.303303, acc 0.90625\n",
      "2017-11-05T20:17:15.111642: step 248, loss 0.380406, acc 0.84375\n",
      "2017-11-05T20:17:19.111129: step 249, loss 0.00573352, acc 1\n",
      "2017-11-05T20:17:23.109905: step 250, loss 0.37808, acc 0.90625\n",
      "2017-11-05T20:17:27.089698: step 251, loss 0.616926, acc 0.9375\n",
      "2017-11-05T20:17:29.705052: step 252, loss 0.0441191, acc 1\n",
      "2017-11-05T20:17:33.722821: step 253, loss 0.349207, acc 0.875\n",
      "2017-11-05T20:17:37.724787: step 254, loss 0.799376, acc 0.875\n",
      "2017-11-05T20:17:41.719851: step 255, loss 0.518854, acc 0.84375\n",
      "2017-11-05T20:17:45.710691: step 256, loss 0.434124, acc 0.90625\n",
      "2017-11-05T20:17:49.714379: step 257, loss 0.0789237, acc 0.96875\n",
      "2017-11-05T20:17:53.710733: step 258, loss 0.555086, acc 0.875\n",
      "2017-11-05T20:17:57.750487: step 259, loss 0.463487, acc 0.875\n",
      "2017-11-05T20:18:01.743379: step 260, loss 0.902232, acc 0.78125\n",
      "2017-11-05T20:18:05.754545: step 261, loss 0.480522, acc 0.875\n",
      "2017-11-05T20:18:09.827921: step 262, loss 0.482071, acc 0.90625\n",
      "2017-11-05T20:18:13.891991: step 263, loss 0.309772, acc 0.875\n",
      "2017-11-05T20:18:17.908192: step 264, loss 0.352687, acc 0.875\n",
      "2017-11-05T20:18:22.009610: step 265, loss 0.300968, acc 0.90625\n",
      "2017-11-05T20:18:26.167744: step 266, loss 0.865588, acc 0.75\n",
      "2017-11-05T20:18:30.192213: step 267, loss 0.216403, acc 0.9375\n",
      "2017-11-05T20:18:34.407598: step 268, loss 0.466728, acc 0.90625\n",
      "2017-11-05T20:18:38.477212: step 269, loss 0.405657, acc 0.90625\n",
      "2017-11-05T20:18:42.718572: step 270, loss 0.71448, acc 0.875\n",
      "2017-11-05T20:18:46.774405: step 271, loss 0.363952, acc 0.9375\n",
      "2017-11-05T20:18:50.857644: step 272, loss 0.018803, acc 1\n",
      "2017-11-05T20:18:55.077513: step 273, loss 0.685711, acc 0.90625\n",
      "2017-11-05T20:18:59.193269: step 274, loss 1.03039, acc 0.84375\n",
      "2017-11-05T20:19:03.252968: step 275, loss 0.373855, acc 0.9375\n",
      "2017-11-05T20:19:07.323904: step 276, loss 0.244043, acc 0.90625\n",
      "2017-11-05T20:19:11.460528: step 277, loss 0.287691, acc 0.9375\n",
      "2017-11-05T20:19:15.521052: step 278, loss 0.934599, acc 0.90625\n",
      "2017-11-05T20:19:19.618404: step 279, loss 0.689517, acc 0.875\n",
      "2017-11-05T20:19:23.696457: step 280, loss 0.324549, acc 0.90625\n",
      "2017-11-05T20:19:27.877204: step 281, loss 0.794601, acc 0.8125\n",
      "2017-11-05T20:19:31.978248: step 282, loss 0.75301, acc 0.78125\n",
      "2017-11-05T20:19:36.064667: step 283, loss 0.936035, acc 0.8125\n",
      "2017-11-05T20:19:40.209972: step 284, loss 0.258325, acc 0.90625\n",
      "2017-11-05T20:19:44.280841: step 285, loss 0.635825, acc 0.90625\n",
      "2017-11-05T20:19:48.387297: step 286, loss 0.478685, acc 0.875\n",
      "2017-11-05T20:19:52.410527: step 287, loss 1.03968, acc 0.84375\n",
      "2017-11-05T20:19:54.951975: step 288, loss 0.417427, acc 0.95\n",
      "2017-11-05T20:19:58.962040: step 289, loss 0.24909, acc 0.9375\n",
      "2017-11-05T20:20:03.246667: step 290, loss 0.113914, acc 0.96875\n",
      "2017-11-05T20:20:07.207328: step 291, loss 0.91256, acc 0.84375\n",
      "2017-11-05T20:20:11.261826: step 292, loss 0.257666, acc 0.90625\n",
      "2017-11-05T20:20:15.254925: step 293, loss 0.089858, acc 0.96875\n",
      "2017-11-05T20:20:19.238213: step 294, loss 0.354467, acc 0.90625\n",
      "2017-11-05T20:20:23.194769: step 295, loss 0.219637, acc 0.96875\n",
      "2017-11-05T20:20:27.138207: step 296, loss 0.0353662, acc 0.96875\n",
      "2017-11-05T20:20:31.151561: step 297, loss 0.223695, acc 0.9375\n",
      "2017-11-05T20:20:35.316049: step 298, loss 0.314956, acc 0.96875\n",
      "2017-11-05T20:20:39.287196: step 299, loss 0.551126, acc 0.90625\n",
      "2017-11-05T20:20:43.226320: step 300, loss 0.187196, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:20:45.772442: step 300, loss 1.58988, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-300\n",
      "\n",
      "2017-11-05T20:20:51.073079: step 301, loss 0.684567, acc 0.875\n",
      "2017-11-05T20:20:55.037659: step 302, loss 0.580425, acc 0.90625\n",
      "2017-11-05T20:20:59.047342: step 303, loss 0.219523, acc 0.96875\n",
      "2017-11-05T20:21:03.026486: step 304, loss 0.28452, acc 0.90625\n",
      "2017-11-05T20:21:07.007805: step 305, loss 0.173085, acc 0.9375\n",
      "2017-11-05T20:21:10.952398: step 306, loss 0.233431, acc 0.9375\n",
      "2017-11-05T20:21:14.905387: step 307, loss 0.398901, acc 0.84375\n",
      "2017-11-05T20:21:18.853275: step 308, loss 0.116098, acc 0.96875\n",
      "2017-11-05T20:21:22.804679: step 309, loss 0.645242, acc 0.90625\n",
      "2017-11-05T20:21:26.750567: step 310, loss 0.807377, acc 0.75\n",
      "2017-11-05T20:21:30.665489: step 311, loss 0.246082, acc 0.9375\n",
      "2017-11-05T20:21:34.574456: step 312, loss 0.703323, acc 0.8125\n",
      "2017-11-05T20:21:38.508107: step 313, loss 0.387557, acc 0.90625\n",
      "2017-11-05T20:21:42.452100: step 314, loss 1.05113, acc 0.84375\n",
      "2017-11-05T20:21:46.372029: step 315, loss 1.40342, acc 0.78125\n",
      "2017-11-05T20:21:50.315863: step 316, loss 0.435315, acc 0.84375\n",
      "2017-11-05T20:21:54.241779: step 317, loss 0.828983, acc 0.8125\n",
      "2017-11-05T20:21:58.185481: step 318, loss 1.24831, acc 0.8125\n",
      "2017-11-05T20:22:02.210086: step 319, loss 0.12107, acc 0.96875\n",
      "2017-11-05T20:22:06.103952: step 320, loss 0.437931, acc 0.875\n",
      "2017-11-05T20:22:10.070214: step 321, loss 0.755988, acc 0.8125\n",
      "2017-11-05T20:22:13.998943: step 322, loss 0.225882, acc 0.9375\n",
      "2017-11-05T20:22:17.905257: step 323, loss 0.884294, acc 0.84375\n",
      "2017-11-05T20:22:20.439455: step 324, loss 0.275032, acc 0.85\n",
      "2017-11-05T20:22:24.404755: step 325, loss 0.192023, acc 0.875\n",
      "2017-11-05T20:22:28.306386: step 326, loss 0.0666155, acc 0.96875\n",
      "2017-11-05T20:22:32.281972: step 327, loss 0.213719, acc 0.9375\n",
      "2017-11-05T20:22:36.406169: step 328, loss 0.453754, acc 0.875\n",
      "2017-11-05T20:22:40.411014: step 329, loss 0.560105, acc 0.90625\n",
      "2017-11-05T20:22:44.354955: step 330, loss 0.543401, acc 0.84375\n",
      "2017-11-05T20:22:48.271977: step 331, loss 0.341155, acc 0.875\n",
      "2017-11-05T20:22:52.216919: step 332, loss 0.372966, acc 0.875\n",
      "2017-11-05T20:22:56.176932: step 333, loss 0.534094, acc 0.90625\n",
      "2017-11-05T20:23:00.149515: step 334, loss 0.0566817, acc 0.96875\n",
      "2017-11-05T20:23:04.086872: step 335, loss 0.834683, acc 0.8125\n",
      "2017-11-05T20:23:08.058010: step 336, loss 0.611281, acc 0.90625\n",
      "2017-11-05T20:23:12.014216: step 337, loss 0.740786, acc 0.9375\n",
      "2017-11-05T20:23:15.928249: step 338, loss 0.177738, acc 0.96875\n",
      "2017-11-05T20:23:19.932197: step 339, loss 0.560425, acc 0.875\n",
      "2017-11-05T20:23:24.015527: step 340, loss 0.121257, acc 0.96875\n",
      "2017-11-05T20:23:28.120647: step 341, loss 0.536489, acc 0.875\n",
      "2017-11-05T20:23:32.063874: step 342, loss 0.899027, acc 0.8125\n",
      "2017-11-05T20:23:36.107525: step 343, loss 0.750835, acc 0.8125\n",
      "2017-11-05T20:23:40.068116: step 344, loss 0.168434, acc 0.96875\n",
      "2017-11-05T20:23:44.038724: step 345, loss 0.641212, acc 0.84375\n",
      "2017-11-05T20:23:48.058535: step 346, loss 0.546247, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T20:23:52.100325: step 347, loss 0.910712, acc 0.75\n",
      "2017-11-05T20:23:56.132780: step 348, loss 0.612591, acc 0.875\n",
      "2017-11-05T20:24:00.101326: step 349, loss 0.307382, acc 0.96875\n",
      "2017-11-05T20:24:04.107530: step 350, loss 1.08884, acc 0.84375\n",
      "2017-11-05T20:24:08.016833: step 351, loss 1.30375, acc 0.75\n",
      "2017-11-05T20:24:11.970264: step 352, loss 1.0967, acc 0.78125\n",
      "2017-11-05T20:24:15.901722: step 353, loss 0.459565, acc 0.84375\n",
      "2017-11-05T20:24:19.873964: step 354, loss 0.219703, acc 0.875\n",
      "2017-11-05T20:24:23.818926: step 355, loss 0.129203, acc 0.90625\n",
      "2017-11-05T20:24:27.737814: step 356, loss 0.985708, acc 0.75\n",
      "2017-11-05T20:24:31.636302: step 357, loss 0.601224, acc 0.8125\n",
      "2017-11-05T20:24:35.686336: step 358, loss 0.0988141, acc 0.96875\n",
      "2017-11-05T20:24:39.759575: step 359, loss 0.3507, acc 0.90625\n",
      "2017-11-05T20:24:42.310874: step 360, loss 0.47219, acc 0.95\n",
      "2017-11-05T20:24:46.273043: step 361, loss 0.174908, acc 0.96875\n",
      "2017-11-05T20:24:50.176579: step 362, loss 0.528099, acc 0.9375\n",
      "2017-11-05T20:24:54.173342: step 363, loss 0.783229, acc 0.875\n",
      "2017-11-05T20:24:58.313423: step 364, loss 0.238974, acc 0.96875\n",
      "2017-11-05T20:25:02.306282: step 365, loss 0.25916, acc 0.90625\n",
      "2017-11-05T20:25:06.289500: step 366, loss 0.212636, acc 0.9375\n",
      "2017-11-05T20:25:10.244789: step 367, loss 0.749018, acc 0.875\n",
      "2017-11-05T20:25:14.178542: step 368, loss 0.447751, acc 0.9375\n",
      "2017-11-05T20:25:18.085799: step 369, loss 0.881271, acc 0.8125\n",
      "2017-11-05T20:25:22.082367: step 370, loss 0.802072, acc 0.875\n",
      "2017-11-05T20:25:26.087435: step 371, loss 0.395292, acc 0.9375\n",
      "2017-11-05T20:25:29.998723: step 372, loss 0.707336, acc 0.84375\n",
      "2017-11-05T20:25:33.942045: step 373, loss 0.296378, acc 0.84375\n",
      "2017-11-05T20:25:37.876423: step 374, loss 0.117489, acc 0.90625\n",
      "2017-11-05T20:25:41.789752: step 375, loss 1.05794, acc 0.75\n",
      "2017-11-05T20:25:45.767198: step 376, loss 0.528165, acc 0.90625\n",
      "2017-11-05T20:25:49.698097: step 377, loss 0.162901, acc 0.9375\n",
      "2017-11-05T20:25:53.645475: step 378, loss 1.0441, acc 0.8125\n",
      "2017-11-05T20:25:57.563459: step 379, loss 0.921767, acc 0.78125\n",
      "2017-11-05T20:26:01.544098: step 380, loss 0.593951, acc 0.8125\n",
      "2017-11-05T20:26:05.490072: step 381, loss 0.0195676, acc 1\n",
      "2017-11-05T20:26:09.447961: step 382, loss 0.465352, acc 0.875\n",
      "2017-11-05T20:26:13.374956: step 383, loss 0.75861, acc 0.875\n",
      "2017-11-05T20:26:17.329738: step 384, loss 0.716588, acc 0.90625\n",
      "2017-11-05T20:26:21.226379: step 385, loss 0.193402, acc 0.90625\n",
      "2017-11-05T20:26:25.300851: step 386, loss 0.000775633, acc 1\n",
      "2017-11-05T20:26:29.246383: step 387, loss 0.195989, acc 0.9375\n",
      "2017-11-05T20:26:33.244831: step 388, loss 0.695429, acc 0.875\n",
      "2017-11-05T20:26:37.243880: step 389, loss 0.289717, acc 0.9375\n",
      "2017-11-05T20:26:41.179783: step 390, loss 0.532565, acc 0.9375\n",
      "2017-11-05T20:26:45.115301: step 391, loss 0.740783, acc 0.8125\n",
      "2017-11-05T20:26:49.081262: step 392, loss 0.688918, acc 0.90625\n",
      "2017-11-05T20:26:53.010326: step 393, loss 0.0503268, acc 0.96875\n",
      "2017-11-05T20:26:56.984126: step 394, loss 1.40448, acc 0.8125\n",
      "2017-11-05T20:27:00.984542: step 395, loss 0.197385, acc 0.90625\n",
      "2017-11-05T20:27:03.495741: step 396, loss 0.219032, acc 0.9\n",
      "2017-11-05T20:27:07.442276: step 397, loss 0.50851, acc 0.875\n",
      "2017-11-05T20:27:11.442031: step 398, loss 0.344073, acc 0.875\n",
      "2017-11-05T20:27:15.370402: step 399, loss 0.602726, acc 0.84375\n",
      "2017-11-05T20:27:19.329878: step 400, loss 0.461617, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:27:21.836249: step 400, loss 1.33584, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-400\n",
      "\n",
      "2017-11-05T20:27:27.331958: step 401, loss 0.482732, acc 0.875\n",
      "2017-11-05T20:27:31.294767: step 402, loss 0.769148, acc 0.75\n",
      "2017-11-05T20:27:35.228626: step 403, loss 0.889048, acc 0.8125\n",
      "2017-11-05T20:27:39.224967: step 404, loss 0.13198, acc 0.9375\n",
      "2017-11-05T20:27:43.210627: step 405, loss 0.406693, acc 0.90625\n",
      "2017-11-05T20:27:47.194957: step 406, loss 0.319564, acc 0.90625\n",
      "2017-11-05T20:27:51.187755: step 407, loss 0.458945, acc 0.90625\n",
      "2017-11-05T20:27:55.143810: step 408, loss 0.540733, acc 0.9375\n",
      "2017-11-05T20:27:59.107934: step 409, loss 0.363438, acc 0.84375\n",
      "2017-11-05T20:28:03.052011: step 410, loss 0.2289, acc 0.96875\n",
      "2017-11-05T20:28:06.967164: step 411, loss 0.305504, acc 0.9375\n",
      "2017-11-05T20:28:10.942009: step 412, loss 0.824975, acc 0.875\n",
      "2017-11-05T20:28:14.901769: step 413, loss 0.258522, acc 0.9375\n",
      "2017-11-05T20:28:18.843479: step 414, loss 0.103592, acc 0.9375\n",
      "2017-11-05T20:28:22.885423: step 415, loss 0.908207, acc 0.875\n",
      "2017-11-05T20:28:26.942355: step 416, loss 0.803828, acc 0.84375\n",
      "2017-11-05T20:28:30.967564: step 417, loss 0.374886, acc 0.90625\n",
      "2017-11-05T20:28:35.066791: step 418, loss 0.0745657, acc 0.96875\n",
      "2017-11-05T20:28:39.077573: step 419, loss 0.19506, acc 0.90625\n",
      "2017-11-05T20:28:43.048532: step 420, loss 0.551247, acc 0.875\n",
      "2017-11-05T20:28:47.027765: step 421, loss 0.0194916, acc 1\n",
      "2017-11-05T20:28:50.991557: step 422, loss 0.399831, acc 0.875\n",
      "2017-11-05T20:28:54.917645: step 423, loss 0.306026, acc 0.9375\n",
      "2017-11-05T20:28:58.852745: step 424, loss 0.826012, acc 0.78125\n",
      "2017-11-05T20:29:02.766321: step 425, loss 0.343288, acc 0.875\n",
      "2017-11-05T20:29:06.712430: step 426, loss 0.374185, acc 0.84375\n",
      "2017-11-05T20:29:10.707858: step 427, loss 0.73613, acc 0.875\n",
      "2017-11-05T20:29:14.694147: step 428, loss 0.238789, acc 0.9375\n",
      "2017-11-05T20:29:18.622371: step 429, loss 0.334469, acc 0.875\n",
      "2017-11-05T20:29:22.572503: step 430, loss 0.631201, acc 0.78125\n",
      "2017-11-05T20:29:26.517968: step 431, loss 0.284806, acc 0.9375\n",
      "2017-11-05T20:29:29.040480: step 432, loss 0.200072, acc 0.95\n",
      "2017-11-05T20:29:33.005746: step 433, loss 0.3033, acc 0.875\n",
      "2017-11-05T20:29:36.947665: step 434, loss 0.198035, acc 0.96875\n",
      "2017-11-05T20:29:40.922382: step 435, loss 0.128711, acc 0.9375\n",
      "2017-11-05T20:29:44.879897: step 436, loss 0.591689, acc 0.84375\n",
      "2017-11-05T20:29:48.788476: step 437, loss 0.536563, acc 0.90625\n",
      "2017-11-05T20:29:52.732499: step 438, loss 0.980813, acc 0.8125\n",
      "2017-11-05T20:29:56.694772: step 439, loss 0.124603, acc 0.9375\n",
      "2017-11-05T20:30:00.819059: step 440, loss 0.121792, acc 0.96875\n",
      "2017-11-05T20:30:04.877530: step 441, loss 0.775903, acc 0.90625\n",
      "2017-11-05T20:30:08.790469: step 442, loss 0.388503, acc 0.90625\n",
      "2017-11-05T20:30:12.800836: step 443, loss 0.0884446, acc 0.96875\n",
      "2017-11-05T20:30:16.723256: step 444, loss 0.258478, acc 0.9375\n",
      "2017-11-05T20:30:20.652220: step 445, loss 0.0247809, acc 1\n",
      "2017-11-05T20:30:24.582442: step 446, loss 1.04693, acc 0.78125\n",
      "2017-11-05T20:30:28.517990: step 447, loss 0.0947222, acc 0.9375\n",
      "2017-11-05T20:30:32.425492: step 448, loss 0.283216, acc 0.875\n",
      "2017-11-05T20:30:36.558892: step 449, loss 0.21234, acc 0.9375\n",
      "2017-11-05T20:30:40.500476: step 450, loss 0.683451, acc 0.90625\n",
      "2017-11-05T20:30:44.520578: step 451, loss 0.172839, acc 0.9375\n",
      "2017-11-05T20:30:48.410675: step 452, loss 0.547239, acc 0.8125\n",
      "2017-11-05T20:30:52.316451: step 453, loss 0.455756, acc 0.84375\n",
      "2017-11-05T20:30:56.296599: step 454, loss 0.151871, acc 0.9375\n",
      "2017-11-05T20:31:00.222486: step 455, loss 0.404479, acc 0.875\n",
      "2017-11-05T20:31:04.177204: step 456, loss 0.383672, acc 0.875\n",
      "2017-11-05T20:31:08.170360: step 457, loss 0.417052, acc 0.875\n",
      "2017-11-05T20:31:12.179133: step 458, loss 0.333075, acc 0.90625\n",
      "2017-11-05T20:31:16.184620: step 459, loss 0.191768, acc 0.875\n",
      "2017-11-05T20:31:20.091181: step 460, loss 0.571296, acc 0.875\n",
      "2017-11-05T20:31:24.091942: step 461, loss 0.550762, acc 0.875\n",
      "2017-11-05T20:31:28.070090: step 462, loss 0.153257, acc 0.9375\n",
      "2017-11-05T20:31:31.990496: step 463, loss 0.555481, acc 0.90625\n",
      "2017-11-05T20:31:35.968019: step 464, loss 0.0394594, acc 1\n",
      "2017-11-05T20:31:39.924178: step 465, loss 0.599799, acc 0.875\n",
      "2017-11-05T20:31:43.846561: step 466, loss 0.737211, acc 0.875\n",
      "2017-11-05T20:31:47.809275: step 467, loss 0.473527, acc 0.75\n",
      "2017-11-05T20:31:50.349238: step 468, loss 0.149443, acc 0.9\n",
      "2017-11-05T20:31:54.274224: step 469, loss 0.0455545, acc 1\n",
      "2017-11-05T20:31:58.333550: step 470, loss 0.0384644, acc 1\n",
      "2017-11-05T20:32:02.282054: step 471, loss 0.123855, acc 0.9375\n",
      "2017-11-05T20:32:06.228254: step 472, loss 0.181055, acc 0.9375\n",
      "2017-11-05T20:32:10.174774: step 473, loss 0.361436, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T20:32:14.176833: step 474, loss 0.258796, acc 0.9375\n",
      "2017-11-05T20:32:18.143741: step 475, loss 0.471574, acc 0.8125\n",
      "2017-11-05T20:32:22.118380: step 476, loss 0.0150675, acc 1\n",
      "2017-11-05T20:32:26.097105: step 477, loss 0.326763, acc 0.9375\n",
      "2017-11-05T20:32:30.060211: step 478, loss 0.256993, acc 0.9375\n",
      "2017-11-05T20:32:34.142446: step 479, loss 0.925357, acc 0.75\n",
      "2017-11-05T20:32:38.103300: step 480, loss 0.823636, acc 0.875\n",
      "2017-11-05T20:32:42.005449: step 481, loss 0.298298, acc 0.875\n",
      "2017-11-05T20:32:45.979345: step 482, loss 0.0854501, acc 0.9375\n",
      "2017-11-05T20:32:49.961561: step 483, loss 0.337444, acc 0.84375\n",
      "2017-11-05T20:32:53.983031: step 484, loss 0.531088, acc 0.90625\n",
      "2017-11-05T20:32:57.997965: step 485, loss 0.535435, acc 0.875\n",
      "2017-11-05T20:33:01.995145: step 486, loss 0.437367, acc 0.875\n",
      "2017-11-05T20:33:05.954972: step 487, loss 0.688339, acc 0.875\n",
      "2017-11-05T20:33:09.951561: step 488, loss 0.100373, acc 0.96875\n",
      "2017-11-05T20:33:13.898153: step 489, loss 0.273836, acc 0.90625\n",
      "2017-11-05T20:33:17.943377: step 490, loss 0.136495, acc 0.90625\n",
      "2017-11-05T20:33:22.024285: step 491, loss 0.266675, acc 0.90625\n",
      "2017-11-05T20:33:26.129901: step 492, loss 0.265921, acc 0.9375\n",
      "2017-11-05T20:33:30.079252: step 493, loss 0.240185, acc 0.9375\n",
      "2017-11-05T20:33:34.054689: step 494, loss 0.508906, acc 0.875\n",
      "2017-11-05T20:33:38.120553: step 495, loss 0.360469, acc 0.90625\n",
      "2017-11-05T20:33:42.024576: step 496, loss 0.34741, acc 0.90625\n",
      "2017-11-05T20:33:46.000120: step 497, loss 0.445532, acc 0.90625\n",
      "2017-11-05T20:33:50.014282: step 498, loss 0.171281, acc 0.9375\n",
      "2017-11-05T20:33:54.022435: step 499, loss 0.706042, acc 0.84375\n",
      "2017-11-05T20:33:58.001783: step 500, loss 0.0768895, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:34:00.517262: step 500, loss 1.5701, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-500\n",
      "\n",
      "2017-11-05T20:34:05.792182: step 501, loss 0.270957, acc 0.9375\n",
      "2017-11-05T20:34:09.776282: step 502, loss 0.793327, acc 0.78125\n",
      "2017-11-05T20:34:13.743277: step 503, loss 0.137541, acc 0.96875\n",
      "2017-11-05T20:34:16.290973: step 504, loss 0.336692, acc 0.85\n",
      "2017-11-05T20:34:20.274545: step 505, loss 0.194997, acc 0.90625\n",
      "2017-11-05T20:34:24.216863: step 506, loss 0.492782, acc 0.84375\n",
      "2017-11-05T20:34:28.242378: step 507, loss 0.241981, acc 0.875\n",
      "2017-11-05T20:34:32.188021: step 508, loss 0.697393, acc 0.875\n",
      "2017-11-05T20:34:36.276913: step 509, loss 0.705885, acc 0.875\n",
      "2017-11-05T20:34:40.245859: step 510, loss 0.9625, acc 0.6875\n",
      "2017-11-05T20:34:44.199617: step 511, loss 0.308416, acc 0.9375\n",
      "2017-11-05T20:34:48.129187: step 512, loss 0.4503, acc 0.875\n",
      "2017-11-05T20:34:52.026597: step 513, loss 0.377072, acc 0.90625\n",
      "2017-11-05T20:34:55.959103: step 514, loss 0.563798, acc 0.8125\n",
      "2017-11-05T20:34:59.882483: step 515, loss 0.361787, acc 0.96875\n",
      "2017-11-05T20:35:03.759772: step 516, loss 0.540572, acc 0.9375\n",
      "2017-11-05T20:35:07.752068: step 517, loss 0.244284, acc 0.90625\n",
      "2017-11-05T20:35:11.685875: step 518, loss 0.698552, acc 0.875\n",
      "2017-11-05T20:35:15.706124: step 519, loss 0.656671, acc 0.84375\n",
      "2017-11-05T20:35:19.738892: step 520, loss 0.00210692, acc 1\n",
      "2017-11-05T20:35:23.663973: step 521, loss 0.40294, acc 0.875\n",
      "2017-11-05T20:35:27.625745: step 522, loss 0.248861, acc 0.875\n",
      "2017-11-05T20:35:31.614026: step 523, loss 0.217775, acc 0.90625\n",
      "2017-11-05T20:35:35.577414: step 524, loss 0.461118, acc 0.9375\n",
      "2017-11-05T20:35:39.555654: step 525, loss 0.044563, acc 0.96875\n",
      "2017-11-05T20:35:43.552474: step 526, loss 0.18254, acc 0.9375\n",
      "2017-11-05T20:35:47.514458: step 527, loss 0.326889, acc 0.90625\n",
      "2017-11-05T20:35:51.495563: step 528, loss 0.521597, acc 0.78125\n",
      "2017-11-05T20:35:55.457954: step 529, loss 0.823331, acc 0.84375\n",
      "2017-11-05T20:35:59.395112: step 530, loss 0.381938, acc 0.84375\n",
      "2017-11-05T20:36:03.316615: step 531, loss 0.822638, acc 0.84375\n",
      "2017-11-05T20:36:07.260949: step 532, loss 0.592023, acc 0.78125\n",
      "2017-11-05T20:36:11.235405: step 533, loss 0.491118, acc 0.90625\n",
      "2017-11-05T20:36:15.236086: step 534, loss 0.684616, acc 0.875\n",
      "2017-11-05T20:36:19.237663: step 535, loss 0.745927, acc 0.84375\n",
      "2017-11-05T20:36:23.202640: step 536, loss 0.308786, acc 0.9375\n",
      "2017-11-05T20:36:27.215414: step 537, loss 0.107375, acc 0.9375\n",
      "2017-11-05T20:36:31.162967: step 538, loss 0.397424, acc 0.875\n",
      "2017-11-05T20:36:35.271778: step 539, loss 0.496149, acc 0.875\n",
      "2017-11-05T20:36:37.773084: step 540, loss 0.775097, acc 0.7\n",
      "2017-11-05T20:36:41.716162: step 541, loss 0.0409317, acc 1\n",
      "2017-11-05T20:36:45.586193: step 542, loss 0.291263, acc 0.9375\n",
      "2017-11-05T20:36:49.606998: step 543, loss 0.323132, acc 0.90625\n",
      "2017-11-05T20:36:53.513788: step 544, loss 0.0263865, acc 0.96875\n",
      "2017-11-05T20:36:57.545008: step 545, loss 0.814294, acc 0.8125\n",
      "2017-11-05T20:37:01.512873: step 546, loss 0.0350187, acc 0.96875\n",
      "2017-11-05T20:37:05.447872: step 547, loss 0.265403, acc 0.90625\n",
      "2017-11-05T20:37:09.424852: step 548, loss 0.0320911, acc 1\n",
      "2017-11-05T20:37:13.371274: step 549, loss 0.373021, acc 0.9375\n",
      "2017-11-05T20:37:17.425366: step 550, loss 0.305128, acc 0.90625\n",
      "2017-11-05T20:37:21.414356: step 551, loss 0.133537, acc 0.9375\n",
      "2017-11-05T20:37:25.400586: step 552, loss 0.363822, acc 0.90625\n",
      "2017-11-05T20:37:29.390747: step 553, loss 0.619479, acc 0.875\n",
      "2017-11-05T20:37:33.356256: step 554, loss 0.317238, acc 0.9375\n",
      "2017-11-05T20:37:37.309833: step 555, loss 0.40369, acc 0.9375\n",
      "2017-11-05T20:37:41.285509: step 556, loss 0.0836392, acc 0.96875\n",
      "2017-11-05T20:37:45.301125: step 557, loss 0.191701, acc 0.96875\n",
      "2017-11-05T20:37:49.247757: step 558, loss 0.450304, acc 0.875\n",
      "2017-11-05T20:37:53.263876: step 559, loss 0.552213, acc 0.84375\n",
      "2017-11-05T20:37:57.257765: step 560, loss 0.186462, acc 0.9375\n",
      "2017-11-05T20:38:01.152744: step 561, loss 0.207401, acc 0.9375\n",
      "2017-11-05T20:38:05.078691: step 562, loss 0.41831, acc 0.875\n",
      "2017-11-05T20:38:09.088752: step 563, loss 0.711506, acc 0.8125\n",
      "2017-11-05T20:38:12.991223: step 564, loss 0.513759, acc 0.875\n",
      "2017-11-05T20:38:16.999783: step 565, loss 0.0859983, acc 0.96875\n",
      "2017-11-05T20:38:20.991606: step 566, loss 0.531993, acc 0.90625\n",
      "2017-11-05T20:38:25.216977: step 567, loss 0.325801, acc 0.90625\n",
      "2017-11-05T20:38:29.259911: step 568, loss 0.274658, acc 0.90625\n",
      "2017-11-05T20:38:33.341485: step 569, loss 0.21259, acc 0.96875\n",
      "2017-11-05T20:38:37.381915: step 570, loss 0.130503, acc 0.9375\n",
      "2017-11-05T20:38:41.390877: step 571, loss 0.217428, acc 0.9375\n",
      "2017-11-05T20:38:45.400870: step 572, loss 0.710683, acc 0.8125\n",
      "2017-11-05T20:38:49.345940: step 573, loss 0.544363, acc 0.9375\n",
      "2017-11-05T20:38:53.369226: step 574, loss 0.527872, acc 0.84375\n",
      "2017-11-05T20:38:57.347439: step 575, loss 0.100792, acc 0.96875\n",
      "2017-11-05T20:38:59.898595: step 576, loss 0.645714, acc 0.8\n",
      "2017-11-05T20:39:03.903406: step 577, loss 0.219505, acc 0.9375\n",
      "2017-11-05T20:39:07.850958: step 578, loss 0.38599, acc 0.875\n",
      "2017-11-05T20:39:11.900177: step 579, loss 0.296084, acc 0.90625\n",
      "2017-11-05T20:39:15.882528: step 580, loss 0.0941959, acc 0.96875\n",
      "2017-11-05T20:39:19.863175: step 581, loss 0.22504, acc 0.90625\n",
      "2017-11-05T20:39:23.920278: step 582, loss 0.769987, acc 0.90625\n",
      "2017-11-05T20:39:27.849394: step 583, loss 0.0653091, acc 0.96875\n",
      "2017-11-05T20:39:31.744926: step 584, loss 0.129052, acc 0.96875\n",
      "2017-11-05T20:39:35.726384: step 585, loss 0.0724672, acc 0.96875\n",
      "2017-11-05T20:39:39.708343: step 586, loss 0.61684, acc 0.875\n",
      "2017-11-05T20:39:43.699290: step 587, loss 0.518964, acc 0.875\n",
      "2017-11-05T20:39:47.629923: step 588, loss 0.410445, acc 0.96875\n",
      "2017-11-05T20:39:51.598237: step 589, loss 0.395901, acc 0.8125\n",
      "2017-11-05T20:39:55.557786: step 590, loss 0.536325, acc 0.875\n",
      "2017-11-05T20:39:59.545934: step 591, loss 0.203636, acc 0.9375\n",
      "2017-11-05T20:40:03.728529: step 592, loss 0.291777, acc 0.9375\n",
      "2017-11-05T20:40:07.711422: step 593, loss 0.169536, acc 0.96875\n",
      "2017-11-05T20:40:11.731590: step 594, loss 0.338956, acc 0.90625\n",
      "2017-11-05T20:40:15.666701: step 595, loss 0.14751, acc 0.96875\n",
      "2017-11-05T20:40:19.609392: step 596, loss 0.305245, acc 0.875\n",
      "2017-11-05T20:40:23.650426: step 597, loss 0.134516, acc 0.9375\n",
      "2017-11-05T20:40:27.582585: step 598, loss 0.243448, acc 0.90625\n",
      "2017-11-05T20:40:31.590790: step 599, loss 0.356562, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T20:40:35.697692: step 600, loss 0.367986, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:40:38.226197: step 600, loss 1.25979, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-600\n",
      "\n",
      "2017-11-05T20:40:43.404097: step 601, loss 0.842578, acc 0.84375\n",
      "2017-11-05T20:40:47.424277: step 602, loss 0.41014, acc 0.90625\n",
      "2017-11-05T20:40:51.391347: step 603, loss 0.103366, acc 0.9375\n",
      "2017-11-05T20:40:55.378737: step 604, loss 0.25739, acc 0.84375\n",
      "2017-11-05T20:40:59.361101: step 605, loss 0.388508, acc 0.875\n",
      "2017-11-05T20:41:03.325492: step 606, loss 0.234749, acc 0.875\n",
      "2017-11-05T20:41:07.260034: step 607, loss 0.262681, acc 0.96875\n",
      "2017-11-05T20:41:11.904285: step 608, loss 0.136554, acc 0.9375\n",
      "2017-11-05T20:41:16.721597: step 609, loss 0.641566, acc 0.8125\n",
      "2017-11-05T20:41:20.743493: step 610, loss 0.490019, acc 0.875\n",
      "2017-11-05T20:41:25.348949: step 611, loss 0.173618, acc 0.90625\n",
      "2017-11-05T20:41:27.861483: step 612, loss 0.340103, acc 0.8\n",
      "2017-11-05T20:41:31.869655: step 613, loss 0.346348, acc 0.90625\n",
      "2017-11-05T20:41:35.855895: step 614, loss 0.019798, acc 1\n",
      "2017-11-05T20:41:39.810510: step 615, loss 0.142844, acc 0.875\n",
      "2017-11-05T20:41:43.776961: step 616, loss 0.229733, acc 0.90625\n",
      "2017-11-05T20:41:47.740401: step 617, loss 0.294872, acc 0.96875\n",
      "2017-11-05T20:41:51.837923: step 618, loss 0.3437, acc 0.90625\n",
      "2017-11-05T20:41:55.743947: step 619, loss 0.170133, acc 0.90625\n",
      "2017-11-05T20:41:59.664385: step 620, loss 0.0368013, acc 0.96875\n",
      "2017-11-05T20:42:03.620913: step 621, loss 0.1504, acc 0.9375\n",
      "2017-11-05T20:42:07.534040: step 622, loss 0.595673, acc 0.84375\n",
      "2017-11-05T20:42:11.655049: step 623, loss 0.348921, acc 0.875\n",
      "2017-11-05T20:42:15.679093: step 624, loss 0.145389, acc 0.96875\n",
      "2017-11-05T20:42:19.633479: step 625, loss 0.125195, acc 0.96875\n",
      "2017-11-05T20:42:23.608147: step 626, loss 0.172254, acc 0.96875\n",
      "2017-11-05T20:42:27.572916: step 627, loss 0.747493, acc 0.84375\n",
      "2017-11-05T20:42:31.520374: step 628, loss 0.11049, acc 0.9375\n",
      "2017-11-05T20:42:35.614635: step 629, loss 0.318305, acc 0.90625\n",
      "2017-11-05T20:42:39.557538: step 630, loss 0.135267, acc 0.90625\n",
      "2017-11-05T20:42:43.527425: step 631, loss 0.155627, acc 0.96875\n",
      "2017-11-05T20:42:47.450347: step 632, loss 0.58632, acc 0.875\n",
      "2017-11-05T20:42:51.432814: step 633, loss 0.666415, acc 0.8125\n",
      "2017-11-05T20:42:55.314472: step 634, loss 0.115192, acc 0.96875\n",
      "2017-11-05T20:42:59.273067: step 635, loss 0.186036, acc 0.875\n",
      "2017-11-05T20:43:03.236703: step 636, loss 0.0901202, acc 0.9375\n",
      "2017-11-05T20:43:07.173939: step 637, loss 0.132552, acc 0.9375\n",
      "2017-11-05T20:43:11.129749: step 638, loss 0.324851, acc 0.875\n",
      "2017-11-05T20:43:15.122296: step 639, loss 0.200819, acc 0.9375\n",
      "2017-11-05T20:43:19.024247: step 640, loss 0.284667, acc 0.90625\n",
      "2017-11-05T20:43:23.072079: step 641, loss 0.167979, acc 0.90625\n",
      "2017-11-05T20:43:27.132691: step 642, loss 0.808469, acc 0.75\n",
      "2017-11-05T20:43:31.105422: step 643, loss 0.556551, acc 0.84375\n",
      "2017-11-05T20:43:35.035982: step 644, loss 0.388259, acc 0.84375\n",
      "2017-11-05T20:43:39.050688: step 645, loss 0.467065, acc 0.875\n",
      "2017-11-05T20:43:42.992555: step 646, loss 0.449488, acc 0.875\n",
      "2017-11-05T20:43:46.875022: step 647, loss 0.14503, acc 0.9375\n",
      "2017-11-05T20:43:49.387666: step 648, loss 0.0454521, acc 0.95\n",
      "2017-11-05T20:43:53.306496: step 649, loss 0.00841928, acc 1\n",
      "2017-11-05T20:43:57.213704: step 650, loss 0.131077, acc 0.9375\n",
      "2017-11-05T20:44:01.114469: step 651, loss 1.02693, acc 0.8125\n",
      "2017-11-05T20:44:05.053177: step 652, loss 0.372107, acc 0.875\n",
      "2017-11-05T20:44:08.944082: step 653, loss 0.289765, acc 0.90625\n",
      "2017-11-05T20:44:12.852821: step 654, loss 0.313748, acc 0.90625\n",
      "2017-11-05T20:44:16.792331: step 655, loss 0.130014, acc 0.9375\n",
      "2017-11-05T20:44:20.742172: step 656, loss 0.480858, acc 0.875\n",
      "2017-11-05T20:44:24.664110: step 657, loss 0.340361, acc 0.875\n",
      "2017-11-05T20:44:28.651046: step 658, loss 0.369994, acc 0.9375\n",
      "2017-11-05T20:44:32.623771: step 659, loss 0.290738, acc 0.875\n",
      "2017-11-05T20:44:36.648275: step 660, loss 0.166265, acc 0.90625\n",
      "2017-11-05T20:44:40.578077: step 661, loss 0.703696, acc 0.84375\n",
      "2017-11-05T20:44:44.520618: step 662, loss 0.0753813, acc 1\n",
      "2017-11-05T20:44:48.420141: step 663, loss 0.650163, acc 0.875\n",
      "2017-11-05T20:44:52.403905: step 664, loss 0.463125, acc 0.875\n",
      "2017-11-05T20:44:56.302912: step 665, loss 0.762109, acc 0.90625\n",
      "2017-11-05T20:45:00.218866: step 666, loss 0.56431, acc 0.875\n",
      "2017-11-05T20:45:04.166878: step 667, loss 0.339607, acc 0.90625\n",
      "2017-11-05T20:45:08.125214: step 668, loss 0.103863, acc 0.96875\n",
      "2017-11-05T20:45:12.069849: step 669, loss 1.13477, acc 0.8125\n",
      "2017-11-05T20:45:16.070124: step 670, loss 0.220986, acc 0.875\n",
      "2017-11-05T20:45:20.095455: step 671, loss 0.397607, acc 0.90625\n",
      "2017-11-05T20:45:24.057705: step 672, loss 0.225127, acc 0.90625\n",
      "2017-11-05T20:45:28.070367: step 673, loss 0.234738, acc 0.9375\n",
      "2017-11-05T20:45:32.100584: step 674, loss 0.219655, acc 0.90625\n",
      "2017-11-05T20:45:36.065802: step 675, loss 0.536947, acc 0.84375\n",
      "2017-11-05T20:45:40.071808: step 676, loss 0.633757, acc 0.84375\n",
      "2017-11-05T20:45:43.998714: step 677, loss 0.18427, acc 0.90625\n",
      "2017-11-05T20:45:47.948569: step 678, loss 0.184152, acc 0.9375\n",
      "2017-11-05T20:45:51.917371: step 679, loss 0.25783, acc 0.875\n",
      "2017-11-05T20:45:55.931160: step 680, loss 0.165402, acc 0.9375\n",
      "2017-11-05T20:45:59.908813: step 681, loss 0.0352781, acc 1\n",
      "2017-11-05T20:46:03.901134: step 682, loss 0.281514, acc 0.9375\n",
      "2017-11-05T20:46:07.881297: step 683, loss 0.337492, acc 0.90625\n",
      "2017-11-05T20:46:10.446290: step 684, loss 0.462343, acc 0.8\n",
      "2017-11-05T20:46:14.448348: step 685, loss 0.0604753, acc 0.96875\n",
      "2017-11-05T20:46:18.464300: step 686, loss 0.199841, acc 0.90625\n",
      "2017-11-05T20:46:22.486280: step 687, loss 0.0346677, acc 0.96875\n",
      "2017-11-05T20:46:26.433078: step 688, loss 0.950871, acc 0.84375\n",
      "2017-11-05T20:46:30.411219: step 689, loss 0.814847, acc 0.875\n",
      "2017-11-05T20:46:34.686597: step 690, loss 0.732108, acc 0.8125\n",
      "2017-11-05T20:46:38.873143: step 691, loss 0.181498, acc 0.875\n",
      "2017-11-05T20:46:42.892802: step 692, loss 0.0674382, acc 0.96875\n",
      "2017-11-05T20:46:46.906886: step 693, loss 0.630648, acc 0.84375\n",
      "2017-11-05T20:46:50.883750: step 694, loss 0.252468, acc 0.875\n",
      "2017-11-05T20:46:54.995022: step 695, loss 0.297374, acc 0.90625\n",
      "2017-11-05T20:46:58.945280: step 696, loss 0.511326, acc 0.8125\n",
      "2017-11-05T20:47:03.067212: step 697, loss 0.0790793, acc 0.96875\n",
      "2017-11-05T20:47:07.146194: step 698, loss 0.305929, acc 0.84375\n",
      "2017-11-05T20:47:11.231624: step 699, loss 0.805243, acc 0.78125\n",
      "2017-11-05T20:47:15.205199: step 700, loss 0.299237, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:47:17.768714: step 700, loss 1.00628, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-700\n",
      "\n",
      "2017-11-05T20:47:24.128565: step 701, loss 0.467047, acc 0.875\n",
      "2017-11-05T20:47:28.206723: step 702, loss 0.3183, acc 0.875\n",
      "2017-11-05T20:47:32.184514: step 703, loss 0.222613, acc 0.9375\n",
      "2017-11-05T20:47:36.190594: step 704, loss 0.0436802, acc 0.96875\n",
      "2017-11-05T20:47:40.189207: step 705, loss 0.528165, acc 0.875\n",
      "2017-11-05T20:47:44.127846: step 706, loss 0.28942, acc 0.90625\n",
      "2017-11-05T20:47:48.086388: step 707, loss 0.505839, acc 0.84375\n",
      "2017-11-05T20:47:52.067659: step 708, loss 0.0491003, acc 0.96875\n",
      "2017-11-05T20:47:56.026599: step 709, loss 0.00418787, acc 1\n",
      "2017-11-05T20:47:59.982663: step 710, loss 0.478529, acc 0.875\n",
      "2017-11-05T20:48:03.943011: step 711, loss 0.000892907, acc 1\n",
      "2017-11-05T20:48:07.892228: step 712, loss 0.374051, acc 0.875\n",
      "2017-11-05T20:48:11.839023: step 713, loss 0.496966, acc 0.90625\n",
      "2017-11-05T20:48:15.808748: step 714, loss 0.362156, acc 0.90625\n",
      "2017-11-05T20:48:19.783446: step 715, loss 0.5138, acc 0.875\n",
      "2017-11-05T20:48:23.949842: step 716, loss 0.0519711, acc 0.96875\n",
      "2017-11-05T20:48:28.210007: step 717, loss 0.11119, acc 0.9375\n",
      "2017-11-05T20:48:32.196684: step 718, loss 0.429178, acc 0.875\n",
      "2017-11-05T20:48:36.375704: step 719, loss 0.284562, acc 0.96875\n",
      "2017-11-05T20:48:38.886917: step 720, loss 0.199197, acc 0.85\n",
      "2017-11-05T20:48:43.081884: step 721, loss 0.515099, acc 0.875\n",
      "2017-11-05T20:48:47.322776: step 722, loss 0.449718, acc 0.875\n",
      "2017-11-05T20:48:51.345784: step 723, loss 0.134982, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T20:48:55.532723: step 724, loss 0.105538, acc 0.96875\n",
      "2017-11-05T20:48:59.639398: step 725, loss 0.2973, acc 0.9375\n",
      "2017-11-05T20:49:03.882382: step 726, loss 0.285073, acc 0.90625\n",
      "2017-11-05T20:49:08.063261: step 727, loss 0.413355, acc 0.90625\n",
      "2017-11-05T20:49:12.250394: step 728, loss 0.0647694, acc 0.9375\n",
      "2017-11-05T20:49:16.351635: step 729, loss 0.048359, acc 0.96875\n",
      "2017-11-05T20:49:20.576791: step 730, loss 0.526979, acc 0.90625\n",
      "2017-11-05T20:49:24.647452: step 731, loss 0.127052, acc 0.9375\n",
      "2017-11-05T20:49:28.838940: step 732, loss 0.439136, acc 0.9375\n",
      "2017-11-05T20:49:32.976105: step 733, loss 0.34776, acc 0.90625\n",
      "2017-11-05T20:49:37.205820: step 734, loss 0.198143, acc 0.9375\n",
      "2017-11-05T20:49:41.349667: step 735, loss 0.366489, acc 0.875\n",
      "2017-11-05T20:49:45.334371: step 736, loss 0.311781, acc 0.90625\n",
      "2017-11-05T20:49:49.322721: step 737, loss 0.388072, acc 0.90625\n",
      "2017-11-05T20:49:53.277043: step 738, loss 0.598793, acc 0.78125\n",
      "2017-11-05T20:49:57.221005: step 739, loss 0.490635, acc 0.90625\n",
      "2017-11-05T20:50:01.444276: step 740, loss 0.214816, acc 0.90625\n",
      "2017-11-05T20:50:05.438174: step 741, loss 0.267208, acc 0.90625\n",
      "2017-11-05T20:50:09.441791: step 742, loss 0.220263, acc 0.90625\n",
      "2017-11-05T20:50:13.400276: step 743, loss 0.334237, acc 0.9375\n",
      "2017-11-05T20:50:17.381188: step 744, loss 0.334822, acc 0.9375\n",
      "2017-11-05T20:50:21.340278: step 745, loss 0.200517, acc 0.90625\n",
      "2017-11-05T20:50:25.260543: step 746, loss 0.0601825, acc 0.96875\n",
      "2017-11-05T20:50:29.225814: step 747, loss 0.607036, acc 0.875\n",
      "2017-11-05T20:50:33.274945: step 748, loss 0.548321, acc 0.84375\n",
      "2017-11-05T20:50:37.298926: step 749, loss 0.34948, acc 0.84375\n",
      "2017-11-05T20:50:41.289716: step 750, loss 0.158838, acc 0.9375\n",
      "2017-11-05T20:50:45.232688: step 751, loss 0.382758, acc 0.84375\n",
      "2017-11-05T20:50:49.154259: step 752, loss 0.316503, acc 0.875\n",
      "2017-11-05T20:50:53.118933: step 753, loss 0.269759, acc 0.875\n",
      "2017-11-05T20:50:57.071562: step 754, loss 0.391248, acc 0.875\n",
      "2017-11-05T20:51:01.047468: step 755, loss 0.283334, acc 0.84375\n",
      "2017-11-05T20:51:03.576758: step 756, loss 0.557445, acc 0.75\n",
      "2017-11-05T20:51:07.520262: step 757, loss 0.0209136, acc 1\n",
      "2017-11-05T20:51:11.467863: step 758, loss 0.283102, acc 0.9375\n",
      "2017-11-05T20:51:15.425551: step 759, loss 0.407622, acc 0.875\n",
      "2017-11-05T20:51:19.371240: step 760, loss 0.149247, acc 0.90625\n",
      "2017-11-05T20:51:23.316340: step 761, loss 0.460258, acc 0.875\n",
      "2017-11-05T20:51:27.219837: step 762, loss 0.358938, acc 0.84375\n",
      "2017-11-05T20:51:31.225845: step 763, loss 0.136662, acc 0.90625\n",
      "2017-11-05T20:51:35.175267: step 764, loss 0.541067, acc 0.875\n",
      "2017-11-05T20:51:39.182764: step 765, loss 0.134032, acc 0.9375\n",
      "2017-11-05T20:51:43.161438: step 766, loss 0.0732736, acc 0.9375\n",
      "2017-11-05T20:51:47.087410: step 767, loss 0.0169473, acc 1\n",
      "2017-11-05T20:51:51.029089: step 768, loss 0.405403, acc 0.84375\n",
      "2017-11-05T20:51:54.955190: step 769, loss 0.233336, acc 0.96875\n",
      "2017-11-05T20:51:58.900219: step 770, loss 0.597759, acc 0.875\n",
      "2017-11-05T20:52:02.823793: step 771, loss 0.270185, acc 0.9375\n",
      "2017-11-05T20:52:06.758176: step 772, loss 0.291622, acc 0.875\n",
      "2017-11-05T20:52:10.719160: step 773, loss 0.101646, acc 0.96875\n",
      "2017-11-05T20:52:14.744743: step 774, loss 0.34544, acc 0.9375\n",
      "2017-11-05T20:52:18.673583: step 775, loss 0.379008, acc 0.84375\n",
      "2017-11-05T20:52:22.571624: step 776, loss 0.0930372, acc 0.96875\n",
      "2017-11-05T20:52:26.498710: step 777, loss 0.291161, acc 0.875\n",
      "2017-11-05T20:52:30.399032: step 778, loss 0.479046, acc 0.875\n",
      "2017-11-05T20:52:34.548349: step 779, loss 0.152279, acc 0.9375\n",
      "2017-11-05T20:52:38.577296: step 780, loss 0.286364, acc 0.9375\n",
      "2017-11-05T20:52:42.597854: step 781, loss 0.398212, acc 0.84375\n",
      "2017-11-05T20:52:46.536268: step 782, loss 0.337335, acc 0.875\n",
      "2017-11-05T20:52:50.453270: step 783, loss 0.304323, acc 0.9375\n",
      "2017-11-05T20:52:54.418665: step 784, loss 0.324826, acc 0.90625\n",
      "2017-11-05T20:52:58.362460: step 785, loss 0.0227096, acc 1\n",
      "2017-11-05T20:53:02.308152: step 786, loss 0.0826549, acc 0.96875\n",
      "2017-11-05T20:53:06.333190: step 787, loss 1.00848, acc 0.8125\n",
      "2017-11-05T20:53:10.305150: step 788, loss 0.817764, acc 0.84375\n",
      "2017-11-05T20:53:14.226443: step 789, loss 0.306538, acc 0.875\n",
      "2017-11-05T20:53:18.210910: step 790, loss 0.825206, acc 0.78125\n",
      "2017-11-05T20:53:22.266139: step 791, loss 0.0677329, acc 0.96875\n",
      "2017-11-05T20:53:24.817133: step 792, loss 0.1303, acc 0.95\n",
      "2017-11-05T20:53:28.718220: step 793, loss 0.190866, acc 0.9375\n",
      "2017-11-05T20:53:32.714888: step 794, loss 0.0842182, acc 1\n",
      "2017-11-05T20:53:36.661732: step 795, loss 0.427051, acc 0.90625\n",
      "2017-11-05T20:53:40.706839: step 796, loss 0.472475, acc 0.875\n",
      "2017-11-05T20:53:44.666325: step 797, loss 0.267988, acc 0.9375\n",
      "2017-11-05T20:53:48.587384: step 798, loss 0.0466303, acc 0.96875\n",
      "2017-11-05T20:53:52.528601: step 799, loss 0.531576, acc 0.84375\n",
      "2017-11-05T20:53:56.479371: step 800, loss 0.0371717, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T20:53:59.028183: step 800, loss 1.14964, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-800\n",
      "\n",
      "2017-11-05T20:54:04.992658: step 801, loss 0.425723, acc 0.875\n",
      "2017-11-05T20:54:09.002462: step 802, loss 0.544553, acc 0.875\n",
      "2017-11-05T20:54:12.946195: step 803, loss 0.120587, acc 0.9375\n",
      "2017-11-05T20:54:16.895643: step 804, loss 0.415828, acc 0.9375\n",
      "2017-11-05T20:54:20.851060: step 805, loss 0.31485, acc 0.84375\n",
      "2017-11-05T20:54:24.846207: step 806, loss 0.235124, acc 0.9375\n",
      "2017-11-05T20:54:28.762937: step 807, loss 0.239243, acc 0.9375\n",
      "2017-11-05T20:54:32.784668: step 808, loss 0.0297178, acc 1\n",
      "2017-11-05T20:54:36.863140: step 809, loss 0.346301, acc 0.9375\n",
      "2017-11-05T20:54:40.812211: step 810, loss 0.281598, acc 0.90625\n",
      "2017-11-05T20:54:44.824579: step 811, loss 0.269271, acc 0.875\n",
      "2017-11-05T20:54:48.755946: step 812, loss 0.181036, acc 0.90625\n",
      "2017-11-05T20:54:52.682959: step 813, loss 0.213017, acc 0.90625\n",
      "2017-11-05T20:54:56.663957: step 814, loss 0.848322, acc 0.84375\n",
      "2017-11-05T20:55:00.635254: step 815, loss 0.504445, acc 0.875\n",
      "2017-11-05T20:55:04.582590: step 816, loss 0.0839006, acc 0.9375\n",
      "2017-11-05T20:55:08.495927: step 817, loss 0.189478, acc 0.9375\n",
      "2017-11-05T20:55:12.430689: step 818, loss 0.159504, acc 0.9375\n",
      "2017-11-05T20:55:16.469364: step 819, loss 0.634626, acc 0.84375\n",
      "2017-11-05T20:55:20.442514: step 820, loss 0.297637, acc 0.875\n",
      "2017-11-05T20:55:24.376325: step 821, loss 0.0873641, acc 0.96875\n",
      "2017-11-05T20:55:28.319651: step 822, loss 0.188307, acc 0.9375\n",
      "2017-11-05T20:55:32.274629: step 823, loss 0.260494, acc 0.875\n",
      "2017-11-05T20:55:36.261257: step 824, loss 0.301033, acc 0.84375\n",
      "2017-11-05T20:55:40.226469: step 825, loss 0.151231, acc 0.90625\n",
      "2017-11-05T20:55:44.150103: step 826, loss 0.202981, acc 0.96875\n",
      "2017-11-05T20:55:48.147165: step 827, loss 0.301587, acc 0.875\n",
      "2017-11-05T20:55:50.652505: step 828, loss 0.27448, acc 0.85\n",
      "2017-11-05T20:55:54.575152: step 829, loss 0.172635, acc 0.90625\n",
      "2017-11-05T20:55:58.512818: step 830, loss 0.167648, acc 0.90625\n",
      "2017-11-05T20:56:02.534617: step 831, loss 0.377704, acc 0.90625\n",
      "2017-11-05T20:56:06.460273: step 832, loss 0.593431, acc 0.84375\n",
      "2017-11-05T20:56:10.445710: step 833, loss 0.12054, acc 0.9375\n",
      "2017-11-05T20:56:14.411487: step 834, loss 0.152057, acc 0.90625\n",
      "2017-11-05T20:56:18.370779: step 835, loss 0.172658, acc 0.96875\n",
      "2017-11-05T20:56:22.328164: step 836, loss 0.0917228, acc 0.9375\n",
      "2017-11-05T20:56:26.335588: step 837, loss 0.225035, acc 0.90625\n",
      "2017-11-05T20:56:30.286111: step 838, loss 0.264884, acc 0.90625\n",
      "2017-11-05T20:56:34.376580: step 839, loss 0.191996, acc 0.9375\n",
      "2017-11-05T20:56:38.365480: step 840, loss 0.318821, acc 0.9375\n",
      "2017-11-05T20:56:42.264304: step 841, loss 0.322803, acc 0.9375\n",
      "2017-11-05T20:56:46.288995: step 842, loss 0.244787, acc 0.90625\n",
      "2017-11-05T20:56:50.233251: step 843, loss 0.305545, acc 0.9375\n",
      "2017-11-05T20:56:54.183426: step 844, loss 0.454334, acc 0.875\n",
      "2017-11-05T20:56:58.155115: step 845, loss 0.27844, acc 0.90625\n",
      "2017-11-05T20:57:02.224181: step 846, loss 0.395073, acc 0.84375\n",
      "2017-11-05T20:57:06.196880: step 847, loss 0.22193, acc 0.90625\n",
      "2017-11-05T20:57:10.205897: step 848, loss 0.234647, acc 0.9375\n",
      "2017-11-05T20:57:14.157682: step 849, loss 0.138992, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T20:57:18.173183: step 850, loss 0.239162, acc 0.90625\n",
      "2017-11-05T20:57:22.123949: step 851, loss 0.150301, acc 0.9375\n",
      "2017-11-05T20:57:26.138408: step 852, loss 0.250216, acc 0.84375\n",
      "2017-11-05T20:57:30.084254: step 853, loss 0.518582, acc 0.875\n",
      "2017-11-05T20:57:34.026820: step 854, loss 0.364768, acc 0.84375\n",
      "2017-11-05T20:57:38.005204: step 855, loss 0.327485, acc 0.875\n",
      "2017-11-05T20:57:41.913869: step 856, loss 0.220698, acc 0.9375\n",
      "2017-11-05T20:57:45.902146: step 857, loss 0.409392, acc 0.90625\n",
      "2017-11-05T20:57:49.886736: step 858, loss 0.314391, acc 0.875\n",
      "2017-11-05T20:57:53.857785: step 859, loss 0.321633, acc 0.90625\n",
      "2017-11-05T20:57:57.798060: step 860, loss 0.0457462, acc 0.96875\n",
      "2017-11-05T20:58:01.827295: step 861, loss 0.269127, acc 0.84375\n",
      "2017-11-05T20:58:05.757413: step 862, loss 0.0383528, acc 0.96875\n",
      "2017-11-05T20:58:09.712364: step 863, loss 0.0597105, acc 1\n",
      "2017-11-05T20:58:12.201077: step 864, loss 0.223105, acc 0.95\n",
      "2017-11-05T20:58:16.151497: step 865, loss 0.0205089, acc 1\n",
      "2017-11-05T20:58:20.110799: step 866, loss 0.216118, acc 0.9375\n",
      "2017-11-05T20:58:24.198354: step 867, loss 0.0739453, acc 0.96875\n",
      "2017-11-05T20:58:28.250305: step 868, loss 0.507086, acc 0.84375\n",
      "2017-11-05T20:58:32.223347: step 869, loss 0.16678, acc 0.96875\n",
      "2017-11-05T20:58:36.303074: step 870, loss 0.41526, acc 0.84375\n",
      "2017-11-05T20:58:40.251202: step 871, loss 0.168335, acc 0.96875\n",
      "2017-11-05T20:58:44.250995: step 872, loss 0.210483, acc 0.9375\n",
      "2017-11-05T20:58:48.203360: step 873, loss 0.42897, acc 0.90625\n",
      "2017-11-05T20:58:52.241989: step 874, loss 0.18976, acc 0.90625\n",
      "2017-11-05T20:58:56.180451: step 875, loss 0.433001, acc 0.8125\n",
      "2017-11-05T20:59:00.148292: step 876, loss 0.752008, acc 0.8125\n",
      "2017-11-05T20:59:04.114492: step 877, loss 0.396583, acc 0.90625\n",
      "2017-11-05T20:59:08.091050: step 878, loss 0.1101, acc 0.96875\n",
      "2017-11-05T20:59:12.066143: step 879, loss 0.280764, acc 0.90625\n",
      "2017-11-05T20:59:16.107429: step 880, loss 0.253096, acc 0.90625\n",
      "2017-11-05T20:59:20.087042: step 881, loss 0.143021, acc 0.96875\n",
      "2017-11-05T20:59:24.063779: step 882, loss 0.240864, acc 0.9375\n",
      "2017-11-05T20:59:27.978582: step 883, loss 0.160166, acc 0.90625\n",
      "2017-11-05T20:59:31.951412: step 884, loss 0.149081, acc 0.96875\n",
      "2017-11-05T20:59:35.917804: step 885, loss 0.403458, acc 0.875\n",
      "2017-11-05T20:59:39.901800: step 886, loss 0.645063, acc 0.78125\n",
      "2017-11-05T20:59:43.845371: step 887, loss 0.0909642, acc 0.96875\n",
      "2017-11-05T20:59:47.805813: step 888, loss 0.161366, acc 0.90625\n",
      "2017-11-05T20:59:51.764662: step 889, loss 0.180086, acc 0.96875\n",
      "2017-11-05T20:59:55.730987: step 890, loss 0.132022, acc 0.96875\n",
      "2017-11-05T20:59:59.672514: step 891, loss 0.0502708, acc 1\n",
      "2017-11-05T21:00:03.871344: step 892, loss 0.203949, acc 0.96875\n",
      "2017-11-05T21:00:07.820193: step 893, loss 0.588311, acc 0.84375\n",
      "2017-11-05T21:00:11.779603: step 894, loss 0.473682, acc 0.84375\n",
      "2017-11-05T21:00:15.821246: step 895, loss 0.563113, acc 0.84375\n",
      "2017-11-05T21:00:19.781651: step 896, loss 0.585447, acc 0.84375\n",
      "2017-11-05T21:00:23.791912: step 897, loss 0.109778, acc 0.9375\n",
      "2017-11-05T21:00:27.769424: step 898, loss 0.309713, acc 0.9375\n",
      "2017-11-05T21:00:31.745895: step 899, loss 0.280541, acc 0.875\n",
      "2017-11-05T21:00:34.330544: step 900, loss 0.127356, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:00:36.877886: step 900, loss 0.994409, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-900\n",
      "\n",
      "2017-11-05T21:00:42.656453: step 901, loss 0.30995, acc 0.875\n",
      "2017-11-05T21:00:46.602187: step 902, loss 0.0384575, acc 1\n",
      "2017-11-05T21:00:50.562983: step 903, loss 0.314247, acc 0.84375\n",
      "2017-11-05T21:00:54.572127: step 904, loss 0.209488, acc 0.9375\n",
      "2017-11-05T21:00:58.484928: step 905, loss 0.257355, acc 0.875\n",
      "2017-11-05T21:01:02.402315: step 906, loss 0.260205, acc 0.90625\n",
      "2017-11-05T21:01:06.411599: step 907, loss 0.0394024, acc 1\n",
      "2017-11-05T21:01:10.418721: step 908, loss 0.213671, acc 0.90625\n",
      "2017-11-05T21:01:14.354238: step 909, loss 0.798273, acc 0.75\n",
      "2017-11-05T21:01:18.329135: step 910, loss 0.132586, acc 0.9375\n",
      "2017-11-05T21:01:22.213647: step 911, loss 0.190178, acc 0.9375\n",
      "2017-11-05T21:01:26.199507: step 912, loss 0.781073, acc 0.75\n",
      "2017-11-05T21:01:30.126410: step 913, loss 0.432388, acc 0.875\n",
      "2017-11-05T21:01:34.075698: step 914, loss 0.282715, acc 0.875\n",
      "2017-11-05T21:01:38.019921: step 915, loss 0.0720599, acc 1\n",
      "2017-11-05T21:01:42.011413: step 916, loss 0.19049, acc 0.96875\n",
      "2017-11-05T21:01:45.970745: step 917, loss 0.477269, acc 0.875\n",
      "2017-11-05T21:01:49.921246: step 918, loss 0.36395, acc 0.875\n",
      "2017-11-05T21:01:53.917604: step 919, loss 0.108355, acc 0.9375\n",
      "2017-11-05T21:01:57.872091: step 920, loss 0.499672, acc 0.8125\n",
      "2017-11-05T21:02:01.939396: step 921, loss 0.207474, acc 0.9375\n",
      "2017-11-05T21:02:05.907736: step 922, loss 0.280705, acc 0.84375\n",
      "2017-11-05T21:02:09.856025: step 923, loss 0.352191, acc 0.875\n",
      "2017-11-05T21:02:13.832863: step 924, loss 0.162764, acc 0.9375\n",
      "2017-11-05T21:02:17.793880: step 925, loss 0.377026, acc 0.9375\n",
      "2017-11-05T21:02:21.725322: step 926, loss 0.287277, acc 0.90625\n",
      "2017-11-05T21:02:25.705681: step 927, loss 0.00998842, acc 1\n",
      "2017-11-05T21:02:29.596336: step 928, loss 0.159407, acc 0.9375\n",
      "2017-11-05T21:02:33.671841: step 929, loss 0.212802, acc 0.90625\n",
      "2017-11-05T21:02:37.609554: step 930, loss 0.226151, acc 0.875\n",
      "2017-11-05T21:02:41.539210: step 931, loss 0.44368, acc 0.875\n",
      "2017-11-05T21:02:45.487566: step 932, loss 0.152291, acc 0.9375\n",
      "2017-11-05T21:02:49.424131: step 933, loss 0.468888, acc 0.875\n",
      "2017-11-05T21:02:53.366553: step 934, loss 0.279197, acc 0.90625\n",
      "2017-11-05T21:02:57.299779: step 935, loss 0.400145, acc 0.84375\n",
      "2017-11-05T21:02:59.790129: step 936, loss 0.0294393, acc 1\n",
      "2017-11-05T21:03:03.820186: step 937, loss 0.102515, acc 0.96875\n",
      "2017-11-05T21:03:07.761324: step 938, loss 0.188197, acc 0.90625\n",
      "2017-11-05T21:03:11.736118: step 939, loss 0.110523, acc 0.9375\n",
      "2017-11-05T21:03:15.680914: step 940, loss 0.384129, acc 0.84375\n",
      "2017-11-05T21:03:19.648002: step 941, loss 0.133842, acc 0.875\n",
      "2017-11-05T21:03:23.783368: step 942, loss 0.273193, acc 0.875\n",
      "2017-11-05T21:03:27.746547: step 943, loss 0.107278, acc 0.96875\n",
      "2017-11-05T21:03:31.674643: step 944, loss 0.119179, acc 0.9375\n",
      "2017-11-05T21:03:35.653978: step 945, loss 0.268436, acc 0.90625\n",
      "2017-11-05T21:03:39.649010: step 946, loss 0.131932, acc 0.9375\n",
      "2017-11-05T21:03:43.711938: step 947, loss 0.263057, acc 0.90625\n",
      "2017-11-05T21:03:47.631535: step 948, loss 0.204108, acc 0.9375\n",
      "2017-11-05T21:03:51.607687: step 949, loss 0.180134, acc 0.96875\n",
      "2017-11-05T21:03:55.616901: step 950, loss 0.183858, acc 0.9375\n",
      "2017-11-05T21:03:59.546600: step 951, loss 0.173501, acc 0.90625\n",
      "2017-11-05T21:04:03.478478: step 952, loss 0.159415, acc 0.9375\n",
      "2017-11-05T21:04:07.439203: step 953, loss 0.198813, acc 0.90625\n",
      "2017-11-05T21:04:11.417405: step 954, loss 0.389348, acc 0.90625\n",
      "2017-11-05T21:04:15.334169: step 955, loss 0.156543, acc 0.96875\n",
      "2017-11-05T21:04:19.348868: step 956, loss 0.169224, acc 0.9375\n",
      "2017-11-05T21:04:23.331304: step 957, loss 0.159, acc 0.9375\n",
      "2017-11-05T21:04:27.223324: step 958, loss 0.0562509, acc 0.96875\n",
      "2017-11-05T21:04:31.202908: step 959, loss 0.106356, acc 0.96875\n",
      "2017-11-05T21:04:35.275467: step 960, loss 0.0621075, acc 0.96875\n",
      "2017-11-05T21:04:39.236147: step 961, loss 0.551744, acc 0.875\n",
      "2017-11-05T21:04:43.179871: step 962, loss 0.577422, acc 0.75\n",
      "2017-11-05T21:04:47.082889: step 963, loss 0.22843, acc 0.9375\n",
      "2017-11-05T21:04:51.036853: step 964, loss 0.590365, acc 0.8125\n",
      "2017-11-05T21:04:54.967754: step 965, loss 0.287585, acc 0.9375\n",
      "2017-11-05T21:04:58.913212: step 966, loss 0.358852, acc 0.875\n",
      "2017-11-05T21:05:02.878938: step 967, loss 0.0678479, acc 1\n",
      "2017-11-05T21:05:06.841464: step 968, loss 0.66055, acc 0.75\n",
      "2017-11-05T21:05:10.797114: step 969, loss 0.142402, acc 0.9375\n",
      "2017-11-05T21:05:14.704861: step 970, loss 0.241817, acc 0.90625\n",
      "2017-11-05T21:05:18.714153: step 971, loss 0.023143, acc 1\n",
      "2017-11-05T21:05:21.220901: step 972, loss 0.126737, acc 0.95\n",
      "2017-11-05T21:05:25.244737: step 973, loss 0.269963, acc 0.875\n",
      "2017-11-05T21:05:29.203799: step 974, loss 0.574871, acc 0.875\n",
      "2017-11-05T21:05:33.120246: step 975, loss 0.296453, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T21:05:37.002704: step 976, loss 0.0128487, acc 1\n",
      "2017-11-05T21:05:40.948209: step 977, loss 0.521633, acc 0.84375\n",
      "2017-11-05T21:05:44.911204: step 978, loss 0.34084, acc 0.9375\n",
      "2017-11-05T21:05:48.859227: step 979, loss 0.0165185, acc 1\n",
      "2017-11-05T21:05:52.816457: step 980, loss 0.201791, acc 0.9375\n",
      "2017-11-05T21:05:56.767272: step 981, loss 0.314346, acc 0.90625\n",
      "2017-11-05T21:06:00.755627: step 982, loss 0.128834, acc 0.9375\n",
      "2017-11-05T21:06:04.684941: step 983, loss 0.186024, acc 0.96875\n",
      "2017-11-05T21:06:08.598056: step 984, loss 0.152026, acc 0.9375\n",
      "2017-11-05T21:06:12.562187: step 985, loss 0.18943, acc 0.90625\n",
      "2017-11-05T21:06:16.494537: step 986, loss 0.251805, acc 0.90625\n",
      "2017-11-05T21:06:20.485785: step 987, loss 0.145273, acc 0.96875\n",
      "2017-11-05T21:06:24.444077: step 988, loss 0.0959371, acc 0.9375\n",
      "2017-11-05T21:06:28.365700: step 989, loss 0.351775, acc 0.84375\n",
      "2017-11-05T21:06:32.311447: step 990, loss 0.220877, acc 0.90625\n",
      "2017-11-05T21:06:36.405653: step 991, loss 0.092947, acc 0.96875\n",
      "2017-11-05T21:06:40.381688: step 992, loss 0.227742, acc 0.9375\n",
      "2017-11-05T21:06:44.319137: step 993, loss 0.479914, acc 0.875\n",
      "2017-11-05T21:06:48.319908: step 994, loss 0.273201, acc 0.84375\n",
      "2017-11-05T21:06:52.298512: step 995, loss 0.452992, acc 0.875\n",
      "2017-11-05T21:06:56.210796: step 996, loss 0.105902, acc 0.9375\n",
      "2017-11-05T21:07:00.246794: step 997, loss 0.245089, acc 0.9375\n",
      "2017-11-05T21:07:04.198286: step 998, loss 0.0578611, acc 0.96875\n",
      "2017-11-05T21:07:08.117343: step 999, loss 0.472823, acc 0.75\n",
      "2017-11-05T21:07:12.120982: step 1000, loss 0.12737, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:07:14.658780: step 1000, loss 0.743568, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-05T21:07:19.904025: step 1001, loss 0.354851, acc 0.875\n",
      "2017-11-05T21:07:23.914653: step 1002, loss 0.20096, acc 0.90625\n",
      "2017-11-05T21:07:27.914147: step 1003, loss 0.333169, acc 0.8125\n",
      "2017-11-05T21:07:31.948909: step 1004, loss 0.257233, acc 0.90625\n",
      "2017-11-05T21:07:35.940873: step 1005, loss 0.152287, acc 0.9375\n",
      "2017-11-05T21:07:39.931133: step 1006, loss 0.316501, acc 0.84375\n",
      "2017-11-05T21:07:43.931248: step 1007, loss 0.398208, acc 0.84375\n",
      "2017-11-05T21:07:46.520968: step 1008, loss 0.310144, acc 0.9\n",
      "2017-11-05T21:07:50.483054: step 1009, loss 0.388842, acc 0.84375\n",
      "2017-11-05T21:07:54.548772: step 1010, loss 0.205951, acc 0.90625\n",
      "2017-11-05T21:07:58.537234: step 1011, loss 0.0915251, acc 1\n",
      "2017-11-05T21:08:02.617514: step 1012, loss 0.237796, acc 0.9375\n",
      "2017-11-05T21:08:06.571834: step 1013, loss 0.222892, acc 0.9375\n",
      "2017-11-05T21:08:10.714721: step 1014, loss 0.217823, acc 0.90625\n",
      "2017-11-05T21:08:14.633245: step 1015, loss 0.250676, acc 0.875\n",
      "2017-11-05T21:08:18.651003: step 1016, loss 0.231249, acc 0.875\n",
      "2017-11-05T21:08:22.754165: step 1017, loss 0.0929633, acc 0.96875\n",
      "2017-11-05T21:08:27.124991: step 1018, loss 0.0867541, acc 0.9375\n",
      "2017-11-05T21:08:31.054313: step 1019, loss 0.311444, acc 0.84375\n",
      "2017-11-05T21:08:35.162610: step 1020, loss 0.353079, acc 0.9375\n",
      "2017-11-05T21:08:39.144276: step 1021, loss 0.0277873, acc 1\n",
      "2017-11-05T21:08:43.107171: step 1022, loss 0.133513, acc 0.9375\n",
      "2017-11-05T21:08:47.062314: step 1023, loss 0.268663, acc 0.9375\n",
      "2017-11-05T21:08:51.064097: step 1024, loss 0.279296, acc 0.90625\n",
      "2017-11-05T21:08:55.030687: step 1025, loss 0.0555804, acc 1\n",
      "2017-11-05T21:08:58.984997: step 1026, loss 0.168396, acc 0.9375\n",
      "2017-11-05T21:09:03.012394: step 1027, loss 0.310685, acc 0.875\n",
      "2017-11-05T21:09:06.953868: step 1028, loss 0.0209292, acc 1\n",
      "2017-11-05T21:09:10.928521: step 1029, loss 0.134411, acc 0.9375\n",
      "2017-11-05T21:09:14.891118: step 1030, loss 0.302051, acc 0.875\n",
      "2017-11-05T21:09:18.891823: step 1031, loss 0.0239405, acc 1\n",
      "2017-11-05T21:09:22.868704: step 1032, loss 0.491125, acc 0.875\n",
      "2017-11-05T21:09:26.816344: step 1033, loss 0.0376622, acc 1\n",
      "2017-11-05T21:09:30.755983: step 1034, loss 0.248142, acc 0.90625\n",
      "2017-11-05T21:09:34.728812: step 1035, loss 0.107866, acc 0.9375\n",
      "2017-11-05T21:09:38.701924: step 1036, loss 0.197297, acc 0.875\n",
      "2017-11-05T21:09:42.742345: step 1037, loss 0.283746, acc 0.90625\n",
      "2017-11-05T21:09:46.729715: step 1038, loss 0.221141, acc 0.875\n",
      "2017-11-05T21:09:50.804164: step 1039, loss 0.5238, acc 0.875\n",
      "2017-11-05T21:09:54.721176: step 1040, loss 0.394093, acc 0.875\n",
      "2017-11-05T21:09:58.675931: step 1041, loss 0.233739, acc 0.84375\n",
      "2017-11-05T21:10:02.990185: step 1042, loss 0.447771, acc 0.84375\n",
      "2017-11-05T21:10:07.007268: step 1043, loss 0.213315, acc 0.90625\n",
      "2017-11-05T21:10:09.539110: step 1044, loss 0.314845, acc 0.9\n",
      "2017-11-05T21:10:13.578118: step 1045, loss 0.304374, acc 0.84375\n",
      "2017-11-05T21:10:17.529609: step 1046, loss 0.303542, acc 0.90625\n",
      "2017-11-05T21:10:21.544709: step 1047, loss 0.0696507, acc 0.9375\n",
      "2017-11-05T21:10:25.473952: step 1048, loss 0.085719, acc 0.96875\n",
      "2017-11-05T21:10:29.514034: step 1049, loss 0.266754, acc 0.9375\n",
      "2017-11-05T21:10:33.549746: step 1050, loss 0.215563, acc 0.90625\n",
      "2017-11-05T21:10:37.588954: step 1051, loss 0.285898, acc 0.90625\n",
      "2017-11-05T21:10:41.544562: step 1052, loss 0.222786, acc 0.90625\n",
      "2017-11-05T21:10:45.473647: step 1053, loss 0.305552, acc 0.875\n",
      "2017-11-05T21:10:49.508689: step 1054, loss 0.230139, acc 0.90625\n",
      "2017-11-05T21:10:53.487494: step 1055, loss 0.148732, acc 0.9375\n",
      "2017-11-05T21:10:57.455692: step 1056, loss 0.304742, acc 0.875\n",
      "2017-11-05T21:11:01.480056: step 1057, loss 0.222723, acc 0.875\n",
      "2017-11-05T21:11:05.455861: step 1058, loss 0.107879, acc 0.96875\n",
      "2017-11-05T21:11:09.455026: step 1059, loss 0.329829, acc 0.9375\n",
      "2017-11-05T21:11:13.379862: step 1060, loss 0.404738, acc 0.875\n",
      "2017-11-05T21:11:17.297398: step 1061, loss 0.17842, acc 0.875\n",
      "2017-11-05T21:11:21.272962: step 1062, loss 0.168667, acc 0.90625\n",
      "2017-11-05T21:11:25.223283: step 1063, loss 0.0218585, acc 1\n",
      "2017-11-05T21:11:29.217560: step 1064, loss 0.178422, acc 0.9375\n",
      "2017-11-05T21:11:33.193558: step 1065, loss 0.0948752, acc 0.9375\n",
      "2017-11-05T21:11:37.124568: step 1066, loss 0.209347, acc 0.96875\n",
      "2017-11-05T21:11:41.052138: step 1067, loss 0.225227, acc 0.9375\n",
      "2017-11-05T21:11:45.045427: step 1068, loss 0.314354, acc 0.90625\n",
      "2017-11-05T21:11:49.061160: step 1069, loss 0.325716, acc 0.875\n",
      "2017-11-05T21:11:53.068897: step 1070, loss 0.283575, acc 0.875\n",
      "2017-11-05T21:11:56.993927: step 1071, loss 0.121112, acc 0.96875\n",
      "2017-11-05T21:12:00.983787: step 1072, loss 0.157821, acc 0.96875\n",
      "2017-11-05T21:12:04.950091: step 1073, loss 0.173685, acc 0.9375\n",
      "2017-11-05T21:12:09.064933: step 1074, loss 0.110077, acc 0.9375\n",
      "2017-11-05T21:12:13.051082: step 1075, loss 0.298775, acc 0.84375\n",
      "2017-11-05T21:12:17.037080: step 1076, loss 0.153391, acc 0.9375\n",
      "2017-11-05T21:12:20.951079: step 1077, loss 0.516514, acc 0.78125\n",
      "2017-11-05T21:12:24.904699: step 1078, loss 0.304693, acc 0.9375\n",
      "2017-11-05T21:12:28.839294: step 1079, loss 0.215501, acc 0.90625\n",
      "2017-11-05T21:12:31.308977: step 1080, loss 0.325469, acc 0.9\n",
      "2017-11-05T21:12:35.414166: step 1081, loss 0.482999, acc 0.8125\n",
      "2017-11-05T21:12:39.402333: step 1082, loss 0.019225, acc 1\n",
      "2017-11-05T21:12:43.344834: step 1083, loss 0.211384, acc 0.9375\n",
      "2017-11-05T21:12:47.299350: step 1084, loss 0.116883, acc 0.9375\n",
      "2017-11-05T21:12:51.262499: step 1085, loss 0.200748, acc 0.90625\n",
      "2017-11-05T21:12:55.182356: step 1086, loss 0.214343, acc 0.90625\n",
      "2017-11-05T21:12:59.078773: step 1087, loss 0.446807, acc 0.875\n",
      "2017-11-05T21:13:03.026823: step 1088, loss 0.400606, acc 0.8125\n",
      "2017-11-05T21:13:07.008045: step 1089, loss 0.0703904, acc 0.9375\n",
      "2017-11-05T21:13:11.013434: step 1090, loss 0.296781, acc 0.84375\n",
      "2017-11-05T21:13:14.984287: step 1091, loss 0.178838, acc 0.96875\n",
      "2017-11-05T21:13:19.002120: step 1092, loss 0.114154, acc 0.96875\n",
      "2017-11-05T21:13:23.102176: step 1093, loss 0.127608, acc 0.9375\n",
      "2017-11-05T21:13:27.218284: step 1094, loss 0.176734, acc 0.96875\n",
      "2017-11-05T21:13:31.224023: step 1095, loss 0.160455, acc 0.9375\n",
      "2017-11-05T21:13:35.202062: step 1096, loss 0.192231, acc 0.90625\n",
      "2017-11-05T21:13:39.213145: step 1097, loss 0.176284, acc 0.96875\n",
      "2017-11-05T21:13:43.314309: step 1098, loss 0.18874, acc 0.90625\n",
      "2017-11-05T21:13:47.274178: step 1099, loss 0.377558, acc 0.84375\n",
      "2017-11-05T21:13:51.228913: step 1100, loss 0.227802, acc 0.875\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T21:13:53.821552: step 1100, loss 0.731476, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-05T21:13:59.191142: step 1101, loss 0.205118, acc 0.9375\n",
      "2017-11-05T21:14:03.200321: step 1102, loss 0.107413, acc 0.96875\n",
      "2017-11-05T21:14:07.161288: step 1103, loss 0.257455, acc 0.90625\n",
      "2017-11-05T21:14:11.136087: step 1104, loss 0.531286, acc 0.8125\n",
      "2017-11-05T21:14:15.063119: step 1105, loss 0.153735, acc 0.96875\n",
      "2017-11-05T21:14:18.994955: step 1106, loss 0.158886, acc 0.90625\n",
      "2017-11-05T21:14:22.897347: step 1107, loss 0.390194, acc 0.875\n",
      "2017-11-05T21:14:26.804266: step 1108, loss 0.069537, acc 0.96875\n",
      "2017-11-05T21:14:30.745056: step 1109, loss 0.48675, acc 0.84375\n",
      "2017-11-05T21:14:34.764554: step 1110, loss 0.0900477, acc 0.96875\n",
      "2017-11-05T21:14:38.715538: step 1111, loss 0.198903, acc 0.9375\n",
      "2017-11-05T21:14:42.679909: step 1112, loss 0.170202, acc 0.9375\n",
      "2017-11-05T21:14:46.594338: step 1113, loss 0.201239, acc 0.9375\n",
      "2017-11-05T21:14:50.500675: step 1114, loss 0.123485, acc 0.9375\n",
      "2017-11-05T21:14:54.409756: step 1115, loss 0.340656, acc 0.84375\n",
      "2017-11-05T21:14:56.947187: step 1116, loss 0.0561484, acc 1\n",
      "2017-11-05T21:15:00.861841: step 1117, loss 0.115926, acc 0.96875\n",
      "2017-11-05T21:15:04.825495: step 1118, loss 0.524301, acc 0.71875\n",
      "2017-11-05T21:15:08.766445: step 1119, loss 0.113202, acc 0.9375\n",
      "2017-11-05T21:15:12.720922: step 1120, loss 0.238971, acc 0.84375\n",
      "2017-11-05T21:15:16.646214: step 1121, loss 0.194497, acc 0.90625\n",
      "2017-11-05T21:15:20.621226: step 1122, loss 0.122257, acc 0.9375\n",
      "2017-11-05T21:15:24.590651: step 1123, loss 0.215635, acc 0.90625\n",
      "2017-11-05T21:15:28.546602: step 1124, loss 0.064208, acc 1\n",
      "2017-11-05T21:15:32.497100: step 1125, loss 0.196688, acc 0.96875\n",
      "2017-11-05T21:15:36.470100: step 1126, loss 0.936747, acc 0.8125\n",
      "2017-11-05T21:15:40.460395: step 1127, loss 0.50065, acc 0.875\n",
      "2017-11-05T21:15:44.399325: step 1128, loss 0.552754, acc 0.84375\n",
      "2017-11-05T21:15:48.313845: step 1129, loss 0.170237, acc 0.9375\n",
      "2017-11-05T21:15:52.277831: step 1130, loss 0.0233576, acc 1\n",
      "2017-11-05T21:15:56.286658: step 1131, loss 0.116991, acc 0.9375\n",
      "2017-11-05T21:16:00.228688: step 1132, loss 0.284971, acc 0.8125\n",
      "2017-11-05T21:16:04.220447: step 1133, loss 0.123326, acc 0.96875\n",
      "2017-11-05T21:16:08.194658: step 1134, loss 0.292234, acc 0.84375\n",
      "2017-11-05T21:16:12.174680: step 1135, loss 0.382452, acc 0.875\n",
      "2017-11-05T21:16:16.169545: step 1136, loss 0.168041, acc 0.9375\n",
      "2017-11-05T21:16:20.082631: step 1137, loss 0.144772, acc 0.9375\n",
      "2017-11-05T21:16:24.074166: step 1138, loss 0.0651623, acc 0.96875\n",
      "2017-11-05T21:16:28.029153: step 1139, loss 0.227829, acc 0.84375\n",
      "2017-11-05T21:16:31.947384: step 1140, loss 0.288093, acc 0.84375\n",
      "2017-11-05T21:16:36.244989: step 1141, loss 0.133437, acc 0.9375\n",
      "2017-11-05T21:16:40.289562: step 1142, loss 0.245744, acc 0.90625\n",
      "2017-11-05T21:16:44.256901: step 1143, loss 0.0838176, acc 0.96875\n",
      "2017-11-05T21:16:48.209430: step 1144, loss 0.223451, acc 0.9375\n",
      "2017-11-05T21:16:52.100942: step 1145, loss 0.133004, acc 0.9375\n",
      "2017-11-05T21:16:56.032973: step 1146, loss 0.319463, acc 0.9375\n",
      "2017-11-05T21:17:00.023585: step 1147, loss 0.354173, acc 0.90625\n",
      "2017-11-05T21:17:03.970430: step 1148, loss 0.317052, acc 0.90625\n",
      "2017-11-05T21:17:07.900765: step 1149, loss 0.187982, acc 0.875\n",
      "2017-11-05T21:17:11.891352: step 1150, loss 0.275957, acc 0.875\n",
      "2017-11-05T21:17:15.806896: step 1151, loss 0.231607, acc 0.84375\n",
      "2017-11-05T21:17:18.328856: step 1152, loss 0.378658, acc 0.8\n",
      "2017-11-05T21:17:22.276631: step 1153, loss 0.089976, acc 0.9375\n",
      "2017-11-05T21:17:26.286647: step 1154, loss 0.113508, acc 0.96875\n",
      "2017-11-05T21:17:30.199298: step 1155, loss 0.410064, acc 0.875\n",
      "2017-11-05T21:17:34.115222: step 1156, loss 0.192058, acc 0.90625\n",
      "2017-11-05T21:17:38.016492: step 1157, loss 0.0483361, acc 0.96875\n",
      "2017-11-05T21:17:41.948450: step 1158, loss 0.174054, acc 0.9375\n",
      "2017-11-05T21:17:45.874636: step 1159, loss 0.269278, acc 0.90625\n",
      "2017-11-05T21:17:49.841445: step 1160, loss 0.233175, acc 0.9375\n",
      "2017-11-05T21:17:53.764380: step 1161, loss 0.238151, acc 0.90625\n",
      "2017-11-05T21:17:57.764714: step 1162, loss 0.321467, acc 0.875\n",
      "2017-11-05T21:18:01.717497: step 1163, loss 0.476357, acc 0.8125\n",
      "2017-11-05T21:18:05.665855: step 1164, loss 0.0459609, acc 1\n",
      "2017-11-05T21:18:09.611137: step 1165, loss 0.199424, acc 0.9375\n",
      "2017-11-05T21:18:13.632216: step 1166, loss 0.203641, acc 0.9375\n",
      "2017-11-05T21:18:17.667512: step 1167, loss 0.245876, acc 0.84375\n",
      "2017-11-05T21:18:21.636459: step 1168, loss 0.0298391, acc 1\n",
      "2017-11-05T21:18:25.838201: step 1169, loss 0.0785783, acc 0.96875\n",
      "2017-11-05T21:18:29.956626: step 1170, loss 0.36152, acc 0.875\n",
      "2017-11-05T21:18:33.980317: step 1171, loss 0.216923, acc 0.90625\n",
      "2017-11-05T21:18:38.091659: step 1172, loss 0.0780049, acc 0.96875\n",
      "2017-11-05T21:18:42.255094: step 1173, loss 0.262206, acc 0.9375\n",
      "2017-11-05T21:18:46.249836: step 1174, loss 0.19337, acc 0.90625\n",
      "2017-11-05T21:18:50.354381: step 1175, loss 0.0665466, acc 0.96875\n",
      "2017-11-05T21:18:54.434623: step 1176, loss 0.204273, acc 0.9375\n",
      "2017-11-05T21:18:58.553830: step 1177, loss 0.141795, acc 0.90625\n",
      "2017-11-05T21:19:02.630751: step 1178, loss 0.148807, acc 0.90625\n",
      "2017-11-05T21:19:06.788354: step 1179, loss 0.176722, acc 0.9375\n",
      "2017-11-05T21:19:10.812089: step 1180, loss 0.109344, acc 0.9375\n",
      "2017-11-05T21:19:15.037827: step 1181, loss 0.446517, acc 0.78125\n",
      "2017-11-05T21:19:19.153525: step 1182, loss 0.16986, acc 0.9375\n",
      "2017-11-05T21:19:23.260281: step 1183, loss 0.517276, acc 0.8125\n",
      "2017-11-05T21:19:27.232037: step 1184, loss 0.29526, acc 0.8125\n",
      "2017-11-05T21:19:31.385239: step 1185, loss 0.215802, acc 0.90625\n",
      "2017-11-05T21:19:35.539904: step 1186, loss 0.568273, acc 0.84375\n",
      "2017-11-05T21:19:39.623048: step 1187, loss 0.0628465, acc 0.96875\n",
      "2017-11-05T21:19:42.306508: step 1188, loss 0.164058, acc 0.95\n",
      "2017-11-05T21:19:46.341465: step 1189, loss 0.0611559, acc 0.96875\n",
      "2017-11-05T21:19:50.350944: step 1190, loss 0.235363, acc 0.90625\n",
      "2017-11-05T21:19:54.318901: step 1191, loss 0.327659, acc 0.90625\n",
      "2017-11-05T21:19:58.289136: step 1192, loss 0.171953, acc 0.90625\n",
      "2017-11-05T21:20:02.601640: step 1193, loss 0.0598005, acc 0.96875\n",
      "2017-11-05T21:20:06.551528: step 1194, loss 0.059745, acc 0.96875\n",
      "2017-11-05T21:20:10.494219: step 1195, loss 0.0448016, acc 1\n",
      "2017-11-05T21:20:14.420418: step 1196, loss 0.161295, acc 0.90625\n",
      "2017-11-05T21:20:18.387475: step 1197, loss 0.212089, acc 0.9375\n",
      "2017-11-05T21:20:22.334931: step 1198, loss 0.00610392, acc 1\n",
      "2017-11-05T21:20:26.305103: step 1199, loss 0.115562, acc 0.9375\n",
      "2017-11-05T21:20:30.228449: step 1200, loss 0.268619, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:20:32.768286: step 1200, loss 0.985318, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-05T21:20:38.620992: step 1201, loss 0.317327, acc 0.90625\n",
      "2017-11-05T21:20:42.598294: step 1202, loss 0.438159, acc 0.84375\n",
      "2017-11-05T21:20:46.591451: step 1203, loss 0.104884, acc 0.9375\n",
      "2017-11-05T21:20:50.555578: step 1204, loss 0.127972, acc 0.96875\n",
      "2017-11-05T21:20:54.498003: step 1205, loss 0.50147, acc 0.875\n",
      "2017-11-05T21:20:58.407944: step 1206, loss 0.346386, acc 0.875\n",
      "2017-11-05T21:21:02.406868: step 1207, loss 0.165849, acc 0.9375\n",
      "2017-11-05T21:21:06.319093: step 1208, loss 0.113467, acc 0.9375\n",
      "2017-11-05T21:21:10.301478: step 1209, loss 0.130012, acc 0.9375\n",
      "2017-11-05T21:21:14.265103: step 1210, loss 0.14499, acc 0.875\n",
      "2017-11-05T21:21:18.188951: step 1211, loss 0.334467, acc 0.875\n",
      "2017-11-05T21:21:22.148997: step 1212, loss 0.321863, acc 0.875\n",
      "2017-11-05T21:21:26.110374: step 1213, loss 0.25907, acc 0.90625\n",
      "2017-11-05T21:21:30.013637: step 1214, loss 0.115734, acc 0.9375\n",
      "2017-11-05T21:21:33.992134: step 1215, loss 0.327025, acc 0.84375\n",
      "2017-11-05T21:21:37.898854: step 1216, loss 0.191474, acc 0.90625\n",
      "2017-11-05T21:21:41.811340: step 1217, loss 0.0387595, acc 1\n",
      "2017-11-05T21:21:45.802145: step 1218, loss 0.406575, acc 0.875\n",
      "2017-11-05T21:21:49.735620: step 1219, loss 0.26103, acc 0.90625\n",
      "2017-11-05T21:21:53.690875: step 1220, loss 0.421245, acc 0.8125\n",
      "2017-11-05T21:21:57.618878: step 1221, loss 0.109044, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T21:22:01.649322: step 1222, loss 0.102021, acc 0.96875\n",
      "2017-11-05T21:22:05.564772: step 1223, loss 0.249045, acc 0.875\n",
      "2017-11-05T21:22:08.090871: step 1224, loss 0.0608278, acc 1\n",
      "2017-11-05T21:22:12.020266: step 1225, loss 0.0415735, acc 0.96875\n",
      "2017-11-05T21:22:16.010589: step 1226, loss 0.169939, acc 0.96875\n",
      "2017-11-05T21:22:19.992850: step 1227, loss 0.0977236, acc 0.9375\n",
      "2017-11-05T21:22:23.938509: step 1228, loss 0.159356, acc 0.96875\n",
      "2017-11-05T21:22:27.907882: step 1229, loss 0.201275, acc 0.90625\n",
      "2017-11-05T21:22:31.870464: step 1230, loss 0.115129, acc 0.96875\n",
      "2017-11-05T21:22:35.991543: step 1231, loss 0.215391, acc 0.90625\n",
      "2017-11-05T21:22:39.892053: step 1232, loss 0.470115, acc 0.8125\n",
      "2017-11-05T21:22:43.816570: step 1233, loss 0.0827349, acc 0.96875\n",
      "2017-11-05T21:22:47.785126: step 1234, loss 0.367299, acc 0.84375\n",
      "2017-11-05T21:22:51.748901: step 1235, loss 0.121611, acc 0.9375\n",
      "2017-11-05T21:22:55.708441: step 1236, loss 0.171725, acc 0.9375\n",
      "2017-11-05T21:22:59.651395: step 1237, loss 0.358524, acc 0.84375\n",
      "2017-11-05T21:23:03.616664: step 1238, loss 0.181861, acc 0.9375\n",
      "2017-11-05T21:23:07.565034: step 1239, loss 0.170207, acc 0.90625\n",
      "2017-11-05T21:23:11.505180: step 1240, loss 0.0240543, acc 1\n",
      "2017-11-05T21:23:15.432059: step 1241, loss 0.301454, acc 0.84375\n",
      "2017-11-05T21:23:19.399209: step 1242, loss 0.0193753, acc 1\n",
      "2017-11-05T21:23:23.405563: step 1243, loss 0.178054, acc 0.9375\n",
      "2017-11-05T21:23:27.374963: step 1244, loss 0.149768, acc 0.90625\n",
      "2017-11-05T21:23:31.291090: step 1245, loss 0.402635, acc 0.84375\n",
      "2017-11-05T21:23:35.270984: step 1246, loss 0.244656, acc 0.90625\n",
      "2017-11-05T21:23:39.183348: step 1247, loss 0.101102, acc 0.96875\n",
      "2017-11-05T21:23:43.257319: step 1248, loss 0.146442, acc 0.9375\n",
      "2017-11-05T21:23:47.206686: step 1249, loss 0.0696954, acc 0.96875\n",
      "2017-11-05T21:23:51.171259: step 1250, loss 0.141678, acc 0.96875\n",
      "2017-11-05T21:23:55.036551: step 1251, loss 0.28899, acc 0.84375\n",
      "2017-11-05T21:23:58.992986: step 1252, loss 0.251043, acc 0.90625\n",
      "2017-11-05T21:24:02.963698: step 1253, loss 0.129347, acc 0.90625\n",
      "2017-11-05T21:24:06.967955: step 1254, loss 0.51003, acc 0.8125\n",
      "2017-11-05T21:24:10.989416: step 1255, loss 0.144749, acc 0.90625\n",
      "2017-11-05T21:24:14.948059: step 1256, loss 0.110289, acc 0.9375\n",
      "2017-11-05T21:24:18.914294: step 1257, loss 0.246612, acc 0.875\n",
      "2017-11-05T21:24:23.108626: step 1258, loss 0.330462, acc 0.90625\n",
      "2017-11-05T21:24:27.346431: step 1259, loss 0.236544, acc 0.90625\n",
      "2017-11-05T21:24:29.859726: step 1260, loss 0.341902, acc 0.85\n",
      "2017-11-05T21:24:33.896279: step 1261, loss 0.324728, acc 0.84375\n",
      "2017-11-05T21:24:37.890957: step 1262, loss 0.225674, acc 0.9375\n",
      "2017-11-05T21:24:41.915341: step 1263, loss 0.216512, acc 0.9375\n",
      "2017-11-05T21:24:45.952610: step 1264, loss 0.351637, acc 0.84375\n",
      "2017-11-05T21:24:49.933079: step 1265, loss 0.212211, acc 0.90625\n",
      "2017-11-05T21:24:53.883630: step 1266, loss 0.298536, acc 0.9375\n",
      "2017-11-05T21:24:57.895463: step 1267, loss 0.454282, acc 0.78125\n",
      "2017-11-05T21:25:01.883701: step 1268, loss 0.134391, acc 0.96875\n",
      "2017-11-05T21:25:05.856359: step 1269, loss 0.205886, acc 0.9375\n",
      "2017-11-05T21:25:09.843894: step 1270, loss 0.202458, acc 0.90625\n",
      "2017-11-05T21:25:13.786929: step 1271, loss 0.329872, acc 0.90625\n",
      "2017-11-05T21:25:17.766331: step 1272, loss 0.409152, acc 0.84375\n",
      "2017-11-05T21:25:21.758916: step 1273, loss 0.330376, acc 0.90625\n",
      "2017-11-05T21:25:25.707720: step 1274, loss 0.361098, acc 0.875\n",
      "2017-11-05T21:25:29.676984: step 1275, loss 0.128291, acc 0.90625\n",
      "2017-11-05T21:25:33.616165: step 1276, loss 0.104669, acc 0.9375\n",
      "2017-11-05T21:25:37.639781: step 1277, loss 0.286075, acc 0.90625\n",
      "2017-11-05T21:25:41.633117: step 1278, loss 0.0860584, acc 0.96875\n",
      "2017-11-05T21:25:45.582097: step 1279, loss 0.168913, acc 0.9375\n",
      "2017-11-05T21:25:49.613259: step 1280, loss 0.20753, acc 0.875\n",
      "2017-11-05T21:25:53.586530: step 1281, loss 0.23539, acc 0.875\n",
      "2017-11-05T21:25:57.535378: step 1282, loss 0.157022, acc 0.90625\n",
      "2017-11-05T21:26:01.627319: step 1283, loss 0.857358, acc 0.75\n",
      "2017-11-05T21:26:05.619293: step 1284, loss 0.180085, acc 0.90625\n",
      "2017-11-05T21:26:09.626472: step 1285, loss 0.167691, acc 0.90625\n",
      "2017-11-05T21:26:13.571035: step 1286, loss 0.103414, acc 0.9375\n",
      "2017-11-05T21:26:17.555976: step 1287, loss 0.126082, acc 0.96875\n",
      "2017-11-05T21:26:21.492237: step 1288, loss 0.160857, acc 0.90625\n",
      "2017-11-05T21:26:25.484977: step 1289, loss 0.417166, acc 0.78125\n",
      "2017-11-05T21:26:29.587557: step 1290, loss 0.191464, acc 0.90625\n",
      "2017-11-05T21:26:33.713517: step 1291, loss 0.261927, acc 0.90625\n",
      "2017-11-05T21:26:37.711505: step 1292, loss 0.175804, acc 0.90625\n",
      "2017-11-05T21:26:41.672497: step 1293, loss 0.236201, acc 0.875\n",
      "2017-11-05T21:26:45.635946: step 1294, loss 0.190161, acc 0.875\n",
      "2017-11-05T21:26:49.578028: step 1295, loss 0.251145, acc 0.90625\n",
      "2017-11-05T21:26:52.072126: step 1296, loss 0.142887, acc 0.95\n",
      "2017-11-05T21:26:56.104038: step 1297, loss 0.114327, acc 0.9375\n",
      "2017-11-05T21:27:00.084828: step 1298, loss 0.271956, acc 0.9375\n",
      "2017-11-05T21:27:04.054732: step 1299, loss 0.0297021, acc 1\n",
      "2017-11-05T21:27:08.020673: step 1300, loss 0.12244, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:27:10.557341: step 1300, loss 0.88928, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-05T21:27:16.335174: step 1301, loss 0.250648, acc 0.875\n",
      "2017-11-05T21:27:20.338114: step 1302, loss 0.329884, acc 0.90625\n",
      "2017-11-05T21:27:24.360670: step 1303, loss 0.0765527, acc 0.96875\n",
      "2017-11-05T21:27:28.371964: step 1304, loss 0.459134, acc 0.8125\n",
      "2017-11-05T21:27:32.351165: step 1305, loss 0.256155, acc 0.90625\n",
      "2017-11-05T21:27:36.348169: step 1306, loss 0.0672319, acc 0.96875\n",
      "2017-11-05T21:27:40.352730: step 1307, loss 0.254626, acc 0.90625\n",
      "2017-11-05T21:27:44.323594: step 1308, loss 0.122827, acc 0.96875\n",
      "2017-11-05T21:27:48.333705: step 1309, loss 0.227747, acc 0.875\n",
      "2017-11-05T21:27:52.247515: step 1310, loss 0.128169, acc 0.9375\n",
      "2017-11-05T21:27:56.205731: step 1311, loss 0.258661, acc 0.84375\n",
      "2017-11-05T21:28:00.168505: step 1312, loss 0.16538, acc 0.9375\n",
      "2017-11-05T21:28:04.089432: step 1313, loss 0.16033, acc 0.9375\n",
      "2017-11-05T21:28:08.093068: step 1314, loss 0.216128, acc 0.90625\n",
      "2017-11-05T21:28:12.072906: step 1315, loss 0.0856027, acc 0.96875\n",
      "2017-11-05T21:28:16.035090: step 1316, loss 0.349503, acc 0.84375\n",
      "2017-11-05T21:28:20.088468: step 1317, loss 0.110598, acc 0.90625\n",
      "2017-11-05T21:28:24.111977: step 1318, loss 0.0797793, acc 0.96875\n",
      "2017-11-05T21:28:28.175089: step 1319, loss 0.213498, acc 0.9375\n",
      "2017-11-05T21:28:32.163986: step 1320, loss 0.267241, acc 0.875\n",
      "2017-11-05T21:28:36.390008: step 1321, loss 0.350275, acc 0.875\n",
      "2017-11-05T21:28:40.357879: step 1322, loss 0.153145, acc 0.90625\n",
      "2017-11-05T21:28:44.330504: step 1323, loss 0.366803, acc 0.84375\n",
      "2017-11-05T21:28:48.342622: step 1324, loss 0.331018, acc 0.875\n",
      "2017-11-05T21:28:52.297337: step 1325, loss 0.0890965, acc 0.96875\n",
      "2017-11-05T21:28:56.279585: step 1326, loss 0.146125, acc 0.96875\n",
      "2017-11-05T21:29:00.251560: step 1327, loss 0.13745, acc 0.9375\n",
      "2017-11-05T21:29:04.214709: step 1328, loss 0.0391223, acc 1\n",
      "2017-11-05T21:29:08.229235: step 1329, loss 0.186816, acc 0.875\n",
      "2017-11-05T21:29:12.177219: step 1330, loss 0.420942, acc 0.84375\n",
      "2017-11-05T21:29:16.180333: step 1331, loss 0.0346357, acc 1\n",
      "2017-11-05T21:29:18.768194: step 1332, loss 0.258456, acc 0.85\n",
      "2017-11-05T21:29:22.948367: step 1333, loss 0.0692021, acc 0.96875\n",
      "2017-11-05T21:29:27.183211: step 1334, loss 0.0748202, acc 1\n",
      "2017-11-05T21:29:31.233468: step 1335, loss 0.105152, acc 0.96875\n",
      "2017-11-05T21:29:35.247758: step 1336, loss 0.122051, acc 0.9375\n",
      "2017-11-05T21:29:39.304844: step 1337, loss 0.1903, acc 0.90625\n",
      "2017-11-05T21:29:43.360695: step 1338, loss 0.405814, acc 0.84375\n",
      "2017-11-05T21:29:47.478211: step 1339, loss 0.0838075, acc 0.96875\n",
      "2017-11-05T21:29:51.553403: step 1340, loss 0.150029, acc 0.9375\n",
      "2017-11-05T21:29:55.625018: step 1341, loss 0.187675, acc 0.9375\n",
      "2017-11-05T21:29:59.618735: step 1342, loss 0.134644, acc 0.9375\n",
      "2017-11-05T21:30:03.900705: step 1343, loss 0.214006, acc 0.90625\n",
      "2017-11-05T21:30:07.920234: step 1344, loss 0.115699, acc 0.9375\n",
      "2017-11-05T21:30:11.976932: step 1345, loss 0.264841, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T21:30:15.984475: step 1346, loss 0.285156, acc 0.875\n",
      "2017-11-05T21:30:20.030317: step 1347, loss 0.192318, acc 0.90625\n",
      "2017-11-05T21:30:24.032019: step 1348, loss 0.0962883, acc 0.96875\n",
      "2017-11-05T21:30:27.996187: step 1349, loss 0.186963, acc 0.90625\n",
      "2017-11-05T21:30:31.989055: step 1350, loss 0.098689, acc 0.96875\n",
      "2017-11-05T21:30:36.170788: step 1351, loss 0.226133, acc 0.90625\n",
      "2017-11-05T21:30:40.179751: step 1352, loss 0.297383, acc 0.9375\n",
      "2017-11-05T21:30:44.113884: step 1353, loss 0.096946, acc 0.96875\n",
      "2017-11-05T21:30:48.089728: step 1354, loss 0.05197, acc 0.96875\n",
      "2017-11-05T21:30:52.060976: step 1355, loss 0.297631, acc 0.875\n",
      "2017-11-05T21:30:56.074326: step 1356, loss 0.122212, acc 0.9375\n",
      "2017-11-05T21:30:59.998549: step 1357, loss 0.245463, acc 0.875\n",
      "2017-11-05T21:31:04.002387: step 1358, loss 0.117182, acc 0.96875\n",
      "2017-11-05T21:31:07.990109: step 1359, loss 0.206187, acc 0.875\n",
      "2017-11-05T21:31:12.078628: step 1360, loss 0.0792367, acc 0.9375\n",
      "2017-11-05T21:31:16.023497: step 1361, loss 0.339274, acc 0.84375\n",
      "2017-11-05T21:31:20.006257: step 1362, loss 0.0638986, acc 0.96875\n",
      "2017-11-05T21:31:23.953195: step 1363, loss 0.252093, acc 0.875\n",
      "2017-11-05T21:31:27.900769: step 1364, loss 0.187421, acc 0.90625\n",
      "2017-11-05T21:31:31.867103: step 1365, loss 0.248351, acc 0.875\n",
      "2017-11-05T21:31:35.856124: step 1366, loss 0.356995, acc 0.875\n",
      "2017-11-05T21:31:39.818361: step 1367, loss 0.084336, acc 0.96875\n",
      "2017-11-05T21:31:42.354851: step 1368, loss 0.244221, acc 0.85\n",
      "2017-11-05T21:31:46.335808: step 1369, loss 0.179543, acc 0.90625\n",
      "2017-11-05T21:31:50.307049: step 1370, loss 0.0622528, acc 0.96875\n",
      "2017-11-05T21:31:54.241375: step 1371, loss 0.268788, acc 0.875\n",
      "2017-11-05T21:31:58.211482: step 1372, loss 0.196991, acc 0.90625\n",
      "2017-11-05T21:32:02.211293: step 1373, loss 0.175847, acc 0.9375\n",
      "2017-11-05T21:32:06.173367: step 1374, loss 0.197411, acc 0.9375\n",
      "2017-11-05T21:32:10.149224: step 1375, loss 0.35819, acc 0.8125\n",
      "2017-11-05T21:32:14.227093: step 1376, loss 0.14915, acc 0.9375\n",
      "2017-11-05T21:32:18.140600: step 1377, loss 0.167269, acc 0.875\n",
      "2017-11-05T21:32:22.101372: step 1378, loss 0.112314, acc 0.9375\n",
      "2017-11-05T21:32:26.127133: step 1379, loss 0.415848, acc 0.84375\n",
      "2017-11-05T21:32:30.103287: step 1380, loss 0.117192, acc 0.90625\n",
      "2017-11-05T21:32:34.214411: step 1381, loss 0.248381, acc 0.875\n",
      "2017-11-05T21:32:38.313722: step 1382, loss 0.492709, acc 0.875\n",
      "2017-11-05T21:32:42.330452: step 1383, loss 0.211421, acc 0.90625\n",
      "2017-11-05T21:32:46.288834: step 1384, loss 0.204056, acc 0.9375\n",
      "2017-11-05T21:32:50.268380: step 1385, loss 0.166369, acc 0.9375\n",
      "2017-11-05T21:32:54.294512: step 1386, loss 0.0637135, acc 0.96875\n",
      "2017-11-05T21:32:58.264336: step 1387, loss 0.0538026, acc 0.96875\n",
      "2017-11-05T21:33:02.283226: step 1388, loss 0.0831013, acc 0.96875\n",
      "2017-11-05T21:33:06.219887: step 1389, loss 0.317589, acc 0.875\n",
      "2017-11-05T21:33:10.194300: step 1390, loss 0.25671, acc 0.875\n",
      "2017-11-05T21:33:14.144038: step 1391, loss 0.211269, acc 0.90625\n",
      "2017-11-05T21:33:18.091346: step 1392, loss 0.147004, acc 0.96875\n",
      "2017-11-05T21:33:22.162848: step 1393, loss 0.0448062, acc 0.96875\n",
      "2017-11-05T21:33:26.302074: step 1394, loss 0.117047, acc 0.9375\n",
      "2017-11-05T21:33:30.235740: step 1395, loss 0.232819, acc 0.90625\n",
      "2017-11-05T21:33:34.196610: step 1396, loss 0.182577, acc 0.90625\n",
      "2017-11-05T21:33:38.225929: step 1397, loss 0.0167066, acc 1\n",
      "2017-11-05T21:33:42.142490: step 1398, loss 0.156715, acc 0.96875\n",
      "2017-11-05T21:33:46.267544: step 1399, loss 0.198234, acc 0.9375\n",
      "2017-11-05T21:33:50.208591: step 1400, loss 0.0680598, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:33:52.760334: step 1400, loss 1.17726, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-05T21:33:58.141430: step 1401, loss 0.67237, acc 0.84375\n",
      "2017-11-05T21:34:02.162552: step 1402, loss 0.0343959, acc 1\n",
      "2017-11-05T21:34:06.084946: step 1403, loss 0.291103, acc 0.90625\n",
      "2017-11-05T21:34:08.631950: step 1404, loss 0.446878, acc 0.85\n",
      "2017-11-05T21:34:12.617322: step 1405, loss 0.153715, acc 0.9375\n",
      "2017-11-05T21:34:16.592143: step 1406, loss 0.273473, acc 0.90625\n",
      "2017-11-05T21:34:20.548975: step 1407, loss 0.118235, acc 0.96875\n",
      "2017-11-05T21:34:24.524948: step 1408, loss 0.201622, acc 0.9375\n",
      "2017-11-05T21:34:28.476416: step 1409, loss 0.149467, acc 0.9375\n",
      "2017-11-05T21:34:32.568959: step 1410, loss 0.167167, acc 0.90625\n",
      "2017-11-05T21:34:36.594343: step 1411, loss 0.349946, acc 0.8125\n",
      "2017-11-05T21:34:40.504979: step 1412, loss 0.105964, acc 0.96875\n",
      "2017-11-05T21:34:44.486589: step 1413, loss 0.240509, acc 0.90625\n",
      "2017-11-05T21:34:48.463758: step 1414, loss 0.0795844, acc 0.96875\n",
      "2017-11-05T21:34:52.392900: step 1415, loss 0.0811559, acc 0.96875\n",
      "2017-11-05T21:34:56.367088: step 1416, loss 0.194621, acc 0.875\n",
      "2017-11-05T21:35:00.340944: step 1417, loss 0.390837, acc 0.84375\n",
      "2017-11-05T21:35:04.269874: step 1418, loss 0.418049, acc 0.875\n",
      "2017-11-05T21:35:08.281557: step 1419, loss 0.23444, acc 0.875\n",
      "2017-11-05T21:35:12.241943: step 1420, loss 0.131574, acc 0.9375\n",
      "2017-11-05T21:35:16.210370: step 1421, loss 0.0631641, acc 0.96875\n",
      "2017-11-05T21:35:20.179733: step 1422, loss 0.12321, acc 0.90625\n",
      "2017-11-05T21:35:24.058914: step 1423, loss 0.309325, acc 0.875\n",
      "2017-11-05T21:35:27.963908: step 1424, loss 0.280527, acc 0.875\n",
      "2017-11-05T21:35:31.934541: step 1425, loss 0.196372, acc 0.875\n",
      "2017-11-05T21:35:35.874987: step 1426, loss 0.0848899, acc 0.96875\n",
      "2017-11-05T21:35:39.874412: step 1427, loss 0.328105, acc 0.84375\n",
      "2017-11-05T21:35:43.837031: step 1428, loss 0.184164, acc 0.9375\n",
      "2017-11-05T21:35:47.812863: step 1429, loss 0.163048, acc 0.90625\n",
      "2017-11-05T21:35:51.795569: step 1430, loss 0.290827, acc 0.875\n",
      "2017-11-05T21:35:55.737941: step 1431, loss 0.0406807, acc 0.96875\n",
      "2017-11-05T21:35:59.730368: step 1432, loss 0.232235, acc 0.9375\n",
      "2017-11-05T21:36:03.725290: step 1433, loss 0.0897217, acc 0.96875\n",
      "2017-11-05T21:36:07.695014: step 1434, loss 0.761081, acc 0.8125\n",
      "2017-11-05T21:36:11.664071: step 1435, loss 0.344245, acc 0.875\n",
      "2017-11-05T21:36:15.612263: step 1436, loss 0.344625, acc 0.9375\n",
      "2017-11-05T21:36:19.618981: step 1437, loss 0.0180001, acc 1\n",
      "2017-11-05T21:36:23.530347: step 1438, loss 0.398761, acc 0.875\n",
      "2017-11-05T21:36:27.427502: step 1439, loss 0.381057, acc 0.84375\n",
      "2017-11-05T21:36:29.936738: step 1440, loss 0.533071, acc 0.85\n",
      "2017-11-05T21:36:34.096642: step 1441, loss 0.0612504, acc 0.96875\n",
      "2017-11-05T21:36:38.073540: step 1442, loss 0.0376538, acc 1\n",
      "2017-11-05T21:36:42.027599: step 1443, loss 0.183111, acc 0.90625\n",
      "2017-11-05T21:36:45.939707: step 1444, loss 0.139344, acc 0.90625\n",
      "2017-11-05T21:36:49.858455: step 1445, loss 0.23939, acc 0.875\n",
      "2017-11-05T21:36:53.833442: step 1446, loss 0.17189, acc 0.9375\n",
      "2017-11-05T21:36:57.852277: step 1447, loss 0.140741, acc 0.96875\n",
      "2017-11-05T21:37:01.812628: step 1448, loss 0.211105, acc 0.90625\n",
      "2017-11-05T21:37:05.733121: step 1449, loss 0.24308, acc 0.90625\n",
      "2017-11-05T21:37:09.692498: step 1450, loss 0.313275, acc 0.84375\n",
      "2017-11-05T21:37:13.695411: step 1451, loss 0.272175, acc 0.84375\n",
      "2017-11-05T21:37:17.673610: step 1452, loss 0.163306, acc 0.90625\n",
      "2017-11-05T21:37:21.580596: step 1453, loss 0.281831, acc 0.84375\n",
      "2017-11-05T21:37:25.542429: step 1454, loss 0.296781, acc 0.875\n",
      "2017-11-05T21:37:29.460374: step 1455, loss 0.210718, acc 0.90625\n",
      "2017-11-05T21:37:33.390445: step 1456, loss 0.33512, acc 0.84375\n",
      "2017-11-05T21:37:37.386236: step 1457, loss 0.193831, acc 0.875\n",
      "2017-11-05T21:37:41.343996: step 1458, loss 0.162341, acc 0.90625\n",
      "2017-11-05T21:37:45.338660: step 1459, loss 0.107617, acc 0.96875\n",
      "2017-11-05T21:37:49.307033: step 1460, loss 0.0740987, acc 0.96875\n",
      "2017-11-05T21:37:53.310142: step 1461, loss 0.397165, acc 0.84375\n",
      "2017-11-05T21:37:57.280734: step 1462, loss 0.453822, acc 0.84375\n",
      "2017-11-05T21:38:01.221345: step 1463, loss 0.0834551, acc 0.96875\n",
      "2017-11-05T21:38:05.166738: step 1464, loss 0.290618, acc 0.875\n",
      "2017-11-05T21:38:09.136130: step 1465, loss 0.163097, acc 0.9375\n",
      "2017-11-05T21:38:13.153440: step 1466, loss 0.0813152, acc 0.9375\n",
      "2017-11-05T21:38:17.175937: step 1467, loss 0.110352, acc 0.9375\n",
      "2017-11-05T21:38:21.280680: step 1468, loss 0.183415, acc 0.96875\n",
      "2017-11-05T21:38:25.525650: step 1469, loss 0.283033, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T21:38:29.782904: step 1470, loss 0.175093, acc 0.9375\n",
      "2017-11-05T21:38:33.870799: step 1471, loss 0.181293, acc 0.90625\n",
      "2017-11-05T21:38:37.988456: step 1472, loss 0.351919, acc 0.84375\n",
      "2017-11-05T21:38:42.108174: step 1473, loss 0.202387, acc 0.90625\n",
      "2017-11-05T21:38:46.145486: step 1474, loss 0.420632, acc 0.84375\n",
      "2017-11-05T21:38:50.277664: step 1475, loss 0.104846, acc 1\n",
      "2017-11-05T21:38:52.868982: step 1476, loss 0.123366, acc 0.95\n",
      "2017-11-05T21:38:56.946086: step 1477, loss 0.0890565, acc 0.9375\n",
      "2017-11-05T21:39:00.936284: step 1478, loss 0.400272, acc 0.78125\n",
      "2017-11-05T21:39:04.819993: step 1479, loss 0.0342662, acc 1\n",
      "2017-11-05T21:39:08.794804: step 1480, loss 0.234915, acc 0.90625\n",
      "2017-11-05T21:39:12.801454: step 1481, loss 0.225521, acc 0.9375\n",
      "2017-11-05T21:39:16.755535: step 1482, loss 0.28897, acc 0.875\n",
      "2017-11-05T21:39:20.695226: step 1483, loss 0.253948, acc 0.90625\n",
      "2017-11-05T21:39:24.631893: step 1484, loss 0.24535, acc 0.9375\n",
      "2017-11-05T21:39:28.585808: step 1485, loss 0.178256, acc 0.9375\n",
      "2017-11-05T21:39:32.632560: step 1486, loss 0.325599, acc 0.84375\n",
      "2017-11-05T21:39:36.660913: step 1487, loss 0.1385, acc 0.9375\n",
      "2017-11-05T21:39:40.609877: step 1488, loss 0.228433, acc 0.84375\n",
      "2017-11-05T21:39:44.584497: step 1489, loss 0.0854604, acc 1\n",
      "2017-11-05T21:39:48.514479: step 1490, loss 0.200183, acc 0.90625\n",
      "2017-11-05T21:39:52.481002: step 1491, loss 0.240002, acc 0.90625\n",
      "2017-11-05T21:39:56.470329: step 1492, loss 0.210546, acc 0.90625\n",
      "2017-11-05T21:40:00.477297: step 1493, loss 0.252872, acc 0.875\n",
      "2017-11-05T21:40:04.604259: step 1494, loss 0.242181, acc 0.90625\n",
      "2017-11-05T21:40:08.604438: step 1495, loss 0.051985, acc 0.96875\n",
      "2017-11-05T21:40:12.695413: step 1496, loss 0.0827795, acc 0.9375\n",
      "2017-11-05T21:40:16.752311: step 1497, loss 0.263341, acc 0.90625\n",
      "2017-11-05T21:40:20.738245: step 1498, loss 0.206779, acc 0.9375\n",
      "2017-11-05T21:40:24.734040: step 1499, loss 0.175466, acc 0.9375\n",
      "2017-11-05T21:40:28.738857: step 1500, loss 0.251154, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:40:31.291075: step 1500, loss 0.793531, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-05T21:40:36.662781: step 1501, loss 0.106183, acc 0.9375\n",
      "2017-11-05T21:40:40.608165: step 1502, loss 0.17413, acc 0.9375\n",
      "2017-11-05T21:40:44.544920: step 1503, loss 0.191256, acc 0.90625\n",
      "2017-11-05T21:40:48.508305: step 1504, loss 0.469415, acc 0.71875\n",
      "2017-11-05T21:40:52.407900: step 1505, loss 0.11113, acc 0.96875\n",
      "2017-11-05T21:40:56.366911: step 1506, loss 0.262818, acc 0.875\n",
      "2017-11-05T21:41:00.265070: step 1507, loss 0.312976, acc 0.875\n",
      "2017-11-05T21:41:04.167793: step 1508, loss 0.165486, acc 0.9375\n",
      "2017-11-05T21:41:08.116357: step 1509, loss 0.0408528, acc 1\n",
      "2017-11-05T21:41:12.880256: step 1510, loss 0.183292, acc 0.90625\n",
      "2017-11-05T21:41:17.345902: step 1511, loss 0.0922424, acc 0.9375\n",
      "2017-11-05T21:41:19.848456: step 1512, loss 0.274421, acc 0.85\n",
      "2017-11-05T21:41:23.905178: step 1513, loss 0.0952365, acc 0.9375\n",
      "2017-11-05T21:41:27.851223: step 1514, loss 0.184662, acc 0.875\n",
      "2017-11-05T21:41:31.779139: step 1515, loss 0.219078, acc 0.84375\n",
      "2017-11-05T21:41:35.719163: step 1516, loss 0.227596, acc 0.875\n",
      "2017-11-05T21:41:39.647581: step 1517, loss 0.280545, acc 0.875\n",
      "2017-11-05T21:41:43.538734: step 1518, loss 0.121677, acc 0.9375\n",
      "2017-11-05T21:41:47.542208: step 1519, loss 0.170303, acc 0.9375\n",
      "2017-11-05T21:41:51.506702: step 1520, loss 0.24265, acc 0.90625\n",
      "2017-11-05T21:41:55.486670: step 1521, loss 0.210981, acc 0.90625\n",
      "2017-11-05T21:41:59.418973: step 1522, loss 0.117236, acc 0.9375\n",
      "2017-11-05T21:42:03.348495: step 1523, loss 0.115081, acc 0.9375\n",
      "2017-11-05T21:42:07.310455: step 1524, loss 0.25137, acc 0.875\n",
      "2017-11-05T21:42:11.267805: step 1525, loss 0.21369, acc 0.875\n",
      "2017-11-05T21:42:15.265374: step 1526, loss 0.501595, acc 0.84375\n",
      "2017-11-05T21:42:19.179426: step 1527, loss 0.291956, acc 0.84375\n",
      "2017-11-05T21:42:23.108616: step 1528, loss 0.131808, acc 0.9375\n",
      "2017-11-05T21:42:27.102836: step 1529, loss 0.118323, acc 0.9375\n",
      "2017-11-05T21:42:30.999894: step 1530, loss 0.117831, acc 0.9375\n",
      "2017-11-05T21:42:35.121302: step 1531, loss 0.0637586, acc 0.96875\n",
      "2017-11-05T21:42:39.035958: step 1532, loss 0.361204, acc 0.8125\n",
      "2017-11-05T21:42:42.973254: step 1533, loss 0.140299, acc 0.90625\n",
      "2017-11-05T21:42:46.912955: step 1534, loss 0.175614, acc 0.96875\n",
      "2017-11-05T21:42:50.880407: step 1535, loss 0.133344, acc 0.90625\n",
      "2017-11-05T21:42:54.898066: step 1536, loss 0.776711, acc 0.8125\n",
      "2017-11-05T21:42:58.807047: step 1537, loss 0.189788, acc 0.9375\n",
      "2017-11-05T21:43:02.723116: step 1538, loss 0.0708106, acc 0.96875\n",
      "2017-11-05T21:43:06.646915: step 1539, loss 0.392681, acc 0.84375\n",
      "2017-11-05T21:43:10.583338: step 1540, loss 0.106556, acc 0.96875\n",
      "2017-11-05T21:43:14.523933: step 1541, loss 0.11555, acc 0.9375\n",
      "2017-11-05T21:43:18.467570: step 1542, loss 0.154496, acc 0.9375\n",
      "2017-11-05T21:43:22.557581: step 1543, loss 0.220671, acc 0.90625\n",
      "2017-11-05T21:43:26.679687: step 1544, loss 0.157191, acc 0.9375\n",
      "2017-11-05T21:43:30.607266: step 1545, loss 0.230227, acc 0.9375\n",
      "2017-11-05T21:43:34.571013: step 1546, loss 0.348896, acc 0.875\n",
      "2017-11-05T21:43:38.497429: step 1547, loss 0.245179, acc 0.84375\n",
      "2017-11-05T21:43:41.026719: step 1548, loss 0.00681138, acc 1\n",
      "2017-11-05T21:43:44.913035: step 1549, loss 0.177569, acc 0.9375\n",
      "2017-11-05T21:43:48.980028: step 1550, loss 0.458362, acc 0.84375\n",
      "2017-11-05T21:43:52.862007: step 1551, loss 0.112924, acc 0.9375\n",
      "2017-11-05T21:43:56.810624: step 1552, loss 0.109745, acc 0.96875\n",
      "2017-11-05T21:44:00.759497: step 1553, loss 0.408125, acc 0.875\n",
      "2017-11-05T21:44:04.688931: step 1554, loss 0.327835, acc 0.8125\n",
      "2017-11-05T21:44:08.611181: step 1555, loss 0.117326, acc 0.96875\n",
      "2017-11-05T21:44:12.517607: step 1556, loss 0.233085, acc 0.84375\n",
      "2017-11-05T21:44:16.478109: step 1557, loss 0.178829, acc 0.90625\n",
      "2017-11-05T21:44:20.425353: step 1558, loss 0.273999, acc 0.84375\n",
      "2017-11-05T21:44:24.300680: step 1559, loss 0.104663, acc 0.9375\n",
      "2017-11-05T21:44:28.257372: step 1560, loss 0.389241, acc 0.875\n",
      "2017-11-05T21:44:32.158632: step 1561, loss 0.267739, acc 0.90625\n",
      "2017-11-05T21:44:36.265212: step 1562, loss 0.237411, acc 0.875\n",
      "2017-11-05T21:44:40.206726: step 1563, loss 0.208013, acc 0.875\n",
      "2017-11-05T21:44:44.122205: step 1564, loss 0.096614, acc 0.96875\n",
      "2017-11-05T21:44:48.053472: step 1565, loss 0.0917002, acc 0.90625\n",
      "2017-11-05T21:44:52.024912: step 1566, loss 0.181235, acc 0.90625\n",
      "2017-11-05T21:44:55.942454: step 1567, loss 0.447955, acc 0.875\n",
      "2017-11-05T21:44:59.874068: step 1568, loss 0.16368, acc 0.90625\n",
      "2017-11-05T21:45:03.843415: step 1569, loss 0.28095, acc 0.875\n",
      "2017-11-05T21:45:07.812896: step 1570, loss 0.037973, acc 0.96875\n",
      "2017-11-05T21:45:11.776369: step 1571, loss 0.208218, acc 0.9375\n",
      "2017-11-05T21:45:15.782588: step 1572, loss 0.0885791, acc 0.96875\n",
      "2017-11-05T21:45:19.780671: step 1573, loss 0.410361, acc 0.84375\n",
      "2017-11-05T21:45:23.713785: step 1574, loss 0.17697, acc 0.9375\n",
      "2017-11-05T21:45:27.674851: step 1575, loss 0.219903, acc 0.875\n",
      "2017-11-05T21:45:31.589560: step 1576, loss 0.263256, acc 0.84375\n",
      "2017-11-05T21:45:35.574199: step 1577, loss 0.134579, acc 0.9375\n",
      "2017-11-05T21:45:39.517673: step 1578, loss 0.203515, acc 0.9375\n",
      "2017-11-05T21:45:43.446710: step 1579, loss 0.37511, acc 0.8125\n",
      "2017-11-05T21:45:47.326374: step 1580, loss 0.0948127, acc 0.96875\n",
      "2017-11-05T21:45:51.284499: step 1581, loss 0.216916, acc 0.90625\n",
      "2017-11-05T21:45:55.213627: step 1582, loss 0.0761123, acc 0.96875\n",
      "2017-11-05T21:45:59.170398: step 1583, loss 0.124019, acc 0.96875\n",
      "2017-11-05T21:46:01.706064: step 1584, loss 0.195796, acc 0.95\n",
      "2017-11-05T21:46:05.683272: step 1585, loss 0.218164, acc 0.875\n",
      "2017-11-05T21:46:09.671091: step 1586, loss 0.235634, acc 0.875\n",
      "2017-11-05T21:46:13.577603: step 1587, loss 0.0986554, acc 0.9375\n",
      "2017-11-05T21:46:17.511699: step 1588, loss 0.179957, acc 0.90625\n",
      "2017-11-05T21:46:21.456085: step 1589, loss 0.128763, acc 0.90625\n",
      "2017-11-05T21:46:25.345288: step 1590, loss 0.233925, acc 0.90625\n",
      "2017-11-05T21:46:29.317958: step 1591, loss 0.3867, acc 0.90625\n",
      "2017-11-05T21:46:33.353660: step 1592, loss 0.216204, acc 0.875\n",
      "2017-11-05T21:46:37.557862: step 1593, loss 0.0882423, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T21:46:41.597825: step 1594, loss 0.0458157, acc 1\n",
      "2017-11-05T21:46:45.533393: step 1595, loss 0.176998, acc 0.9375\n",
      "2017-11-05T21:46:49.447712: step 1596, loss 0.27302, acc 0.90625\n",
      "2017-11-05T21:46:53.414384: step 1597, loss 0.263814, acc 0.875\n",
      "2017-11-05T21:46:57.352508: step 1598, loss 0.270731, acc 0.90625\n",
      "2017-11-05T21:47:01.270060: step 1599, loss 0.240502, acc 0.84375\n",
      "2017-11-05T21:47:05.230572: step 1600, loss 0.107366, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:47:07.769641: step 1600, loss 0.814925, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-05T21:47:13.133526: step 1601, loss 0.0970199, acc 0.96875\n",
      "2017-11-05T21:47:17.115669: step 1602, loss 0.254821, acc 0.875\n",
      "2017-11-05T21:47:21.051288: step 1603, loss 0.0725315, acc 1\n",
      "2017-11-05T21:47:25.028172: step 1604, loss 0.180415, acc 0.9375\n",
      "2017-11-05T21:47:29.017708: step 1605, loss 0.362633, acc 0.90625\n",
      "2017-11-05T21:47:32.932915: step 1606, loss 0.167796, acc 0.9375\n",
      "2017-11-05T21:47:36.929285: step 1607, loss 0.171499, acc 0.84375\n",
      "2017-11-05T21:47:40.900481: step 1608, loss 0.0840452, acc 0.96875\n",
      "2017-11-05T21:47:44.823523: step 1609, loss 0.194069, acc 0.90625\n",
      "2017-11-05T21:47:48.827985: step 1610, loss 0.289396, acc 0.875\n",
      "2017-11-05T21:47:52.862139: step 1611, loss 0.127655, acc 0.90625\n",
      "2017-11-05T21:47:56.840857: step 1612, loss 0.13886, acc 0.9375\n",
      "2017-11-05T21:48:00.821194: step 1613, loss 0.162618, acc 0.96875\n",
      "2017-11-05T21:48:04.773881: step 1614, loss 0.207126, acc 0.90625\n",
      "2017-11-05T21:48:08.704392: step 1615, loss 0.463583, acc 0.84375\n",
      "2017-11-05T21:48:12.625512: step 1616, loss 0.214185, acc 0.9375\n",
      "2017-11-05T21:48:16.626618: step 1617, loss 0.196226, acc 0.90625\n",
      "2017-11-05T21:48:20.589943: step 1618, loss 0.186444, acc 0.9375\n",
      "2017-11-05T21:48:24.632349: step 1619, loss 0.278836, acc 0.875\n",
      "2017-11-05T21:48:27.367929: step 1620, loss 0.109878, acc 0.95\n",
      "2017-11-05T21:48:31.342533: step 1621, loss 0.10375, acc 0.9375\n",
      "2017-11-05T21:48:35.495688: step 1622, loss 0.378131, acc 0.875\n",
      "2017-11-05T21:48:39.629958: step 1623, loss 0.14806, acc 0.90625\n",
      "2017-11-05T21:48:43.685334: step 1624, loss 0.339978, acc 0.875\n",
      "2017-11-05T21:48:47.821928: step 1625, loss 0.0514065, acc 1\n",
      "2017-11-05T21:48:51.829285: step 1626, loss 0.415693, acc 0.84375\n",
      "2017-11-05T21:48:55.996060: step 1627, loss 0.179251, acc 0.90625\n",
      "2017-11-05T21:49:00.114872: step 1628, loss 0.22214, acc 0.9375\n",
      "2017-11-05T21:49:04.314562: step 1629, loss 0.266198, acc 0.875\n",
      "2017-11-05T21:49:08.281357: step 1630, loss 0.236728, acc 0.90625\n",
      "2017-11-05T21:49:12.458504: step 1631, loss 0.434045, acc 0.8125\n",
      "2017-11-05T21:49:16.483195: step 1632, loss 0.145438, acc 0.9375\n",
      "2017-11-05T21:49:20.680710: step 1633, loss 0.109277, acc 0.96875\n",
      "2017-11-05T21:49:24.678207: step 1634, loss 0.277127, acc 0.875\n",
      "2017-11-05T21:49:28.772961: step 1635, loss 0.0982863, acc 0.96875\n",
      "2017-11-05T21:49:32.845077: step 1636, loss 0.276679, acc 0.84375\n",
      "2017-11-05T21:49:36.980353: step 1637, loss 0.0443814, acc 1\n",
      "2017-11-05T21:49:41.141569: step 1638, loss 0.166373, acc 0.90625\n",
      "2017-11-05T21:49:45.062181: step 1639, loss 0.113461, acc 0.9375\n",
      "2017-11-05T21:49:49.057589: step 1640, loss 0.0416318, acc 1\n",
      "2017-11-05T21:49:52.981107: step 1641, loss 0.246947, acc 0.90625\n",
      "2017-11-05T21:49:56.933949: step 1642, loss 0.228349, acc 0.9375\n",
      "2017-11-05T21:50:01.105078: step 1643, loss 0.16272, acc 0.875\n",
      "2017-11-05T21:50:05.087480: step 1644, loss 0.10055, acc 0.90625\n",
      "2017-11-05T21:50:09.023934: step 1645, loss 0.113185, acc 0.96875\n",
      "2017-11-05T21:50:12.947344: step 1646, loss 0.188163, acc 0.9375\n",
      "2017-11-05T21:50:16.862600: step 1647, loss 0.295803, acc 0.84375\n",
      "2017-11-05T21:50:20.890551: step 1648, loss 0.448484, acc 0.8125\n",
      "2017-11-05T21:50:24.897528: step 1649, loss 0.378583, acc 0.84375\n",
      "2017-11-05T21:50:28.929808: step 1650, loss 0.233347, acc 0.84375\n",
      "2017-11-05T21:50:33.011442: step 1651, loss 0.0558942, acc 1\n",
      "2017-11-05T21:50:37.141444: step 1652, loss 0.187225, acc 0.9375\n",
      "2017-11-05T21:50:41.080264: step 1653, loss 0.381609, acc 0.8125\n",
      "2017-11-05T21:50:45.034006: step 1654, loss 0.237151, acc 0.9375\n",
      "2017-11-05T21:50:48.984210: step 1655, loss 0.308385, acc 0.90625\n",
      "2017-11-05T21:50:51.478903: step 1656, loss 0.283543, acc 0.85\n",
      "2017-11-05T21:50:55.442937: step 1657, loss 0.132421, acc 0.96875\n",
      "2017-11-05T21:50:59.437978: step 1658, loss 0.155518, acc 0.90625\n",
      "2017-11-05T21:51:03.406449: step 1659, loss 0.167261, acc 0.96875\n",
      "2017-11-05T21:51:07.366750: step 1660, loss 0.0739545, acc 0.96875\n",
      "2017-11-05T21:51:11.333505: step 1661, loss 0.152693, acc 0.9375\n",
      "2017-11-05T21:51:15.342005: step 1662, loss 0.18725, acc 0.90625\n",
      "2017-11-05T21:51:19.347101: step 1663, loss 0.0749946, acc 0.96875\n",
      "2017-11-05T21:51:23.286496: step 1664, loss 0.0919534, acc 0.96875\n",
      "2017-11-05T21:51:27.194288: step 1665, loss 0.153961, acc 0.90625\n",
      "2017-11-05T21:51:31.166911: step 1666, loss 0.256908, acc 0.90625\n",
      "2017-11-05T21:51:35.169146: step 1667, loss 0.151669, acc 0.875\n",
      "2017-11-05T21:51:39.204132: step 1668, loss 0.104642, acc 0.96875\n",
      "2017-11-05T21:51:43.164222: step 1669, loss 0.276655, acc 0.90625\n",
      "2017-11-05T21:51:47.087047: step 1670, loss 0.10994, acc 0.90625\n",
      "2017-11-05T21:51:51.071024: step 1671, loss 0.119661, acc 0.9375\n",
      "2017-11-05T21:51:54.999057: step 1672, loss 0.239205, acc 0.875\n",
      "2017-11-05T21:51:58.979415: step 1673, loss 0.233643, acc 0.9375\n",
      "2017-11-05T21:52:02.943103: step 1674, loss 0.327552, acc 0.8125\n",
      "2017-11-05T21:52:06.915235: step 1675, loss 0.102742, acc 0.96875\n",
      "2017-11-05T21:52:10.895649: step 1676, loss 0.217957, acc 0.90625\n",
      "2017-11-05T21:52:14.880738: step 1677, loss 0.102319, acc 0.96875\n",
      "2017-11-05T21:52:18.841785: step 1678, loss 0.0736284, acc 0.96875\n",
      "2017-11-05T21:52:22.774411: step 1679, loss 0.453834, acc 0.8125\n",
      "2017-11-05T21:52:26.752483: step 1680, loss 0.148132, acc 0.9375\n",
      "2017-11-05T21:52:30.696730: step 1681, loss 0.16488, acc 0.90625\n",
      "2017-11-05T21:52:34.787011: step 1682, loss 0.309028, acc 0.875\n",
      "2017-11-05T21:52:38.794368: step 1683, loss 0.284204, acc 0.875\n",
      "2017-11-05T21:52:42.772254: step 1684, loss 0.223798, acc 0.875\n",
      "2017-11-05T21:52:46.696997: step 1685, loss 0.229011, acc 0.9375\n",
      "2017-11-05T21:52:50.644609: step 1686, loss 0.308487, acc 0.875\n",
      "2017-11-05T21:52:54.583279: step 1687, loss 0.18992, acc 0.90625\n",
      "2017-11-05T21:52:58.532640: step 1688, loss 0.380472, acc 0.84375\n",
      "2017-11-05T21:53:02.536871: step 1689, loss 0.335155, acc 0.875\n",
      "2017-11-05T21:53:06.472449: step 1690, loss 0.0737414, acc 0.96875\n",
      "2017-11-05T21:53:10.385161: step 1691, loss 0.345657, acc 0.875\n",
      "2017-11-05T21:53:12.928095: step 1692, loss 0.205938, acc 0.9\n",
      "2017-11-05T21:53:16.877187: step 1693, loss 0.209601, acc 0.90625\n",
      "2017-11-05T21:53:20.884678: step 1694, loss 0.121596, acc 0.90625\n",
      "2017-11-05T21:53:25.009774: step 1695, loss 0.200163, acc 0.875\n",
      "2017-11-05T21:53:29.065197: step 1696, loss 0.142354, acc 0.96875\n",
      "2017-11-05T21:53:33.020258: step 1697, loss 0.0668632, acc 1\n",
      "2017-11-05T21:53:37.010454: step 1698, loss 0.0967182, acc 0.96875\n",
      "2017-11-05T21:53:40.966999: step 1699, loss 0.151981, acc 0.9375\n",
      "2017-11-05T21:53:44.919153: step 1700, loss 0.39861, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T21:53:47.535191: step 1700, loss 0.810116, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-05T21:53:52.698980: step 1701, loss 0.123515, acc 0.96875\n",
      "2017-11-05T21:53:56.763416: step 1702, loss 0.415016, acc 0.84375\n",
      "2017-11-05T21:54:00.753521: step 1703, loss 0.148113, acc 0.875\n",
      "2017-11-05T21:54:04.747814: step 1704, loss 0.160187, acc 0.9375\n",
      "2017-11-05T21:54:08.756687: step 1705, loss 0.19832, acc 0.90625\n",
      "2017-11-05T21:54:12.788119: step 1706, loss 0.101618, acc 1\n",
      "2017-11-05T21:54:16.802891: step 1707, loss 0.231068, acc 0.90625\n",
      "2017-11-05T21:54:20.794563: step 1708, loss 0.190819, acc 0.96875\n",
      "2017-11-05T21:54:24.836245: step 1709, loss 0.208758, acc 0.875\n",
      "2017-11-05T21:54:28.838764: step 1710, loss 0.23232, acc 0.90625\n",
      "2017-11-05T21:54:32.834575: step 1711, loss 0.0514619, acc 0.96875\n",
      "2017-11-05T21:54:36.903316: step 1712, loss 0.0986553, acc 0.9375\n",
      "2017-11-05T21:54:40.960117: step 1713, loss 0.208939, acc 0.9375\n",
      "2017-11-05T21:54:44.943051: step 1714, loss 0.10611, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T21:54:48.934378: step 1715, loss 0.215714, acc 0.90625\n",
      "2017-11-05T21:54:52.883172: step 1716, loss 0.197488, acc 0.96875\n",
      "2017-11-05T21:54:56.862227: step 1717, loss 0.233027, acc 0.875\n",
      "2017-11-05T21:55:00.926353: step 1718, loss 0.0761215, acc 0.96875\n",
      "2017-11-05T21:55:04.950225: step 1719, loss 0.105924, acc 0.90625\n",
      "2017-11-05T21:55:08.959216: step 1720, loss 0.222126, acc 0.875\n",
      "2017-11-05T21:55:12.968953: step 1721, loss 0.208415, acc 0.90625\n",
      "2017-11-05T21:55:17.008410: step 1722, loss 0.373746, acc 0.84375\n",
      "2017-11-05T21:55:21.016426: step 1723, loss 0.409427, acc 0.8125\n",
      "2017-11-05T21:55:24.986712: step 1724, loss 0.186278, acc 0.90625\n",
      "2017-11-05T21:55:28.939884: step 1725, loss 0.254054, acc 0.875\n",
      "2017-11-05T21:55:32.947946: step 1726, loss 0.14612, acc 0.9375\n",
      "2017-11-05T21:55:36.911291: step 1727, loss 0.271435, acc 0.90625\n",
      "2017-11-05T21:55:39.482333: step 1728, loss 0.27756, acc 0.9\n",
      "2017-11-05T21:55:43.490522: step 1729, loss 0.0785981, acc 0.96875\n",
      "2017-11-05T21:55:47.480605: step 1730, loss 0.257259, acc 0.84375\n",
      "2017-11-05T21:55:51.432641: step 1731, loss 0.211007, acc 0.9375\n",
      "2017-11-05T21:55:55.439857: step 1732, loss 0.270244, acc 0.90625\n",
      "2017-11-05T21:55:59.485196: step 1733, loss 0.179861, acc 0.90625\n",
      "2017-11-05T21:56:03.510753: step 1734, loss 0.118571, acc 0.96875\n",
      "2017-11-05T21:56:07.611894: step 1735, loss 0.0842339, acc 0.96875\n",
      "2017-11-05T21:56:11.621853: step 1736, loss 0.0272752, acc 1\n",
      "2017-11-05T21:56:15.637523: step 1737, loss 0.257008, acc 0.875\n",
      "2017-11-05T21:56:19.714246: step 1738, loss 0.239434, acc 0.84375\n",
      "2017-11-05T21:56:23.683465: step 1739, loss 0.188895, acc 0.90625\n",
      "2017-11-05T21:56:27.733071: step 1740, loss 0.369204, acc 0.84375\n",
      "2017-11-05T21:56:31.747607: step 1741, loss 0.323603, acc 0.84375\n",
      "2017-11-05T21:56:35.967914: step 1742, loss 0.231363, acc 0.9375\n",
      "2017-11-05T21:56:39.967717: step 1743, loss 0.288019, acc 0.875\n",
      "2017-11-05T21:56:43.952507: step 1744, loss 0.158684, acc 0.90625\n",
      "2017-11-05T21:56:47.994248: step 1745, loss 0.114976, acc 0.9375\n",
      "2017-11-05T21:56:51.930348: step 1746, loss 0.428772, acc 0.875\n",
      "2017-11-05T21:56:55.887497: step 1747, loss 0.320835, acc 0.875\n",
      "2017-11-05T21:56:59.862072: step 1748, loss 0.150582, acc 0.9375\n",
      "2017-11-05T21:57:03.798466: step 1749, loss 0.099249, acc 0.96875\n",
      "2017-11-05T21:57:07.849380: step 1750, loss 0.175086, acc 0.96875\n",
      "2017-11-05T21:57:11.812577: step 1751, loss 0.0676163, acc 0.96875\n",
      "2017-11-05T21:57:15.869722: step 1752, loss 0.344052, acc 0.84375\n",
      "2017-11-05T21:57:19.826986: step 1753, loss 0.301122, acc 0.875\n",
      "2017-11-05T21:57:23.833929: step 1754, loss 0.607276, acc 0.75\n",
      "2017-11-05T21:57:27.791618: step 1755, loss 0.372822, acc 0.84375\n",
      "2017-11-05T21:57:31.808008: step 1756, loss 0.131114, acc 0.9375\n",
      "2017-11-05T21:57:35.811385: step 1757, loss 0.0803858, acc 0.96875\n",
      "2017-11-05T21:57:39.740937: step 1758, loss 0.073367, acc 0.96875\n",
      "2017-11-05T21:57:43.714667: step 1759, loss 0.158701, acc 0.90625\n",
      "2017-11-05T21:57:47.712365: step 1760, loss 0.332421, acc 0.875\n",
      "2017-11-05T21:57:51.658250: step 1761, loss 0.119009, acc 0.9375\n",
      "2017-11-05T21:57:55.645083: step 1762, loss 0.229062, acc 0.90625\n",
      "2017-11-05T21:57:59.643243: step 1763, loss 0.129033, acc 0.9375\n",
      "2017-11-05T21:58:02.161192: step 1764, loss 0.034586, acc 1\n",
      "2017-11-05T21:58:06.188770: step 1765, loss 0.138519, acc 0.90625\n",
      "2017-11-05T21:58:10.191251: step 1766, loss 0.347509, acc 0.84375\n",
      "2017-11-05T21:58:14.282687: step 1767, loss 0.0853353, acc 0.96875\n",
      "2017-11-05T21:58:18.218781: step 1768, loss 0.0391854, acc 1\n",
      "2017-11-05T21:58:22.227298: step 1769, loss 0.13626, acc 0.9375\n",
      "2017-11-05T21:58:26.514697: step 1770, loss 0.201125, acc 0.90625\n",
      "2017-11-05T21:58:30.735811: step 1771, loss 0.170543, acc 0.90625\n",
      "2017-11-05T21:58:34.870260: step 1772, loss 0.136302, acc 0.9375\n",
      "2017-11-05T21:58:38.843212: step 1773, loss 0.144363, acc 0.90625\n",
      "2017-11-05T21:58:42.792984: step 1774, loss 0.173758, acc 0.9375\n",
      "2017-11-05T21:58:46.766434: step 1775, loss 0.0834594, acc 0.96875\n",
      "2017-11-05T21:58:50.773999: step 1776, loss 0.0799797, acc 0.96875\n",
      "2017-11-05T21:58:54.751158: step 1777, loss 0.148402, acc 0.9375\n",
      "2017-11-05T21:58:58.799712: step 1778, loss 0.0520071, acc 0.96875\n",
      "2017-11-05T21:59:02.833880: step 1779, loss 0.199815, acc 0.90625\n",
      "2017-11-05T21:59:06.777669: step 1780, loss 0.333469, acc 0.84375\n",
      "2017-11-05T21:59:10.848631: step 1781, loss 0.0953761, acc 0.96875\n",
      "2017-11-05T21:59:14.777352: step 1782, loss 0.398618, acc 0.8125\n",
      "2017-11-05T21:59:18.736524: step 1783, loss 0.31982, acc 0.875\n",
      "2017-11-05T21:59:22.712137: step 1784, loss 0.23363, acc 0.875\n",
      "2017-11-05T21:59:26.665261: step 1785, loss 0.360194, acc 0.8125\n",
      "2017-11-05T21:59:30.598744: step 1786, loss 0.148082, acc 0.90625\n",
      "2017-11-05T21:59:34.580877: step 1787, loss 0.118277, acc 0.96875\n",
      "2017-11-05T21:59:38.566406: step 1788, loss 0.269304, acc 0.90625\n",
      "2017-11-05T21:59:42.537590: step 1789, loss 0.197924, acc 0.875\n",
      "2017-11-05T21:59:46.496832: step 1790, loss 0.0755909, acc 1\n",
      "2017-11-05T21:59:50.517247: step 1791, loss 0.143348, acc 0.9375\n",
      "2017-11-05T21:59:54.534262: step 1792, loss 0.0936971, acc 0.9375\n",
      "2017-11-05T21:59:58.505470: step 1793, loss 0.0391903, acc 1\n",
      "2017-11-05T22:00:02.849954: step 1794, loss 0.0640314, acc 1\n",
      "2017-11-05T22:00:06.835426: step 1795, loss 0.354666, acc 0.90625\n",
      "2017-11-05T22:00:10.884324: step 1796, loss 0.167148, acc 0.9375\n",
      "2017-11-05T22:00:14.802968: step 1797, loss 0.207853, acc 0.9375\n",
      "2017-11-05T22:00:18.802628: step 1798, loss 0.42534, acc 0.84375\n",
      "2017-11-05T22:00:22.790216: step 1799, loss 0.0856772, acc 0.9375\n",
      "2017-11-05T22:00:25.275812: step 1800, loss 0.023481, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:00:27.862107: step 1800, loss 0.85936, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-05T22:00:33.155323: step 1801, loss 0.122587, acc 0.96875\n",
      "2017-11-05T22:00:37.300340: step 1802, loss 0.0800856, acc 1\n",
      "2017-11-05T22:00:41.384203: step 1803, loss 0.302707, acc 0.84375\n",
      "2017-11-05T22:00:45.422370: step 1804, loss 0.229539, acc 0.90625\n",
      "2017-11-05T22:00:49.525259: step 1805, loss 0.213137, acc 0.90625\n",
      "2017-11-05T22:00:53.624884: step 1806, loss 0.120866, acc 0.90625\n",
      "2017-11-05T22:00:57.707186: step 1807, loss 0.150753, acc 0.9375\n",
      "2017-11-05T22:01:01.674870: step 1808, loss 0.11398, acc 0.96875\n",
      "2017-11-05T22:01:05.761069: step 1809, loss 0.10353, acc 0.96875\n",
      "2017-11-05T22:01:09.904979: step 1810, loss 0.174023, acc 0.9375\n",
      "2017-11-05T22:01:13.946394: step 1811, loss 0.212376, acc 0.875\n",
      "2017-11-05T22:01:18.007654: step 1812, loss 0.204833, acc 0.875\n",
      "2017-11-05T22:01:22.142884: step 1813, loss 0.24614, acc 0.90625\n",
      "2017-11-05T22:01:26.179614: step 1814, loss 0.0849689, acc 0.96875\n",
      "2017-11-05T22:01:30.189110: step 1815, loss 0.181934, acc 0.875\n",
      "2017-11-05T22:01:34.202041: step 1816, loss 0.0381494, acc 1\n",
      "2017-11-05T22:01:38.211198: step 1817, loss 0.150695, acc 0.9375\n",
      "2017-11-05T22:01:42.248743: step 1818, loss 0.356201, acc 0.84375\n",
      "2017-11-05T22:01:46.251157: step 1819, loss 0.139609, acc 0.90625\n",
      "2017-11-05T22:01:50.336831: step 1820, loss 0.222614, acc 0.9375\n",
      "2017-11-05T22:01:54.355726: step 1821, loss 0.228153, acc 0.90625\n",
      "2017-11-05T22:01:58.410720: step 1822, loss 0.0779111, acc 0.96875\n",
      "2017-11-05T22:02:02.452147: step 1823, loss 0.193569, acc 0.84375\n",
      "2017-11-05T22:02:06.460955: step 1824, loss 0.142558, acc 0.9375\n",
      "2017-11-05T22:02:10.563364: step 1825, loss 0.407546, acc 0.84375\n",
      "2017-11-05T22:02:14.672767: step 1826, loss 0.334062, acc 0.84375\n",
      "2017-11-05T22:02:18.755429: step 1827, loss 0.0926589, acc 0.9375\n",
      "2017-11-05T22:02:22.717416: step 1828, loss 0.0871545, acc 0.9375\n",
      "2017-11-05T22:02:26.729656: step 1829, loss 0.262942, acc 0.9375\n",
      "2017-11-05T22:02:30.706406: step 1830, loss 0.165076, acc 0.9375\n",
      "2017-11-05T22:02:34.869699: step 1831, loss 0.439968, acc 0.875\n",
      "2017-11-05T22:02:39.026072: step 1832, loss 0.115855, acc 0.96875\n",
      "2017-11-05T22:02:43.107334: step 1833, loss 0.193722, acc 0.90625\n",
      "2017-11-05T22:02:47.100293: step 1834, loss 0.179141, acc 0.875\n",
      "2017-11-05T22:02:51.170073: step 1835, loss 0.0787597, acc 0.9375\n",
      "2017-11-05T22:02:53.694654: step 1836, loss 0.129947, acc 0.95\n",
      "2017-11-05T22:02:57.725235: step 1837, loss 0.117871, acc 0.9375\n",
      "2017-11-05T22:03:01.758639: step 1838, loss 0.130702, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T22:03:05.813281: step 1839, loss 0.240815, acc 0.90625\n",
      "2017-11-05T22:03:09.846366: step 1840, loss 0.586643, acc 0.8125\n",
      "2017-11-05T22:03:13.899726: step 1841, loss 0.297373, acc 0.875\n",
      "2017-11-05T22:03:17.900347: step 1842, loss 0.226834, acc 0.90625\n",
      "2017-11-05T22:03:21.958772: step 1843, loss 0.218034, acc 0.90625\n",
      "2017-11-05T22:03:26.179136: step 1844, loss 0.135995, acc 0.9375\n",
      "2017-11-05T22:03:30.492963: step 1845, loss 0.287992, acc 0.84375\n",
      "2017-11-05T22:03:34.505756: step 1846, loss 0.301407, acc 0.90625\n",
      "2017-11-05T22:03:38.539080: step 1847, loss 0.140589, acc 0.96875\n",
      "2017-11-05T22:03:42.609695: step 1848, loss 0.180656, acc 0.875\n",
      "2017-11-05T22:03:46.569810: step 1849, loss 0.0680952, acc 1\n",
      "2017-11-05T22:03:50.673909: step 1850, loss 0.0958424, acc 0.96875\n",
      "2017-11-05T22:03:54.714489: step 1851, loss 0.183999, acc 0.90625\n",
      "2017-11-05T22:03:58.757071: step 1852, loss 0.354725, acc 0.84375\n",
      "2017-11-05T22:04:02.777311: step 1853, loss 0.139462, acc 0.9375\n",
      "2017-11-05T22:04:06.863146: step 1854, loss 0.200511, acc 0.90625\n",
      "2017-11-05T22:04:10.859210: step 1855, loss 0.0965807, acc 0.96875\n",
      "2017-11-05T22:04:14.866711: step 1856, loss 0.120027, acc 0.96875\n",
      "2017-11-05T22:04:18.969342: step 1857, loss 0.139589, acc 0.9375\n",
      "2017-11-05T22:04:23.050761: step 1858, loss 0.2455, acc 0.875\n",
      "2017-11-05T22:04:27.080262: step 1859, loss 0.226754, acc 0.90625\n",
      "2017-11-05T22:04:31.128772: step 1860, loss 0.110309, acc 0.9375\n",
      "2017-11-05T22:04:35.235504: step 1861, loss 0.155433, acc 0.96875\n",
      "2017-11-05T22:04:39.247909: step 1862, loss 0.391078, acc 0.8125\n",
      "2017-11-05T22:04:43.328842: step 1863, loss 0.124108, acc 0.9375\n",
      "2017-11-05T22:04:47.517760: step 1864, loss 0.198098, acc 0.90625\n",
      "2017-11-05T22:04:51.476063: step 1865, loss 0.172841, acc 0.9375\n",
      "2017-11-05T22:04:55.439473: step 1866, loss 0.0672254, acc 0.96875\n",
      "2017-11-05T22:04:59.406528: step 1867, loss 0.0636424, acc 0.96875\n",
      "2017-11-05T22:05:03.434261: step 1868, loss 0.110133, acc 0.96875\n",
      "2017-11-05T22:05:07.508645: step 1869, loss 0.354356, acc 0.90625\n",
      "2017-11-05T22:05:11.599592: step 1870, loss 0.311649, acc 0.8125\n",
      "2017-11-05T22:05:15.638822: step 1871, loss 0.314321, acc 0.84375\n",
      "2017-11-05T22:05:18.176490: step 1872, loss 0.560241, acc 0.7\n",
      "2017-11-05T22:05:22.197424: step 1873, loss 0.0817983, acc 0.96875\n",
      "2017-11-05T22:05:26.197156: step 1874, loss 0.210367, acc 0.90625\n",
      "2017-11-05T22:05:30.179163: step 1875, loss 0.238154, acc 0.90625\n",
      "2017-11-05T22:05:34.091545: step 1876, loss 0.061989, acc 0.96875\n",
      "2017-11-05T22:05:38.136877: step 1877, loss 0.351484, acc 0.875\n",
      "2017-11-05T22:05:42.155794: step 1878, loss 0.175333, acc 0.96875\n",
      "2017-11-05T22:05:46.227331: step 1879, loss 0.0602449, acc 1\n",
      "2017-11-05T22:05:50.137161: step 1880, loss 0.444349, acc 0.75\n",
      "2017-11-05T22:05:54.077004: step 1881, loss 0.148948, acc 0.9375\n",
      "2017-11-05T22:05:57.983312: step 1882, loss 0.13432, acc 0.9375\n",
      "2017-11-05T22:06:01.940953: step 1883, loss 0.0929307, acc 0.96875\n",
      "2017-11-05T22:06:05.869814: step 1884, loss 0.147183, acc 0.90625\n",
      "2017-11-05T22:06:09.800930: step 1885, loss 0.281018, acc 0.875\n",
      "2017-11-05T22:06:13.752405: step 1886, loss 0.204248, acc 0.9375\n",
      "2017-11-05T22:06:17.689704: step 1887, loss 0.0467239, acc 1\n",
      "2017-11-05T22:06:21.672619: step 1888, loss 0.352085, acc 0.84375\n",
      "2017-11-05T22:06:25.624537: step 1889, loss 0.201791, acc 0.90625\n",
      "2017-11-05T22:06:29.578629: step 1890, loss 0.232424, acc 0.90625\n",
      "2017-11-05T22:06:33.646347: step 1891, loss 0.276848, acc 0.90625\n",
      "2017-11-05T22:06:37.605902: step 1892, loss 0.243889, acc 0.90625\n",
      "2017-11-05T22:06:41.534369: step 1893, loss 0.344584, acc 0.875\n",
      "2017-11-05T22:06:45.453236: step 1894, loss 0.0845985, acc 0.9375\n",
      "2017-11-05T22:06:49.422148: step 1895, loss 0.231652, acc 0.90625\n",
      "2017-11-05T22:06:53.343635: step 1896, loss 0.329004, acc 0.90625\n",
      "2017-11-05T22:06:57.289207: step 1897, loss 0.217286, acc 0.9375\n",
      "2017-11-05T22:07:01.189502: step 1898, loss 0.26522, acc 0.875\n",
      "2017-11-05T22:07:05.150511: step 1899, loss 0.096843, acc 0.96875\n",
      "2017-11-05T22:07:09.075357: step 1900, loss 0.438288, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:07:11.599405: step 1900, loss 0.727053, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-05T22:07:16.877472: step 1901, loss 0.0813559, acc 0.96875\n",
      "2017-11-05T22:07:20.821273: step 1902, loss 0.273407, acc 0.90625\n",
      "2017-11-05T22:07:24.769678: step 1903, loss 0.11482, acc 0.9375\n",
      "2017-11-05T22:07:28.672101: step 1904, loss 0.25738, acc 0.90625\n",
      "2017-11-05T22:07:32.609350: step 1905, loss 0.28958, acc 0.875\n",
      "2017-11-05T22:07:36.507696: step 1906, loss 0.1248, acc 0.9375\n",
      "2017-11-05T22:07:40.418456: step 1907, loss 0.339402, acc 0.875\n",
      "2017-11-05T22:07:42.923479: step 1908, loss 0.04278, acc 1\n",
      "2017-11-05T22:07:46.843648: step 1909, loss 0.160769, acc 0.9375\n",
      "2017-11-05T22:07:50.788659: step 1910, loss 0.204029, acc 0.875\n",
      "2017-11-05T22:07:54.745920: step 1911, loss 0.117201, acc 0.90625\n",
      "2017-11-05T22:07:58.697312: step 1912, loss 0.274908, acc 0.90625\n",
      "2017-11-05T22:08:02.647156: step 1913, loss 0.454336, acc 0.8125\n",
      "2017-11-05T22:08:06.620532: step 1914, loss 0.0345376, acc 1\n",
      "2017-11-05T22:08:10.540967: step 1915, loss 0.32485, acc 0.875\n",
      "2017-11-05T22:08:14.453264: step 1916, loss 0.175813, acc 0.9375\n",
      "2017-11-05T22:08:18.443285: step 1917, loss 0.129592, acc 0.96875\n",
      "2017-11-05T22:08:22.576021: step 1918, loss 0.32842, acc 0.875\n",
      "2017-11-05T22:08:26.543273: step 1919, loss 0.127309, acc 0.9375\n",
      "2017-11-05T22:08:30.447721: step 1920, loss 0.355866, acc 0.90625\n",
      "2017-11-05T22:08:34.520878: step 1921, loss 0.117914, acc 0.96875\n",
      "2017-11-05T22:08:38.445533: step 1922, loss 0.300157, acc 0.84375\n",
      "2017-11-05T22:08:42.416753: step 1923, loss 0.142527, acc 0.9375\n",
      "2017-11-05T22:08:46.380813: step 1924, loss 0.228055, acc 0.90625\n",
      "2017-11-05T22:08:50.349099: step 1925, loss 0.0687875, acc 0.96875\n",
      "2017-11-05T22:08:54.283703: step 1926, loss 0.288297, acc 0.9375\n",
      "2017-11-05T22:08:58.207835: step 1927, loss 0.346766, acc 0.875\n",
      "2017-11-05T22:09:02.131996: step 1928, loss 0.26527, acc 0.90625\n",
      "2017-11-05T22:09:06.057453: step 1929, loss 0.26639, acc 0.90625\n",
      "2017-11-05T22:09:10.013408: step 1930, loss 0.256873, acc 0.90625\n",
      "2017-11-05T22:09:13.971256: step 1931, loss 0.29289, acc 0.90625\n",
      "2017-11-05T22:09:17.849386: step 1932, loss 0.101372, acc 0.96875\n",
      "2017-11-05T22:09:21.848970: step 1933, loss 0.160974, acc 0.9375\n",
      "2017-11-05T22:09:25.871855: step 1934, loss 0.174898, acc 0.90625\n",
      "2017-11-05T22:09:29.832481: step 1935, loss 0.201371, acc 0.96875\n",
      "2017-11-05T22:09:33.786635: step 1936, loss 0.240702, acc 0.90625\n",
      "2017-11-05T22:09:37.756221: step 1937, loss 0.145926, acc 0.9375\n",
      "2017-11-05T22:09:41.670047: step 1938, loss 0.244387, acc 0.875\n",
      "2017-11-05T22:09:45.660295: step 1939, loss 0.160865, acc 0.90625\n",
      "2017-11-05T22:09:49.653025: step 1940, loss 0.156222, acc 0.90625\n",
      "2017-11-05T22:09:53.567947: step 1941, loss 0.0977212, acc 0.96875\n",
      "2017-11-05T22:09:57.538276: step 1942, loss 0.233936, acc 0.90625\n",
      "2017-11-05T22:10:01.865002: step 1943, loss 0.0469112, acc 0.96875\n",
      "2017-11-05T22:10:04.377184: step 1944, loss 0.320405, acc 0.8\n",
      "2017-11-05T22:10:08.396233: step 1945, loss 0.160714, acc 0.9375\n",
      "2017-11-05T22:10:12.358209: step 1946, loss 0.0442408, acc 1\n",
      "2017-11-05T22:10:16.296759: step 1947, loss 0.0714902, acc 0.96875\n",
      "2017-11-05T22:10:20.345343: step 1948, loss 0.119065, acc 0.9375\n",
      "2017-11-05T22:10:24.323241: step 1949, loss 0.179481, acc 0.90625\n",
      "2017-11-05T22:10:28.271710: step 1950, loss 0.251481, acc 0.84375\n",
      "2017-11-05T22:10:32.213093: step 1951, loss 0.327073, acc 0.875\n",
      "2017-11-05T22:10:36.315082: step 1952, loss 0.212515, acc 0.90625\n",
      "2017-11-05T22:10:40.211688: step 1953, loss 0.17049, acc 0.90625\n",
      "2017-11-05T22:10:44.210166: step 1954, loss 0.0722374, acc 0.96875\n",
      "2017-11-05T22:10:48.177119: step 1955, loss 0.150861, acc 0.90625\n",
      "2017-11-05T22:10:52.186456: step 1956, loss 0.331387, acc 0.84375\n",
      "2017-11-05T22:10:56.152939: step 1957, loss 0.260887, acc 0.875\n",
      "2017-11-05T22:11:00.152675: step 1958, loss 0.151138, acc 0.90625\n",
      "2017-11-05T22:11:04.112983: step 1959, loss 0.230157, acc 0.90625\n",
      "2017-11-05T22:11:08.090924: step 1960, loss 0.103765, acc 0.90625\n",
      "2017-11-05T22:11:12.025401: step 1961, loss 0.297382, acc 0.875\n",
      "2017-11-05T22:11:16.031119: step 1962, loss 0.193579, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T22:11:19.947356: step 1963, loss 0.174824, acc 0.875\n",
      "2017-11-05T22:11:23.926034: step 1964, loss 0.236848, acc 0.90625\n",
      "2017-11-05T22:11:27.888246: step 1965, loss 0.0795781, acc 0.96875\n",
      "2017-11-05T22:11:31.831405: step 1966, loss 0.208651, acc 0.9375\n",
      "2017-11-05T22:11:35.851229: step 1967, loss 0.222448, acc 0.875\n",
      "2017-11-05T22:11:39.793814: step 1968, loss 0.254177, acc 0.875\n",
      "2017-11-05T22:11:43.759520: step 1969, loss 0.256125, acc 0.90625\n",
      "2017-11-05T22:11:47.703796: step 1970, loss 0.294837, acc 0.90625\n",
      "2017-11-05T22:11:51.662707: step 1971, loss 0.402821, acc 0.84375\n",
      "2017-11-05T22:11:55.575887: step 1972, loss 0.107132, acc 0.96875\n",
      "2017-11-05T22:11:59.534098: step 1973, loss 0.130253, acc 0.9375\n",
      "2017-11-05T22:12:03.461851: step 1974, loss 0.0358347, acc 1\n",
      "2017-11-05T22:12:07.539984: step 1975, loss 0.225201, acc 0.875\n",
      "2017-11-05T22:12:11.523003: step 1976, loss 0.116528, acc 0.9375\n",
      "2017-11-05T22:12:15.494159: step 1977, loss 0.263472, acc 0.875\n",
      "2017-11-05T22:12:19.482150: step 1978, loss 0.0136645, acc 1\n",
      "2017-11-05T22:12:23.396423: step 1979, loss 0.254106, acc 0.90625\n",
      "2017-11-05T22:12:25.945962: step 1980, loss 0.0213459, acc 1\n",
      "2017-11-05T22:12:29.909573: step 1981, loss 0.171233, acc 0.90625\n",
      "2017-11-05T22:12:33.970103: step 1982, loss 0.133013, acc 0.9375\n",
      "2017-11-05T22:12:38.012217: step 1983, loss 0.0666859, acc 0.96875\n",
      "2017-11-05T22:12:42.008563: step 1984, loss 0.140792, acc 0.9375\n",
      "2017-11-05T22:12:46.002401: step 1985, loss 0.19447, acc 0.9375\n",
      "2017-11-05T22:12:49.998713: step 1986, loss 0.209495, acc 0.875\n",
      "2017-11-05T22:12:54.024190: step 1987, loss 0.236434, acc 0.90625\n",
      "2017-11-05T22:12:58.025997: step 1988, loss 0.20959, acc 0.90625\n",
      "2017-11-05T22:13:01.999120: step 1989, loss 0.296796, acc 0.84375\n",
      "2017-11-05T22:13:06.008963: step 1990, loss 0.204801, acc 0.9375\n",
      "2017-11-05T22:13:09.946314: step 1991, loss 0.0931689, acc 0.96875\n",
      "2017-11-05T22:13:13.918565: step 1992, loss 0.233993, acc 0.9375\n",
      "2017-11-05T22:13:17.880603: step 1993, loss 0.178289, acc 0.9375\n",
      "2017-11-05T22:13:21.956404: step 1994, loss 0.345682, acc 0.90625\n",
      "2017-11-05T22:13:26.141682: step 1995, loss 0.352455, acc 0.90625\n",
      "2017-11-05T22:13:30.209370: step 1996, loss 0.296377, acc 0.84375\n",
      "2017-11-05T22:13:34.147563: step 1997, loss 0.141802, acc 0.90625\n",
      "2017-11-05T22:13:38.106893: step 1998, loss 0.0547772, acc 0.96875\n",
      "2017-11-05T22:13:42.021258: step 1999, loss 0.143296, acc 0.9375\n",
      "2017-11-05T22:13:45.904888: step 2000, loss 0.192955, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:13:48.474617: step 2000, loss 1.15778, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-05T22:13:54.400440: step 2001, loss 0.357689, acc 0.84375\n",
      "2017-11-05T22:13:58.303796: step 2002, loss 0.296607, acc 0.84375\n",
      "2017-11-05T22:14:02.288977: step 2003, loss 0.122741, acc 0.9375\n",
      "2017-11-05T22:14:06.247117: step 2004, loss 0.184115, acc 0.9375\n",
      "2017-11-05T22:14:10.168150: step 2005, loss 0.128953, acc 0.90625\n",
      "2017-11-05T22:14:14.176172: step 2006, loss 0.197286, acc 0.9375\n",
      "2017-11-05T22:14:18.136281: step 2007, loss 0.176833, acc 0.9375\n",
      "2017-11-05T22:14:22.149181: step 2008, loss 0.174113, acc 0.9375\n",
      "2017-11-05T22:14:26.140767: step 2009, loss 0.326602, acc 0.875\n",
      "2017-11-05T22:14:30.141571: step 2010, loss 0.178993, acc 0.90625\n",
      "2017-11-05T22:14:34.226180: step 2011, loss 0.244193, acc 0.875\n",
      "2017-11-05T22:14:38.153017: step 2012, loss 0.123942, acc 0.9375\n",
      "2017-11-05T22:14:42.100860: step 2013, loss 0.0541257, acc 0.96875\n",
      "2017-11-05T22:14:46.048917: step 2014, loss 0.470472, acc 0.84375\n",
      "2017-11-05T22:14:49.984343: step 2015, loss 0.511104, acc 0.875\n",
      "2017-11-05T22:14:52.502754: step 2016, loss 0.260109, acc 0.85\n",
      "2017-11-05T22:14:56.452655: step 2017, loss 0.133989, acc 0.90625\n",
      "2017-11-05T22:15:00.406169: step 2018, loss 0.265987, acc 0.90625\n",
      "2017-11-05T22:15:04.352038: step 2019, loss 0.0281085, acc 1\n",
      "2017-11-05T22:15:08.369796: step 2020, loss 0.0217204, acc 1\n",
      "2017-11-05T22:15:12.279623: step 2021, loss 0.297174, acc 0.9375\n",
      "2017-11-05T22:15:16.260120: step 2022, loss 0.346714, acc 0.90625\n",
      "2017-11-05T22:15:20.200483: step 2023, loss 0.186657, acc 0.96875\n",
      "2017-11-05T22:15:24.199673: step 2024, loss 0.297983, acc 0.875\n",
      "2017-11-05T22:15:28.165749: step 2025, loss 0.0605939, acc 0.96875\n",
      "2017-11-05T22:15:32.183344: step 2026, loss 0.149571, acc 0.96875\n",
      "2017-11-05T22:15:36.190412: step 2027, loss 0.108291, acc 0.96875\n",
      "2017-11-05T22:15:40.229790: step 2028, loss 0.154928, acc 0.9375\n",
      "2017-11-05T22:15:44.215713: step 2029, loss 0.275771, acc 0.875\n",
      "2017-11-05T22:15:48.172536: step 2030, loss 0.34636, acc 0.84375\n",
      "2017-11-05T22:15:52.177650: step 2031, loss 0.191696, acc 0.9375\n",
      "2017-11-05T22:15:56.132670: step 2032, loss 0.240781, acc 0.84375\n",
      "2017-11-05T22:16:00.205852: step 2033, loss 0.221494, acc 0.875\n",
      "2017-11-05T22:16:04.203075: step 2034, loss 0.179293, acc 0.9375\n",
      "2017-11-05T22:16:08.182853: step 2035, loss 0.204009, acc 0.875\n",
      "2017-11-05T22:16:12.209837: step 2036, loss 0.421416, acc 0.84375\n",
      "2017-11-05T22:16:16.188733: step 2037, loss 0.259249, acc 0.875\n",
      "2017-11-05T22:16:20.114126: step 2038, loss 0.46335, acc 0.6875\n",
      "2017-11-05T22:16:24.114410: step 2039, loss 0.565424, acc 0.75\n",
      "2017-11-05T22:16:28.058684: step 2040, loss 0.115676, acc 0.9375\n",
      "2017-11-05T22:16:32.045314: step 2041, loss 0.243726, acc 0.90625\n",
      "2017-11-05T22:16:36.337165: step 2042, loss 0.132319, acc 0.9375\n",
      "2017-11-05T22:16:40.485352: step 2043, loss 0.22652, acc 0.875\n",
      "2017-11-05T22:16:44.482579: step 2044, loss 0.445521, acc 0.78125\n",
      "2017-11-05T22:16:48.513746: step 2045, loss 0.236356, acc 0.875\n",
      "2017-11-05T22:16:52.533085: step 2046, loss 0.107264, acc 0.96875\n",
      "2017-11-05T22:16:56.515949: step 2047, loss 0.228522, acc 0.90625\n",
      "2017-11-05T22:17:00.536871: step 2048, loss 0.23122, acc 0.875\n",
      "2017-11-05T22:17:04.511240: step 2049, loss 0.277844, acc 0.90625\n",
      "2017-11-05T22:17:08.493527: step 2050, loss 0.0961724, acc 0.9375\n",
      "2017-11-05T22:17:12.543480: step 2051, loss 0.244299, acc 0.875\n",
      "2017-11-05T22:17:15.048015: step 2052, loss 0.102047, acc 0.95\n",
      "2017-11-05T22:17:19.036549: step 2053, loss 0.108523, acc 0.96875\n",
      "2017-11-05T22:17:23.092432: step 2054, loss 0.0207074, acc 1\n",
      "2017-11-05T22:17:27.025042: step 2055, loss 0.0886864, acc 0.9375\n",
      "2017-11-05T22:17:30.957720: step 2056, loss 0.1271, acc 0.90625\n",
      "2017-11-05T22:17:34.980645: step 2057, loss 0.351656, acc 0.84375\n",
      "2017-11-05T22:17:38.925672: step 2058, loss 0.0936597, acc 0.9375\n",
      "2017-11-05T22:17:42.879335: step 2059, loss 0.355067, acc 0.84375\n",
      "2017-11-05T22:17:46.863197: step 2060, loss 0.121291, acc 0.90625\n",
      "2017-11-05T22:17:50.903079: step 2061, loss 0.0547871, acc 0.96875\n",
      "2017-11-05T22:17:54.871639: step 2062, loss 0.0526964, acc 1\n",
      "2017-11-05T22:17:58.877233: step 2063, loss 0.148793, acc 0.90625\n",
      "2017-11-05T22:18:02.856147: step 2064, loss 0.142656, acc 0.90625\n",
      "2017-11-05T22:18:06.798644: step 2065, loss 0.155336, acc 0.96875\n",
      "2017-11-05T22:18:10.776793: step 2066, loss 0.299389, acc 0.84375\n",
      "2017-11-05T22:18:14.768109: step 2067, loss 0.0824796, acc 0.9375\n",
      "2017-11-05T22:18:18.736344: step 2068, loss 0.266057, acc 0.875\n",
      "2017-11-05T22:18:22.850589: step 2069, loss 0.11626, acc 0.9375\n",
      "2017-11-05T22:18:27.071549: step 2070, loss 0.263293, acc 0.90625\n",
      "2017-11-05T22:18:31.059326: step 2071, loss 0.150441, acc 0.9375\n",
      "2017-11-05T22:18:35.173764: step 2072, loss 0.301917, acc 0.875\n",
      "2017-11-05T22:18:39.172256: step 2073, loss 0.240025, acc 0.84375\n",
      "2017-11-05T22:18:43.406099: step 2074, loss 0.235263, acc 0.90625\n",
      "2017-11-05T22:18:47.416312: step 2075, loss 0.26744, acc 0.875\n",
      "2017-11-05T22:18:51.660095: step 2076, loss 0.185902, acc 0.90625\n",
      "2017-11-05T22:18:55.738618: step 2077, loss 0.170966, acc 0.875\n",
      "2017-11-05T22:18:59.919328: step 2078, loss 0.129732, acc 0.96875\n",
      "2017-11-05T22:19:03.910996: step 2079, loss 0.0866691, acc 0.96875\n",
      "2017-11-05T22:19:07.948989: step 2080, loss 0.58754, acc 0.78125\n",
      "2017-11-05T22:19:12.168153: step 2081, loss 0.0698397, acc 0.96875\n",
      "2017-11-05T22:19:16.167786: step 2082, loss 0.298639, acc 0.875\n",
      "2017-11-05T22:19:20.314191: step 2083, loss 0.288562, acc 0.8125\n",
      "2017-11-05T22:19:24.426305: step 2084, loss 0.279161, acc 0.875\n",
      "2017-11-05T22:19:28.595761: step 2085, loss 0.114268, acc 0.96875\n",
      "2017-11-05T22:19:32.765647: step 2086, loss 0.163143, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T22:19:36.899366: step 2087, loss 0.232738, acc 0.875\n",
      "2017-11-05T22:19:39.632917: step 2088, loss 0.0777078, acc 0.95\n",
      "2017-11-05T22:19:43.635931: step 2089, loss 0.114501, acc 0.9375\n",
      "2017-11-05T22:19:47.604156: step 2090, loss 0.168017, acc 0.9375\n",
      "2017-11-05T22:19:51.589003: step 2091, loss 0.0849334, acc 0.9375\n",
      "2017-11-05T22:19:55.523706: step 2092, loss 0.272747, acc 0.875\n",
      "2017-11-05T22:19:59.499201: step 2093, loss 0.0293295, acc 1\n",
      "2017-11-05T22:20:03.738058: step 2094, loss 0.0749898, acc 1\n",
      "2017-11-05T22:20:07.734708: step 2095, loss 0.167843, acc 0.9375\n",
      "2017-11-05T22:20:11.762348: step 2096, loss 0.0573881, acc 1\n",
      "2017-11-05T22:20:15.756049: step 2097, loss 0.223132, acc 0.90625\n",
      "2017-11-05T22:20:19.713123: step 2098, loss 0.0816412, acc 0.96875\n",
      "2017-11-05T22:20:23.696613: step 2099, loss 0.113614, acc 0.96875\n",
      "2017-11-05T22:20:27.668503: step 2100, loss 0.38921, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:20:30.179388: step 2100, loss 0.894569, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-05T22:20:36.357795: step 2101, loss 0.0720576, acc 1\n",
      "2017-11-05T22:20:40.347519: step 2102, loss 0.285841, acc 0.875\n",
      "2017-11-05T22:20:44.319646: step 2103, loss 0.205782, acc 0.90625\n",
      "2017-11-05T22:20:48.288754: step 2104, loss 0.0828464, acc 0.96875\n",
      "2017-11-05T22:20:52.225699: step 2105, loss 0.13658, acc 0.9375\n",
      "2017-11-05T22:20:56.223421: step 2106, loss 0.105066, acc 0.9375\n",
      "2017-11-05T22:21:00.187611: step 2107, loss 0.13633, acc 0.9375\n",
      "2017-11-05T22:21:04.256104: step 2108, loss 0.137111, acc 0.96875\n",
      "2017-11-05T22:21:08.252279: step 2109, loss 0.172408, acc 0.90625\n",
      "2017-11-05T22:21:12.302947: step 2110, loss 0.173355, acc 0.875\n",
      "2017-11-05T22:21:16.301133: step 2111, loss 0.199085, acc 0.875\n",
      "2017-11-05T22:21:20.254232: step 2112, loss 0.162365, acc 0.9375\n",
      "2017-11-05T22:21:24.289537: step 2113, loss 0.189139, acc 0.875\n",
      "2017-11-05T22:21:28.231848: step 2114, loss 0.349079, acc 0.875\n",
      "2017-11-05T22:21:32.249917: step 2115, loss 0.343571, acc 0.78125\n",
      "2017-11-05T22:21:36.234463: step 2116, loss 0.283131, acc 0.84375\n",
      "2017-11-05T22:21:40.255356: step 2117, loss 0.190201, acc 0.875\n",
      "2017-11-05T22:21:44.228831: step 2118, loss 0.178625, acc 0.9375\n",
      "2017-11-05T22:21:48.149764: step 2119, loss 0.0584704, acc 1\n",
      "2017-11-05T22:21:52.149731: step 2120, loss 0.184143, acc 0.9375\n",
      "2017-11-05T22:21:56.137356: step 2121, loss 0.122228, acc 0.90625\n",
      "2017-11-05T22:22:00.145940: step 2122, loss 0.193876, acc 0.9375\n",
      "2017-11-05T22:22:04.152107: step 2123, loss 0.365099, acc 0.875\n",
      "2017-11-05T22:22:06.664109: step 2124, loss 0.36286, acc 0.8\n",
      "2017-11-05T22:22:10.654561: step 2125, loss 0.15899, acc 0.90625\n",
      "2017-11-05T22:22:14.667835: step 2126, loss 0.246494, acc 0.90625\n",
      "2017-11-05T22:22:18.662841: step 2127, loss 0.0509612, acc 1\n",
      "2017-11-05T22:22:22.641392: step 2128, loss 0.0635616, acc 1\n",
      "2017-11-05T22:22:26.651426: step 2129, loss 0.514518, acc 0.875\n",
      "2017-11-05T22:22:30.630021: step 2130, loss 0.19291, acc 0.90625\n",
      "2017-11-05T22:22:34.791591: step 2131, loss 0.328586, acc 0.8125\n",
      "2017-11-05T22:22:38.852046: step 2132, loss 0.176771, acc 0.90625\n",
      "2017-11-05T22:22:42.899530: step 2133, loss 0.182089, acc 0.875\n",
      "2017-11-05T22:22:46.865460: step 2134, loss 0.135574, acc 0.9375\n",
      "2017-11-05T22:22:50.878131: step 2135, loss 0.364421, acc 0.875\n",
      "2017-11-05T22:22:54.873543: step 2136, loss 0.10911, acc 0.9375\n",
      "2017-11-05T22:22:58.895682: step 2137, loss 0.0948791, acc 0.9375\n",
      "2017-11-05T22:23:03.076929: step 2138, loss 0.094892, acc 0.96875\n",
      "2017-11-05T22:23:07.036739: step 2139, loss 0.264529, acc 0.9375\n",
      "2017-11-05T22:23:11.031844: step 2140, loss 0.0872089, acc 0.9375\n",
      "2017-11-05T22:23:15.011298: step 2141, loss 0.249633, acc 0.90625\n",
      "2017-11-05T22:23:18.986788: step 2142, loss 0.218497, acc 0.90625\n",
      "2017-11-05T22:23:23.031426: step 2143, loss 0.20727, acc 0.8125\n",
      "2017-11-05T22:23:27.136794: step 2144, loss 0.193521, acc 0.9375\n",
      "2017-11-05T22:23:31.128847: step 2145, loss 0.0618652, acc 0.96875\n",
      "2017-11-05T22:23:35.088629: step 2146, loss 0.186773, acc 0.90625\n",
      "2017-11-05T22:23:39.030915: step 2147, loss 0.122284, acc 0.96875\n",
      "2017-11-05T22:23:43.023129: step 2148, loss 0.218537, acc 0.875\n",
      "2017-11-05T22:23:46.978423: step 2149, loss 0.134221, acc 0.96875\n",
      "2017-11-05T22:23:51.066340: step 2150, loss 0.141958, acc 0.9375\n",
      "2017-11-05T22:23:55.141142: step 2151, loss 0.141706, acc 0.90625\n",
      "2017-11-05T22:23:59.045729: step 2152, loss 0.138422, acc 0.9375\n",
      "2017-11-05T22:24:03.059945: step 2153, loss 0.148985, acc 0.9375\n",
      "2017-11-05T22:24:07.076204: step 2154, loss 0.326111, acc 0.8125\n",
      "2017-11-05T22:24:11.105540: step 2155, loss 0.313015, acc 0.875\n",
      "2017-11-05T22:24:15.096537: step 2156, loss 0.12711, acc 0.90625\n",
      "2017-11-05T22:24:19.077621: step 2157, loss 0.112345, acc 0.96875\n",
      "2017-11-05T22:24:23.094513: step 2158, loss 0.246726, acc 0.90625\n",
      "2017-11-05T22:24:27.062754: step 2159, loss 0.161063, acc 0.9375\n",
      "2017-11-05T22:24:29.631350: step 2160, loss 0.445871, acc 0.75\n",
      "2017-11-05T22:24:33.680253: step 2161, loss 0.0477164, acc 0.96875\n",
      "2017-11-05T22:24:37.743091: step 2162, loss 0.164216, acc 0.90625\n",
      "2017-11-05T22:24:41.707032: step 2163, loss 0.186111, acc 0.90625\n",
      "2017-11-05T22:24:45.723033: step 2164, loss 0.0354898, acc 1\n",
      "2017-11-05T22:24:49.667408: step 2165, loss 0.420962, acc 0.84375\n",
      "2017-11-05T22:24:53.660849: step 2166, loss 0.204834, acc 0.90625\n",
      "2017-11-05T22:24:57.713028: step 2167, loss 0.289716, acc 0.90625\n",
      "2017-11-05T22:25:01.704641: step 2168, loss 0.148365, acc 0.90625\n",
      "2017-11-05T22:25:05.634375: step 2169, loss 0.301235, acc 0.875\n",
      "2017-11-05T22:25:09.612158: step 2170, loss 0.146332, acc 0.90625\n",
      "2017-11-05T22:25:13.621256: step 2171, loss 0.220994, acc 0.875\n",
      "2017-11-05T22:25:17.647395: step 2172, loss 0.0653247, acc 0.9375\n",
      "2017-11-05T22:25:21.655720: step 2173, loss 0.348448, acc 0.84375\n",
      "2017-11-05T22:25:25.605737: step 2174, loss 0.258965, acc 0.90625\n",
      "2017-11-05T22:25:29.497188: step 2175, loss 0.0490887, acc 0.96875\n",
      "2017-11-05T22:25:33.475030: step 2176, loss 0.209547, acc 0.875\n",
      "2017-11-05T22:25:37.474907: step 2177, loss 0.167538, acc 0.9375\n",
      "2017-11-05T22:25:41.437201: step 2178, loss 0.225164, acc 0.875\n",
      "2017-11-05T22:25:45.477392: step 2179, loss 0.151644, acc 0.9375\n",
      "2017-11-05T22:25:49.456888: step 2180, loss 0.0908063, acc 0.9375\n",
      "2017-11-05T22:25:53.492245: step 2181, loss 0.140621, acc 0.9375\n",
      "2017-11-05T22:25:57.443779: step 2182, loss 0.185391, acc 0.90625\n",
      "2017-11-05T22:26:01.523298: step 2183, loss 0.208305, acc 0.90625\n",
      "2017-11-05T22:26:05.512310: step 2184, loss 0.344263, acc 0.875\n",
      "2017-11-05T22:26:09.490616: step 2185, loss 0.243102, acc 0.84375\n",
      "2017-11-05T22:26:13.467695: step 2186, loss 0.316196, acc 0.875\n",
      "2017-11-05T22:26:17.508123: step 2187, loss 0.20119, acc 0.90625\n",
      "2017-11-05T22:26:21.625092: step 2188, loss 0.14506, acc 0.875\n",
      "2017-11-05T22:26:25.636721: step 2189, loss 0.0994152, acc 0.9375\n",
      "2017-11-05T22:26:29.690514: step 2190, loss 0.0621706, acc 0.96875\n",
      "2017-11-05T22:26:33.901659: step 2191, loss 0.0728786, acc 0.96875\n",
      "2017-11-05T22:26:38.007472: step 2192, loss 0.0595678, acc 0.96875\n",
      "2017-11-05T22:26:42.002867: step 2193, loss 0.505964, acc 0.8125\n",
      "2017-11-05T22:26:46.036223: step 2194, loss 0.23904, acc 0.90625\n",
      "2017-11-05T22:26:50.030557: step 2195, loss 0.271256, acc 0.875\n",
      "2017-11-05T22:26:52.675681: step 2196, loss 0.345573, acc 0.75\n",
      "2017-11-05T22:26:56.686341: step 2197, loss 0.270839, acc 0.875\n",
      "2017-11-05T22:27:00.660904: step 2198, loss 0.185804, acc 0.9375\n",
      "2017-11-05T22:27:04.621530: step 2199, loss 0.312602, acc 0.90625\n",
      "2017-11-05T22:27:08.641831: step 2200, loss 0.157551, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:27:11.170619: step 2200, loss 0.715243, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-05T22:27:16.607904: step 2201, loss 0.0838539, acc 1\n",
      "2017-11-05T22:27:20.601460: step 2202, loss 0.303707, acc 0.875\n",
      "2017-11-05T22:27:24.655418: step 2203, loss 0.185354, acc 0.9375\n",
      "2017-11-05T22:27:28.634116: step 2204, loss 0.397589, acc 0.875\n",
      "2017-11-05T22:27:32.569017: step 2205, loss 0.127237, acc 0.90625\n",
      "2017-11-05T22:27:36.608443: step 2206, loss 0.309203, acc 0.90625\n",
      "2017-11-05T22:27:40.585496: step 2207, loss 0.368497, acc 0.8125\n",
      "2017-11-05T22:27:44.622178: step 2208, loss 0.249324, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T22:27:48.645016: step 2209, loss 0.115306, acc 0.96875\n",
      "2017-11-05T22:27:52.600836: step 2210, loss 0.154135, acc 0.96875\n",
      "2017-11-05T22:27:56.599734: step 2211, loss 0.0369918, acc 1\n",
      "2017-11-05T22:28:00.618495: step 2212, loss 0.0269837, acc 1\n",
      "2017-11-05T22:28:04.566451: step 2213, loss 0.256768, acc 0.90625\n",
      "2017-11-05T22:28:08.494443: step 2214, loss 0.218552, acc 0.875\n",
      "2017-11-05T22:28:12.457577: step 2215, loss 0.132219, acc 0.9375\n",
      "2017-11-05T22:28:16.420460: step 2216, loss 0.165834, acc 0.9375\n",
      "2017-11-05T22:28:20.411190: step 2217, loss 0.174978, acc 0.90625\n",
      "2017-11-05T22:28:24.602285: step 2218, loss 0.495211, acc 0.84375\n",
      "2017-11-05T22:28:29.336115: step 2219, loss 0.355676, acc 0.78125\n",
      "2017-11-05T22:28:34.079295: step 2220, loss 0.337549, acc 0.875\n",
      "2017-11-05T22:28:38.484179: step 2221, loss 0.0635187, acc 0.96875\n",
      "2017-11-05T22:28:42.528917: step 2222, loss 0.247199, acc 0.84375\n",
      "2017-11-05T22:28:46.498860: step 2223, loss 0.134421, acc 0.90625\n",
      "2017-11-05T22:28:50.498664: step 2224, loss 0.108708, acc 0.9375\n",
      "2017-11-05T22:28:54.452315: step 2225, loss 0.387978, acc 0.90625\n",
      "2017-11-05T22:28:58.440542: step 2226, loss 0.0558318, acc 1\n",
      "2017-11-05T22:29:02.392061: step 2227, loss 0.174776, acc 0.9375\n",
      "2017-11-05T22:29:06.369743: step 2228, loss 0.274173, acc 0.875\n",
      "2017-11-05T22:29:10.518364: step 2229, loss 0.128502, acc 0.9375\n",
      "2017-11-05T22:29:14.495631: step 2230, loss 0.152333, acc 0.9375\n",
      "2017-11-05T22:29:18.456443: step 2231, loss 0.0789396, acc 0.96875\n",
      "2017-11-05T22:29:20.985285: step 2232, loss 0.242604, acc 0.85\n",
      "2017-11-05T22:29:24.988393: step 2233, loss 0.287959, acc 0.875\n",
      "2017-11-05T22:29:28.937923: step 2234, loss 0.1549, acc 0.90625\n",
      "2017-11-05T22:29:32.909057: step 2235, loss 0.159, acc 0.9375\n",
      "2017-11-05T22:29:36.904975: step 2236, loss 0.155854, acc 0.9375\n",
      "2017-11-05T22:29:40.900421: step 2237, loss 0.0955865, acc 0.9375\n",
      "2017-11-05T22:29:44.884518: step 2238, loss 0.186044, acc 0.90625\n",
      "2017-11-05T22:29:48.808639: step 2239, loss 0.0652991, acc 0.96875\n",
      "2017-11-05T22:29:52.871227: step 2240, loss 0.2262, acc 0.90625\n",
      "2017-11-05T22:29:56.890548: step 2241, loss 0.0715144, acc 0.96875\n",
      "2017-11-05T22:30:01.016508: step 2242, loss 0.352985, acc 0.8125\n",
      "2017-11-05T22:30:05.088849: step 2243, loss 0.403929, acc 0.875\n",
      "2017-11-05T22:30:09.100884: step 2244, loss 0.222124, acc 0.90625\n",
      "2017-11-05T22:30:13.041211: step 2245, loss 0.146627, acc 0.96875\n",
      "2017-11-05T22:30:17.018429: step 2246, loss 0.0830908, acc 0.96875\n",
      "2017-11-05T22:30:21.014639: step 2247, loss 0.0825892, acc 0.96875\n",
      "2017-11-05T22:30:24.928639: step 2248, loss 0.0474876, acc 1\n",
      "2017-11-05T22:30:28.934390: step 2249, loss 0.27268, acc 0.84375\n",
      "2017-11-05T22:30:33.069873: step 2250, loss 0.125386, acc 0.9375\n",
      "2017-11-05T22:30:37.124542: step 2251, loss 0.0602453, acc 1\n",
      "2017-11-05T22:30:41.098879: step 2252, loss 0.290565, acc 0.875\n",
      "2017-11-05T22:30:45.055440: step 2253, loss 0.134283, acc 0.96875\n",
      "2017-11-05T22:30:48.961802: step 2254, loss 0.20884, acc 0.90625\n",
      "2017-11-05T22:30:52.973903: step 2255, loss 0.33218, acc 0.875\n",
      "2017-11-05T22:30:56.914018: step 2256, loss 0.160748, acc 0.9375\n",
      "2017-11-05T22:31:00.922773: step 2257, loss 0.136416, acc 0.90625\n",
      "2017-11-05T22:31:04.912280: step 2258, loss 0.384574, acc 0.8125\n",
      "2017-11-05T22:31:08.927502: step 2259, loss 0.210981, acc 0.90625\n",
      "2017-11-05T22:31:13.025933: step 2260, loss 0.24657, acc 0.90625\n",
      "2017-11-05T22:31:17.010260: step 2261, loss 0.14661, acc 0.9375\n",
      "2017-11-05T22:31:20.995804: step 2262, loss 0.133894, acc 0.90625\n",
      "2017-11-05T22:31:24.991848: step 2263, loss 0.0371469, acc 1\n",
      "2017-11-05T22:31:28.912953: step 2264, loss 0.165274, acc 0.90625\n",
      "2017-11-05T22:31:32.852589: step 2265, loss 0.141366, acc 0.90625\n",
      "2017-11-05T22:31:36.834352: step 2266, loss 0.117727, acc 0.9375\n",
      "2017-11-05T22:31:40.794205: step 2267, loss 0.19169, acc 0.875\n",
      "2017-11-05T22:31:43.293967: step 2268, loss 0.40307, acc 0.8\n",
      "2017-11-05T22:31:47.223437: step 2269, loss 0.238965, acc 0.9375\n",
      "2017-11-05T22:31:51.177353: step 2270, loss 0.0506756, acc 0.96875\n",
      "2017-11-05T22:31:55.127858: step 2271, loss 0.126541, acc 0.90625\n",
      "2017-11-05T22:31:59.119148: step 2272, loss 0.391375, acc 0.8125\n",
      "2017-11-05T22:32:03.079012: step 2273, loss 0.105943, acc 0.96875\n",
      "2017-11-05T22:32:07.013482: step 2274, loss 0.131097, acc 0.96875\n",
      "2017-11-05T22:32:10.987812: step 2275, loss 0.352228, acc 0.90625\n",
      "2017-11-05T22:32:14.917241: step 2276, loss 0.12969, acc 0.90625\n",
      "2017-11-05T22:32:18.893756: step 2277, loss 0.0614109, acc 0.96875\n",
      "2017-11-05T22:32:22.824329: step 2278, loss 0.245967, acc 0.90625\n",
      "2017-11-05T22:32:26.740423: step 2279, loss 0.282246, acc 0.84375\n",
      "2017-11-05T22:32:30.701964: step 2280, loss 0.0986056, acc 0.96875\n",
      "2017-11-05T22:32:34.798365: step 2281, loss 0.163135, acc 0.9375\n",
      "2017-11-05T22:32:38.811208: step 2282, loss 0.208059, acc 0.90625\n",
      "2017-11-05T22:32:42.723439: step 2283, loss 0.414914, acc 0.78125\n",
      "2017-11-05T22:32:46.627276: step 2284, loss 0.0805321, acc 0.96875\n",
      "2017-11-05T22:32:50.572263: step 2285, loss 0.0917362, acc 0.96875\n",
      "2017-11-05T22:32:54.569057: step 2286, loss 0.278486, acc 0.8125\n",
      "2017-11-05T22:32:58.498325: step 2287, loss 0.25395, acc 0.9375\n",
      "2017-11-05T22:33:02.414391: step 2288, loss 0.236475, acc 0.84375\n",
      "2017-11-05T22:33:06.344455: step 2289, loss 0.23428, acc 0.875\n",
      "2017-11-05T22:33:10.331613: step 2290, loss 0.103323, acc 0.9375\n",
      "2017-11-05T22:33:14.300316: step 2291, loss 0.11882, acc 0.9375\n",
      "2017-11-05T22:33:18.257371: step 2292, loss 0.0793412, acc 0.96875\n",
      "2017-11-05T22:33:22.282236: step 2293, loss 0.138615, acc 0.96875\n",
      "2017-11-05T22:33:26.345562: step 2294, loss 0.369366, acc 0.84375\n",
      "2017-11-05T22:33:30.258956: step 2295, loss 0.389927, acc 0.875\n",
      "2017-11-05T22:33:34.138343: step 2296, loss 0.106638, acc 0.96875\n",
      "2017-11-05T22:33:38.095543: step 2297, loss 0.208844, acc 0.875\n",
      "2017-11-05T22:33:42.059093: step 2298, loss 0.155221, acc 0.9375\n",
      "2017-11-05T22:33:46.007509: step 2299, loss 0.10793, acc 0.9375\n",
      "2017-11-05T22:33:49.952146: step 2300, loss 0.180798, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:33:52.669261: step 2300, loss 0.81803, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-05T22:33:58.383309: step 2301, loss 0.168157, acc 0.9375\n",
      "2017-11-05T22:34:02.366671: step 2302, loss 0.15963, acc 0.9375\n",
      "2017-11-05T22:34:06.283714: step 2303, loss 0.333111, acc 0.9375\n",
      "2017-11-05T22:34:08.802337: step 2304, loss 0.464157, acc 0.85\n",
      "2017-11-05T22:34:12.736840: step 2305, loss 0.0338612, acc 1\n",
      "2017-11-05T22:34:16.682123: step 2306, loss 0.180569, acc 0.84375\n",
      "2017-11-05T22:34:20.623316: step 2307, loss 0.0450887, acc 0.96875\n",
      "2017-11-05T22:34:24.509973: step 2308, loss 0.193876, acc 0.90625\n",
      "2017-11-05T22:34:28.489576: step 2309, loss 0.279894, acc 0.875\n",
      "2017-11-05T22:34:32.485947: step 2310, loss 0.159408, acc 0.875\n",
      "2017-11-05T22:34:36.595169: step 2311, loss 0.2058, acc 0.96875\n",
      "2017-11-05T22:34:40.526593: step 2312, loss 0.162873, acc 0.9375\n",
      "2017-11-05T22:34:44.436450: step 2313, loss 0.370845, acc 0.84375\n",
      "2017-11-05T22:34:48.353847: step 2314, loss 0.136627, acc 0.9375\n",
      "2017-11-05T22:34:52.304230: step 2315, loss 0.172162, acc 0.90625\n",
      "2017-11-05T22:34:56.188976: step 2316, loss 0.177112, acc 0.875\n",
      "2017-11-05T22:35:00.139574: step 2317, loss 0.286418, acc 0.875\n",
      "2017-11-05T22:35:04.084098: step 2318, loss 0.100698, acc 0.9375\n",
      "2017-11-05T22:35:07.994717: step 2319, loss 0.0694135, acc 0.96875\n",
      "2017-11-05T22:35:11.910321: step 2320, loss 0.110416, acc 0.96875\n",
      "2017-11-05T22:35:15.888158: step 2321, loss 0.220919, acc 0.9375\n",
      "2017-11-05T22:35:19.818061: step 2322, loss 0.459957, acc 0.84375\n",
      "2017-11-05T22:35:23.749924: step 2323, loss 0.184694, acc 0.9375\n",
      "2017-11-05T22:35:27.655354: step 2324, loss 0.69635, acc 0.78125\n",
      "2017-11-05T22:35:31.597195: step 2325, loss 0.342865, acc 0.90625\n",
      "2017-11-05T22:35:35.522303: step 2326, loss 0.258319, acc 0.90625\n",
      "2017-11-05T22:35:39.470891: step 2327, loss 0.0869849, acc 1\n",
      "2017-11-05T22:35:43.406580: step 2328, loss 0.46977, acc 0.8125\n",
      "2017-11-05T22:35:47.334759: step 2329, loss 0.201337, acc 0.90625\n",
      "2017-11-05T22:35:51.300007: step 2330, loss 0.21816, acc 0.9375\n",
      "2017-11-05T22:35:55.210442: step 2331, loss 0.181561, acc 0.96875\n",
      "2017-11-05T22:35:59.208746: step 2332, loss 0.173979, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T22:36:03.137437: step 2333, loss 0.142033, acc 0.9375\n",
      "2017-11-05T22:36:07.111696: step 2334, loss 0.111554, acc 0.9375\n",
      "2017-11-05T22:36:11.090970: step 2335, loss 0.231035, acc 0.875\n",
      "2017-11-05T22:36:15.005021: step 2336, loss 0.417393, acc 0.875\n",
      "2017-11-05T22:36:18.966049: step 2337, loss 0.186492, acc 0.90625\n",
      "2017-11-05T22:36:22.917731: step 2338, loss 0.193349, acc 0.90625\n",
      "2017-11-05T22:36:26.869148: step 2339, loss 0.325954, acc 0.875\n",
      "2017-11-05T22:36:29.349518: step 2340, loss 0.0707346, acc 0.95\n",
      "2017-11-05T22:36:33.379940: step 2341, loss 0.174652, acc 0.875\n",
      "2017-11-05T22:36:37.422491: step 2342, loss 0.515405, acc 0.71875\n",
      "2017-11-05T22:36:41.350808: step 2343, loss 0.206577, acc 0.9375\n",
      "2017-11-05T22:36:45.258050: step 2344, loss 0.107934, acc 0.96875\n",
      "2017-11-05T22:36:49.167368: step 2345, loss 0.223501, acc 0.84375\n",
      "2017-11-05T22:36:53.103913: step 2346, loss 0.0752531, acc 0.96875\n",
      "2017-11-05T22:36:57.063529: step 2347, loss 0.319158, acc 0.875\n",
      "2017-11-05T22:37:01.048870: step 2348, loss 0.352554, acc 0.875\n",
      "2017-11-05T22:37:05.048947: step 2349, loss 0.186131, acc 0.9375\n",
      "2017-11-05T22:37:08.951640: step 2350, loss 0.130733, acc 0.90625\n",
      "2017-11-05T22:37:12.912903: step 2351, loss 0.126191, acc 0.9375\n",
      "2017-11-05T22:37:16.905731: step 2352, loss 0.231728, acc 0.9375\n",
      "2017-11-05T22:37:20.875916: step 2353, loss 0.0649085, acc 0.96875\n",
      "2017-11-05T22:37:24.782552: step 2354, loss 0.322165, acc 0.84375\n",
      "2017-11-05T22:37:28.697694: step 2355, loss 0.175992, acc 0.9375\n",
      "2017-11-05T22:37:32.644725: step 2356, loss 0.211351, acc 0.90625\n",
      "2017-11-05T22:37:36.557162: step 2357, loss 0.110674, acc 0.9375\n",
      "2017-11-05T22:37:40.555805: step 2358, loss 0.290799, acc 0.875\n",
      "2017-11-05T22:37:44.497260: step 2359, loss 0.0336848, acc 1\n",
      "2017-11-05T22:37:48.397555: step 2360, loss 0.0205823, acc 1\n",
      "2017-11-05T22:37:52.351448: step 2361, loss 0.163685, acc 0.90625\n",
      "2017-11-05T22:37:56.301564: step 2362, loss 0.176434, acc 0.84375\n",
      "2017-11-05T22:38:00.273383: step 2363, loss 0.103933, acc 0.9375\n",
      "2017-11-05T22:38:04.271825: step 2364, loss 0.149357, acc 0.9375\n",
      "2017-11-05T22:38:08.189796: step 2365, loss 0.196717, acc 0.9375\n",
      "2017-11-05T22:38:12.178191: step 2366, loss 0.298513, acc 0.84375\n",
      "2017-11-05T22:38:16.130212: step 2367, loss 0.179968, acc 0.9375\n",
      "2017-11-05T22:38:20.095924: step 2368, loss 0.243639, acc 0.90625\n",
      "2017-11-05T22:38:24.155421: step 2369, loss 0.198242, acc 0.90625\n",
      "2017-11-05T22:38:28.336713: step 2370, loss 0.366738, acc 0.84375\n",
      "2017-11-05T22:38:32.329162: step 2371, loss 0.0749455, acc 1\n",
      "2017-11-05T22:38:36.421415: step 2372, loss 0.151659, acc 0.9375\n",
      "2017-11-05T22:38:40.354183: step 2373, loss 0.0597157, acc 0.96875\n",
      "2017-11-05T22:38:44.296155: step 2374, loss 0.171151, acc 0.9375\n",
      "2017-11-05T22:38:48.215717: step 2375, loss 0.240519, acc 0.9375\n",
      "2017-11-05T22:38:50.735683: step 2376, loss 0.304675, acc 0.85\n",
      "2017-11-05T22:38:54.677962: step 2377, loss 0.188925, acc 0.9375\n",
      "2017-11-05T22:38:58.598468: step 2378, loss 0.140975, acc 0.90625\n",
      "2017-11-05T22:39:02.557111: step 2379, loss 0.266823, acc 0.90625\n",
      "2017-11-05T22:39:06.462664: step 2380, loss 0.154624, acc 0.9375\n",
      "2017-11-05T22:39:10.418374: step 2381, loss 0.14243, acc 0.9375\n",
      "2017-11-05T22:39:14.337755: step 2382, loss 0.16991, acc 0.90625\n",
      "2017-11-05T22:39:18.345110: step 2383, loss 0.0970124, acc 0.96875\n",
      "2017-11-05T22:39:22.275004: step 2384, loss 0.25625, acc 0.875\n",
      "2017-11-05T22:39:26.223455: step 2385, loss 0.0836745, acc 0.96875\n",
      "2017-11-05T22:39:30.105036: step 2386, loss 0.156486, acc 0.96875\n",
      "2017-11-05T22:39:34.056870: step 2387, loss 0.131623, acc 0.9375\n",
      "2017-11-05T22:39:38.003069: step 2388, loss 0.127693, acc 0.9375\n",
      "2017-11-05T22:39:42.061375: step 2389, loss 0.229716, acc 0.875\n",
      "2017-11-05T22:39:46.071377: step 2390, loss 0.250709, acc 0.875\n",
      "2017-11-05T22:39:50.045082: step 2391, loss 0.235449, acc 0.84375\n",
      "2017-11-05T22:39:53.990335: step 2392, loss 0.228282, acc 0.90625\n",
      "2017-11-05T22:39:57.978528: step 2393, loss 0.101504, acc 0.9375\n",
      "2017-11-05T22:40:02.239280: step 2394, loss 0.236273, acc 0.9375\n",
      "2017-11-05T22:40:06.243297: step 2395, loss 0.0560639, acc 1\n",
      "2017-11-05T22:40:10.238797: step 2396, loss 0.0796361, acc 0.9375\n",
      "2017-11-05T22:40:14.184423: step 2397, loss 0.198512, acc 0.84375\n",
      "2017-11-05T22:40:18.167896: step 2398, loss 0.275881, acc 0.875\n",
      "2017-11-05T22:40:22.128958: step 2399, loss 0.197043, acc 0.875\n",
      "2017-11-05T22:40:26.059394: step 2400, loss 0.206957, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:40:28.581035: step 2400, loss 0.876799, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-05T22:40:33.921963: step 2401, loss 0.234414, acc 0.90625\n",
      "2017-11-05T22:40:37.904736: step 2402, loss 0.213299, acc 0.90625\n",
      "2017-11-05T22:40:41.893136: step 2403, loss 0.13567, acc 0.9375\n",
      "2017-11-05T22:40:45.817932: step 2404, loss 0.0657718, acc 1\n",
      "2017-11-05T22:40:49.766444: step 2405, loss 0.270009, acc 0.84375\n",
      "2017-11-05T22:40:53.686453: step 2406, loss 0.0718083, acc 0.96875\n",
      "2017-11-05T22:40:57.612266: step 2407, loss 0.169091, acc 0.9375\n",
      "2017-11-05T22:41:01.587832: step 2408, loss 0.460849, acc 0.84375\n",
      "2017-11-05T22:41:05.541434: step 2409, loss 0.317253, acc 0.875\n",
      "2017-11-05T22:41:09.618499: step 2410, loss 0.0565078, acc 1\n",
      "2017-11-05T22:41:14.618800: step 2411, loss 0.126295, acc 0.96875\n",
      "2017-11-05T22:41:17.462299: step 2412, loss 0.172593, acc 0.9\n",
      "2017-11-05T22:41:21.538822: step 2413, loss 0.170398, acc 0.9375\n",
      "2017-11-05T22:41:25.581046: step 2414, loss 0.151298, acc 0.90625\n",
      "2017-11-05T22:41:29.678526: step 2415, loss 0.131929, acc 0.9375\n",
      "2017-11-05T22:41:33.687060: step 2416, loss 0.209228, acc 0.90625\n",
      "2017-11-05T22:41:37.728810: step 2417, loss 0.124846, acc 0.96875\n",
      "2017-11-05T22:41:41.862259: step 2418, loss 0.176876, acc 0.90625\n",
      "2017-11-05T22:41:45.860110: step 2419, loss 0.459492, acc 0.875\n",
      "2017-11-05T22:41:49.886151: step 2420, loss 0.0941101, acc 0.9375\n",
      "2017-11-05T22:41:53.868101: step 2421, loss 0.351913, acc 0.84375\n",
      "2017-11-05T22:41:57.790933: step 2422, loss 0.188587, acc 0.9375\n",
      "2017-11-05T22:42:01.737645: step 2423, loss 0.300799, acc 0.8125\n",
      "2017-11-05T22:42:05.737931: step 2424, loss 0.319539, acc 0.84375\n",
      "2017-11-05T22:42:09.744372: step 2425, loss 0.142896, acc 0.9375\n",
      "2017-11-05T22:42:13.700133: step 2426, loss 0.166693, acc 0.9375\n",
      "2017-11-05T22:42:17.699436: step 2427, loss 0.0917491, acc 0.96875\n",
      "2017-11-05T22:42:21.721965: step 2428, loss 0.128749, acc 0.9375\n",
      "2017-11-05T22:42:25.681172: step 2429, loss 0.14107, acc 0.9375\n",
      "2017-11-05T22:42:29.595688: step 2430, loss 0.0806974, acc 0.9375\n",
      "2017-11-05T22:42:33.627701: step 2431, loss 0.095037, acc 0.96875\n",
      "2017-11-05T22:42:37.612109: step 2432, loss 0.152154, acc 0.9375\n",
      "2017-11-05T22:42:41.652025: step 2433, loss 0.169602, acc 0.90625\n",
      "2017-11-05T22:42:45.635267: step 2434, loss 0.0660833, acc 0.96875\n",
      "2017-11-05T22:42:49.606054: step 2435, loss 0.153432, acc 0.90625\n",
      "2017-11-05T22:42:53.547976: step 2436, loss 0.145972, acc 0.9375\n",
      "2017-11-05T22:42:57.514845: step 2437, loss 0.354193, acc 0.875\n",
      "2017-11-05T22:43:01.543850: step 2438, loss 0.200334, acc 0.9375\n",
      "2017-11-05T22:43:05.517806: step 2439, loss 0.0080875, acc 1\n",
      "2017-11-05T22:43:09.493947: step 2440, loss 0.0811237, acc 0.96875\n",
      "2017-11-05T22:43:13.441662: step 2441, loss 0.230297, acc 0.84375\n",
      "2017-11-05T22:43:17.401768: step 2442, loss 0.20422, acc 0.90625\n",
      "2017-11-05T22:43:21.486453: step 2443, loss 0.145581, acc 0.96875\n",
      "2017-11-05T22:43:25.531992: step 2444, loss 0.179685, acc 0.84375\n",
      "2017-11-05T22:43:29.486843: step 2445, loss 0.298447, acc 0.84375\n",
      "2017-11-05T22:43:33.434501: step 2446, loss 0.222641, acc 0.9375\n",
      "2017-11-05T22:43:37.407146: step 2447, loss 0.158403, acc 0.9375\n",
      "2017-11-05T22:43:40.044888: step 2448, loss 0.251264, acc 0.85\n",
      "2017-11-05T22:43:44.038663: step 2449, loss 0.0921208, acc 0.9375\n",
      "2017-11-05T22:43:48.045199: step 2450, loss 0.163402, acc 0.9375\n",
      "2017-11-05T22:43:52.030011: step 2451, loss 0.133691, acc 0.96875\n",
      "2017-11-05T22:43:56.128168: step 2452, loss 0.1466, acc 0.90625\n",
      "2017-11-05T22:44:00.067985: step 2453, loss 0.0529856, acc 0.96875\n",
      "2017-11-05T22:44:04.047524: step 2454, loss 0.149434, acc 0.9375\n",
      "2017-11-05T22:44:08.027789: step 2455, loss 0.125214, acc 0.9375\n",
      "2017-11-05T22:44:12.085182: step 2456, loss 0.0910107, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T22:44:16.060394: step 2457, loss 0.0975984, acc 0.90625\n",
      "2017-11-05T22:44:20.000122: step 2458, loss 0.42583, acc 0.8125\n",
      "2017-11-05T22:44:24.008064: step 2459, loss 0.164637, acc 0.875\n",
      "2017-11-05T22:44:27.984010: step 2460, loss 0.118635, acc 0.96875\n",
      "2017-11-05T22:44:31.890234: step 2461, loss 0.211004, acc 0.90625\n",
      "2017-11-05T22:44:36.022529: step 2462, loss 0.274664, acc 0.90625\n",
      "2017-11-05T22:44:39.954611: step 2463, loss 0.246971, acc 0.90625\n",
      "2017-11-05T22:44:43.943802: step 2464, loss 0.142506, acc 0.9375\n",
      "2017-11-05T22:44:47.918643: step 2465, loss 0.291787, acc 0.78125\n",
      "2017-11-05T22:44:51.868299: step 2466, loss 0.186188, acc 0.90625\n",
      "2017-11-05T22:44:55.829413: step 2467, loss 0.325852, acc 0.84375\n",
      "2017-11-05T22:44:59.807142: step 2468, loss 0.194577, acc 0.9375\n",
      "2017-11-05T22:45:03.797893: step 2469, loss 0.208339, acc 0.90625\n",
      "2017-11-05T22:45:07.774595: step 2470, loss 0.276817, acc 0.875\n",
      "2017-11-05T22:45:11.829434: step 2471, loss 0.355974, acc 0.84375\n",
      "2017-11-05T22:45:15.830282: step 2472, loss 0.261544, acc 0.875\n",
      "2017-11-05T22:45:19.824930: step 2473, loss 0.111209, acc 0.96875\n",
      "2017-11-05T22:45:23.903848: step 2474, loss 0.322486, acc 0.9375\n",
      "2017-11-05T22:45:27.827613: step 2475, loss 0.155275, acc 0.9375\n",
      "2017-11-05T22:45:31.792236: step 2476, loss 0.131187, acc 0.96875\n",
      "2017-11-05T22:45:35.769376: step 2477, loss 0.0095479, acc 1\n",
      "2017-11-05T22:45:39.728592: step 2478, loss 0.507814, acc 0.8125\n",
      "2017-11-05T22:45:43.769327: step 2479, loss 0.182646, acc 0.90625\n",
      "2017-11-05T22:45:47.745059: step 2480, loss 0.109795, acc 0.9375\n",
      "2017-11-05T22:45:51.737730: step 2481, loss 0.212235, acc 0.875\n",
      "2017-11-05T22:45:55.686524: step 2482, loss 0.293313, acc 0.84375\n",
      "2017-11-05T22:45:59.651768: step 2483, loss 0.0890289, acc 0.96875\n",
      "2017-11-05T22:46:02.206707: step 2484, loss 0.182856, acc 0.95\n",
      "2017-11-05T22:46:06.200589: step 2485, loss 0.225174, acc 0.90625\n",
      "2017-11-05T22:46:10.230923: step 2486, loss 0.10764, acc 0.96875\n",
      "2017-11-05T22:46:14.307108: step 2487, loss 0.237953, acc 0.90625\n",
      "2017-11-05T22:46:18.227036: step 2488, loss 0.330338, acc 0.84375\n",
      "2017-11-05T22:46:22.242728: step 2489, loss 0.183948, acc 0.90625\n",
      "2017-11-05T22:46:26.183617: step 2490, loss 0.144218, acc 0.9375\n",
      "2017-11-05T22:46:30.098291: step 2491, loss 0.317647, acc 0.875\n",
      "2017-11-05T22:46:34.206095: step 2492, loss 0.245043, acc 0.875\n",
      "2017-11-05T22:46:38.353733: step 2493, loss 0.0865729, acc 0.96875\n",
      "2017-11-05T22:46:42.425404: step 2494, loss 0.131014, acc 0.9375\n",
      "2017-11-05T22:46:46.392151: step 2495, loss 0.124663, acc 0.9375\n",
      "2017-11-05T22:46:50.334086: step 2496, loss 0.058234, acc 1\n",
      "2017-11-05T22:46:54.333080: step 2497, loss 0.0958335, acc 0.96875\n",
      "2017-11-05T22:46:58.262737: step 2498, loss 0.208057, acc 0.9375\n",
      "2017-11-05T22:47:02.206544: step 2499, loss 0.229041, acc 0.9375\n",
      "2017-11-05T22:47:06.231419: step 2500, loss 0.124715, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:47:08.738091: step 2500, loss 0.82553, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-05T22:47:14.034370: step 2501, loss 0.011573, acc 1\n",
      "2017-11-05T22:47:18.073952: step 2502, loss 0.190755, acc 0.90625\n",
      "2017-11-05T22:47:22.017112: step 2503, loss 0.0502428, acc 0.96875\n",
      "2017-11-05T22:47:26.078170: step 2504, loss 0.103104, acc 0.96875\n",
      "2017-11-05T22:47:30.050546: step 2505, loss 0.212836, acc 0.9375\n",
      "2017-11-05T22:47:34.032102: step 2506, loss 0.239524, acc 0.84375\n",
      "2017-11-05T22:47:38.054799: step 2507, loss 0.266009, acc 0.8125\n",
      "2017-11-05T22:47:42.041337: step 2508, loss 0.208953, acc 0.90625\n",
      "2017-11-05T22:47:46.019243: step 2509, loss 0.392549, acc 0.84375\n",
      "2017-11-05T22:47:49.991964: step 2510, loss 0.00663621, acc 1\n",
      "2017-11-05T22:47:54.018646: step 2511, loss 0.126215, acc 0.96875\n",
      "2017-11-05T22:47:57.902797: step 2512, loss 0.0938213, acc 0.96875\n",
      "2017-11-05T22:48:01.965369: step 2513, loss 0.262784, acc 0.90625\n",
      "2017-11-05T22:48:05.874917: step 2514, loss 0.31728, acc 0.875\n",
      "2017-11-05T22:48:09.871359: step 2515, loss 0.112546, acc 0.90625\n",
      "2017-11-05T22:48:13.821501: step 2516, loss 0.425966, acc 0.875\n",
      "2017-11-05T22:48:17.757328: step 2517, loss 0.130766, acc 0.9375\n",
      "2017-11-05T22:48:21.812464: step 2518, loss 0.234755, acc 0.9375\n",
      "2017-11-05T22:48:25.930471: step 2519, loss 0.129617, acc 0.90625\n",
      "2017-11-05T22:48:28.510596: step 2520, loss 0.154871, acc 0.9\n",
      "2017-11-05T22:48:32.551398: step 2521, loss 0.0826217, acc 0.96875\n",
      "2017-11-05T22:48:36.651085: step 2522, loss 0.147145, acc 0.875\n",
      "2017-11-05T22:48:40.730129: step 2523, loss 0.0991944, acc 0.9375\n",
      "2017-11-05T22:48:44.929585: step 2524, loss 0.405326, acc 0.84375\n",
      "2017-11-05T22:48:49.175992: step 2525, loss 0.0925527, acc 0.9375\n",
      "2017-11-05T22:48:53.185501: step 2526, loss 0.0341153, acc 1\n",
      "2017-11-05T22:48:57.369020: step 2527, loss 0.274445, acc 0.84375\n",
      "2017-11-05T22:49:01.393251: step 2528, loss 0.296411, acc 0.78125\n",
      "2017-11-05T22:49:05.560906: step 2529, loss 0.0795918, acc 0.96875\n",
      "2017-11-05T22:49:09.608623: step 2530, loss 0.246058, acc 0.875\n",
      "2017-11-05T22:49:13.780557: step 2531, loss 0.371749, acc 0.875\n",
      "2017-11-05T22:49:17.811376: step 2532, loss 0.145999, acc 0.875\n",
      "2017-11-05T22:49:21.966989: step 2533, loss 0.00848812, acc 1\n",
      "2017-11-05T22:49:26.030597: step 2534, loss 0.0503732, acc 1\n",
      "2017-11-05T22:49:30.150915: step 2535, loss 0.0974725, acc 0.96875\n",
      "2017-11-05T22:49:34.194470: step 2536, loss 0.281859, acc 0.90625\n",
      "2017-11-05T22:49:38.290791: step 2537, loss 0.340534, acc 0.84375\n",
      "2017-11-05T22:49:42.295980: step 2538, loss 0.0897966, acc 0.9375\n",
      "2017-11-05T22:49:46.262388: step 2539, loss 0.268297, acc 0.875\n",
      "2017-11-05T22:49:50.202671: step 2540, loss 0.192847, acc 0.90625\n",
      "2017-11-05T22:49:54.153275: step 2541, loss 0.224144, acc 0.875\n",
      "2017-11-05T22:49:58.118421: step 2542, loss 0.228585, acc 0.875\n",
      "2017-11-05T22:50:02.472062: step 2543, loss 0.169645, acc 0.90625\n",
      "2017-11-05T22:50:06.527842: step 2544, loss 0.104509, acc 0.96875\n",
      "2017-11-05T22:50:10.534507: step 2545, loss 0.220927, acc 0.875\n",
      "2017-11-05T22:50:14.597386: step 2546, loss 0.228381, acc 0.90625\n",
      "2017-11-05T22:50:18.634316: step 2547, loss 0.242363, acc 0.90625\n",
      "2017-11-05T22:50:22.603358: step 2548, loss 0.203057, acc 0.875\n",
      "2017-11-05T22:50:26.561703: step 2549, loss 0.365684, acc 0.8125\n",
      "2017-11-05T22:50:30.530060: step 2550, loss 0.283303, acc 0.90625\n",
      "2017-11-05T22:50:34.641327: step 2551, loss 0.222566, acc 0.875\n",
      "2017-11-05T22:50:38.678814: step 2552, loss 0.167497, acc 0.9375\n",
      "2017-11-05T22:50:42.641699: step 2553, loss 0.116179, acc 0.96875\n",
      "2017-11-05T22:50:46.618473: step 2554, loss 0.0541708, acc 0.96875\n",
      "2017-11-05T22:50:50.559255: step 2555, loss 0.165247, acc 0.875\n",
      "2017-11-05T22:50:53.106316: step 2556, loss 0.230728, acc 0.9\n",
      "2017-11-05T22:50:57.115567: step 2557, loss 0.196938, acc 0.875\n",
      "2017-11-05T22:51:01.073262: step 2558, loss 0.156626, acc 0.9375\n",
      "2017-11-05T22:51:05.071019: step 2559, loss 0.0518393, acc 0.96875\n",
      "2017-11-05T22:51:09.080239: step 2560, loss 0.157345, acc 0.90625\n",
      "2017-11-05T22:51:13.065246: step 2561, loss 0.302349, acc 0.875\n",
      "2017-11-05T22:51:17.012135: step 2562, loss 0.111534, acc 0.96875\n",
      "2017-11-05T22:51:21.048297: step 2563, loss 0.11799, acc 0.9375\n",
      "2017-11-05T22:51:24.999781: step 2564, loss 0.197536, acc 0.90625\n",
      "2017-11-05T22:51:28.960123: step 2565, loss 0.0700571, acc 0.96875\n",
      "2017-11-05T22:51:32.908631: step 2566, loss 0.312634, acc 0.84375\n",
      "2017-11-05T22:51:36.840706: step 2567, loss 0.175846, acc 0.9375\n",
      "2017-11-05T22:51:40.895289: step 2568, loss 0.39455, acc 0.84375\n",
      "2017-11-05T22:51:44.891393: step 2569, loss 0.115294, acc 0.9375\n",
      "2017-11-05T22:51:48.931139: step 2570, loss 0.0537839, acc 1\n",
      "2017-11-05T22:51:52.985995: step 2571, loss 0.113332, acc 0.90625\n",
      "2017-11-05T22:51:56.964945: step 2572, loss 0.226776, acc 0.875\n",
      "2017-11-05T22:52:00.931373: step 2573, loss 0.197849, acc 0.875\n",
      "2017-11-05T22:52:04.919301: step 2574, loss 0.356701, acc 0.875\n",
      "2017-11-05T22:52:08.845985: step 2575, loss 0.256463, acc 0.90625\n",
      "2017-11-05T22:52:12.864473: step 2576, loss 0.14385, acc 0.90625\n",
      "2017-11-05T22:52:16.868723: step 2577, loss 0.123881, acc 0.9375\n",
      "2017-11-05T22:52:20.833589: step 2578, loss 0.249478, acc 0.875\n",
      "2017-11-05T22:52:24.851323: step 2579, loss 0.120676, acc 0.96875\n",
      "2017-11-05T22:52:28.807884: step 2580, loss 0.0382707, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T22:52:32.846424: step 2581, loss 0.0923323, acc 0.96875\n",
      "2017-11-05T22:52:36.918768: step 2582, loss 0.27436, acc 0.90625\n",
      "2017-11-05T22:52:40.837689: step 2583, loss 0.135061, acc 0.9375\n",
      "2017-11-05T22:52:44.827536: step 2584, loss 0.107427, acc 0.90625\n",
      "2017-11-05T22:52:48.772214: step 2585, loss 0.0930187, acc 0.9375\n",
      "2017-11-05T22:52:52.705489: step 2586, loss 0.330279, acc 0.875\n",
      "2017-11-05T22:52:56.708569: step 2587, loss 0.0811388, acc 0.96875\n",
      "2017-11-05T22:53:00.672064: step 2588, loss 0.151164, acc 0.9375\n",
      "2017-11-05T22:53:04.607973: step 2589, loss 0.254425, acc 0.84375\n",
      "2017-11-05T22:53:08.521350: step 2590, loss 0.0531191, acc 1\n",
      "2017-11-05T22:53:12.596654: step 2591, loss 0.0201617, acc 1\n",
      "2017-11-05T22:53:15.073117: step 2592, loss 0.507326, acc 0.85\n",
      "2017-11-05T22:53:19.051065: step 2593, loss 0.0365321, acc 1\n",
      "2017-11-05T22:53:23.183583: step 2594, loss 0.157275, acc 0.90625\n",
      "2017-11-05T22:53:27.272962: step 2595, loss 0.194642, acc 0.9375\n",
      "2017-11-05T22:53:31.189236: step 2596, loss 0.0533998, acc 0.96875\n",
      "2017-11-05T22:53:35.226090: step 2597, loss 0.107005, acc 0.90625\n",
      "2017-11-05T22:53:39.193951: step 2598, loss 0.219897, acc 0.90625\n",
      "2017-11-05T22:53:43.161403: step 2599, loss 0.138528, acc 0.9375\n",
      "2017-11-05T22:53:47.142536: step 2600, loss 0.0438263, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T22:53:49.711024: step 2600, loss 0.814999, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-05T22:53:54.972745: step 2601, loss 0.426194, acc 0.71875\n",
      "2017-11-05T22:53:59.026325: step 2602, loss 0.205059, acc 0.90625\n",
      "2017-11-05T22:54:03.007892: step 2603, loss 0.136124, acc 0.9375\n",
      "2017-11-05T22:54:07.041494: step 2604, loss 0.120802, acc 0.9375\n",
      "2017-11-05T22:54:11.105535: step 2605, loss 0.233774, acc 0.8125\n",
      "2017-11-05T22:54:15.054160: step 2606, loss 0.0854433, acc 0.96875\n",
      "2017-11-05T22:54:19.067720: step 2607, loss 0.0623547, acc 1\n",
      "2017-11-05T22:54:23.106078: step 2608, loss 0.134642, acc 0.96875\n",
      "2017-11-05T22:54:27.112993: step 2609, loss 0.121508, acc 0.9375\n",
      "2017-11-05T22:54:31.087109: step 2610, loss 0.0940584, acc 0.96875\n",
      "2017-11-05T22:54:35.241959: step 2611, loss 0.121781, acc 0.96875\n",
      "2017-11-05T22:54:39.264676: step 2612, loss 0.056127, acc 0.96875\n",
      "2017-11-05T22:54:43.305054: step 2613, loss 0.18156, acc 0.9375\n",
      "2017-11-05T22:54:47.341799: step 2614, loss 0.193439, acc 0.875\n",
      "2017-11-05T22:54:51.366151: step 2615, loss 0.225937, acc 0.875\n",
      "2017-11-05T22:54:55.421384: step 2616, loss 0.158689, acc 0.90625\n",
      "2017-11-05T22:54:59.465988: step 2617, loss 0.647711, acc 0.84375\n",
      "2017-11-05T22:55:03.425011: step 2618, loss 0.213075, acc 0.9375\n",
      "2017-11-05T22:55:07.451871: step 2619, loss 0.431923, acc 0.8125\n",
      "2017-11-05T22:55:11.491440: step 2620, loss 0.097303, acc 0.9375\n",
      "2017-11-05T22:55:15.477496: step 2621, loss 0.212646, acc 0.90625\n",
      "2017-11-05T22:55:19.600372: step 2622, loss 0.107826, acc 0.96875\n",
      "2017-11-05T22:55:23.688002: step 2623, loss 0.21207, acc 0.90625\n",
      "2017-11-05T22:55:27.728963: step 2624, loss 0.255311, acc 0.84375\n",
      "2017-11-05T22:55:31.736094: step 2625, loss 0.111784, acc 0.96875\n",
      "2017-11-05T22:55:35.671548: step 2626, loss 0.212611, acc 0.90625\n",
      "2017-11-05T22:55:39.696528: step 2627, loss 0.136958, acc 0.9375\n",
      "2017-11-05T22:55:42.282496: step 2628, loss 0.181566, acc 0.85\n",
      "2017-11-05T22:55:46.301185: step 2629, loss 0.0438797, acc 1\n",
      "2017-11-05T22:55:50.349088: step 2630, loss 0.208912, acc 0.90625\n",
      "2017-11-05T22:55:54.406453: step 2631, loss 0.0357261, acc 1\n",
      "2017-11-05T22:55:58.471384: step 2632, loss 0.0747764, acc 0.96875\n",
      "2017-11-05T22:56:02.520100: step 2633, loss 0.119307, acc 0.9375\n",
      "2017-11-05T22:56:06.540853: step 2634, loss 0.237947, acc 0.875\n",
      "2017-11-05T22:56:10.607920: step 2635, loss 0.329597, acc 0.84375\n",
      "2017-11-05T22:56:14.663451: step 2636, loss 0.409114, acc 0.84375\n",
      "2017-11-05T22:56:18.736426: step 2637, loss 0.0991722, acc 0.9375\n",
      "2017-11-05T22:56:22.822874: step 2638, loss 0.0287606, acc 1\n",
      "2017-11-05T22:56:26.865771: step 2639, loss 0.295819, acc 0.90625\n",
      "2017-11-05T22:56:30.925586: step 2640, loss 0.0337548, acc 1\n",
      "2017-11-05T22:56:35.066153: step 2641, loss 0.101863, acc 0.9375\n",
      "2017-11-05T22:56:39.210978: step 2642, loss 0.0121387, acc 1\n",
      "2017-11-05T22:56:43.186933: step 2643, loss 0.282349, acc 0.875\n",
      "2017-11-05T22:56:47.120001: step 2644, loss 0.197332, acc 0.90625\n",
      "2017-11-05T22:56:51.163582: step 2645, loss 0.289306, acc 0.84375\n",
      "2017-11-05T22:56:55.194410: step 2646, loss 0.199676, acc 0.9375\n",
      "2017-11-05T22:56:59.257878: step 2647, loss 0.197092, acc 0.875\n",
      "2017-11-05T22:57:03.344815: step 2648, loss 0.375082, acc 0.8125\n",
      "2017-11-05T22:57:07.340008: step 2649, loss 0.115743, acc 0.96875\n",
      "2017-11-05T22:57:11.367114: step 2650, loss 0.340438, acc 0.8125\n",
      "2017-11-05T22:57:15.435261: step 2651, loss 0.250709, acc 0.875\n",
      "2017-11-05T22:57:19.382168: step 2652, loss 0.291493, acc 0.875\n",
      "2017-11-05T22:57:23.392975: step 2653, loss 0.090104, acc 0.9375\n",
      "2017-11-05T22:57:27.451097: step 2654, loss 0.283125, acc 0.84375\n",
      "2017-11-05T22:57:31.477817: step 2655, loss 0.180932, acc 0.875\n",
      "2017-11-05T22:57:35.485680: step 2656, loss 0.336, acc 0.875\n",
      "2017-11-05T22:57:39.445281: step 2657, loss 0.295128, acc 0.875\n",
      "2017-11-05T22:57:43.484927: step 2658, loss 0.208528, acc 0.875\n",
      "2017-11-05T22:57:47.468636: step 2659, loss 0.0868114, acc 0.96875\n",
      "2017-11-05T22:57:51.505266: step 2660, loss 0.0988642, acc 0.9375\n",
      "2017-11-05T22:57:55.529383: step 2661, loss 0.189554, acc 0.9375\n",
      "2017-11-05T22:57:59.522416: step 2662, loss 0.0692337, acc 0.96875\n",
      "2017-11-05T22:58:03.514535: step 2663, loss 0.18007, acc 0.9375\n",
      "2017-11-05T22:58:06.054481: step 2664, loss 0.068751, acc 1\n",
      "2017-11-05T22:58:10.081477: step 2665, loss 0.0318214, acc 0.96875\n",
      "2017-11-05T22:58:14.159855: step 2666, loss 0.0785445, acc 0.9375\n",
      "2017-11-05T22:58:18.131201: step 2667, loss 0.0658, acc 0.96875\n",
      "2017-11-05T22:58:22.144312: step 2668, loss 0.124586, acc 0.9375\n",
      "2017-11-05T22:58:26.353172: step 2669, loss 0.00751036, acc 1\n",
      "2017-11-05T22:58:30.715055: step 2670, loss 0.18438, acc 0.875\n",
      "2017-11-05T22:58:34.893698: step 2671, loss 0.161266, acc 0.90625\n",
      "2017-11-05T22:58:38.864005: step 2672, loss 0.172491, acc 0.90625\n",
      "2017-11-05T22:58:42.952616: step 2673, loss 0.0995179, acc 0.9375\n",
      "2017-11-05T22:58:46.895611: step 2674, loss 0.276496, acc 0.8125\n",
      "2017-11-05T22:58:50.901996: step 2675, loss 0.0972064, acc 0.96875\n",
      "2017-11-05T22:58:54.848878: step 2676, loss 0.239363, acc 0.84375\n",
      "2017-11-05T22:58:58.767916: step 2677, loss 0.147115, acc 0.9375\n",
      "2017-11-05T22:59:02.791189: step 2678, loss 0.141418, acc 0.9375\n",
      "2017-11-05T22:59:06.842668: step 2679, loss 0.360431, acc 0.84375\n",
      "2017-11-05T22:59:10.858544: step 2680, loss 0.1293, acc 0.96875\n",
      "2017-11-05T22:59:14.769930: step 2681, loss 0.269864, acc 0.875\n",
      "2017-11-05T22:59:18.781803: step 2682, loss 0.262735, acc 0.84375\n",
      "2017-11-05T22:59:22.775980: step 2683, loss 0.164132, acc 0.9375\n",
      "2017-11-05T22:59:26.768393: step 2684, loss 0.166671, acc 0.875\n",
      "2017-11-05T22:59:30.810607: step 2685, loss 0.260033, acc 0.8125\n",
      "2017-11-05T22:59:34.883221: step 2686, loss 0.502545, acc 0.75\n",
      "2017-11-05T22:59:38.856004: step 2687, loss 0.0627499, acc 0.96875\n",
      "2017-11-05T22:59:42.857529: step 2688, loss 0.0645366, acc 0.96875\n",
      "2017-11-05T22:59:46.762672: step 2689, loss 0.0959594, acc 0.9375\n",
      "2017-11-05T22:59:50.749475: step 2690, loss 0.39824, acc 0.84375\n",
      "2017-11-05T22:59:54.738822: step 2691, loss 0.409322, acc 0.78125\n",
      "2017-11-05T22:59:58.733847: step 2692, loss 0.346469, acc 0.875\n",
      "2017-11-05T23:00:03.010733: step 2693, loss 0.114545, acc 0.90625\n",
      "2017-11-05T23:00:07.020276: step 2694, loss 0.321931, acc 0.8125\n",
      "2017-11-05T23:00:11.034054: step 2695, loss 0.249573, acc 0.875\n",
      "2017-11-05T23:00:15.061287: step 2696, loss 0.148604, acc 0.90625\n",
      "2017-11-05T23:00:19.002270: step 2697, loss 0.310878, acc 0.90625\n",
      "2017-11-05T23:00:23.033539: step 2698, loss 0.0609927, acc 0.96875\n",
      "2017-11-05T23:00:27.090156: step 2699, loss 0.320492, acc 0.8125\n",
      "2017-11-05T23:00:29.696753: step 2700, loss 0.0725153, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:00:32.380816: step 2700, loss 0.674038, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-05T23:00:37.923629: step 2701, loss 0.236043, acc 0.875\n",
      "2017-11-05T23:00:41.946420: step 2702, loss 0.187776, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:00:45.987135: step 2703, loss 0.212118, acc 0.875\n",
      "2017-11-05T23:00:49.935233: step 2704, loss 0.221085, acc 0.875\n",
      "2017-11-05T23:00:53.944556: step 2705, loss 0.0277216, acc 1\n",
      "2017-11-05T23:00:57.935254: step 2706, loss 0.137656, acc 0.9375\n",
      "2017-11-05T23:01:01.968408: step 2707, loss 0.147677, acc 0.90625\n",
      "2017-11-05T23:01:05.992813: step 2708, loss 0.256852, acc 0.875\n",
      "2017-11-05T23:01:09.980207: step 2709, loss 0.0908349, acc 0.96875\n",
      "2017-11-05T23:01:13.919589: step 2710, loss 0.300067, acc 0.90625\n",
      "2017-11-05T23:01:17.863472: step 2711, loss 0.0901425, acc 0.96875\n",
      "2017-11-05T23:01:21.796806: step 2712, loss 0.678006, acc 0.8125\n",
      "2017-11-05T23:01:25.711109: step 2713, loss 0.46355, acc 0.84375\n",
      "2017-11-05T23:01:29.611856: step 2714, loss 0.155401, acc 0.9375\n",
      "2017-11-05T23:01:33.541660: step 2715, loss 0.0723433, acc 0.96875\n",
      "2017-11-05T23:01:37.485653: step 2716, loss 0.211509, acc 0.875\n",
      "2017-11-05T23:01:41.370946: step 2717, loss 0.25402, acc 0.90625\n",
      "2017-11-05T23:01:45.294704: step 2718, loss 0.234973, acc 0.875\n",
      "2017-11-05T23:01:49.257284: step 2719, loss 0.168411, acc 0.9375\n",
      "2017-11-05T23:01:53.197370: step 2720, loss 0.230483, acc 0.9375\n",
      "2017-11-05T23:01:57.175590: step 2721, loss 0.216013, acc 0.9375\n",
      "2017-11-05T23:02:01.150609: step 2722, loss 0.10389, acc 0.96875\n",
      "2017-11-05T23:02:05.266293: step 2723, loss 0.0561083, acc 1\n",
      "2017-11-05T23:02:09.259893: step 2724, loss 0.0940193, acc 0.96875\n",
      "2017-11-05T23:02:13.246306: step 2725, loss 0.350016, acc 0.90625\n",
      "2017-11-05T23:02:17.222405: step 2726, loss 0.0516609, acc 0.96875\n",
      "2017-11-05T23:02:21.216681: step 2727, loss 0.490155, acc 0.84375\n",
      "2017-11-05T23:02:25.195737: step 2728, loss 0.27089, acc 0.875\n",
      "2017-11-05T23:02:29.101775: step 2729, loss 0.293411, acc 0.84375\n",
      "2017-11-05T23:02:33.041363: step 2730, loss 0.16118, acc 0.90625\n",
      "2017-11-05T23:02:37.129410: step 2731, loss 0.247927, acc 0.90625\n",
      "2017-11-05T23:02:41.039717: step 2732, loss 0.175938, acc 0.9375\n",
      "2017-11-05T23:02:44.960851: step 2733, loss 0.274734, acc 0.90625\n",
      "2017-11-05T23:02:48.846113: step 2734, loss 0.47595, acc 0.875\n",
      "2017-11-05T23:02:52.765398: step 2735, loss 0.327809, acc 0.875\n",
      "2017-11-05T23:02:55.281169: step 2736, loss 0.182229, acc 0.9\n",
      "2017-11-05T23:02:59.199313: step 2737, loss 0.271562, acc 0.8125\n",
      "2017-11-05T23:03:03.111035: step 2738, loss 0.096527, acc 0.96875\n",
      "2017-11-05T23:03:06.993852: step 2739, loss 0.158831, acc 0.9375\n",
      "2017-11-05T23:03:10.982955: step 2740, loss 0.118076, acc 0.90625\n",
      "2017-11-05T23:03:14.899079: step 2741, loss 0.196307, acc 0.90625\n",
      "2017-11-05T23:03:18.838890: step 2742, loss 0.173064, acc 0.875\n",
      "2017-11-05T23:03:22.906753: step 2743, loss 0.0251769, acc 1\n",
      "2017-11-05T23:03:27.091728: step 2744, loss 0.335584, acc 0.8125\n",
      "2017-11-05T23:03:31.068081: step 2745, loss 0.198177, acc 0.90625\n",
      "2017-11-05T23:03:35.168940: step 2746, loss 0.0571826, acc 0.96875\n",
      "2017-11-05T23:03:39.340661: step 2747, loss 0.20982, acc 0.875\n",
      "2017-11-05T23:03:43.443225: step 2748, loss 0.153628, acc 0.9375\n",
      "2017-11-05T23:03:47.422913: step 2749, loss 0.184594, acc 0.90625\n",
      "2017-11-05T23:03:51.387046: step 2750, loss 0.0907165, acc 0.9375\n",
      "2017-11-05T23:03:55.304208: step 2751, loss 0.103582, acc 0.96875\n",
      "2017-11-05T23:03:59.312874: step 2752, loss 0.196729, acc 0.90625\n",
      "2017-11-05T23:04:03.190496: step 2753, loss 0.254758, acc 0.875\n",
      "2017-11-05T23:04:07.132297: step 2754, loss 0.0995315, acc 0.96875\n",
      "2017-11-05T23:04:11.054638: step 2755, loss 0.101173, acc 0.96875\n",
      "2017-11-05T23:04:14.995636: step 2756, loss 0.301621, acc 0.875\n",
      "2017-11-05T23:04:18.902430: step 2757, loss 0.206573, acc 0.875\n",
      "2017-11-05T23:04:22.804838: step 2758, loss 0.127889, acc 0.9375\n",
      "2017-11-05T23:04:26.744970: step 2759, loss 0.235288, acc 0.9375\n",
      "2017-11-05T23:04:30.686935: step 2760, loss 0.0338813, acc 1\n",
      "2017-11-05T23:04:34.777647: step 2761, loss 0.108008, acc 0.9375\n",
      "2017-11-05T23:04:38.729516: step 2762, loss 0.137108, acc 0.9375\n",
      "2017-11-05T23:04:42.643275: step 2763, loss 0.177612, acc 0.90625\n",
      "2017-11-05T23:04:46.549515: step 2764, loss 0.326723, acc 0.8125\n",
      "2017-11-05T23:04:50.494263: step 2765, loss 0.318767, acc 0.875\n",
      "2017-11-05T23:04:54.457866: step 2766, loss 0.2113, acc 0.90625\n",
      "2017-11-05T23:04:58.366278: step 2767, loss 0.0917959, acc 0.96875\n",
      "2017-11-05T23:05:02.289386: step 2768, loss 0.30072, acc 0.875\n",
      "2017-11-05T23:05:06.221790: step 2769, loss 0.191886, acc 0.90625\n",
      "2017-11-05T23:05:10.154976: step 2770, loss 0.118233, acc 0.9375\n",
      "2017-11-05T23:05:14.132114: step 2771, loss 0.278528, acc 0.875\n",
      "2017-11-05T23:05:16.634874: step 2772, loss 0.178253, acc 0.95\n",
      "2017-11-05T23:05:20.570136: step 2773, loss 0.0761543, acc 0.96875\n",
      "2017-11-05T23:05:24.499222: step 2774, loss 0.174167, acc 0.875\n",
      "2017-11-05T23:05:28.439439: step 2775, loss 0.227065, acc 0.875\n",
      "2017-11-05T23:05:32.396127: step 2776, loss 0.155116, acc 0.9375\n",
      "2017-11-05T23:05:36.294695: step 2777, loss 0.233559, acc 0.90625\n",
      "2017-11-05T23:05:40.225570: step 2778, loss 0.230772, acc 0.90625\n",
      "2017-11-05T23:05:44.124528: step 2779, loss 0.15019, acc 0.90625\n",
      "2017-11-05T23:05:48.136227: step 2780, loss 0.191185, acc 0.90625\n",
      "2017-11-05T23:05:52.020652: step 2781, loss 0.186658, acc 0.875\n",
      "2017-11-05T23:05:55.983557: step 2782, loss 0.252899, acc 0.875\n",
      "2017-11-05T23:05:59.905104: step 2783, loss 0.178981, acc 0.9375\n",
      "2017-11-05T23:06:03.831905: step 2784, loss 0.102222, acc 0.9375\n",
      "2017-11-05T23:06:07.722778: step 2785, loss 0.118178, acc 0.9375\n",
      "2017-11-05T23:06:11.696886: step 2786, loss 0.118779, acc 0.9375\n",
      "2017-11-05T23:06:15.615023: step 2787, loss 0.0489551, acc 1\n",
      "2017-11-05T23:06:19.545176: step 2788, loss 0.103964, acc 0.96875\n",
      "2017-11-05T23:06:23.479598: step 2789, loss 0.0957075, acc 0.9375\n",
      "2017-11-05T23:06:27.423110: step 2790, loss 0.150263, acc 0.9375\n",
      "2017-11-05T23:06:31.351958: step 2791, loss 0.195966, acc 0.90625\n",
      "2017-11-05T23:06:35.383820: step 2792, loss 0.135383, acc 0.90625\n",
      "2017-11-05T23:06:39.338607: step 2793, loss 0.101611, acc 0.9375\n",
      "2017-11-05T23:06:43.272705: step 2794, loss 0.321914, acc 0.8125\n",
      "2017-11-05T23:06:47.144492: step 2795, loss 0.139672, acc 0.9375\n",
      "2017-11-05T23:06:51.110093: step 2796, loss 0.157068, acc 0.96875\n",
      "2017-11-05T23:06:55.046232: step 2797, loss 0.182921, acc 0.875\n",
      "2017-11-05T23:06:58.988910: step 2798, loss 0.109384, acc 0.9375\n",
      "2017-11-05T23:07:02.929112: step 2799, loss 0.112099, acc 0.96875\n",
      "2017-11-05T23:07:06.840327: step 2800, loss 0.146811, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:07:09.374147: step 2800, loss 0.73786, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-05T23:07:15.547040: step 2801, loss 0.22935, acc 0.875\n",
      "2017-11-05T23:07:19.449319: step 2802, loss 0.0521028, acc 1\n",
      "2017-11-05T23:07:23.393992: step 2803, loss 0.153793, acc 0.9375\n",
      "2017-11-05T23:07:27.274552: step 2804, loss 0.126365, acc 0.90625\n",
      "2017-11-05T23:07:31.232765: step 2805, loss 0.206873, acc 0.90625\n",
      "2017-11-05T23:07:35.153880: step 2806, loss 0.207595, acc 0.90625\n",
      "2017-11-05T23:07:39.098674: step 2807, loss 0.211843, acc 0.9375\n",
      "2017-11-05T23:07:41.619586: step 2808, loss 0.197958, acc 0.9\n",
      "2017-11-05T23:07:45.530557: step 2809, loss 0.163574, acc 0.90625\n",
      "2017-11-05T23:07:49.590494: step 2810, loss 0.181832, acc 0.9375\n",
      "2017-11-05T23:07:53.490922: step 2811, loss 0.204901, acc 0.875\n",
      "2017-11-05T23:07:57.428273: step 2812, loss 0.105207, acc 0.96875\n",
      "2017-11-05T23:08:01.353110: step 2813, loss 0.407752, acc 0.90625\n",
      "2017-11-05T23:08:05.297868: step 2814, loss 0.156696, acc 0.9375\n",
      "2017-11-05T23:08:09.250145: step 2815, loss 0.19123, acc 0.90625\n",
      "2017-11-05T23:08:13.222458: step 2816, loss 0.272356, acc 0.875\n",
      "2017-11-05T23:08:17.145142: step 2817, loss 0.251622, acc 0.875\n",
      "2017-11-05T23:08:21.051353: step 2818, loss 0.226047, acc 0.875\n",
      "2017-11-05T23:08:25.138478: step 2819, loss 0.0171292, acc 1\n",
      "2017-11-05T23:08:29.119589: step 2820, loss 0.309034, acc 0.90625\n",
      "2017-11-05T23:08:33.125792: step 2821, loss 0.104022, acc 0.9375\n",
      "2017-11-05T23:08:37.140041: step 2822, loss 0.143775, acc 0.9375\n",
      "2017-11-05T23:08:41.133730: step 2823, loss 0.116268, acc 0.96875\n",
      "2017-11-05T23:08:45.082241: step 2824, loss 0.341818, acc 0.84375\n",
      "2017-11-05T23:08:49.004418: step 2825, loss 0.0438024, acc 0.96875\n",
      "2017-11-05T23:08:52.937554: step 2826, loss 0.225977, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:08:56.894584: step 2827, loss 0.0826319, acc 0.9375\n",
      "2017-11-05T23:09:00.777570: step 2828, loss 0.162504, acc 0.90625\n",
      "2017-11-05T23:09:04.746283: step 2829, loss 0.125043, acc 0.9375\n",
      "2017-11-05T23:09:08.686886: step 2830, loss 0.0438301, acc 1\n",
      "2017-11-05T23:09:12.636091: step 2831, loss 0.0751455, acc 1\n",
      "2017-11-05T23:09:16.539747: step 2832, loss 0.480144, acc 0.84375\n",
      "2017-11-05T23:09:20.479121: step 2833, loss 0.223925, acc 0.90625\n",
      "2017-11-05T23:09:24.420720: step 2834, loss 0.0912147, acc 0.9375\n",
      "2017-11-05T23:09:28.358712: step 2835, loss 0.172198, acc 0.9375\n",
      "2017-11-05T23:09:32.300897: step 2836, loss 0.066912, acc 0.96875\n",
      "2017-11-05T23:09:36.213228: step 2837, loss 0.287591, acc 0.84375\n",
      "2017-11-05T23:09:40.159136: step 2838, loss 0.00942209, acc 1\n",
      "2017-11-05T23:09:44.137139: step 2839, loss 0.164922, acc 0.90625\n",
      "2017-11-05T23:09:48.068682: step 2840, loss 0.680319, acc 0.8125\n",
      "2017-11-05T23:09:52.106799: step 2841, loss 0.537795, acc 0.8125\n",
      "2017-11-05T23:09:56.038575: step 2842, loss 0.0711427, acc 0.96875\n",
      "2017-11-05T23:09:59.965496: step 2843, loss 0.143527, acc 0.90625\n",
      "2017-11-05T23:10:02.679978: step 2844, loss 0.530079, acc 0.8\n",
      "2017-11-05T23:10:06.634792: step 2845, loss 0.221452, acc 0.90625\n",
      "2017-11-05T23:10:10.570344: step 2846, loss 0.062421, acc 0.96875\n",
      "2017-11-05T23:10:14.545320: step 2847, loss 0.231867, acc 0.90625\n",
      "2017-11-05T23:10:18.477623: step 2848, loss 0.183037, acc 0.90625\n",
      "2017-11-05T23:10:22.417403: step 2849, loss 0.232528, acc 0.90625\n",
      "2017-11-05T23:10:26.373924: step 2850, loss 0.117944, acc 0.96875\n",
      "2017-11-05T23:10:30.258447: step 2851, loss 0.30579, acc 0.875\n",
      "2017-11-05T23:10:34.367058: step 2852, loss 0.130007, acc 0.96875\n",
      "2017-11-05T23:10:38.320540: step 2853, loss 0.186703, acc 0.9375\n",
      "2017-11-05T23:10:42.295094: step 2854, loss 0.28023, acc 0.875\n",
      "2017-11-05T23:10:46.240554: step 2855, loss 0.14716, acc 0.9375\n",
      "2017-11-05T23:10:50.185111: step 2856, loss 0.118048, acc 0.96875\n",
      "2017-11-05T23:10:54.128483: step 2857, loss 0.0901726, acc 0.96875\n",
      "2017-11-05T23:10:58.066588: step 2858, loss 0.364213, acc 0.875\n",
      "2017-11-05T23:11:02.004941: step 2859, loss 0.226469, acc 0.84375\n",
      "2017-11-05T23:11:05.920935: step 2860, loss 0.314463, acc 0.875\n",
      "2017-11-05T23:11:09.833752: step 2861, loss 0.278199, acc 0.90625\n",
      "2017-11-05T23:11:13.778386: step 2862, loss 0.0968882, acc 0.96875\n",
      "2017-11-05T23:11:17.684246: step 2863, loss 0.36443, acc 0.8125\n",
      "2017-11-05T23:11:21.654424: step 2864, loss 0.140265, acc 0.90625\n",
      "2017-11-05T23:11:25.608092: step 2865, loss 0.10821, acc 0.96875\n",
      "2017-11-05T23:11:29.522388: step 2866, loss 0.157755, acc 0.90625\n",
      "2017-11-05T23:11:33.434965: step 2867, loss 0.106334, acc 0.90625\n",
      "2017-11-05T23:11:37.398689: step 2868, loss 0.147079, acc 0.9375\n",
      "2017-11-05T23:11:41.550105: step 2869, loss 0.549173, acc 0.78125\n",
      "2017-11-05T23:11:45.522267: step 2870, loss 0.0426414, acc 1\n",
      "2017-11-05T23:11:49.491588: step 2871, loss 0.311823, acc 0.875\n",
      "2017-11-05T23:11:53.516760: step 2872, loss 0.215195, acc 0.84375\n",
      "2017-11-05T23:11:57.473383: step 2873, loss 0.0930413, acc 0.9375\n",
      "2017-11-05T23:12:01.416962: step 2874, loss 0.0703673, acc 0.96875\n",
      "2017-11-05T23:12:05.367671: step 2875, loss 0.225583, acc 0.90625\n",
      "2017-11-05T23:12:09.383497: step 2876, loss 0.0940039, acc 0.96875\n",
      "2017-11-05T23:12:13.379952: step 2877, loss 0.0747314, acc 0.9375\n",
      "2017-11-05T23:12:17.378991: step 2878, loss 0.182671, acc 0.90625\n",
      "2017-11-05T23:12:21.324124: step 2879, loss 0.228004, acc 0.90625\n",
      "2017-11-05T23:12:23.905924: step 2880, loss 0.390335, acc 0.85\n",
      "2017-11-05T23:12:27.871098: step 2881, loss 0.0413019, acc 0.96875\n",
      "2017-11-05T23:12:31.851326: step 2882, loss 0.155706, acc 0.90625\n",
      "2017-11-05T23:12:36.005887: step 2883, loss 0.195846, acc 0.9375\n",
      "2017-11-05T23:12:39.946428: step 2884, loss 0.204706, acc 0.9375\n",
      "2017-11-05T23:12:43.889667: step 2885, loss 0.217572, acc 0.875\n",
      "2017-11-05T23:12:47.890895: step 2886, loss 0.336028, acc 0.90625\n",
      "2017-11-05T23:12:51.865304: step 2887, loss 0.129548, acc 0.90625\n",
      "2017-11-05T23:12:55.844589: step 2888, loss 0.259323, acc 0.875\n",
      "2017-11-05T23:12:59.780791: step 2889, loss 0.273741, acc 0.84375\n",
      "2017-11-05T23:13:03.678186: step 2890, loss 0.170252, acc 0.90625\n",
      "2017-11-05T23:13:07.595382: step 2891, loss 0.0350916, acc 1\n",
      "2017-11-05T23:13:11.605988: step 2892, loss 0.171599, acc 0.90625\n",
      "2017-11-05T23:13:15.610256: step 2893, loss 0.168108, acc 0.9375\n",
      "2017-11-05T23:13:19.514514: step 2894, loss 0.180807, acc 0.9375\n",
      "2017-11-05T23:13:23.647645: step 2895, loss 0.077349, acc 0.96875\n",
      "2017-11-05T23:13:27.786346: step 2896, loss 0.089665, acc 0.96875\n",
      "2017-11-05T23:13:31.697621: step 2897, loss 0.139241, acc 0.96875\n",
      "2017-11-05T23:13:35.657612: step 2898, loss 0.181173, acc 0.9375\n",
      "2017-11-05T23:13:39.602070: step 2899, loss 0.178903, acc 0.90625\n",
      "2017-11-05T23:13:43.583039: step 2900, loss 0.183393, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:13:46.181704: step 2900, loss 0.87258, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-05T23:13:51.596631: step 2901, loss 0.377937, acc 0.875\n",
      "2017-11-05T23:13:55.665760: step 2902, loss 0.206435, acc 0.90625\n",
      "2017-11-05T23:13:59.740777: step 2903, loss 0.254224, acc 0.875\n",
      "2017-11-05T23:14:03.696251: step 2904, loss 0.164794, acc 0.90625\n",
      "2017-11-05T23:14:07.627753: step 2905, loss 0.151516, acc 0.875\n",
      "2017-11-05T23:14:11.694688: step 2906, loss 0.0933529, acc 0.96875\n",
      "2017-11-05T23:14:15.656106: step 2907, loss 0.167065, acc 0.9375\n",
      "2017-11-05T23:14:19.612956: step 2908, loss 0.268605, acc 0.90625\n",
      "2017-11-05T23:14:23.520762: step 2909, loss 0.144894, acc 0.9375\n",
      "2017-11-05T23:14:27.483261: step 2910, loss 0.126082, acc 0.9375\n",
      "2017-11-05T23:14:31.460782: step 2911, loss 0.248958, acc 0.875\n",
      "2017-11-05T23:14:35.567166: step 2912, loss 0.187399, acc 0.90625\n",
      "2017-11-05T23:14:39.527251: step 2913, loss 0.294901, acc 0.875\n",
      "2017-11-05T23:14:43.550411: step 2914, loss 0.232947, acc 0.875\n",
      "2017-11-05T23:14:47.534719: step 2915, loss 0.173306, acc 0.90625\n",
      "2017-11-05T23:14:50.039687: step 2916, loss 0.216395, acc 0.85\n",
      "2017-11-05T23:14:54.057469: step 2917, loss 0.126653, acc 0.9375\n",
      "2017-11-05T23:14:58.049612: step 2918, loss 0.11959, acc 0.9375\n",
      "2017-11-05T23:15:01.993498: step 2919, loss 0.298079, acc 0.84375\n",
      "2017-11-05T23:15:06.035737: step 2920, loss 0.205602, acc 0.90625\n",
      "2017-11-05T23:15:09.942895: step 2921, loss 0.0986524, acc 0.96875\n",
      "2017-11-05T23:15:13.942211: step 2922, loss 0.152314, acc 0.96875\n",
      "2017-11-05T23:15:17.858495: step 2923, loss 0.0419131, acc 1\n",
      "2017-11-05T23:15:21.855928: step 2924, loss 0.163489, acc 0.9375\n",
      "2017-11-05T23:15:25.800529: step 2925, loss 0.146608, acc 0.9375\n",
      "2017-11-05T23:15:29.787709: step 2926, loss 0.0265648, acc 1\n",
      "2017-11-05T23:15:33.771325: step 2927, loss 0.213186, acc 0.9375\n",
      "2017-11-05T23:15:37.739945: step 2928, loss 0.218998, acc 0.875\n",
      "2017-11-05T23:15:41.676811: step 2929, loss 0.119403, acc 0.96875\n",
      "2017-11-05T23:15:45.605140: step 2930, loss 0.114723, acc 0.9375\n",
      "2017-11-05T23:15:49.657895: step 2931, loss 0.118608, acc 0.90625\n",
      "2017-11-05T23:15:53.626180: step 2932, loss 0.291133, acc 0.875\n",
      "2017-11-05T23:15:57.706683: step 2933, loss 0.31989, acc 0.8125\n",
      "2017-11-05T23:16:01.636608: step 2934, loss 0.123938, acc 0.96875\n",
      "2017-11-05T23:16:05.646624: step 2935, loss 0.0438692, acc 0.96875\n",
      "2017-11-05T23:16:09.626505: step 2936, loss 0.16655, acc 0.90625\n",
      "2017-11-05T23:16:13.654016: step 2937, loss 0.0933046, acc 0.96875\n",
      "2017-11-05T23:16:17.599496: step 2938, loss 0.0616418, acc 0.96875\n",
      "2017-11-05T23:16:21.623145: step 2939, loss 0.415051, acc 0.78125\n",
      "2017-11-05T23:16:25.599488: step 2940, loss 0.132244, acc 0.9375\n",
      "2017-11-05T23:16:29.606935: step 2941, loss 0.204304, acc 0.90625\n",
      "2017-11-05T23:16:33.668347: step 2942, loss 0.155665, acc 0.90625\n",
      "2017-11-05T23:16:37.921234: step 2943, loss 0.194421, acc 0.90625\n",
      "2017-11-05T23:16:41.968552: step 2944, loss 0.149285, acc 0.90625\n",
      "2017-11-05T23:16:45.912239: step 2945, loss 0.107226, acc 0.96875\n",
      "2017-11-05T23:16:49.919029: step 2946, loss 0.0920891, acc 0.96875\n",
      "2017-11-05T23:16:53.981246: step 2947, loss 0.171537, acc 0.875\n",
      "2017-11-05T23:16:57.984466: step 2948, loss 0.200368, acc 0.96875\n",
      "2017-11-05T23:17:01.929325: step 2949, loss 0.173404, acc 0.90625\n",
      "2017-11-05T23:17:05.931540: step 2950, loss 0.157311, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:17:09.885575: step 2951, loss 0.269773, acc 0.84375\n",
      "2017-11-05T23:17:12.381063: step 2952, loss 0.112256, acc 0.95\n",
      "2017-11-05T23:17:16.352917: step 2953, loss 0.19543, acc 0.875\n",
      "2017-11-05T23:17:20.361440: step 2954, loss 0.178365, acc 0.90625\n",
      "2017-11-05T23:17:24.354944: step 2955, loss 0.11001, acc 0.9375\n",
      "2017-11-05T23:17:28.301333: step 2956, loss 0.101405, acc 0.96875\n",
      "2017-11-05T23:17:32.343213: step 2957, loss 0.258944, acc 0.875\n",
      "2017-11-05T23:17:36.301847: step 2958, loss 0.120128, acc 0.9375\n",
      "2017-11-05T23:17:40.279017: step 2959, loss 0.107312, acc 0.9375\n",
      "2017-11-05T23:17:44.249517: step 2960, loss 0.0555207, acc 0.96875\n",
      "2017-11-05T23:17:48.240120: step 2961, loss 0.116706, acc 0.90625\n",
      "2017-11-05T23:17:52.209489: step 2962, loss 0.237215, acc 0.84375\n",
      "2017-11-05T23:17:56.249622: step 2963, loss 0.0720423, acc 0.96875\n",
      "2017-11-05T23:18:00.249533: step 2964, loss 0.258412, acc 0.875\n",
      "2017-11-05T23:18:04.220090: step 2965, loss 0.199561, acc 0.90625\n",
      "2017-11-05T23:18:08.191388: step 2966, loss 0.162478, acc 0.9375\n",
      "2017-11-05T23:18:12.135888: step 2967, loss 0.153505, acc 0.96875\n",
      "2017-11-05T23:18:16.053957: step 2968, loss 0.166461, acc 0.875\n",
      "2017-11-05T23:18:20.105781: step 2969, loss 0.289785, acc 0.875\n",
      "2017-11-05T23:18:24.204875: step 2970, loss 0.0695125, acc 0.96875\n",
      "2017-11-05T23:18:28.499231: step 2971, loss 0.115184, acc 0.9375\n",
      "2017-11-05T23:18:32.499535: step 2972, loss 0.106108, acc 0.90625\n",
      "2017-11-05T23:18:36.693194: step 2973, loss 0.0989097, acc 0.9375\n",
      "2017-11-05T23:18:40.825502: step 2974, loss 0.106561, acc 0.9375\n",
      "2017-11-05T23:18:44.929740: step 2975, loss 0.181872, acc 0.9375\n",
      "2017-11-05T23:18:49.000073: step 2976, loss 0.226331, acc 0.90625\n",
      "2017-11-05T23:18:53.026376: step 2977, loss 0.335276, acc 0.8125\n",
      "2017-11-05T23:18:57.120070: step 2978, loss 0.207948, acc 0.875\n",
      "2017-11-05T23:19:01.263922: step 2979, loss 0.152149, acc 0.90625\n",
      "2017-11-05T23:19:05.373635: step 2980, loss 0.192063, acc 0.875\n",
      "2017-11-05T23:19:09.489195: step 2981, loss 0.238504, acc 0.875\n",
      "2017-11-05T23:19:13.622069: step 2982, loss 0.148979, acc 0.90625\n",
      "2017-11-05T23:19:17.711018: step 2983, loss 0.327087, acc 0.90625\n",
      "2017-11-05T23:19:21.694152: step 2984, loss 0.115724, acc 0.9375\n",
      "2017-11-05T23:19:25.722924: step 2985, loss 0.226815, acc 0.9375\n",
      "2017-11-05T23:19:29.831803: step 2986, loss 0.197404, acc 0.90625\n",
      "2017-11-05T23:19:33.819165: step 2987, loss 0.25829, acc 0.84375\n",
      "2017-11-05T23:19:36.534072: step 2988, loss 0.120293, acc 0.9\n",
      "2017-11-05T23:19:40.680266: step 2989, loss 0.27536, acc 0.8125\n",
      "2017-11-05T23:19:44.749412: step 2990, loss 0.349276, acc 0.84375\n",
      "2017-11-05T23:19:48.882843: step 2991, loss 0.135525, acc 0.9375\n",
      "2017-11-05T23:19:52.985310: step 2992, loss 0.131773, acc 0.9375\n",
      "2017-11-05T23:19:57.179262: step 2993, loss 0.113068, acc 0.96875\n",
      "2017-11-05T23:20:01.477850: step 2994, loss 0.210428, acc 0.875\n",
      "2017-11-05T23:20:05.479346: step 2995, loss 0.0593406, acc 0.96875\n",
      "2017-11-05T23:20:09.491340: step 2996, loss 0.204095, acc 0.90625\n",
      "2017-11-05T23:20:13.446863: step 2997, loss 0.147952, acc 0.9375\n",
      "2017-11-05T23:20:17.460840: step 2998, loss 0.0305018, acc 1\n",
      "2017-11-05T23:20:21.413840: step 2999, loss 0.382864, acc 0.84375\n",
      "2017-11-05T23:20:25.400632: step 3000, loss 0.0963649, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:20:27.927827: step 3000, loss 0.79644, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-05T23:20:33.435907: step 3001, loss 0.118656, acc 0.96875\n",
      "2017-11-05T23:20:37.467429: step 3002, loss 0.139587, acc 0.90625\n",
      "2017-11-05T23:20:41.504748: step 3003, loss 0.255334, acc 0.84375\n",
      "2017-11-05T23:20:45.447019: step 3004, loss 0.154773, acc 0.9375\n",
      "2017-11-05T23:20:49.398484: step 3005, loss 0.479665, acc 0.8125\n",
      "2017-11-05T23:20:53.372064: step 3006, loss 0.0745675, acc 1\n",
      "2017-11-05T23:20:57.408914: step 3007, loss 0.338799, acc 0.84375\n",
      "2017-11-05T23:21:01.381550: step 3008, loss 0.372188, acc 0.84375\n",
      "2017-11-05T23:21:05.325007: step 3009, loss 0.122119, acc 0.90625\n",
      "2017-11-05T23:21:09.340252: step 3010, loss 0.0984237, acc 0.96875\n",
      "2017-11-05T23:21:13.336811: step 3011, loss 0.0675278, acc 0.96875\n",
      "2017-11-05T23:21:17.336328: step 3012, loss 0.219577, acc 0.90625\n",
      "2017-11-05T23:21:21.350473: step 3013, loss 0.107502, acc 0.9375\n",
      "2017-11-05T23:21:25.323270: step 3014, loss 0.443914, acc 0.8125\n",
      "2017-11-05T23:21:29.250895: step 3015, loss 0.36583, acc 0.84375\n",
      "2017-11-05T23:21:33.266437: step 3016, loss 0.207393, acc 0.90625\n",
      "2017-11-05T23:21:37.223185: step 3017, loss 0.142928, acc 0.9375\n",
      "2017-11-05T23:21:41.155134: step 3018, loss 0.406946, acc 0.8125\n",
      "2017-11-05T23:21:45.163882: step 3019, loss 0.280107, acc 0.875\n",
      "2017-11-05T23:21:49.138820: step 3020, loss 0.345805, acc 0.875\n",
      "2017-11-05T23:21:53.077030: step 3021, loss 0.0644477, acc 1\n",
      "2017-11-05T23:21:57.077622: step 3022, loss 0.122985, acc 0.9375\n",
      "2017-11-05T23:22:01.055546: step 3023, loss 0.076276, acc 0.96875\n",
      "2017-11-05T23:22:03.641802: step 3024, loss 0.186653, acc 0.9\n",
      "2017-11-05T23:22:07.596910: step 3025, loss 0.246529, acc 0.875\n",
      "2017-11-05T23:22:11.531989: step 3026, loss 0.0752669, acc 0.9375\n",
      "2017-11-05T23:22:15.444845: step 3027, loss 0.095589, acc 0.9375\n",
      "2017-11-05T23:22:19.541952: step 3028, loss 0.110757, acc 0.96875\n",
      "2017-11-05T23:22:23.568619: step 3029, loss 0.421577, acc 0.84375\n",
      "2017-11-05T23:22:27.539336: step 3030, loss 0.146674, acc 0.9375\n",
      "2017-11-05T23:22:31.529579: step 3031, loss 0.301384, acc 0.875\n",
      "2017-11-05T23:22:35.788255: step 3032, loss 0.164801, acc 0.9375\n",
      "2017-11-05T23:22:39.825631: step 3033, loss 0.0795969, acc 0.96875\n",
      "2017-11-05T23:22:43.829003: step 3034, loss 0.209673, acc 0.9375\n",
      "2017-11-05T23:22:47.778288: step 3035, loss 0.331725, acc 0.875\n",
      "2017-11-05T23:22:51.738401: step 3036, loss 0.0764216, acc 1\n",
      "2017-11-05T23:22:55.700708: step 3037, loss 0.442879, acc 0.875\n",
      "2017-11-05T23:22:59.693525: step 3038, loss 0.0941556, acc 0.96875\n",
      "2017-11-05T23:23:03.685321: step 3039, loss 0.0541327, acc 0.96875\n",
      "2017-11-05T23:23:07.696005: step 3040, loss 0.177922, acc 0.9375\n",
      "2017-11-05T23:23:11.638930: step 3041, loss 0.183711, acc 0.875\n",
      "2017-11-05T23:23:15.643240: step 3042, loss 0.0833316, acc 0.9375\n",
      "2017-11-05T23:23:19.536354: step 3043, loss 0.209549, acc 0.9375\n",
      "2017-11-05T23:23:23.555401: step 3044, loss 0.0515935, acc 0.96875\n",
      "2017-11-05T23:23:27.463529: step 3045, loss 0.146437, acc 0.9375\n",
      "2017-11-05T23:23:31.434437: step 3046, loss 0.223536, acc 0.90625\n",
      "2017-11-05T23:23:35.410418: step 3047, loss 0.263688, acc 0.875\n",
      "2017-11-05T23:23:39.360269: step 3048, loss 0.132427, acc 0.9375\n",
      "2017-11-05T23:23:43.370940: step 3049, loss 0.0904517, acc 0.96875\n",
      "2017-11-05T23:23:47.322325: step 3050, loss 0.193525, acc 0.9375\n",
      "2017-11-05T23:23:51.308221: step 3051, loss 0.348153, acc 0.8125\n",
      "2017-11-05T23:23:55.268192: step 3052, loss 0.290027, acc 0.8125\n",
      "2017-11-05T23:23:59.354427: step 3053, loss 0.155424, acc 0.90625\n",
      "2017-11-05T23:24:03.378520: step 3054, loss 0.280945, acc 0.8125\n",
      "2017-11-05T23:24:07.446588: step 3055, loss 0.135125, acc 0.9375\n",
      "2017-11-05T23:24:11.427513: step 3056, loss 0.205084, acc 0.90625\n",
      "2017-11-05T23:24:15.448163: step 3057, loss 0.108212, acc 0.96875\n",
      "2017-11-05T23:24:19.434324: step 3058, loss 0.347429, acc 0.875\n",
      "2017-11-05T23:24:23.519444: step 3059, loss 0.340083, acc 0.84375\n",
      "2017-11-05T23:24:26.199044: step 3060, loss 0.281441, acc 0.9\n",
      "2017-11-05T23:24:30.188339: step 3061, loss 0.0908547, acc 0.96875\n",
      "2017-11-05T23:24:34.307793: step 3062, loss 0.226612, acc 0.875\n",
      "2017-11-05T23:24:38.328212: step 3063, loss 0.111078, acc 0.9375\n",
      "2017-11-05T23:24:42.355025: step 3064, loss 0.198157, acc 0.9375\n",
      "2017-11-05T23:24:46.295657: step 3065, loss 0.44587, acc 0.84375\n",
      "2017-11-05T23:24:50.240086: step 3066, loss 0.207887, acc 0.875\n",
      "2017-11-05T23:24:54.246712: step 3067, loss 0.207858, acc 0.875\n",
      "2017-11-05T23:24:58.224030: step 3068, loss 0.0447931, acc 0.96875\n",
      "2017-11-05T23:25:02.273179: step 3069, loss 0.0707207, acc 0.96875\n",
      "2017-11-05T23:25:06.219567: step 3070, loss 0.0838266, acc 0.9375\n",
      "2017-11-05T23:25:10.215850: step 3071, loss 0.264222, acc 0.8125\n",
      "2017-11-05T23:25:14.168138: step 3072, loss 0.102584, acc 0.9375\n",
      "2017-11-05T23:25:18.121135: step 3073, loss 0.16912, acc 0.90625\n",
      "2017-11-05T23:25:22.121959: step 3074, loss 0.129652, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:25:26.118502: step 3075, loss 0.191756, acc 0.90625\n",
      "2017-11-05T23:25:30.054855: step 3076, loss 0.102744, acc 0.96875\n",
      "2017-11-05T23:25:34.001389: step 3077, loss 0.214729, acc 0.875\n",
      "2017-11-05T23:25:37.966857: step 3078, loss 0.167985, acc 0.90625\n",
      "2017-11-05T23:25:41.877022: step 3079, loss 0.244663, acc 0.84375\n",
      "2017-11-05T23:25:45.820304: step 3080, loss 0.143818, acc 0.9375\n",
      "2017-11-05T23:25:49.814717: step 3081, loss 0.186731, acc 0.9375\n",
      "2017-11-05T23:25:53.729909: step 3082, loss 0.114519, acc 0.9375\n",
      "2017-11-05T23:25:57.720593: step 3083, loss 0.134996, acc 0.9375\n",
      "2017-11-05T23:26:01.836589: step 3084, loss 0.0142156, acc 1\n",
      "2017-11-05T23:26:05.923648: step 3085, loss 0.0553794, acc 0.96875\n",
      "2017-11-05T23:26:09.900600: step 3086, loss 0.33336, acc 0.90625\n",
      "2017-11-05T23:26:13.829161: step 3087, loss 0.377211, acc 0.84375\n",
      "2017-11-05T23:26:17.822702: step 3088, loss 0.180343, acc 0.9375\n",
      "2017-11-05T23:26:21.753077: step 3089, loss 0.389766, acc 0.8125\n",
      "2017-11-05T23:26:25.730904: step 3090, loss 0.0454579, acc 0.96875\n",
      "2017-11-05T23:26:29.730553: step 3091, loss 0.158261, acc 0.90625\n",
      "2017-11-05T23:26:33.836794: step 3092, loss 0.153743, acc 0.9375\n",
      "2017-11-05T23:26:38.008798: step 3093, loss 0.422851, acc 0.84375\n",
      "2017-11-05T23:26:41.994604: step 3094, loss 0.107006, acc 0.96875\n",
      "2017-11-05T23:26:45.930062: step 3095, loss 0.492244, acc 0.84375\n",
      "2017-11-05T23:26:48.451687: step 3096, loss 0.280554, acc 0.95\n",
      "2017-11-05T23:26:52.378895: step 3097, loss 0.169235, acc 0.90625\n",
      "2017-11-05T23:26:56.323701: step 3098, loss 0.293883, acc 0.84375\n",
      "2017-11-05T23:27:00.336534: step 3099, loss 0.0700292, acc 0.96875\n",
      "2017-11-05T23:27:04.279410: step 3100, loss 0.233286, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:27:06.829623: step 3100, loss 1.13727, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-05T23:27:12.154014: step 3101, loss 0.194409, acc 0.9375\n",
      "2017-11-05T23:27:16.103231: step 3102, loss 0.39412, acc 0.90625\n",
      "2017-11-05T23:27:20.041433: step 3103, loss 0.353494, acc 0.875\n",
      "2017-11-05T23:27:24.028384: step 3104, loss 0.132239, acc 0.9375\n",
      "2017-11-05T23:27:27.943051: step 3105, loss 0.0712464, acc 0.96875\n",
      "2017-11-05T23:27:31.870614: step 3106, loss 0.293732, acc 0.90625\n",
      "2017-11-05T23:27:35.793816: step 3107, loss 0.244372, acc 0.90625\n",
      "2017-11-05T23:27:39.776949: step 3108, loss 0.160883, acc 0.9375\n",
      "2017-11-05T23:27:43.702543: step 3109, loss 0.0588365, acc 1\n",
      "2017-11-05T23:27:47.674698: step 3110, loss 0.220573, acc 0.9375\n",
      "2017-11-05T23:27:51.692833: step 3111, loss 0.255927, acc 0.90625\n",
      "2017-11-05T23:27:55.639821: step 3112, loss 0.151033, acc 0.9375\n",
      "2017-11-05T23:27:59.641992: step 3113, loss 0.107008, acc 0.96875\n",
      "2017-11-05T23:28:03.642085: step 3114, loss 0.217659, acc 0.90625\n",
      "2017-11-05T23:28:07.607918: step 3115, loss 0.0332968, acc 1\n",
      "2017-11-05T23:28:11.602743: step 3116, loss 0.205657, acc 0.9375\n",
      "2017-11-05T23:28:15.502901: step 3117, loss 0.123886, acc 0.96875\n",
      "2017-11-05T23:28:19.382709: step 3118, loss 0.0717022, acc 0.96875\n",
      "2017-11-05T23:28:23.499276: step 3119, loss 0.226109, acc 0.96875\n",
      "2017-11-05T23:28:27.496692: step 3120, loss 0.349517, acc 0.9375\n",
      "2017-11-05T23:28:31.424119: step 3121, loss 0.2198, acc 0.875\n",
      "2017-11-05T23:28:35.540943: step 3122, loss 0.393922, acc 0.84375\n",
      "2017-11-05T23:28:39.518770: step 3123, loss 0.188495, acc 0.96875\n",
      "2017-11-05T23:28:43.450620: step 3124, loss 0.325591, acc 0.875\n",
      "2017-11-05T23:28:47.360380: step 3125, loss 0.0799318, acc 0.96875\n",
      "2017-11-05T23:28:51.268812: step 3126, loss 0.304512, acc 0.84375\n",
      "2017-11-05T23:28:55.203479: step 3127, loss 0.221337, acc 0.90625\n",
      "2017-11-05T23:28:59.115465: step 3128, loss 0.100218, acc 0.90625\n",
      "2017-11-05T23:29:03.059184: step 3129, loss 0.221255, acc 0.875\n",
      "2017-11-05T23:29:07.008282: step 3130, loss 0.147451, acc 0.90625\n",
      "2017-11-05T23:29:10.985801: step 3131, loss 0.324703, acc 0.90625\n",
      "2017-11-05T23:29:13.508207: step 3132, loss 0.139466, acc 0.9\n",
      "2017-11-05T23:29:17.425184: step 3133, loss 0.193999, acc 0.90625\n",
      "2017-11-05T23:29:21.384044: step 3134, loss 0.146636, acc 0.9375\n",
      "2017-11-05T23:29:25.327252: step 3135, loss 0.245721, acc 0.90625\n",
      "2017-11-05T23:29:29.216221: step 3136, loss 0.192554, acc 0.90625\n",
      "2017-11-05T23:29:33.146230: step 3137, loss 0.260825, acc 0.875\n",
      "2017-11-05T23:29:37.086429: step 3138, loss 0.194515, acc 0.9375\n",
      "2017-11-05T23:29:41.040159: step 3139, loss 0.0803887, acc 0.96875\n",
      "2017-11-05T23:29:44.963759: step 3140, loss 0.18253, acc 0.90625\n",
      "2017-11-05T23:29:48.884408: step 3141, loss 0.223073, acc 0.9375\n",
      "2017-11-05T23:29:52.801459: step 3142, loss 0.119242, acc 0.9375\n",
      "2017-11-05T23:29:56.699308: step 3143, loss 0.0443329, acc 0.96875\n",
      "2017-11-05T23:30:00.815536: step 3144, loss 0.0206039, acc 1\n",
      "2017-11-05T23:30:04.869123: step 3145, loss 0.136743, acc 0.9375\n",
      "2017-11-05T23:30:08.790883: step 3146, loss 0.246505, acc 0.9375\n",
      "2017-11-05T23:30:12.817787: step 3147, loss 0.258999, acc 0.875\n",
      "2017-11-05T23:30:16.715540: step 3148, loss 0.0791343, acc 0.96875\n",
      "2017-11-05T23:30:20.698452: step 3149, loss 0.228389, acc 0.90625\n",
      "2017-11-05T23:30:24.641840: step 3150, loss 0.0574663, acc 1\n",
      "2017-11-05T23:30:28.576060: step 3151, loss 0.283947, acc 0.84375\n",
      "2017-11-05T23:30:32.544567: step 3152, loss 0.151145, acc 0.90625\n",
      "2017-11-05T23:30:36.578663: step 3153, loss 0.0817267, acc 0.96875\n",
      "2017-11-05T23:30:40.557991: step 3154, loss 0.242806, acc 0.90625\n",
      "2017-11-05T23:30:44.487932: step 3155, loss 0.327611, acc 0.875\n",
      "2017-11-05T23:30:48.424540: step 3156, loss 0.355031, acc 0.90625\n",
      "2017-11-05T23:30:52.318142: step 3157, loss 0.116828, acc 0.9375\n",
      "2017-11-05T23:30:56.278975: step 3158, loss 0.464626, acc 0.8125\n",
      "2017-11-05T23:31:00.202349: step 3159, loss 0.0653698, acc 0.96875\n",
      "2017-11-05T23:31:04.129467: step 3160, loss 0.227669, acc 0.875\n",
      "2017-11-05T23:31:08.031143: step 3161, loss 0.126405, acc 0.9375\n",
      "2017-11-05T23:31:12.069236: step 3162, loss 0.170215, acc 0.875\n",
      "2017-11-05T23:31:16.015776: step 3163, loss 0.131391, acc 0.90625\n",
      "2017-11-05T23:31:19.931487: step 3164, loss 0.37451, acc 0.75\n",
      "2017-11-05T23:31:23.894084: step 3165, loss 0.332034, acc 0.78125\n",
      "2017-11-05T23:31:27.841155: step 3166, loss 0.422692, acc 0.84375\n",
      "2017-11-05T23:31:31.803904: step 3167, loss 0.238527, acc 0.875\n",
      "2017-11-05T23:31:34.336328: step 3168, loss 0.417739, acc 0.85\n",
      "2017-11-05T23:31:38.278416: step 3169, loss 0.07713, acc 0.96875\n",
      "2017-11-05T23:31:42.252526: step 3170, loss 0.172633, acc 0.90625\n",
      "2017-11-05T23:31:46.207493: step 3171, loss 0.0553697, acc 0.96875\n",
      "2017-11-05T23:31:50.164452: step 3172, loss 0.0575052, acc 0.96875\n",
      "2017-11-05T23:31:54.091054: step 3173, loss 0.213671, acc 0.90625\n",
      "2017-11-05T23:31:58.027305: step 3174, loss 0.104839, acc 0.96875\n",
      "2017-11-05T23:32:02.027131: step 3175, loss 0.288513, acc 0.90625\n",
      "2017-11-05T23:32:05.939502: step 3176, loss 0.219763, acc 0.9375\n",
      "2017-11-05T23:32:09.870636: step 3177, loss 0.243472, acc 0.90625\n",
      "2017-11-05T23:32:13.863794: step 3178, loss 0.367057, acc 0.84375\n",
      "2017-11-05T23:32:17.858427: step 3179, loss 0.353039, acc 0.78125\n",
      "2017-11-05T23:32:21.791248: step 3180, loss 0.147744, acc 0.9375\n",
      "2017-11-05T23:32:25.735007: step 3181, loss 0.315938, acc 0.875\n",
      "2017-11-05T23:32:29.665218: step 3182, loss 0.294265, acc 0.875\n",
      "2017-11-05T23:32:33.717840: step 3183, loss 0.214562, acc 0.90625\n",
      "2017-11-05T23:32:37.759405: step 3184, loss 0.273697, acc 0.90625\n",
      "2017-11-05T23:32:41.747502: step 3185, loss 0.0871155, acc 0.9375\n",
      "2017-11-05T23:32:45.768090: step 3186, loss 0.143119, acc 0.9375\n",
      "2017-11-05T23:32:49.774511: step 3187, loss 0.142794, acc 0.9375\n",
      "2017-11-05T23:32:53.717201: step 3188, loss 0.0613878, acc 0.96875\n",
      "2017-11-05T23:32:57.710841: step 3189, loss 0.120199, acc 0.9375\n",
      "2017-11-05T23:33:01.679343: step 3190, loss 0.126585, acc 0.96875\n",
      "2017-11-05T23:33:05.718025: step 3191, loss 0.328685, acc 0.875\n",
      "2017-11-05T23:33:09.663191: step 3192, loss 0.220776, acc 0.90625\n",
      "2017-11-05T23:33:13.643094: step 3193, loss 0.201, acc 0.9375\n",
      "2017-11-05T23:33:17.591763: step 3194, loss 0.269556, acc 0.875\n",
      "2017-11-05T23:33:21.631691: step 3195, loss 0.373986, acc 0.90625\n",
      "2017-11-05T23:33:25.878926: step 3196, loss 0.273996, acc 0.90625\n",
      "2017-11-05T23:33:29.805946: step 3197, loss 0.13191, acc 0.96875\n",
      "2017-11-05T23:33:33.727765: step 3198, loss 0.14014, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:33:37.697267: step 3199, loss 0.210501, acc 0.90625\n",
      "2017-11-05T23:33:41.681454: step 3200, loss 0.0523189, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:33:44.208512: step 3200, loss 0.825294, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-05T23:33:49.588189: step 3201, loss 0.186174, acc 0.90625\n",
      "2017-11-05T23:33:53.593624: step 3202, loss 0.0773599, acc 0.96875\n",
      "2017-11-05T23:33:57.529144: step 3203, loss 0.139736, acc 0.9375\n",
      "2017-11-05T23:34:00.033104: step 3204, loss 0.416855, acc 0.8\n",
      "2017-11-05T23:34:04.147061: step 3205, loss 0.204195, acc 0.90625\n",
      "2017-11-05T23:34:08.069048: step 3206, loss 0.0825141, acc 0.96875\n",
      "2017-11-05T23:34:12.061363: step 3207, loss 0.0261576, acc 1\n",
      "2017-11-05T23:34:15.986357: step 3208, loss 0.289898, acc 0.84375\n",
      "2017-11-05T23:34:19.983602: step 3209, loss 0.135415, acc 0.9375\n",
      "2017-11-05T23:34:23.954897: step 3210, loss 0.168785, acc 0.875\n",
      "2017-11-05T23:34:27.868303: step 3211, loss 0.281129, acc 0.84375\n",
      "2017-11-05T23:34:31.785705: step 3212, loss 0.322192, acc 0.875\n",
      "2017-11-05T23:34:35.888979: step 3213, loss 0.207923, acc 0.875\n",
      "2017-11-05T23:34:39.813796: step 3214, loss 0.243267, acc 0.875\n",
      "2017-11-05T23:34:43.735029: step 3215, loss 0.208246, acc 0.9375\n",
      "2017-11-05T23:34:47.693821: step 3216, loss 0.0818255, acc 1\n",
      "2017-11-05T23:34:51.613910: step 3217, loss 0.0904353, acc 0.96875\n",
      "2017-11-05T23:34:55.528847: step 3218, loss 0.125371, acc 0.9375\n",
      "2017-11-05T23:34:59.469296: step 3219, loss 0.243455, acc 0.90625\n",
      "2017-11-05T23:35:03.439013: step 3220, loss 0.418527, acc 0.8125\n",
      "2017-11-05T23:35:07.395590: step 3221, loss 0.174317, acc 0.9375\n",
      "2017-11-05T23:35:11.400305: step 3222, loss 0.188491, acc 0.9375\n",
      "2017-11-05T23:35:15.376880: step 3223, loss 0.180853, acc 0.9375\n",
      "2017-11-05T23:35:19.348126: step 3224, loss 0.0317763, acc 1\n",
      "2017-11-05T23:35:23.263030: step 3225, loss 0.11204, acc 0.90625\n",
      "2017-11-05T23:35:27.206965: step 3226, loss 0.159441, acc 0.90625\n",
      "2017-11-05T23:35:31.259165: step 3227, loss 0.194107, acc 0.90625\n",
      "2017-11-05T23:35:35.763407: step 3228, loss 0.157313, acc 0.90625\n",
      "2017-11-05T23:35:40.257396: step 3229, loss 0.186622, acc 0.90625\n",
      "2017-11-05T23:35:44.488686: step 3230, loss 0.221834, acc 0.84375\n",
      "2017-11-05T23:35:48.626339: step 3231, loss 0.106767, acc 0.9375\n",
      "2017-11-05T23:35:53.193385: step 3232, loss 0.333586, acc 0.84375\n",
      "2017-11-05T23:35:57.647096: step 3233, loss 0.228111, acc 0.9375\n",
      "2017-11-05T23:36:01.992888: step 3234, loss 0.265234, acc 0.875\n",
      "2017-11-05T23:36:06.741146: step 3235, loss 0.262006, acc 0.90625\n",
      "2017-11-05T23:36:10.894027: step 3236, loss 0.210414, acc 0.90625\n",
      "2017-11-05T23:36:15.302357: step 3237, loss 0.274793, acc 0.78125\n",
      "2017-11-05T23:36:19.915835: step 3238, loss 0.269542, acc 0.90625\n",
      "2017-11-05T23:36:24.491446: step 3239, loss 0.128308, acc 0.9375\n",
      "2017-11-05T23:36:27.329462: step 3240, loss 0.129906, acc 0.9\n",
      "2017-11-05T23:36:31.348332: step 3241, loss 0.116112, acc 0.96875\n",
      "2017-11-05T23:36:35.451626: step 3242, loss 0.315157, acc 0.875\n",
      "2017-11-05T23:36:39.433031: step 3243, loss 0.28891, acc 0.84375\n",
      "2017-11-05T23:36:43.441484: step 3244, loss 0.171816, acc 0.90625\n",
      "2017-11-05T23:36:47.344709: step 3245, loss 0.187985, acc 0.90625\n",
      "2017-11-05T23:36:51.239252: step 3246, loss 0.117387, acc 0.9375\n",
      "2017-11-05T23:36:55.173646: step 3247, loss 0.0806389, acc 0.96875\n",
      "2017-11-05T23:36:59.083655: step 3248, loss 0.0467188, acc 1\n",
      "2017-11-05T23:37:03.068776: step 3249, loss 0.203399, acc 0.875\n",
      "2017-11-05T23:37:07.001734: step 3250, loss 0.26174, acc 0.875\n",
      "2017-11-05T23:37:10.963364: step 3251, loss 0.138396, acc 0.90625\n",
      "2017-11-05T23:37:14.893639: step 3252, loss 0.274849, acc 0.875\n",
      "2017-11-05T23:37:18.808893: step 3253, loss 0.0821127, acc 0.96875\n",
      "2017-11-05T23:37:22.799830: step 3254, loss 0.148346, acc 0.9375\n",
      "2017-11-05T23:37:26.701317: step 3255, loss 0.185585, acc 0.9375\n",
      "2017-11-05T23:37:30.632879: step 3256, loss 0.277395, acc 0.875\n",
      "2017-11-05T23:37:34.543089: step 3257, loss 0.0472595, acc 0.96875\n",
      "2017-11-05T23:37:38.431012: step 3258, loss 0.146497, acc 0.90625\n",
      "2017-11-05T23:37:42.356134: step 3259, loss 0.16197, acc 0.90625\n",
      "2017-11-05T23:37:46.270185: step 3260, loss 0.100738, acc 0.96875\n",
      "2017-11-05T23:37:50.233658: step 3261, loss 0.155393, acc 0.9375\n",
      "2017-11-05T23:37:54.171716: step 3262, loss 0.518607, acc 0.84375\n",
      "2017-11-05T23:37:58.096745: step 3263, loss 0.123875, acc 0.9375\n",
      "2017-11-05T23:38:02.017745: step 3264, loss 0.250028, acc 0.875\n",
      "2017-11-05T23:38:05.969528: step 3265, loss 0.353, acc 0.8125\n",
      "2017-11-05T23:38:09.945865: step 3266, loss 0.203119, acc 0.90625\n",
      "2017-11-05T23:38:13.922594: step 3267, loss 0.20492, acc 0.9375\n",
      "2017-11-05T23:38:17.825821: step 3268, loss 0.239193, acc 0.875\n",
      "2017-11-05T23:38:21.933406: step 3269, loss 0.156268, acc 0.90625\n",
      "2017-11-05T23:38:26.052964: step 3270, loss 0.0958854, acc 0.96875\n",
      "2017-11-05T23:38:29.996836: step 3271, loss 0.222662, acc 0.84375\n",
      "2017-11-05T23:38:34.082484: step 3272, loss 0.0905662, acc 0.9375\n",
      "2017-11-05T23:38:38.032572: step 3273, loss 0.145531, acc 0.90625\n",
      "2017-11-05T23:38:41.949898: step 3274, loss 0.113177, acc 0.9375\n",
      "2017-11-05T23:38:45.880663: step 3275, loss 0.120345, acc 0.9375\n",
      "2017-11-05T23:38:48.384824: step 3276, loss 0.17912, acc 0.9\n",
      "2017-11-05T23:38:52.275539: step 3277, loss 0.128955, acc 0.9375\n",
      "2017-11-05T23:38:56.217585: step 3278, loss 0.119865, acc 0.9375\n",
      "2017-11-05T23:39:00.170404: step 3279, loss 0.0211352, acc 1\n",
      "2017-11-05T23:39:04.093217: step 3280, loss 0.259446, acc 0.84375\n",
      "2017-11-05T23:39:07.989485: step 3281, loss 0.189557, acc 0.90625\n",
      "2017-11-05T23:39:11.923035: step 3282, loss 0.237144, acc 0.90625\n",
      "2017-11-05T23:39:15.851596: step 3283, loss 0.168091, acc 0.9375\n",
      "2017-11-05T23:39:19.766889: step 3284, loss 0.323998, acc 0.84375\n",
      "2017-11-05T23:39:23.711372: step 3285, loss 0.111019, acc 0.96875\n",
      "2017-11-05T23:39:27.644645: step 3286, loss 0.0953482, acc 0.96875\n",
      "2017-11-05T23:39:31.583044: step 3287, loss 0.0757369, acc 0.96875\n",
      "2017-11-05T23:39:35.494385: step 3288, loss 0.213409, acc 0.90625\n",
      "2017-11-05T23:39:39.463666: step 3289, loss 0.0484979, acc 1\n",
      "2017-11-05T23:39:43.441494: step 3290, loss 0.0367631, acc 1\n",
      "2017-11-05T23:39:47.377061: step 3291, loss 0.217085, acc 0.875\n",
      "2017-11-05T23:39:51.330654: step 3292, loss 0.0735999, acc 0.96875\n",
      "2017-11-05T23:39:55.239520: step 3293, loss 0.223349, acc 0.875\n",
      "2017-11-05T23:39:59.168571: step 3294, loss 0.26923, acc 0.8125\n",
      "2017-11-05T23:40:03.334534: step 3295, loss 0.29516, acc 0.8125\n",
      "2017-11-05T23:40:07.296175: step 3296, loss 0.139486, acc 0.9375\n",
      "2017-11-05T23:40:11.280183: step 3297, loss 0.305207, acc 0.90625\n",
      "2017-11-05T23:40:15.183990: step 3298, loss 0.216205, acc 0.90625\n",
      "2017-11-05T23:40:19.133446: step 3299, loss 0.227598, acc 0.875\n",
      "2017-11-05T23:40:23.172422: step 3300, loss 0.169355, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:40:25.717708: step 3300, loss 0.768635, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-05T23:40:31.002565: step 3301, loss 0.3364, acc 0.8125\n",
      "2017-11-05T23:40:35.163147: step 3302, loss 0.25475, acc 0.90625\n",
      "2017-11-05T23:40:39.210808: step 3303, loss 0.209139, acc 0.875\n",
      "2017-11-05T23:40:43.136504: step 3304, loss 0.196385, acc 0.875\n",
      "2017-11-05T23:40:47.101802: step 3305, loss 0.443445, acc 0.78125\n",
      "2017-11-05T23:40:51.027475: step 3306, loss 0.200634, acc 0.96875\n",
      "2017-11-05T23:40:54.954742: step 3307, loss 0.146429, acc 0.9375\n",
      "2017-11-05T23:40:58.903507: step 3308, loss 0.159187, acc 0.9375\n",
      "2017-11-05T23:41:02.876246: step 3309, loss 0.0615218, acc 0.9375\n",
      "2017-11-05T23:41:06.813961: step 3310, loss 0.260882, acc 0.875\n",
      "2017-11-05T23:41:11.278343: step 3311, loss 0.163371, acc 0.96875\n",
      "2017-11-05T23:41:14.415100: step 3312, loss 0.10574, acc 0.95\n",
      "2017-11-05T23:41:18.526689: step 3313, loss 0.153976, acc 0.96875\n",
      "2017-11-05T23:41:22.622615: step 3314, loss 0.114703, acc 0.9375\n",
      "2017-11-05T23:41:26.639808: step 3315, loss 0.134944, acc 0.9375\n",
      "2017-11-05T23:41:30.608819: step 3316, loss 0.17081, acc 0.90625\n",
      "2017-11-05T23:41:34.545366: step 3317, loss 0.0918859, acc 0.9375\n",
      "2017-11-05T23:41:38.506281: step 3318, loss 0.0871558, acc 0.96875\n",
      "2017-11-05T23:41:42.455498: step 3319, loss 0.141267, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:41:46.413855: step 3320, loss 0.164208, acc 0.96875\n",
      "2017-11-05T23:41:50.447513: step 3321, loss 0.0825715, acc 0.96875\n",
      "2017-11-05T23:41:54.399158: step 3322, loss 0.185649, acc 0.9375\n",
      "2017-11-05T23:41:58.344705: step 3323, loss 0.164001, acc 0.90625\n",
      "2017-11-05T23:42:02.254174: step 3324, loss 0.122462, acc 0.9375\n",
      "2017-11-05T23:42:06.224066: step 3325, loss 0.191072, acc 0.90625\n",
      "2017-11-05T23:42:10.154854: step 3326, loss 0.145035, acc 0.90625\n",
      "2017-11-05T23:42:14.138405: step 3327, loss 0.212368, acc 0.875\n",
      "2017-11-05T23:42:18.155091: step 3328, loss 0.303167, acc 0.90625\n",
      "2017-11-05T23:42:22.110769: step 3329, loss 0.213489, acc 0.90625\n",
      "2017-11-05T23:42:26.158296: step 3330, loss 0.107811, acc 0.9375\n",
      "2017-11-05T23:42:30.127976: step 3331, loss 0.191663, acc 0.90625\n",
      "2017-11-05T23:42:34.229524: step 3332, loss 0.160005, acc 0.9375\n",
      "2017-11-05T23:42:38.258625: step 3333, loss 0.115067, acc 0.9375\n",
      "2017-11-05T23:42:42.258171: step 3334, loss 0.311706, acc 0.875\n",
      "2017-11-05T23:42:46.290664: step 3335, loss 0.318309, acc 0.875\n",
      "2017-11-05T23:42:50.285070: step 3336, loss 0.129204, acc 0.9375\n",
      "2017-11-05T23:42:54.252775: step 3337, loss 0.249255, acc 0.90625\n",
      "2017-11-05T23:42:58.281600: step 3338, loss 0.139742, acc 0.90625\n",
      "2017-11-05T23:43:02.288820: step 3339, loss 0.277604, acc 0.875\n",
      "2017-11-05T23:43:06.303058: step 3340, loss 0.25036, acc 0.875\n",
      "2017-11-05T23:43:10.311545: step 3341, loss 0.222915, acc 0.875\n",
      "2017-11-05T23:43:14.302524: step 3342, loss 0.380938, acc 0.8125\n",
      "2017-11-05T23:43:18.319258: step 3343, loss 0.253763, acc 0.90625\n",
      "2017-11-05T23:43:22.468266: step 3344, loss 0.166139, acc 0.875\n",
      "2017-11-05T23:43:26.666534: step 3345, loss 0.195289, acc 0.875\n",
      "2017-11-05T23:43:30.674329: step 3346, loss 0.121838, acc 0.96875\n",
      "2017-11-05T23:43:34.643496: step 3347, loss 0.181825, acc 0.9375\n",
      "2017-11-05T23:43:37.234970: step 3348, loss 0.631444, acc 0.7\n",
      "2017-11-05T23:43:41.234161: step 3349, loss 0.130873, acc 0.9375\n",
      "2017-11-05T23:43:45.206220: step 3350, loss 0.27979, acc 0.875\n",
      "2017-11-05T23:43:49.184715: step 3351, loss 0.104626, acc 0.9375\n",
      "2017-11-05T23:43:53.123337: step 3352, loss 0.0773529, acc 0.96875\n",
      "2017-11-05T23:43:57.005822: step 3353, loss 0.154257, acc 0.9375\n",
      "2017-11-05T23:44:00.972942: step 3354, loss 0.0800722, acc 0.96875\n",
      "2017-11-05T23:44:05.014995: step 3355, loss 0.101536, acc 0.9375\n",
      "2017-11-05T23:44:08.937701: step 3356, loss 0.027298, acc 1\n",
      "2017-11-05T23:44:12.906853: step 3357, loss 0.132246, acc 0.96875\n",
      "2017-11-05T23:44:16.819672: step 3358, loss 0.163944, acc 0.9375\n",
      "2017-11-05T23:44:20.731006: step 3359, loss 0.0558638, acc 0.96875\n",
      "2017-11-05T23:44:24.658467: step 3360, loss 0.148535, acc 0.90625\n",
      "2017-11-05T23:44:28.651210: step 3361, loss 0.185312, acc 0.9375\n",
      "2017-11-05T23:44:32.605440: step 3362, loss 0.436315, acc 0.8125\n",
      "2017-11-05T23:44:36.667023: step 3363, loss 0.142073, acc 0.875\n",
      "2017-11-05T23:44:40.610507: step 3364, loss 0.0237409, acc 1\n",
      "2017-11-05T23:44:44.537324: step 3365, loss 0.146316, acc 0.875\n",
      "2017-11-05T23:44:48.474756: step 3366, loss 0.263394, acc 0.90625\n",
      "2017-11-05T23:44:52.441917: step 3367, loss 0.458604, acc 0.8125\n",
      "2017-11-05T23:44:56.387587: step 3368, loss 0.209821, acc 0.84375\n",
      "2017-11-05T23:45:00.339512: step 3369, loss 0.368348, acc 0.75\n",
      "2017-11-05T23:45:04.295356: step 3370, loss 0.145498, acc 0.90625\n",
      "2017-11-05T23:45:08.213492: step 3371, loss 0.022298, acc 1\n",
      "2017-11-05T23:45:12.145381: step 3372, loss 0.322045, acc 0.875\n",
      "2017-11-05T23:45:16.108415: step 3373, loss 0.254018, acc 0.875\n",
      "2017-11-05T23:45:20.110949: step 3374, loss 0.185531, acc 0.9375\n",
      "2017-11-05T23:45:24.057440: step 3375, loss 0.151671, acc 0.90625\n",
      "2017-11-05T23:45:28.009054: step 3376, loss 0.162151, acc 0.90625\n",
      "2017-11-05T23:45:32.001521: step 3377, loss 0.278953, acc 0.8125\n",
      "2017-11-05T23:45:36.009476: step 3378, loss 0.231723, acc 0.875\n",
      "2017-11-05T23:45:39.942122: step 3379, loss 0.0916292, acc 0.96875\n",
      "2017-11-05T23:45:43.866943: step 3380, loss 0.186221, acc 0.875\n",
      "2017-11-05T23:45:47.835400: step 3381, loss 0.266498, acc 0.875\n",
      "2017-11-05T23:45:51.794926: step 3382, loss 0.279643, acc 0.9375\n",
      "2017-11-05T23:45:55.735321: step 3383, loss 0.366817, acc 0.875\n",
      "2017-11-05T23:45:58.214866: step 3384, loss 0.257786, acc 0.85\n",
      "2017-11-05T23:46:02.169573: step 3385, loss 0.10656, acc 0.9375\n",
      "2017-11-05T23:46:06.128716: step 3386, loss 0.0782439, acc 0.96875\n",
      "2017-11-05T23:46:10.065578: step 3387, loss 0.15942, acc 0.90625\n",
      "2017-11-05T23:46:14.028295: step 3388, loss 0.206618, acc 0.9375\n",
      "2017-11-05T23:46:17.919888: step 3389, loss 0.0853742, acc 0.96875\n",
      "2017-11-05T23:46:21.852439: step 3390, loss 0.102472, acc 0.96875\n",
      "2017-11-05T23:46:25.781473: step 3391, loss 0.144088, acc 0.9375\n",
      "2017-11-05T23:46:29.780710: step 3392, loss 0.20797, acc 0.90625\n",
      "2017-11-05T23:46:33.848871: step 3393, loss 0.349184, acc 0.84375\n",
      "2017-11-05T23:46:38.068999: step 3394, loss 0.0227325, acc 1\n",
      "2017-11-05T23:46:42.061872: step 3395, loss 0.160963, acc 0.9375\n",
      "2017-11-05T23:46:45.980948: step 3396, loss 0.073889, acc 0.9375\n",
      "2017-11-05T23:46:49.943603: step 3397, loss 0.283973, acc 0.875\n",
      "2017-11-05T23:46:53.864863: step 3398, loss 0.261185, acc 0.90625\n",
      "2017-11-05T23:46:57.794076: step 3399, loss 0.178167, acc 0.9375\n",
      "2017-11-05T23:47:01.718899: step 3400, loss 0.130527, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:47:04.263286: step 3400, loss 0.867571, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-05T23:47:09.677783: step 3401, loss 0.325329, acc 0.90625\n",
      "2017-11-05T23:47:13.676086: step 3402, loss 0.210002, acc 0.90625\n",
      "2017-11-05T23:47:17.661847: step 3403, loss 0.106043, acc 0.96875\n",
      "2017-11-05T23:47:21.649625: step 3404, loss 0.16544, acc 0.90625\n",
      "2017-11-05T23:47:25.654259: step 3405, loss 0.202983, acc 0.875\n",
      "2017-11-05T23:47:29.615938: step 3406, loss 0.106291, acc 0.96875\n",
      "2017-11-05T23:47:33.596828: step 3407, loss 0.175824, acc 0.90625\n",
      "2017-11-05T23:47:37.573148: step 3408, loss 0.171005, acc 0.96875\n",
      "2017-11-05T23:47:41.592580: step 3409, loss 0.25013, acc 0.84375\n",
      "2017-11-05T23:47:45.526487: step 3410, loss 0.16048, acc 0.9375\n",
      "2017-11-05T23:47:49.487126: step 3411, loss 0.0460037, acc 0.96875\n",
      "2017-11-05T23:47:53.412343: step 3412, loss 0.185323, acc 0.90625\n",
      "2017-11-05T23:47:57.378584: step 3413, loss 0.214207, acc 0.9375\n",
      "2017-11-05T23:48:01.352705: step 3414, loss 0.149149, acc 0.9375\n",
      "2017-11-05T23:48:05.309116: step 3415, loss 0.324044, acc 0.90625\n",
      "2017-11-05T23:48:09.238791: step 3416, loss 0.346617, acc 0.8125\n",
      "2017-11-05T23:48:13.202388: step 3417, loss 0.172794, acc 0.9375\n",
      "2017-11-05T23:48:17.144139: step 3418, loss 0.0459529, acc 0.96875\n",
      "2017-11-05T23:48:21.107834: step 3419, loss 0.058537, acc 0.96875\n",
      "2017-11-05T23:48:23.628661: step 3420, loss 0.25878, acc 0.85\n",
      "2017-11-05T23:48:27.586763: step 3421, loss 0.144039, acc 0.96875\n",
      "2017-11-05T23:48:31.756949: step 3422, loss 0.163377, acc 0.9375\n",
      "2017-11-05T23:48:35.914539: step 3423, loss 0.17404, acc 0.90625\n",
      "2017-11-05T23:48:40.000388: step 3424, loss 0.213757, acc 0.90625\n",
      "2017-11-05T23:48:44.095165: step 3425, loss 0.0728402, acc 0.9375\n",
      "2017-11-05T23:48:48.131928: step 3426, loss 0.255251, acc 0.875\n",
      "2017-11-05T23:48:52.298127: step 3427, loss 0.236135, acc 0.90625\n",
      "2017-11-05T23:48:56.343849: step 3428, loss 0.0698902, acc 0.96875\n",
      "2017-11-05T23:49:00.556250: step 3429, loss 0.306726, acc 0.875\n",
      "2017-11-05T23:49:04.640875: step 3430, loss 0.24945, acc 0.90625\n",
      "2017-11-05T23:49:08.826971: step 3431, loss 0.034136, acc 1\n",
      "2017-11-05T23:49:12.947488: step 3432, loss 0.120727, acc 0.9375\n",
      "2017-11-05T23:49:17.147432: step 3433, loss 0.122829, acc 0.96875\n",
      "2017-11-05T23:49:21.242010: step 3434, loss 0.012818, acc 1\n",
      "2017-11-05T23:49:25.516437: step 3435, loss 0.0426095, acc 0.96875\n",
      "2017-11-05T23:49:29.859677: step 3436, loss 0.189647, acc 0.90625\n",
      "2017-11-05T23:49:33.991438: step 3437, loss 0.0616066, acc 0.96875\n",
      "2017-11-05T23:49:38.039687: step 3438, loss 0.229767, acc 0.9375\n",
      "2017-11-05T23:49:42.155805: step 3439, loss 0.153692, acc 0.90625\n",
      "2017-11-05T23:49:46.122508: step 3440, loss 0.252052, acc 0.875\n",
      "2017-11-05T23:49:50.152843: step 3441, loss 0.149766, acc 0.90625\n",
      "2017-11-05T23:49:54.115335: step 3442, loss 0.0246018, acc 1\n",
      "2017-11-05T23:49:58.090168: step 3443, loss 0.232753, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:50:02.322840: step 3444, loss 0.146943, acc 0.9375\n",
      "2017-11-05T23:50:06.308308: step 3445, loss 0.194437, acc 0.9375\n",
      "2017-11-05T23:50:10.263084: step 3446, loss 0.0744598, acc 0.96875\n",
      "2017-11-05T23:50:14.298191: step 3447, loss 0.267097, acc 0.84375\n",
      "2017-11-05T23:50:18.200728: step 3448, loss 0.214192, acc 0.875\n",
      "2017-11-05T23:50:22.161230: step 3449, loss 0.298473, acc 0.84375\n",
      "2017-11-05T23:50:26.182792: step 3450, loss 0.300183, acc 0.8125\n",
      "2017-11-05T23:50:30.156510: step 3451, loss 0.335905, acc 0.78125\n",
      "2017-11-05T23:50:34.270576: step 3452, loss 0.175715, acc 0.9375\n",
      "2017-11-05T23:50:38.374671: step 3453, loss 0.17286, acc 0.90625\n",
      "2017-11-05T23:50:42.364071: step 3454, loss 0.105798, acc 0.96875\n",
      "2017-11-05T23:50:46.309704: step 3455, loss 0.325915, acc 0.78125\n",
      "2017-11-05T23:50:48.816392: step 3456, loss 0.136821, acc 0.9\n",
      "2017-11-05T23:50:52.823395: step 3457, loss 0.114346, acc 0.9375\n",
      "2017-11-05T23:50:56.783297: step 3458, loss 0.155119, acc 0.9375\n",
      "2017-11-05T23:51:00.794571: step 3459, loss 0.114089, acc 0.9375\n",
      "2017-11-05T23:51:04.742234: step 3460, loss 0.252907, acc 0.90625\n",
      "2017-11-05T23:51:08.693545: step 3461, loss 0.223392, acc 0.875\n",
      "2017-11-05T23:51:12.674950: step 3462, loss 0.111986, acc 0.9375\n",
      "2017-11-05T23:51:16.632152: step 3463, loss 0.202619, acc 0.875\n",
      "2017-11-05T23:51:20.623154: step 3464, loss 0.129274, acc 0.9375\n",
      "2017-11-05T23:51:24.597820: step 3465, loss 0.332303, acc 0.84375\n",
      "2017-11-05T23:51:28.551367: step 3466, loss 0.117926, acc 0.9375\n",
      "2017-11-05T23:51:32.582188: step 3467, loss 0.219473, acc 0.84375\n",
      "2017-11-05T23:51:36.592864: step 3468, loss 0.243898, acc 0.875\n",
      "2017-11-05T23:51:40.584735: step 3469, loss 0.134499, acc 0.90625\n",
      "2017-11-05T23:51:44.581265: step 3470, loss 0.335239, acc 0.84375\n",
      "2017-11-05T23:51:48.576435: step 3471, loss 0.117222, acc 0.9375\n",
      "2017-11-05T23:51:52.547914: step 3472, loss 0.193698, acc 0.875\n",
      "2017-11-05T23:51:56.454162: step 3473, loss 0.109179, acc 0.9375\n",
      "2017-11-05T23:52:00.479935: step 3474, loss 0.187856, acc 0.9375\n",
      "2017-11-05T23:52:04.453588: step 3475, loss 0.172283, acc 0.9375\n",
      "2017-11-05T23:52:08.405971: step 3476, loss 0.13294, acc 0.90625\n",
      "2017-11-05T23:52:12.388978: step 3477, loss 0.130077, acc 0.96875\n",
      "2017-11-05T23:52:16.360747: step 3478, loss 0.178961, acc 0.90625\n",
      "2017-11-05T23:52:20.396212: step 3479, loss 0.128149, acc 0.90625\n",
      "2017-11-05T23:52:24.366482: step 3480, loss 0.271406, acc 0.875\n",
      "2017-11-05T23:52:28.349705: step 3481, loss 0.113642, acc 0.90625\n",
      "2017-11-05T23:52:32.373684: step 3482, loss 0.138018, acc 0.90625\n",
      "2017-11-05T23:52:36.561191: step 3483, loss 0.261757, acc 0.9375\n",
      "2017-11-05T23:52:40.578735: step 3484, loss 0.337764, acc 0.84375\n",
      "2017-11-05T23:52:44.600045: step 3485, loss 0.147427, acc 0.96875\n",
      "2017-11-05T23:52:48.565307: step 3486, loss 0.0612821, acc 0.96875\n",
      "2017-11-05T23:52:52.523196: step 3487, loss 0.125824, acc 0.9375\n",
      "2017-11-05T23:52:56.535580: step 3488, loss 0.470894, acc 0.84375\n",
      "2017-11-05T23:53:00.542794: step 3489, loss 0.2496, acc 0.90625\n",
      "2017-11-05T23:53:04.669424: step 3490, loss 0.240415, acc 0.84375\n",
      "2017-11-05T23:53:08.681525: step 3491, loss 0.27507, acc 0.875\n",
      "2017-11-05T23:53:11.161413: step 3492, loss 0.243984, acc 0.9\n",
      "2017-11-05T23:53:15.185138: step 3493, loss 0.036197, acc 0.96875\n",
      "2017-11-05T23:53:19.122999: step 3494, loss 0.241036, acc 0.90625\n",
      "2017-11-05T23:53:23.135728: step 3495, loss 0.114223, acc 0.9375\n",
      "2017-11-05T23:53:27.243936: step 3496, loss 0.133674, acc 0.90625\n",
      "2017-11-05T23:53:31.240458: step 3497, loss 0.207536, acc 0.875\n",
      "2017-11-05T23:53:35.262143: step 3498, loss 0.0953236, acc 0.96875\n",
      "2017-11-05T23:53:39.210151: step 3499, loss 0.143602, acc 0.9375\n",
      "2017-11-05T23:53:43.250309: step 3500, loss 0.0849307, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-05T23:53:45.810694: step 3500, loss 0.761191, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-05T23:53:51.350013: step 3501, loss 0.218199, acc 0.875\n",
      "2017-11-05T23:53:55.382267: step 3502, loss 0.167409, acc 0.90625\n",
      "2017-11-05T23:53:59.326580: step 3503, loss 0.226024, acc 0.90625\n",
      "2017-11-05T23:54:03.385016: step 3504, loss 0.1524, acc 0.9375\n",
      "2017-11-05T23:54:07.513298: step 3505, loss 0.038251, acc 1\n",
      "2017-11-05T23:54:11.440889: step 3506, loss 0.15422, acc 0.90625\n",
      "2017-11-05T23:54:15.478607: step 3507, loss 0.146396, acc 0.9375\n",
      "2017-11-05T23:54:19.462188: step 3508, loss 0.0867903, acc 0.9375\n",
      "2017-11-05T23:54:23.628388: step 3509, loss 0.211995, acc 0.90625\n",
      "2017-11-05T23:54:27.779777: step 3510, loss 0.399059, acc 0.875\n",
      "2017-11-05T23:54:31.782334: step 3511, loss 0.142111, acc 0.9375\n",
      "2017-11-05T23:54:35.981692: step 3512, loss 0.0494611, acc 1\n",
      "2017-11-05T23:54:40.132456: step 3513, loss 0.148539, acc 0.90625\n",
      "2017-11-05T23:54:44.194214: step 3514, loss 0.18959, acc 0.875\n",
      "2017-11-05T23:54:48.135898: step 3515, loss 0.149945, acc 0.9375\n",
      "2017-11-05T23:54:52.102807: step 3516, loss 0.197219, acc 0.9375\n",
      "2017-11-05T23:54:56.206105: step 3517, loss 0.188928, acc 0.90625\n",
      "2017-11-05T23:55:00.194401: step 3518, loss 0.489173, acc 0.78125\n",
      "2017-11-05T23:55:04.241519: step 3519, loss 0.181989, acc 0.90625\n",
      "2017-11-05T23:55:08.215824: step 3520, loss 0.111922, acc 0.9375\n",
      "2017-11-05T23:55:12.248316: step 3521, loss 0.317589, acc 0.78125\n",
      "2017-11-05T23:55:16.287671: step 3522, loss 0.107383, acc 0.96875\n",
      "2017-11-05T23:55:20.298786: step 3523, loss 0.269313, acc 0.9375\n",
      "2017-11-05T23:55:24.371789: step 3524, loss 0.191554, acc 0.9375\n",
      "2017-11-05T23:55:28.377490: step 3525, loss 0.274943, acc 0.875\n",
      "2017-11-05T23:55:32.411232: step 3526, loss 0.168796, acc 0.90625\n",
      "2017-11-05T23:55:36.368564: step 3527, loss 0.306719, acc 0.875\n",
      "2017-11-05T23:55:38.926655: step 3528, loss 0.0582164, acc 0.95\n",
      "2017-11-05T23:55:42.957331: step 3529, loss 0.231377, acc 0.9375\n",
      "2017-11-05T23:55:46.913136: step 3530, loss 0.112837, acc 0.9375\n",
      "2017-11-05T23:55:50.897373: step 3531, loss 0.205179, acc 0.90625\n",
      "2017-11-05T23:55:54.844928: step 3532, loss 0.253081, acc 0.90625\n",
      "2017-11-05T23:55:58.796375: step 3533, loss 0.207866, acc 0.9375\n",
      "2017-11-05T23:56:02.734808: step 3534, loss 0.251121, acc 0.84375\n",
      "2017-11-05T23:56:06.694644: step 3535, loss 0.305142, acc 0.8125\n",
      "2017-11-05T23:56:10.648374: step 3536, loss 0.157976, acc 0.96875\n",
      "2017-11-05T23:56:14.587585: step 3537, loss 0.140652, acc 0.90625\n",
      "2017-11-05T23:56:18.564157: step 3538, loss 0.0637253, acc 0.96875\n",
      "2017-11-05T23:56:22.537944: step 3539, loss 0.538141, acc 0.8125\n",
      "2017-11-05T23:56:26.535287: step 3540, loss 0.176136, acc 0.875\n",
      "2017-11-05T23:56:30.463895: step 3541, loss 0.26371, acc 0.90625\n",
      "2017-11-05T23:56:34.600977: step 3542, loss 0.288265, acc 0.90625\n",
      "2017-11-05T23:56:38.617244: step 3543, loss 0.157674, acc 0.90625\n",
      "2017-11-05T23:56:42.630098: step 3544, loss 0.154426, acc 0.9375\n",
      "2017-11-05T23:56:46.594511: step 3545, loss 0.162388, acc 0.90625\n",
      "2017-11-05T23:56:50.612095: step 3546, loss 0.216596, acc 0.875\n",
      "2017-11-05T23:56:54.647528: step 3547, loss 0.0747288, acc 0.96875\n",
      "2017-11-05T23:56:58.594604: step 3548, loss 0.219033, acc 0.96875\n",
      "2017-11-05T23:57:02.548093: step 3549, loss 0.139706, acc 0.96875\n",
      "2017-11-05T23:57:06.482546: step 3550, loss 0.154063, acc 0.96875\n",
      "2017-11-05T23:57:10.458789: step 3551, loss 0.233031, acc 0.875\n",
      "2017-11-05T23:57:14.461216: step 3552, loss 0.183793, acc 0.90625\n",
      "2017-11-05T23:57:18.415407: step 3553, loss 0.40263, acc 0.84375\n",
      "2017-11-05T23:57:22.341866: step 3554, loss 0.117314, acc 0.96875\n",
      "2017-11-05T23:57:26.348751: step 3555, loss 0.0261451, acc 1\n",
      "2017-11-05T23:57:30.281840: step 3556, loss 0.19911, acc 0.875\n",
      "2017-11-05T23:57:34.258794: step 3557, loss 0.300332, acc 0.875\n",
      "2017-11-05T23:57:38.245299: step 3558, loss 0.313176, acc 0.84375\n",
      "2017-11-05T23:57:42.167478: step 3559, loss 0.156832, acc 0.9375\n",
      "2017-11-05T23:57:46.157524: step 3560, loss 0.186817, acc 0.90625\n",
      "2017-11-05T23:57:50.200825: step 3561, loss 0.188192, acc 0.9375\n",
      "2017-11-05T23:57:54.155135: step 3562, loss 0.239685, acc 0.90625\n",
      "2017-11-05T23:57:58.129744: step 3563, loss 0.317408, acc 0.8125\n",
      "2017-11-05T23:58:00.668294: step 3564, loss 0.268188, acc 0.9\n",
      "2017-11-05T23:58:04.653105: step 3565, loss 0.192911, acc 0.90625\n",
      "2017-11-05T23:58:08.597706: step 3566, loss 0.272858, acc 0.8125\n",
      "2017-11-05T23:58:12.549526: step 3567, loss 0.064918, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-05T23:58:16.503580: step 3568, loss 0.0964926, acc 0.96875\n",
      "2017-11-05T23:58:20.451717: step 3569, loss 0.171891, acc 0.9375\n",
      "2017-11-05T23:58:24.590557: step 3570, loss 0.0369851, acc 1\n",
      "2017-11-05T23:58:28.599204: step 3571, loss 0.102565, acc 0.96875\n",
      "2017-11-05T23:58:32.587352: step 3572, loss 0.220643, acc 0.875\n",
      "2017-11-05T23:58:36.742268: step 3573, loss 0.0435826, acc 1\n",
      "2017-11-05T23:58:40.690576: step 3574, loss 0.181194, acc 0.90625\n",
      "2017-11-05T23:58:44.678473: step 3575, loss 0.15991, acc 0.9375\n",
      "2017-11-05T23:58:48.697453: step 3576, loss 0.250765, acc 0.875\n",
      "2017-11-05T23:58:52.634154: step 3577, loss 0.0514403, acc 1\n",
      "2017-11-05T23:58:56.591372: step 3578, loss 0.13142, acc 0.9375\n",
      "2017-11-05T23:59:00.597926: step 3579, loss 0.222513, acc 0.90625\n",
      "2017-11-05T23:59:04.557544: step 3580, loss 0.409994, acc 0.78125\n",
      "2017-11-05T23:59:08.505206: step 3581, loss 0.194488, acc 0.875\n",
      "2017-11-05T23:59:12.504022: step 3582, loss 0.26334, acc 0.875\n",
      "2017-11-05T23:59:16.455923: step 3583, loss 0.0810479, acc 0.9375\n",
      "2017-11-05T23:59:20.411948: step 3584, loss 0.11365, acc 0.9375\n",
      "2017-11-05T23:59:24.387176: step 3585, loss 0.164021, acc 0.875\n",
      "2017-11-05T23:59:28.297027: step 3586, loss 0.213616, acc 0.90625\n",
      "2017-11-05T23:59:32.226676: step 3587, loss 0.0537242, acc 0.96875\n",
      "2017-11-05T23:59:36.228158: step 3588, loss 0.0263364, acc 1\n",
      "2017-11-05T23:59:40.148254: step 3589, loss 0.174433, acc 0.90625\n",
      "2017-11-05T23:59:44.116084: step 3590, loss 0.195573, acc 0.90625\n",
      "2017-11-05T23:59:48.042632: step 3591, loss 0.197776, acc 0.875\n",
      "2017-11-05T23:59:52.126392: step 3592, loss 0.341644, acc 0.8125\n",
      "2017-11-05T23:59:56.079589: step 3593, loss 0.188596, acc 0.875\n",
      "2017-11-06T00:00:00.046315: step 3594, loss 0.191437, acc 0.90625\n",
      "2017-11-06T00:00:04.316374: step 3595, loss 0.307348, acc 0.84375\n",
      "2017-11-06T00:00:08.336790: step 3596, loss 0.14745, acc 0.90625\n",
      "2017-11-06T00:00:12.348244: step 3597, loss 0.203456, acc 0.90625\n",
      "2017-11-06T00:00:16.273793: step 3598, loss 0.0609752, acc 0.96875\n",
      "2017-11-06T00:00:20.204106: step 3599, loss 0.26228, acc 0.875\n",
      "2017-11-06T00:00:22.743482: step 3600, loss 0.150063, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:00:25.266793: step 3600, loss 0.863958, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\3\\1509886845\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113800133C8>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\n",
      "\n",
      "2017-11-06T00:00:35.603537: step 1, loss 1.27292, acc 0.71875\n",
      "2017-11-06T00:00:39.601315: step 2, loss 0.761744, acc 0.84375\n",
      "2017-11-06T00:00:43.614393: step 3, loss 1.54185, acc 0.875\n",
      "2017-11-06T00:00:47.650869: step 4, loss 0.639193, acc 0.90625\n",
      "2017-11-06T00:00:51.681157: step 5, loss 0.00409722, acc 1\n",
      "2017-11-06T00:00:55.684991: step 6, loss 0.400546, acc 0.9375\n",
      "2017-11-06T00:00:59.695256: step 7, loss 1.38385, acc 0.9375\n",
      "2017-11-06T00:01:03.705292: step 8, loss 1.19119, acc 0.875\n",
      "2017-11-06T00:01:07.660826: step 9, loss 0.542074, acc 0.90625\n",
      "2017-11-06T00:01:11.710676: step 10, loss 1.9175, acc 0.78125\n",
      "2017-11-06T00:01:15.666454: step 11, loss 0.55242, acc 0.9375\n",
      "2017-11-06T00:01:19.681682: step 12, loss 1.28297, acc 0.75\n",
      "2017-11-06T00:01:23.685147: step 13, loss 1.15521, acc 0.8125\n",
      "2017-11-06T00:01:27.656728: step 14, loss 1.141, acc 0.78125\n",
      "2017-11-06T00:01:31.703918: step 15, loss 1.64806, acc 0.6875\n",
      "2017-11-06T00:01:35.712639: step 16, loss 1.63924, acc 0.6875\n",
      "2017-11-06T00:01:39.862597: step 17, loss 1.63784, acc 0.78125\n",
      "2017-11-06T00:01:44.056959: step 18, loss 2.38469, acc 0.65625\n",
      "2017-11-06T00:01:48.216993: step 19, loss 0.666269, acc 0.8125\n",
      "2017-11-06T00:01:52.401394: step 20, loss 1.68062, acc 0.65625\n",
      "2017-11-06T00:01:56.566840: step 21, loss 1.63975, acc 0.59375\n",
      "2017-11-06T00:02:00.771241: step 22, loss 2.01802, acc 0.78125\n",
      "2017-11-06T00:02:05.017744: step 23, loss 1.30815, acc 0.84375\n",
      "2017-11-06T00:02:09.272774: step 24, loss 1.1609, acc 0.875\n",
      "2017-11-06T00:02:13.610548: step 25, loss 0.921935, acc 0.90625\n",
      "2017-11-06T00:02:17.697209: step 26, loss 0.939232, acc 0.8125\n",
      "2017-11-06T00:02:22.040994: step 27, loss 0.376948, acc 0.9375\n",
      "2017-11-06T00:02:26.111172: step 28, loss 1.24844, acc 0.84375\n",
      "2017-11-06T00:02:30.325661: step 29, loss 0.387645, acc 0.875\n",
      "2017-11-06T00:02:34.697850: step 30, loss 1.25082, acc 0.875\n",
      "2017-11-06T00:02:39.013911: step 31, loss 1.52336, acc 0.8125\n",
      "2017-11-06T00:02:43.210372: step 32, loss 0.968754, acc 0.875\n",
      "2017-11-06T00:02:47.334930: step 33, loss 0.383569, acc 0.96875\n",
      "2017-11-06T00:02:51.473752: step 34, loss 0.913619, acc 0.875\n",
      "2017-11-06T00:02:55.605454: step 35, loss 2.65469, acc 0.78125\n",
      "2017-11-06T00:02:58.257711: step 36, loss 0.497058, acc 0.85\n",
      "2017-11-06T00:03:02.416505: step 37, loss 0.42832, acc 0.84375\n",
      "2017-11-06T00:03:06.690347: step 38, loss 0.442783, acc 0.875\n",
      "2017-11-06T00:03:10.914695: step 39, loss 0.115122, acc 0.9375\n",
      "2017-11-06T00:03:15.048017: step 40, loss 0.408827, acc 0.90625\n",
      "2017-11-06T00:03:19.259580: step 41, loss 1.0443, acc 0.84375\n",
      "2017-11-06T00:03:23.347897: step 42, loss 0.904467, acc 0.90625\n",
      "2017-11-06T00:03:27.344862: step 43, loss 0.339931, acc 0.90625\n",
      "2017-11-06T00:03:31.524822: step 44, loss 0.796712, acc 0.84375\n",
      "2017-11-06T00:03:36.031981: step 45, loss 1.253, acc 0.78125\n",
      "2017-11-06T00:03:40.661580: step 46, loss 0.939432, acc 0.75\n",
      "2017-11-06T00:03:45.384207: step 47, loss 0.986669, acc 0.84375\n",
      "2017-11-06T00:03:49.851732: step 48, loss 0.488972, acc 0.84375\n",
      "2017-11-06T00:03:54.104257: step 49, loss 1.00627, acc 0.8125\n",
      "2017-11-06T00:03:58.979568: step 50, loss 0.836941, acc 0.84375\n",
      "2017-11-06T00:04:03.650544: step 51, loss 0.985236, acc 0.84375\n",
      "2017-11-06T00:04:08.184956: step 52, loss 0.614183, acc 0.84375\n",
      "2017-11-06T00:04:12.695576: step 53, loss 1.3093, acc 0.875\n",
      "2017-11-06T00:04:17.454976: step 54, loss 0.654118, acc 0.875\n",
      "2017-11-06T00:04:21.907180: step 55, loss 1.15627, acc 0.875\n",
      "2017-11-06T00:04:26.799634: step 56, loss 0.36592, acc 0.9375\n",
      "2017-11-06T00:04:32.037531: step 57, loss 0.784497, acc 0.9375\n",
      "2017-11-06T00:04:37.091707: step 58, loss 0.0915931, acc 0.9375\n",
      "2017-11-06T00:04:41.898149: step 59, loss 0.297226, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T00:04:46.757528: step 60, loss 0.730624, acc 0.8125\n",
      "2017-11-06T00:04:51.554586: step 61, loss 1.66037, acc 0.78125\n",
      "2017-11-06T00:04:55.993582: step 62, loss 1.02586, acc 0.8125\n",
      "2017-11-06T00:05:00.393392: step 63, loss 0.491679, acc 0.8125\n",
      "2017-11-06T00:05:04.778684: step 64, loss 1.55099, acc 0.78125\n",
      "2017-11-06T00:05:08.962204: step 65, loss 1.23932, acc 0.8125\n",
      "2017-11-06T00:05:13.216942: step 66, loss 0.780838, acc 0.84375\n",
      "2017-11-06T00:05:17.583635: step 67, loss 1.82155, acc 0.71875\n",
      "2017-11-06T00:05:21.963765: step 68, loss 0.990976, acc 0.84375\n",
      "2017-11-06T00:05:26.336380: step 69, loss 1.69884, acc 0.8125\n",
      "2017-11-06T00:05:30.536037: step 70, loss 0.798549, acc 0.78125\n",
      "2017-11-06T00:05:35.251901: step 71, loss 0.545897, acc 0.8125\n",
      "2017-11-06T00:05:38.272652: step 72, loss 1.02073, acc 0.8\n",
      "2017-11-06T00:05:42.941489: step 73, loss 0.672604, acc 0.8125\n",
      "2017-11-06T00:05:47.662916: step 74, loss 0.961286, acc 0.75\n",
      "2017-11-06T00:05:52.014194: step 75, loss 0.572825, acc 0.8125\n",
      "2017-11-06T00:05:55.956671: step 76, loss 0.66267, acc 0.875\n",
      "2017-11-06T00:05:59.919496: step 77, loss 1.19206, acc 0.75\n",
      "2017-11-06T00:06:03.949279: step 78, loss 0.394443, acc 0.90625\n",
      "2017-11-06T00:06:07.885058: step 79, loss 0.658884, acc 0.875\n",
      "2017-11-06T00:06:11.872855: step 80, loss 0.0689161, acc 0.96875\n",
      "2017-11-06T00:06:16.013879: step 81, loss 0.484674, acc 0.9375\n",
      "2017-11-06T00:06:20.166934: step 82, loss 0.381918, acc 0.875\n",
      "2017-11-06T00:06:24.350640: step 83, loss 0.790064, acc 0.90625\n",
      "2017-11-06T00:06:28.367150: step 84, loss 0.478504, acc 0.90625\n",
      "2017-11-06T00:06:32.465752: step 85, loss 1.09847, acc 0.8125\n",
      "2017-11-06T00:06:37.395284: step 86, loss 1.51185, acc 0.8125\n",
      "2017-11-06T00:06:41.704684: step 87, loss 0.321755, acc 0.90625\n",
      "2017-11-06T00:06:45.821657: step 88, loss 0.334647, acc 0.9375\n",
      "2017-11-06T00:06:49.881571: step 89, loss 0.284415, acc 0.875\n",
      "2017-11-06T00:06:53.801663: step 90, loss 0.645292, acc 0.90625\n",
      "2017-11-06T00:06:57.701095: step 91, loss 0.256383, acc 0.9375\n",
      "2017-11-06T00:07:01.631322: step 92, loss 0.289948, acc 0.9375\n",
      "2017-11-06T00:07:05.594553: step 93, loss 0.295688, acc 0.875\n",
      "2017-11-06T00:07:09.485488: step 94, loss 0.569436, acc 0.84375\n",
      "2017-11-06T00:07:13.453213: step 95, loss 0.394908, acc 0.90625\n",
      "2017-11-06T00:07:17.367064: step 96, loss 0.32138, acc 0.90625\n",
      "2017-11-06T00:07:21.346656: step 97, loss 0.462901, acc 0.84375\n",
      "2017-11-06T00:07:25.262814: step 98, loss 0.84749, acc 0.75\n",
      "2017-11-06T00:07:29.269837: step 99, loss 0.527945, acc 0.9375\n",
      "2017-11-06T00:07:33.229351: step 100, loss 0.349465, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:07:35.774295: step 100, loss 1.17583, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-100\n",
      "\n",
      "2017-11-06T00:07:41.286690: step 101, loss 0.351241, acc 0.96875\n",
      "2017-11-06T00:07:45.331390: step 102, loss 0.366025, acc 0.84375\n",
      "2017-11-06T00:07:49.366306: step 103, loss 1.00462, acc 0.78125\n",
      "2017-11-06T00:07:53.376897: step 104, loss 0.171236, acc 0.96875\n",
      "2017-11-06T00:07:57.430304: step 105, loss 0.324905, acc 0.90625\n",
      "2017-11-06T00:08:01.523714: step 106, loss 1.14207, acc 0.875\n",
      "2017-11-06T00:08:05.580392: step 107, loss 0.999369, acc 0.90625\n",
      "2017-11-06T00:08:08.197306: step 108, loss 0.995051, acc 0.9\n",
      "2017-11-06T00:08:12.216159: step 109, loss 0.12542, acc 0.9375\n",
      "2017-11-06T00:08:16.204972: step 110, loss 0.386607, acc 0.84375\n",
      "2017-11-06T00:08:20.232246: step 111, loss 0.982627, acc 0.84375\n",
      "2017-11-06T00:08:24.293422: step 112, loss 0.22774, acc 0.90625\n",
      "2017-11-06T00:08:28.466809: step 113, loss 0.549112, acc 0.9375\n",
      "2017-11-06T00:08:32.482604: step 114, loss 1.07359, acc 0.8125\n",
      "2017-11-06T00:08:36.792047: step 115, loss 0.239195, acc 0.96875\n",
      "2017-11-06T00:08:40.835879: step 116, loss 0.528806, acc 0.8125\n",
      "2017-11-06T00:08:44.857146: step 117, loss 0.104605, acc 0.96875\n",
      "2017-11-06T00:08:49.007190: step 118, loss 0.377508, acc 0.90625\n",
      "2017-11-06T00:08:53.110182: step 119, loss 0.886704, acc 0.8125\n",
      "2017-11-06T00:08:57.416018: step 120, loss 0.464656, acc 0.9375\n",
      "2017-11-06T00:09:01.517126: step 121, loss 0.713953, acc 0.84375\n",
      "2017-11-06T00:09:05.805662: step 122, loss 0.907069, acc 0.78125\n",
      "2017-11-06T00:09:09.867779: step 123, loss 0.307517, acc 0.96875\n",
      "2017-11-06T00:09:14.084980: step 124, loss 0.271414, acc 0.9375\n",
      "2017-11-06T00:09:18.122911: step 125, loss 0.251305, acc 0.96875\n",
      "2017-11-06T00:09:22.289138: step 126, loss 0.541809, acc 0.84375\n",
      "2017-11-06T00:09:26.559051: step 127, loss 0.360907, acc 0.90625\n",
      "2017-11-06T00:09:30.670732: step 128, loss 0.780809, acc 0.8125\n",
      "2017-11-06T00:09:34.808045: step 129, loss 0.483216, acc 0.875\n",
      "2017-11-06T00:09:38.853869: step 130, loss 0.00769435, acc 1\n",
      "2017-11-06T00:09:43.046572: step 131, loss 0.590232, acc 0.90625\n",
      "2017-11-06T00:09:47.052001: step 132, loss 0.421176, acc 0.875\n",
      "2017-11-06T00:09:51.093653: step 133, loss 0.261054, acc 0.9375\n",
      "2017-11-06T00:09:55.035420: step 134, loss 1.19926, acc 0.875\n",
      "2017-11-06T00:09:59.065100: step 135, loss 0.726071, acc 0.875\n",
      "2017-11-06T00:10:03.330093: step 136, loss 0.770966, acc 0.78125\n",
      "2017-11-06T00:10:07.435297: step 137, loss 0.417509, acc 0.875\n",
      "2017-11-06T00:10:11.401248: step 138, loss 0.27137, acc 0.90625\n",
      "2017-11-06T00:10:15.437304: step 139, loss 0.65786, acc 0.84375\n",
      "2017-11-06T00:10:19.441964: step 140, loss 0.917723, acc 0.84375\n",
      "2017-11-06T00:10:23.575302: step 141, loss 0.824765, acc 0.78125\n",
      "2017-11-06T00:10:27.558315: step 142, loss 0.71555, acc 0.90625\n",
      "2017-11-06T00:10:31.556986: step 143, loss 1.02334, acc 0.875\n",
      "2017-11-06T00:10:34.251182: step 144, loss 0.370983, acc 0.9\n",
      "2017-11-06T00:10:38.291236: step 145, loss 0.661789, acc 0.875\n",
      "2017-11-06T00:10:42.261380: step 146, loss 0.553353, acc 0.8125\n",
      "2017-11-06T00:10:46.276165: step 147, loss 0.724322, acc 0.84375\n",
      "2017-11-06T00:10:50.281379: step 148, loss 0.688459, acc 0.875\n",
      "2017-11-06T00:10:54.286204: step 149, loss 0.89535, acc 0.8125\n",
      "2017-11-06T00:10:58.285201: step 150, loss 0.621013, acc 0.875\n",
      "2017-11-06T00:11:02.298090: step 151, loss 0.343562, acc 0.90625\n",
      "2017-11-06T00:11:06.318814: step 152, loss 0.856286, acc 0.8125\n",
      "2017-11-06T00:11:10.276717: step 153, loss 0.329343, acc 0.96875\n",
      "2017-11-06T00:11:14.305167: step 154, loss 0.66809, acc 0.84375\n",
      "2017-11-06T00:11:18.288568: step 155, loss 0.37333, acc 0.9375\n",
      "2017-11-06T00:11:22.317961: step 156, loss 0.188745, acc 0.9375\n",
      "2017-11-06T00:11:26.274050: step 157, loss 0.258041, acc 0.9375\n",
      "2017-11-06T00:11:30.262445: step 158, loss 0.606297, acc 0.90625\n",
      "2017-11-06T00:11:34.244116: step 159, loss 0.899985, acc 0.9375\n",
      "2017-11-06T00:11:38.282996: step 160, loss 1.15188, acc 0.8125\n",
      "2017-11-06T00:11:42.330634: step 161, loss 1.60963, acc 0.875\n",
      "2017-11-06T00:11:46.310967: step 162, loss 0.679291, acc 0.9375\n",
      "2017-11-06T00:11:50.287062: step 163, loss 0.0140734, acc 1\n",
      "2017-11-06T00:11:54.345740: step 164, loss 0.579723, acc 0.84375\n",
      "2017-11-06T00:11:58.331237: step 165, loss 0.208962, acc 0.90625\n",
      "2017-11-06T00:12:02.324283: step 166, loss 0.0855355, acc 0.96875\n",
      "2017-11-06T00:12:06.391884: step 167, loss 0.577995, acc 0.90625\n",
      "2017-11-06T00:12:10.369128: step 168, loss 0.918872, acc 0.78125\n",
      "2017-11-06T00:12:14.524250: step 169, loss 0.691005, acc 0.90625\n",
      "2017-11-06T00:12:18.495612: step 170, loss 0.0781623, acc 0.9375\n",
      "2017-11-06T00:12:22.592882: step 171, loss 0.335554, acc 0.90625\n",
      "2017-11-06T00:12:26.674965: step 172, loss 0.216736, acc 0.90625\n",
      "2017-11-06T00:12:30.629176: step 173, loss 0.425831, acc 0.90625\n",
      "2017-11-06T00:12:34.806534: step 174, loss 1.13274, acc 0.75\n",
      "2017-11-06T00:12:38.805329: step 175, loss 0.288077, acc 0.90625\n",
      "2017-11-06T00:12:42.806321: step 176, loss 0.291299, acc 0.96875\n",
      "2017-11-06T00:12:46.797380: step 177, loss 0.423052, acc 0.875\n",
      "2017-11-06T00:12:50.855015: step 178, loss 0.252814, acc 0.96875\n",
      "2017-11-06T00:12:54.852749: step 179, loss 0.514065, acc 0.9375\n",
      "2017-11-06T00:12:57.405580: step 180, loss 0.000984072, acc 1\n",
      "2017-11-06T00:13:01.459148: step 181, loss 0.86165, acc 0.8125\n",
      "2017-11-06T00:13:05.422555: step 182, loss 0.422795, acc 0.9375\n",
      "2017-11-06T00:13:09.437094: step 183, loss 0.685224, acc 0.875\n",
      "2017-11-06T00:13:13.479145: step 184, loss 0.303874, acc 0.9375\n",
      "2017-11-06T00:13:17.519324: step 185, loss 0.840427, acc 0.9375\n",
      "2017-11-06T00:13:21.467902: step 186, loss 0.0410596, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T00:13:25.784146: step 187, loss 1.05741, acc 0.78125\n",
      "2017-11-06T00:13:29.721614: step 188, loss 0.593228, acc 0.84375\n",
      "2017-11-06T00:13:33.655609: step 189, loss 0.909706, acc 0.8125\n",
      "2017-11-06T00:13:37.576394: step 190, loss 0.297355, acc 0.90625\n",
      "2017-11-06T00:13:41.532205: step 191, loss 0.324391, acc 0.875\n",
      "2017-11-06T00:13:45.555065: step 192, loss 0.635593, acc 0.90625\n",
      "2017-11-06T00:13:49.558373: step 193, loss 0.0909621, acc 0.96875\n",
      "2017-11-06T00:13:53.526706: step 194, loss 0.232361, acc 0.96875\n",
      "2017-11-06T00:13:57.516804: step 195, loss 0.167074, acc 0.90625\n",
      "2017-11-06T00:14:01.506791: step 196, loss 0.177864, acc 0.96875\n",
      "2017-11-06T00:14:05.406622: step 197, loss 0.886072, acc 0.84375\n",
      "2017-11-06T00:14:09.504249: step 198, loss 0.0548071, acc 1\n",
      "2017-11-06T00:14:13.472001: step 199, loss 0.29939, acc 0.875\n",
      "2017-11-06T00:14:17.468842: step 200, loss 0.265771, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:14:20.047570: step 200, loss 1.20854, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-200\n",
      "\n",
      "2017-11-06T00:14:25.483010: step 201, loss 0.0242586, acc 1\n",
      "2017-11-06T00:14:29.484108: step 202, loss 0.0110832, acc 1\n",
      "2017-11-06T00:14:33.493170: step 203, loss 0.618522, acc 0.90625\n",
      "2017-11-06T00:14:37.505500: step 204, loss 0.974843, acc 0.84375\n",
      "2017-11-06T00:14:41.600908: step 205, loss 0.248424, acc 0.9375\n",
      "2017-11-06T00:14:45.574923: step 206, loss 1.21254, acc 0.8125\n",
      "2017-11-06T00:14:49.521708: step 207, loss 0.509734, acc 0.875\n",
      "2017-11-06T00:14:53.473569: step 208, loss 0.470242, acc 0.90625\n",
      "2017-11-06T00:14:57.449894: step 209, loss 0.0512966, acc 0.9375\n",
      "2017-11-06T00:15:01.368368: step 210, loss 0.217413, acc 0.96875\n",
      "2017-11-06T00:15:05.284546: step 211, loss 0.50139, acc 0.90625\n",
      "2017-11-06T00:15:09.220675: step 212, loss 0.627719, acc 0.84375\n",
      "2017-11-06T00:15:13.182410: step 213, loss 0.274716, acc 0.90625\n",
      "2017-11-06T00:15:17.111045: step 214, loss 0.662476, acc 0.90625\n",
      "2017-11-06T00:15:21.102655: step 215, loss 0.228394, acc 0.90625\n",
      "2017-11-06T00:15:23.642056: step 216, loss 0.154612, acc 0.9\n",
      "2017-11-06T00:15:27.649123: step 217, loss 0.562936, acc 0.90625\n",
      "2017-11-06T00:15:31.552873: step 218, loss 0.41065, acc 0.9375\n",
      "2017-11-06T00:15:35.524919: step 219, loss 0.471818, acc 0.875\n",
      "2017-11-06T00:15:39.445806: step 220, loss 0.639754, acc 0.90625\n",
      "2017-11-06T00:15:43.406457: step 221, loss 0.550598, acc 0.875\n",
      "2017-11-06T00:15:47.319213: step 222, loss 0.16108, acc 0.90625\n",
      "2017-11-06T00:15:51.296236: step 223, loss 0.0522225, acc 0.96875\n",
      "2017-11-06T00:15:55.243276: step 224, loss 0.187271, acc 0.875\n",
      "2017-11-06T00:15:59.172204: step 225, loss 0.417553, acc 0.9375\n",
      "2017-11-06T00:16:03.132237: step 226, loss 0.74368, acc 0.90625\n",
      "2017-11-06T00:16:07.154641: step 227, loss 0.319064, acc 0.90625\n",
      "2017-11-06T00:16:11.167053: step 228, loss 0.11853, acc 0.9375\n",
      "2017-11-06T00:16:15.109133: step 229, loss 0.375712, acc 0.9375\n",
      "2017-11-06T00:16:19.089994: step 230, loss 0.320698, acc 0.9375\n",
      "2017-11-06T00:16:23.114483: step 231, loss 0.27856, acc 0.9375\n",
      "2017-11-06T00:16:27.071196: step 232, loss 0.624071, acc 0.84375\n",
      "2017-11-06T00:16:31.037539: step 233, loss 0.86943, acc 0.75\n",
      "2017-11-06T00:16:35.164301: step 234, loss 0.349719, acc 0.8125\n",
      "2017-11-06T00:16:39.150163: step 235, loss 0.279878, acc 0.96875\n",
      "2017-11-06T00:16:43.185333: step 236, loss 0.287377, acc 0.90625\n",
      "2017-11-06T00:16:47.137722: step 237, loss 0.983465, acc 0.8125\n",
      "2017-11-06T00:16:51.094916: step 238, loss 0.506596, acc 0.9375\n",
      "2017-11-06T00:16:55.045157: step 239, loss 0.709905, acc 0.78125\n",
      "2017-11-06T00:16:58.943232: step 240, loss 0.283185, acc 0.9375\n",
      "2017-11-06T00:17:02.853695: step 241, loss 0.33164, acc 0.875\n",
      "2017-11-06T00:17:06.862427: step 242, loss 0.169811, acc 0.9375\n",
      "2017-11-06T00:17:10.775826: step 243, loss 0.102021, acc 0.9375\n",
      "2017-11-06T00:17:14.727271: step 244, loss 0.310229, acc 0.875\n",
      "2017-11-06T00:17:18.653085: step 245, loss 0.471633, acc 0.90625\n",
      "2017-11-06T00:17:22.612601: step 246, loss 0.492974, acc 0.9375\n",
      "2017-11-06T00:17:26.621363: step 247, loss 0.861731, acc 0.90625\n",
      "2017-11-06T00:17:30.567641: step 248, loss 0.11934, acc 0.96875\n",
      "2017-11-06T00:17:34.522495: step 249, loss 0.602062, acc 0.9375\n",
      "2017-11-06T00:17:38.441401: step 250, loss 0.411981, acc 0.9375\n",
      "2017-11-06T00:17:42.340858: step 251, loss 0.902766, acc 0.90625\n",
      "2017-11-06T00:17:44.891619: step 252, loss 0.399423, acc 0.95\n",
      "2017-11-06T00:17:48.893885: step 253, loss 0.137521, acc 0.9375\n",
      "2017-11-06T00:17:52.850644: step 254, loss 0.561084, acc 0.9375\n",
      "2017-11-06T00:17:56.787925: step 255, loss 0.126191, acc 0.9375\n",
      "2017-11-06T00:18:00.744561: step 256, loss 0.128323, acc 0.9375\n",
      "2017-11-06T00:18:04.717400: step 257, loss 0.254612, acc 0.9375\n",
      "2017-11-06T00:18:08.736983: step 258, loss 0.403998, acc 0.90625\n",
      "2017-11-06T00:18:12.674747: step 259, loss 0.602986, acc 0.875\n",
      "2017-11-06T00:18:16.645432: step 260, loss 0.278627, acc 0.90625\n",
      "2017-11-06T00:18:20.599832: step 261, loss 0.277862, acc 0.9375\n",
      "2017-11-06T00:18:24.563101: step 262, loss 0.516445, acc 0.875\n",
      "2017-11-06T00:18:28.545930: step 263, loss 0.703595, acc 0.8125\n",
      "2017-11-06T00:18:32.489283: step 264, loss 0.135452, acc 0.9375\n",
      "2017-11-06T00:18:36.569032: step 265, loss 0.474217, acc 0.9375\n",
      "2017-11-06T00:18:40.470007: step 266, loss 0.0816238, acc 0.9375\n",
      "2017-11-06T00:18:44.449811: step 267, loss 0.231856, acc 0.9375\n",
      "2017-11-06T00:18:48.456125: step 268, loss 0.125954, acc 0.90625\n",
      "2017-11-06T00:18:52.499782: step 269, loss 0.251738, acc 0.9375\n",
      "2017-11-06T00:18:56.397553: step 270, loss 0.42042, acc 0.96875\n",
      "2017-11-06T00:19:00.338111: step 271, loss 0.375501, acc 0.90625\n",
      "2017-11-06T00:19:04.274695: step 272, loss 0.255367, acc 0.90625\n",
      "2017-11-06T00:19:08.202795: step 273, loss 0.0215035, acc 1\n",
      "2017-11-06T00:19:12.163588: step 274, loss 0.819301, acc 0.84375\n",
      "2017-11-06T00:19:16.195681: step 275, loss 0.0216714, acc 1\n",
      "2017-11-06T00:19:20.103733: step 276, loss 0.538839, acc 0.90625\n",
      "2017-11-06T00:19:24.287636: step 277, loss 0.708786, acc 0.90625\n",
      "2017-11-06T00:19:28.450943: step 278, loss 0.583436, acc 0.84375\n",
      "2017-11-06T00:19:32.403538: step 279, loss 0.561134, acc 0.84375\n",
      "2017-11-06T00:19:36.348933: step 280, loss 0.543077, acc 0.90625\n",
      "2017-11-06T00:19:40.295373: step 281, loss 0.521367, acc 0.90625\n",
      "2017-11-06T00:19:44.274951: step 282, loss 0.840305, acc 0.90625\n",
      "2017-11-06T00:19:48.200657: step 283, loss 0.84521, acc 0.8125\n",
      "2017-11-06T00:19:52.184381: step 284, loss 0.0450162, acc 0.96875\n",
      "2017-11-06T00:19:56.122081: step 285, loss 0.464161, acc 0.875\n",
      "2017-11-06T00:20:00.065908: step 286, loss 0.344899, acc 0.90625\n",
      "2017-11-06T00:20:04.261605: step 287, loss 0.146465, acc 0.96875\n",
      "2017-11-06T00:20:06.794462: step 288, loss 0.488965, acc 0.85\n",
      "2017-11-06T00:20:10.766567: step 289, loss 0.213544, acc 0.90625\n",
      "2017-11-06T00:20:14.711142: step 290, loss 0.195545, acc 0.875\n",
      "2017-11-06T00:20:18.728244: step 291, loss 0.666671, acc 0.90625\n",
      "2017-11-06T00:20:22.659697: step 292, loss 0.717454, acc 0.84375\n",
      "2017-11-06T00:20:26.587960: step 293, loss 0.406989, acc 0.90625\n",
      "2017-11-06T00:20:30.549225: step 294, loss 0.121028, acc 0.96875\n",
      "2017-11-06T00:20:34.677901: step 295, loss 0.0850436, acc 0.96875\n",
      "2017-11-06T00:20:38.652183: step 296, loss 0.174985, acc 0.9375\n",
      "2017-11-06T00:20:42.628152: step 297, loss 1.44193, acc 0.8125\n",
      "2017-11-06T00:20:46.622485: step 298, loss 0.134162, acc 0.96875\n",
      "2017-11-06T00:20:50.581205: step 299, loss 0.579384, acc 0.90625\n",
      "2017-11-06T00:20:54.518173: step 300, loss 0.893368, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:20:57.107654: step 300, loss 1.32254, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-300\n",
      "\n",
      "2017-11-06T00:21:02.416771: step 301, loss 0.164197, acc 0.96875\n",
      "2017-11-06T00:21:06.503775: step 302, loss 0.627823, acc 0.875\n",
      "2017-11-06T00:21:10.483944: step 303, loss 0.359182, acc 0.96875\n",
      "2017-11-06T00:21:14.435812: step 304, loss 0.491553, acc 0.9375\n",
      "2017-11-06T00:21:18.427306: step 305, loss 0.447223, acc 0.9375\n",
      "2017-11-06T00:21:22.322766: step 306, loss 0.376956, acc 0.9375\n",
      "2017-11-06T00:21:26.283504: step 307, loss 0.330835, acc 0.90625\n",
      "2017-11-06T00:21:30.280844: step 308, loss 0.662587, acc 0.875\n",
      "2017-11-06T00:21:34.247581: step 309, loss 0.249347, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T00:21:38.258819: step 310, loss 1.25253, acc 0.8125\n",
      "2017-11-06T00:21:42.150165: step 311, loss 0.647158, acc 0.90625\n",
      "2017-11-06T00:21:46.084956: step 312, loss 1.74711, acc 0.8125\n",
      "2017-11-06T00:21:50.087009: step 313, loss 0.591209, acc 0.875\n",
      "2017-11-06T00:21:54.065584: step 314, loss 0.731409, acc 0.84375\n",
      "2017-11-06T00:21:58.047134: step 315, loss 0.223164, acc 0.90625\n",
      "2017-11-06T00:22:01.952470: step 316, loss 0.136396, acc 0.9375\n",
      "2017-11-06T00:22:05.873584: step 317, loss 0.0467319, acc 0.9375\n",
      "2017-11-06T00:22:09.861341: step 318, loss 0.355183, acc 0.8125\n",
      "2017-11-06T00:22:13.766566: step 319, loss 0.369439, acc 0.875\n",
      "2017-11-06T00:22:17.756278: step 320, loss 0.301181, acc 0.90625\n",
      "2017-11-06T00:22:21.737404: step 321, loss 0.0239572, acc 1\n",
      "2017-11-06T00:22:25.694933: step 322, loss 0.254791, acc 0.9375\n",
      "2017-11-06T00:22:29.651751: step 323, loss 0.527689, acc 0.90625\n",
      "2017-11-06T00:22:32.207088: step 324, loss 0.368284, acc 0.9\n",
      "2017-11-06T00:22:36.365304: step 325, loss 0.238699, acc 0.96875\n",
      "2017-11-06T00:22:40.280237: step 326, loss 0.034806, acc 0.96875\n",
      "2017-11-06T00:22:44.238621: step 327, loss 0.179411, acc 0.96875\n",
      "2017-11-06T00:22:48.223734: step 328, loss 0.443427, acc 0.90625\n",
      "2017-11-06T00:22:52.190673: step 329, loss 0.35906, acc 0.9375\n",
      "2017-11-06T00:22:56.131866: step 330, loss 0.861599, acc 0.875\n",
      "2017-11-06T00:23:00.126324: step 331, loss 0.00435802, acc 1\n",
      "2017-11-06T00:23:04.050828: step 332, loss 0.404534, acc 0.9375\n",
      "2017-11-06T00:23:08.016301: step 333, loss 0.312616, acc 0.90625\n",
      "2017-11-06T00:23:11.981236: step 334, loss 0.35672, acc 0.9375\n",
      "2017-11-06T00:23:15.927266: step 335, loss 0.543511, acc 0.9375\n",
      "2017-11-06T00:23:19.910997: step 336, loss 0.263909, acc 0.9375\n",
      "2017-11-06T00:23:24.082491: step 337, loss 0.329658, acc 0.875\n",
      "2017-11-06T00:23:28.004029: step 338, loss 0.350473, acc 0.90625\n",
      "2017-11-06T00:23:31.973234: step 339, loss 0.0298095, acc 0.96875\n",
      "2017-11-06T00:23:35.937793: step 340, loss 0.103489, acc 0.96875\n",
      "2017-11-06T00:23:39.872880: step 341, loss 0.299847, acc 0.875\n",
      "2017-11-06T00:23:43.875252: step 342, loss 0.242462, acc 0.9375\n",
      "2017-11-06T00:23:47.822463: step 343, loss 0.239415, acc 0.875\n",
      "2017-11-06T00:23:51.822942: step 344, loss 0.144905, acc 0.96875\n",
      "2017-11-06T00:23:55.798510: step 345, loss 0.297203, acc 0.90625\n",
      "2017-11-06T00:23:59.728399: step 346, loss 0.471824, acc 0.90625\n",
      "2017-11-06T00:24:03.765827: step 347, loss 0.102826, acc 0.96875\n",
      "2017-11-06T00:24:07.824852: step 348, loss 0.194449, acc 0.96875\n",
      "2017-11-06T00:24:11.874206: step 349, loss 0.361827, acc 0.875\n",
      "2017-11-06T00:24:15.911924: step 350, loss 0.333136, acc 0.90625\n",
      "2017-11-06T00:24:19.880709: step 351, loss 0.224449, acc 0.90625\n",
      "2017-11-06T00:24:23.952231: step 352, loss 0.189425, acc 0.90625\n",
      "2017-11-06T00:24:27.928888: step 353, loss 0.459295, acc 0.96875\n",
      "2017-11-06T00:24:31.937469: step 354, loss 0.307135, acc 0.90625\n",
      "2017-11-06T00:24:36.074440: step 355, loss 0.660209, acc 0.90625\n",
      "2017-11-06T00:24:40.057094: step 356, loss 1.10964, acc 0.78125\n",
      "2017-11-06T00:24:44.009974: step 357, loss 0.266431, acc 0.90625\n",
      "2017-11-06T00:24:47.942383: step 358, loss 0.173712, acc 0.875\n",
      "2017-11-06T00:24:51.954822: step 359, loss 0.440889, acc 0.875\n",
      "2017-11-06T00:24:54.504781: step 360, loss 0.321326, acc 0.95\n",
      "2017-11-06T00:24:58.537690: step 361, loss 0.0710365, acc 0.96875\n",
      "2017-11-06T00:25:02.468611: step 362, loss 0.272677, acc 0.96875\n",
      "2017-11-06T00:25:06.416538: step 363, loss 0.194507, acc 0.9375\n",
      "2017-11-06T00:25:10.395091: step 364, loss 0.251221, acc 0.96875\n",
      "2017-11-06T00:25:14.363728: step 365, loss 0.163118, acc 0.9375\n",
      "2017-11-06T00:25:18.329046: step 366, loss 0.376057, acc 0.90625\n",
      "2017-11-06T00:25:22.337983: step 367, loss 0.315418, acc 0.875\n",
      "2017-11-06T00:25:26.317403: step 368, loss 0.172915, acc 0.90625\n",
      "2017-11-06T00:25:30.346523: step 369, loss 0.421726, acc 0.875\n",
      "2017-11-06T00:25:34.289557: step 370, loss 0.569178, acc 0.9375\n",
      "2017-11-06T00:25:38.249336: step 371, loss 0.0197703, acc 1\n",
      "2017-11-06T00:25:42.223690: step 372, loss 0.0673016, acc 0.96875\n",
      "2017-11-06T00:25:46.234261: step 373, loss 0.295548, acc 0.90625\n",
      "2017-11-06T00:25:50.255511: step 374, loss 0.0559113, acc 0.96875\n",
      "2017-11-06T00:25:54.219180: step 375, loss 0.0681958, acc 0.96875\n",
      "2017-11-06T00:25:58.163847: step 376, loss 0.643883, acc 0.84375\n",
      "2017-11-06T00:26:02.270242: step 377, loss 0.770554, acc 0.84375\n",
      "2017-11-06T00:26:06.278913: step 378, loss 0.0955653, acc 0.96875\n",
      "2017-11-06T00:26:10.303132: step 379, loss 0.494791, acc 0.875\n",
      "2017-11-06T00:26:14.250089: step 380, loss 0.385961, acc 0.90625\n",
      "2017-11-06T00:26:18.254986: step 381, loss 0.794186, acc 0.875\n",
      "2017-11-06T00:26:22.211985: step 382, loss 0.534311, acc 0.84375\n",
      "2017-11-06T00:26:26.154511: step 383, loss 0.478776, acc 0.84375\n",
      "2017-11-06T00:26:30.132113: step 384, loss 0.228473, acc 0.9375\n",
      "2017-11-06T00:26:34.292051: step 385, loss 0.383617, acc 0.90625\n",
      "2017-11-06T00:26:38.324325: step 386, loss 0.10283, acc 0.96875\n",
      "2017-11-06T00:26:42.253828: step 387, loss 0.0133629, acc 1\n",
      "2017-11-06T00:26:46.247264: step 388, loss 0.149067, acc 0.9375\n",
      "2017-11-06T00:26:50.203383: step 389, loss 0.720108, acc 0.90625\n",
      "2017-11-06T00:26:54.233139: step 390, loss 0.195922, acc 0.9375\n",
      "2017-11-06T00:26:58.200910: step 391, loss 0.526288, acc 0.875\n",
      "2017-11-06T00:27:02.202913: step 392, loss 0.20229, acc 0.9375\n",
      "2017-11-06T00:27:06.194757: step 393, loss 0.155612, acc 0.90625\n",
      "2017-11-06T00:27:10.153275: step 394, loss 0.468355, acc 0.90625\n",
      "2017-11-06T00:27:14.084579: step 395, loss 0.0678672, acc 0.96875\n",
      "2017-11-06T00:27:16.631060: step 396, loss 0.424395, acc 0.85\n",
      "2017-11-06T00:27:20.606688: step 397, loss 0.710105, acc 0.8125\n",
      "2017-11-06T00:27:24.599428: step 398, loss 0.00813933, acc 1\n",
      "2017-11-06T00:27:28.716052: step 399, loss 0.307165, acc 0.90625\n",
      "2017-11-06T00:27:32.636054: step 400, loss 0.138017, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:27:35.265276: step 400, loss 1.05724, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-400\n",
      "\n",
      "2017-11-06T00:27:40.736755: step 401, loss 0.175106, acc 0.96875\n",
      "2017-11-06T00:27:44.771769: step 402, loss 0.313874, acc 0.9375\n",
      "2017-11-06T00:27:48.682869: step 403, loss 0.219159, acc 0.90625\n",
      "2017-11-06T00:27:52.642912: step 404, loss 0.174213, acc 0.9375\n",
      "2017-11-06T00:27:56.648459: step 405, loss 0.506632, acc 0.9375\n",
      "2017-11-06T00:28:00.624160: step 406, loss 0.0819569, acc 0.96875\n",
      "2017-11-06T00:28:04.590794: step 407, loss 0.394221, acc 0.9375\n",
      "2017-11-06T00:28:08.556566: step 408, loss 0.740163, acc 0.875\n",
      "2017-11-06T00:28:12.620337: step 409, loss 0.23062, acc 0.90625\n",
      "2017-11-06T00:28:16.534627: step 410, loss 0.197368, acc 0.96875\n",
      "2017-11-06T00:28:20.472093: step 411, loss 0.319206, acc 0.9375\n",
      "2017-11-06T00:28:24.607605: step 412, loss 0.188771, acc 0.9375\n",
      "2017-11-06T00:28:28.695629: step 413, loss 0.386625, acc 0.96875\n",
      "2017-11-06T00:28:32.699298: step 414, loss 0.406231, acc 0.9375\n",
      "2017-11-06T00:28:36.754944: step 415, loss 0.0908377, acc 0.96875\n",
      "2017-11-06T00:28:40.726872: step 416, loss 0.534626, acc 0.90625\n",
      "2017-11-06T00:28:44.653923: step 417, loss 0.535803, acc 0.90625\n",
      "2017-11-06T00:28:48.613713: step 418, loss 0.093752, acc 0.96875\n",
      "2017-11-06T00:28:52.589582: step 419, loss 0.167343, acc 0.9375\n",
      "2017-11-06T00:28:56.602125: step 420, loss 0.882496, acc 0.875\n",
      "2017-11-06T00:29:00.579231: step 421, loss 0.040001, acc 1\n",
      "2017-11-06T00:29:04.558290: step 422, loss 0.124608, acc 0.9375\n",
      "2017-11-06T00:29:08.481865: step 423, loss 0.437949, acc 0.9375\n",
      "2017-11-06T00:29:12.506040: step 424, loss 0.755312, acc 0.8125\n",
      "2017-11-06T00:29:16.514007: step 425, loss 0.454, acc 0.875\n",
      "2017-11-06T00:29:20.416015: step 426, loss 0.519327, acc 0.90625\n",
      "2017-11-06T00:29:24.417934: step 427, loss 0.267498, acc 0.90625\n",
      "2017-11-06T00:29:28.387858: step 428, loss 0.105105, acc 0.96875\n",
      "2017-11-06T00:29:32.339842: step 429, loss 0.196862, acc 0.90625\n",
      "2017-11-06T00:29:36.300360: step 430, loss 0.509681, acc 0.875\n",
      "2017-11-06T00:29:40.248151: step 431, loss 0.274186, acc 0.90625\n",
      "2017-11-06T00:29:42.747625: step 432, loss 0.193265, acc 0.95\n",
      "2017-11-06T00:29:46.743085: step 433, loss 0.38074, acc 0.90625\n",
      "2017-11-06T00:29:50.758161: step 434, loss 0.217442, acc 0.96875\n",
      "2017-11-06T00:29:54.770188: step 435, loss 0.00701144, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T00:29:58.681959: step 436, loss 0.177671, acc 0.9375\n",
      "2017-11-06T00:30:03.063636: step 437, loss 0.177732, acc 0.96875\n",
      "2017-11-06T00:30:07.034211: step 438, loss 0.258754, acc 0.90625\n",
      "2017-11-06T00:30:11.002928: step 439, loss 0.167822, acc 0.96875\n",
      "2017-11-06T00:30:14.966004: step 440, loss 0.115047, acc 0.9375\n",
      "2017-11-06T00:30:18.896714: step 441, loss 0.858818, acc 0.84375\n",
      "2017-11-06T00:30:22.846004: step 442, loss 0.404472, acc 0.90625\n",
      "2017-11-06T00:30:26.856588: step 443, loss 0.107301, acc 0.96875\n",
      "2017-11-06T00:30:30.842284: step 444, loss 0.0868783, acc 0.9375\n",
      "2017-11-06T00:30:34.965908: step 445, loss 0.00068514, acc 1\n",
      "2017-11-06T00:30:38.920393: step 446, loss 0.64745, acc 0.90625\n",
      "2017-11-06T00:30:42.895644: step 447, loss 0.0801475, acc 0.96875\n",
      "2017-11-06T00:30:46.888039: step 448, loss 0.369158, acc 0.9375\n",
      "2017-11-06T00:30:50.890060: step 449, loss 0.208728, acc 0.90625\n",
      "2017-11-06T00:30:54.881947: step 450, loss 0.21687, acc 0.9375\n",
      "2017-11-06T00:30:58.831425: step 451, loss 0.0153411, acc 1\n",
      "2017-11-06T00:31:02.833606: step 452, loss 0.131251, acc 0.96875\n",
      "2017-11-06T00:31:06.874565: step 453, loss 0.459702, acc 0.8125\n",
      "2017-11-06T00:31:10.858429: step 454, loss 0.535822, acc 0.84375\n",
      "2017-11-06T00:31:14.839765: step 455, loss 0.394269, acc 0.9375\n",
      "2017-11-06T00:31:18.771194: step 456, loss 0.0348655, acc 1\n",
      "2017-11-06T00:31:22.760192: step 457, loss 0.613504, acc 0.9375\n",
      "2017-11-06T00:31:26.703819: step 458, loss 0.113398, acc 0.96875\n",
      "2017-11-06T00:31:30.675396: step 459, loss 0.267022, acc 0.875\n",
      "2017-11-06T00:31:34.811238: step 460, loss 0.166625, acc 0.9375\n",
      "2017-11-06T00:31:38.889163: step 461, loss 0.324789, acc 0.875\n",
      "2017-11-06T00:31:42.959719: step 462, loss 0.391726, acc 0.9375\n",
      "2017-11-06T00:31:47.003891: step 463, loss 0.0892144, acc 0.96875\n",
      "2017-11-06T00:31:50.941800: step 464, loss 0.279724, acc 0.9375\n",
      "2017-11-06T00:31:54.967691: step 465, loss 0.0916272, acc 0.96875\n",
      "2017-11-06T00:31:58.887489: step 466, loss 0.413737, acc 0.84375\n",
      "2017-11-06T00:32:02.889941: step 467, loss 0.125164, acc 0.9375\n",
      "2017-11-06T00:32:05.493595: step 468, loss 0.00148326, acc 1\n",
      "2017-11-06T00:32:09.474369: step 469, loss 0.288114, acc 0.90625\n",
      "2017-11-06T00:32:13.494198: step 470, loss 0.391704, acc 0.90625\n",
      "2017-11-06T00:32:17.444416: step 471, loss 0.00187247, acc 1\n",
      "2017-11-06T00:32:21.405657: step 472, loss 0.0816002, acc 0.96875\n",
      "2017-11-06T00:32:25.400610: step 473, loss 0.0127386, acc 1\n",
      "2017-11-06T00:32:29.404132: step 474, loss 0.147039, acc 0.90625\n",
      "2017-11-06T00:32:33.585071: step 475, loss 0.204545, acc 0.875\n",
      "2017-11-06T00:32:37.622259: step 476, loss 0.0651547, acc 0.9375\n",
      "2017-11-06T00:32:41.674554: step 477, loss 0.172482, acc 0.96875\n",
      "2017-11-06T00:32:45.640522: step 478, loss 0.652664, acc 0.875\n",
      "2017-11-06T00:32:49.592454: step 479, loss 0.408747, acc 0.9375\n",
      "2017-11-06T00:32:53.548005: step 480, loss 0.0745779, acc 0.96875\n",
      "2017-11-06T00:32:57.593565: step 481, loss 0.0184316, acc 1\n",
      "2017-11-06T00:33:01.553589: step 482, loss 0.105623, acc 0.96875\n",
      "2017-11-06T00:33:05.506867: step 483, loss 0.613447, acc 0.8125\n",
      "2017-11-06T00:33:09.494044: step 484, loss 0.285736, acc 0.96875\n",
      "2017-11-06T00:33:13.438188: step 485, loss 0.0451009, acc 1\n",
      "2017-11-06T00:33:17.397998: step 486, loss 0.205205, acc 0.96875\n",
      "2017-11-06T00:33:21.406785: step 487, loss 0.596184, acc 0.875\n",
      "2017-11-06T00:33:25.457955: step 488, loss 0.252933, acc 0.9375\n",
      "2017-11-06T00:33:29.766278: step 489, loss 0.387102, acc 0.9375\n",
      "2017-11-06T00:33:33.722437: step 490, loss 0.249181, acc 0.90625\n",
      "2017-11-06T00:33:37.692595: step 491, loss 0.158593, acc 0.90625\n",
      "2017-11-06T00:33:41.756639: step 492, loss 0.105526, acc 0.96875\n",
      "2017-11-06T00:33:45.824354: step 493, loss 0.246812, acc 0.90625\n",
      "2017-11-06T00:33:49.883504: step 494, loss 0.390004, acc 0.9375\n",
      "2017-11-06T00:33:54.096913: step 495, loss 0.331242, acc 0.875\n",
      "2017-11-06T00:33:58.079675: step 496, loss 0.365894, acc 0.9375\n",
      "2017-11-06T00:34:02.283158: step 497, loss 0.325838, acc 0.96875\n",
      "2017-11-06T00:34:06.319344: step 498, loss 0.299015, acc 0.90625\n",
      "2017-11-06T00:34:10.457012: step 499, loss 0.242874, acc 0.90625\n",
      "2017-11-06T00:34:14.616300: step 500, loss 0.01421, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:34:17.289916: step 500, loss 1.44304, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-500\n",
      "\n",
      "2017-11-06T00:34:22.768427: step 501, loss 0.199613, acc 0.9375\n",
      "2017-11-06T00:34:26.934962: step 502, loss 0.26662, acc 0.9375\n",
      "2017-11-06T00:34:31.024206: step 503, loss 0.230282, acc 0.96875\n",
      "2017-11-06T00:34:33.906643: step 504, loss 0.10226, acc 0.95\n",
      "2017-11-06T00:34:38.162378: step 505, loss 0.192166, acc 0.96875\n",
      "2017-11-06T00:34:42.400563: step 506, loss 0.0868987, acc 0.96875\n",
      "2017-11-06T00:34:46.451911: step 507, loss 0.273946, acc 0.9375\n",
      "2017-11-06T00:34:50.430961: step 508, loss 0.710373, acc 0.875\n",
      "2017-11-06T00:34:54.417261: step 509, loss 0.239252, acc 0.9375\n",
      "2017-11-06T00:34:58.406848: step 510, loss 0.166695, acc 0.9375\n",
      "2017-11-06T00:35:02.404101: step 511, loss 0.0906276, acc 0.96875\n",
      "2017-11-06T00:35:06.387026: step 512, loss 0.231709, acc 0.96875\n",
      "2017-11-06T00:35:10.398208: step 513, loss 0.520218, acc 0.875\n",
      "2017-11-06T00:35:14.429321: step 514, loss 0.331331, acc 0.90625\n",
      "2017-11-06T00:35:18.418977: step 515, loss 0.24002, acc 0.90625\n",
      "2017-11-06T00:35:22.427697: step 516, loss 0.251587, acc 0.90625\n",
      "2017-11-06T00:35:26.383383: step 517, loss 0.0632779, acc 0.96875\n",
      "2017-11-06T00:35:30.381734: step 518, loss 0.0808247, acc 0.9375\n",
      "2017-11-06T00:35:34.404538: step 519, loss 0.268801, acc 0.90625\n",
      "2017-11-06T00:35:38.504173: step 520, loss 0.187984, acc 0.9375\n",
      "2017-11-06T00:35:42.546037: step 521, loss 0.111888, acc 0.96875\n",
      "2017-11-06T00:35:46.624331: step 522, loss 0.312042, acc 0.90625\n",
      "2017-11-06T00:35:50.603557: step 523, loss 0.133865, acc 0.96875\n",
      "2017-11-06T00:35:54.605011: step 524, loss 0.168863, acc 0.9375\n",
      "2017-11-06T00:35:58.572696: step 525, loss 0.128515, acc 0.9375\n",
      "2017-11-06T00:36:02.626488: step 526, loss 0.39063, acc 0.9375\n",
      "2017-11-06T00:36:06.552627: step 527, loss 0.127377, acc 0.9375\n",
      "2017-11-06T00:36:10.605781: step 528, loss 0.167474, acc 0.96875\n",
      "2017-11-06T00:36:14.576298: step 529, loss 0.30644, acc 0.9375\n",
      "2017-11-06T00:36:18.561807: step 530, loss 0.165481, acc 0.9375\n",
      "2017-11-06T00:36:22.589502: step 531, loss 0.722486, acc 0.875\n",
      "2017-11-06T00:36:26.603643: step 532, loss 0.188927, acc 0.9375\n",
      "2017-11-06T00:36:30.653157: step 533, loss 0.122336, acc 0.96875\n",
      "2017-11-06T00:36:34.787705: step 534, loss 0.471448, acc 0.875\n",
      "2017-11-06T00:36:38.865062: step 535, loss 0.395181, acc 0.90625\n",
      "2017-11-06T00:36:42.946918: step 536, loss 0.480657, acc 0.90625\n",
      "2017-11-06T00:36:46.938200: step 537, loss 0.267624, acc 0.90625\n",
      "2017-11-06T00:36:50.912556: step 538, loss 0.0314859, acc 1\n",
      "2017-11-06T00:36:54.920542: step 539, loss 0.255646, acc 0.90625\n",
      "2017-11-06T00:36:57.497654: step 540, loss 0.316597, acc 0.95\n",
      "2017-11-06T00:37:01.560159: step 541, loss 0.366466, acc 0.9375\n",
      "2017-11-06T00:37:05.569458: step 542, loss 0.0568317, acc 0.96875\n",
      "2017-11-06T00:37:09.601254: step 543, loss 0.56172, acc 0.90625\n",
      "2017-11-06T00:37:13.636324: step 544, loss 0.169691, acc 0.9375\n",
      "2017-11-06T00:37:17.576730: step 545, loss 0.328946, acc 0.875\n",
      "2017-11-06T00:37:21.556558: step 546, loss 0.0430743, acc 0.96875\n",
      "2017-11-06T00:37:25.604996: step 547, loss 0.00288044, acc 1\n",
      "2017-11-06T00:37:29.525213: step 548, loss 0.00263783, acc 1\n",
      "2017-11-06T00:37:33.651140: step 549, loss 0.0957074, acc 0.9375\n",
      "2017-11-06T00:37:37.734967: step 550, loss 0.224291, acc 0.875\n",
      "2017-11-06T00:37:41.887536: step 551, loss 0.0669215, acc 0.96875\n",
      "2017-11-06T00:37:45.959951: step 552, loss 0.0791656, acc 0.96875\n",
      "2017-11-06T00:37:50.098643: step 553, loss 0.302926, acc 0.84375\n",
      "2017-11-06T00:37:54.099352: step 554, loss 0.573238, acc 0.90625\n",
      "2017-11-06T00:37:58.067946: step 555, loss 0.322895, acc 0.9375\n",
      "2017-11-06T00:38:02.126251: step 556, loss 0.255758, acc 0.9375\n",
      "2017-11-06T00:38:06.114284: step 557, loss 0.198113, acc 0.9375\n",
      "2017-11-06T00:38:10.195868: step 558, loss 0.625655, acc 0.875\n",
      "2017-11-06T00:38:14.120754: step 559, loss 0.83619, acc 0.78125\n",
      "2017-11-06T00:38:18.109386: step 560, loss 0.165289, acc 0.96875\n",
      "2017-11-06T00:38:22.193125: step 561, loss 0.135725, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T00:38:26.322346: step 562, loss 0.238869, acc 0.96875\n",
      "2017-11-06T00:38:30.339671: step 563, loss 0.226892, acc 0.9375\n",
      "2017-11-06T00:38:34.542241: step 564, loss 0.301642, acc 0.90625\n",
      "2017-11-06T00:38:38.556745: step 565, loss 0.323029, acc 0.875\n",
      "2017-11-06T00:38:42.607800: step 566, loss 0.395352, acc 0.84375\n",
      "2017-11-06T00:38:46.499063: step 567, loss 0.018296, acc 1\n",
      "2017-11-06T00:38:50.586030: step 568, loss 0.145607, acc 0.90625\n",
      "2017-11-06T00:38:54.553456: step 569, loss 0.395888, acc 0.90625\n",
      "2017-11-06T00:38:58.504666: step 570, loss 0.23648, acc 0.9375\n",
      "2017-11-06T00:39:02.434282: step 571, loss 0.808962, acc 0.8125\n",
      "2017-11-06T00:39:06.403311: step 572, loss 0.710315, acc 0.875\n",
      "2017-11-06T00:39:10.295332: step 573, loss 0.515068, acc 0.84375\n",
      "2017-11-06T00:39:14.222580: step 574, loss 0.24887, acc 0.90625\n",
      "2017-11-06T00:39:18.096795: step 575, loss 0.348453, acc 0.9375\n",
      "2017-11-06T00:39:20.604048: step 576, loss 0.43498, acc 0.85\n",
      "2017-11-06T00:39:24.571791: step 577, loss 0.180268, acc 0.9375\n",
      "2017-11-06T00:39:28.426607: step 578, loss 0.0606991, acc 0.96875\n",
      "2017-11-06T00:39:32.333140: step 579, loss 0.654465, acc 0.90625\n",
      "2017-11-06T00:39:36.233452: step 580, loss 0.386507, acc 0.90625\n",
      "2017-11-06T00:39:40.172475: step 581, loss 0.0927879, acc 0.96875\n",
      "2017-11-06T00:39:44.121539: step 582, loss 0.167127, acc 0.9375\n",
      "2017-11-06T00:39:48.028137: step 583, loss 0.0949304, acc 0.96875\n",
      "2017-11-06T00:39:51.948549: step 584, loss 0.0947074, acc 0.96875\n",
      "2017-11-06T00:39:55.924035: step 585, loss 0.00896675, acc 1\n",
      "2017-11-06T00:39:59.866028: step 586, loss 0.452823, acc 0.90625\n",
      "2017-11-06T00:40:04.082174: step 587, loss 0.133046, acc 0.96875\n",
      "2017-11-06T00:40:07.964660: step 588, loss 0.0507921, acc 0.96875\n",
      "2017-11-06T00:40:11.867960: step 589, loss 0.0142889, acc 1\n",
      "2017-11-06T00:40:15.790858: step 590, loss 0.140762, acc 0.9375\n",
      "2017-11-06T00:40:19.692309: step 591, loss 0.344567, acc 0.90625\n",
      "2017-11-06T00:40:23.626050: step 592, loss 0.525472, acc 0.90625\n",
      "2017-11-06T00:40:27.488618: step 593, loss 0.0230946, acc 1\n",
      "2017-11-06T00:40:31.397227: step 594, loss 0.0349394, acc 0.96875\n",
      "2017-11-06T00:40:35.541223: step 595, loss 0.0695583, acc 0.96875\n",
      "2017-11-06T00:40:39.420357: step 596, loss 0.310438, acc 0.9375\n",
      "2017-11-06T00:40:43.394808: step 597, loss 0.00118732, acc 1\n",
      "2017-11-06T00:40:47.334866: step 598, loss 0.303826, acc 0.90625\n",
      "2017-11-06T00:40:51.222519: step 599, loss 0.234974, acc 0.90625\n",
      "2017-11-06T00:40:55.166573: step 600, loss 0.161686, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:40:57.705354: step 600, loss 1.18265, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-600\n",
      "\n",
      "2017-11-06T00:41:02.948524: step 601, loss 0.852262, acc 0.84375\n",
      "2017-11-06T00:41:06.921371: step 602, loss 0.131204, acc 0.96875\n",
      "2017-11-06T00:41:11.282684: step 603, loss 0.423846, acc 0.9375\n",
      "2017-11-06T00:41:16.088350: step 604, loss 0.263867, acc 0.9375\n",
      "2017-11-06T00:41:19.989850: step 605, loss 0.152056, acc 0.9375\n",
      "2017-11-06T00:41:24.062775: step 606, loss 0.324769, acc 0.90625\n",
      "2017-11-06T00:41:27.955226: step 607, loss 0.0118788, acc 1\n",
      "2017-11-06T00:41:31.822105: step 608, loss 0.333572, acc 0.90625\n",
      "2017-11-06T00:41:35.772990: step 609, loss 0.50417, acc 0.84375\n",
      "2017-11-06T00:41:39.662617: step 610, loss 0.41608, acc 0.875\n",
      "2017-11-06T00:41:43.607300: step 611, loss 0.460714, acc 0.84375\n",
      "2017-11-06T00:41:46.109715: step 612, loss 0.340184, acc 0.95\n",
      "2017-11-06T00:41:50.022318: step 613, loss 0.501825, acc 0.90625\n",
      "2017-11-06T00:41:53.905479: step 614, loss 0.174743, acc 0.96875\n",
      "2017-11-06T00:41:57.819102: step 615, loss 0.0541226, acc 0.96875\n",
      "2017-11-06T00:42:01.788762: step 616, loss 0.294268, acc 0.875\n",
      "2017-11-06T00:42:05.733764: step 617, loss 0.36616, acc 0.96875\n",
      "2017-11-06T00:42:09.622743: step 618, loss 0.0976297, acc 0.96875\n",
      "2017-11-06T00:42:13.544911: step 619, loss 0.217199, acc 0.96875\n",
      "2017-11-06T00:42:17.494374: step 620, loss 0.294478, acc 0.9375\n",
      "2017-11-06T00:42:21.444194: step 621, loss 0.000966596, acc 1\n",
      "2017-11-06T00:42:25.451013: step 622, loss 0.230484, acc 0.9375\n",
      "2017-11-06T00:42:29.360383: step 623, loss 0.473717, acc 0.9375\n",
      "2017-11-06T00:42:33.400869: step 624, loss 0.307128, acc 0.90625\n",
      "2017-11-06T00:42:37.428162: step 625, loss 0.131853, acc 0.96875\n",
      "2017-11-06T00:42:41.364338: step 626, loss 0.597468, acc 0.875\n",
      "2017-11-06T00:42:45.277486: step 627, loss 0.424791, acc 0.875\n",
      "2017-11-06T00:42:49.219530: step 628, loss 0.285838, acc 0.90625\n",
      "2017-11-06T00:42:53.101064: step 629, loss 0.455146, acc 0.875\n",
      "2017-11-06T00:42:57.069192: step 630, loss 0.296652, acc 0.90625\n",
      "2017-11-06T00:43:01.010989: step 631, loss 0.116722, acc 0.96875\n",
      "2017-11-06T00:43:04.934536: step 632, loss 0.407275, acc 0.90625\n",
      "2017-11-06T00:43:08.867165: step 633, loss 0.37281, acc 0.84375\n",
      "2017-11-06T00:43:12.762087: step 634, loss 0.0821755, acc 0.9375\n",
      "2017-11-06T00:43:16.666434: step 635, loss 0.239206, acc 0.9375\n",
      "2017-11-06T00:43:20.657085: step 636, loss 0.214217, acc 0.96875\n",
      "2017-11-06T00:43:24.643048: step 637, loss 0.695088, acc 0.90625\n",
      "2017-11-06T00:43:28.547644: step 638, loss 0.146859, acc 0.9375\n",
      "2017-11-06T00:43:32.497206: step 639, loss 0.532983, acc 0.90625\n",
      "2017-11-06T00:43:36.411345: step 640, loss 0.242178, acc 0.90625\n",
      "2017-11-06T00:43:40.347941: step 641, loss 0.0635057, acc 0.96875\n",
      "2017-11-06T00:43:44.298197: step 642, loss 0.0722436, acc 0.96875\n",
      "2017-11-06T00:43:48.246001: step 643, loss 0.0172223, acc 1\n",
      "2017-11-06T00:43:52.222979: step 644, loss 0.214291, acc 0.96875\n",
      "2017-11-06T00:43:56.100361: step 645, loss 0.136089, acc 0.96875\n",
      "2017-11-06T00:44:00.001750: step 646, loss 0.418663, acc 0.875\n",
      "2017-11-06T00:44:03.908003: step 647, loss 0.448704, acc 0.90625\n",
      "2017-11-06T00:44:06.386387: step 648, loss 0.00759254, acc 1\n",
      "2017-11-06T00:44:10.331569: step 649, loss 0.160675, acc 0.90625\n",
      "2017-11-06T00:44:14.316251: step 650, loss 0.180025, acc 0.9375\n",
      "2017-11-06T00:44:18.225461: step 651, loss 0.32393, acc 0.9375\n",
      "2017-11-06T00:44:22.207097: step 652, loss 0.561984, acc 0.875\n",
      "2017-11-06T00:44:26.472028: step 653, loss 0.0598378, acc 0.96875\n",
      "2017-11-06T00:44:30.412252: step 654, loss 0.10674, acc 0.96875\n",
      "2017-11-06T00:44:34.507111: step 655, loss 0.347364, acc 0.90625\n",
      "2017-11-06T00:44:38.503996: step 656, loss 0.146855, acc 0.96875\n",
      "2017-11-06T00:44:42.455793: step 657, loss 0.331404, acc 0.90625\n",
      "2017-11-06T00:44:46.395820: step 658, loss 0.216924, acc 0.90625\n",
      "2017-11-06T00:44:50.427309: step 659, loss 0.09825, acc 0.9375\n",
      "2017-11-06T00:44:54.364657: step 660, loss 0.00560726, acc 1\n",
      "2017-11-06T00:44:58.337903: step 661, loss 0.140689, acc 0.96875\n",
      "2017-11-06T00:45:02.266227: step 662, loss 0.294894, acc 0.9375\n",
      "2017-11-06T00:45:06.202076: step 663, loss 0.42221, acc 0.9375\n",
      "2017-11-06T00:45:10.206404: step 664, loss 0.0474012, acc 0.96875\n",
      "2017-11-06T00:45:14.158378: step 665, loss 0.37297, acc 0.9375\n",
      "2017-11-06T00:45:18.133231: step 666, loss 0.0702184, acc 0.96875\n",
      "2017-11-06T00:45:22.052287: step 667, loss 0.121526, acc 0.9375\n",
      "2017-11-06T00:45:25.956500: step 668, loss 0.656705, acc 0.875\n",
      "2017-11-06T00:45:29.963478: step 669, loss 0.525551, acc 0.84375\n",
      "2017-11-06T00:45:33.876678: step 670, loss 0.333582, acc 0.9375\n",
      "2017-11-06T00:45:37.785832: step 671, loss 0.275904, acc 0.90625\n",
      "2017-11-06T00:45:41.722734: step 672, loss 0.154484, acc 0.9375\n",
      "2017-11-06T00:45:45.721785: step 673, loss 0.0445416, acc 0.96875\n",
      "2017-11-06T00:45:49.622126: step 674, loss 0.0943191, acc 0.9375\n",
      "2017-11-06T00:45:53.606616: step 675, loss 0.360974, acc 0.90625\n",
      "2017-11-06T00:45:57.616806: step 676, loss 0.253742, acc 0.9375\n",
      "2017-11-06T00:46:01.534529: step 677, loss 0.174904, acc 0.96875\n",
      "2017-11-06T00:46:05.498133: step 678, loss 0.218355, acc 0.96875\n",
      "2017-11-06T00:46:09.403698: step 679, loss 0.239211, acc 0.90625\n",
      "2017-11-06T00:46:13.517555: step 680, loss 0.139448, acc 0.96875\n",
      "2017-11-06T00:46:17.421962: step 681, loss 0.32082, acc 0.90625\n",
      "2017-11-06T00:46:21.376386: step 682, loss 0.0966548, acc 0.96875\n",
      "2017-11-06T00:46:25.361848: step 683, loss 0.538123, acc 0.875\n",
      "2017-11-06T00:46:27.868886: step 684, loss 0.0142991, acc 1\n",
      "2017-11-06T00:46:31.772627: step 685, loss 0.532084, acc 0.8125\n",
      "2017-11-06T00:46:35.904343: step 686, loss 0.341123, acc 0.90625\n",
      "2017-11-06T00:46:39.870740: step 687, loss 0.188557, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T00:46:43.766254: step 688, loss 0.572516, acc 0.875\n",
      "2017-11-06T00:46:47.712017: step 689, loss 0.129208, acc 0.96875\n",
      "2017-11-06T00:46:51.603023: step 690, loss 0.169182, acc 0.9375\n",
      "2017-11-06T00:46:55.555691: step 691, loss 0.0319694, acc 1\n",
      "2017-11-06T00:46:59.481027: step 692, loss 0.494936, acc 0.90625\n",
      "2017-11-06T00:47:03.406709: step 693, loss 0.53768, acc 0.9375\n",
      "2017-11-06T00:47:07.324366: step 694, loss 0.0608514, acc 0.96875\n",
      "2017-11-06T00:47:11.269008: step 695, loss 0.302829, acc 0.84375\n",
      "2017-11-06T00:47:15.204087: step 696, loss 0.174976, acc 0.9375\n",
      "2017-11-06T00:47:19.087329: step 697, loss 0.0242287, acc 1\n",
      "2017-11-06T00:47:23.020238: step 698, loss 0.454635, acc 0.9375\n",
      "2017-11-06T00:47:26.935473: step 699, loss 0.0173072, acc 1\n",
      "2017-11-06T00:47:30.829435: step 700, loss 0.255732, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:47:33.369346: step 700, loss 0.858683, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-700\n",
      "\n",
      "2017-11-06T00:47:38.957066: step 701, loss 0.173658, acc 0.9375\n",
      "2017-11-06T00:47:42.909359: step 702, loss 0.0452582, acc 1\n",
      "2017-11-06T00:47:46.865017: step 703, loss 0.330288, acc 0.90625\n",
      "2017-11-06T00:47:50.872817: step 704, loss 0.0740536, acc 0.96875\n",
      "2017-11-06T00:47:54.880462: step 705, loss 0.736247, acc 0.875\n",
      "2017-11-06T00:47:58.820396: step 706, loss 0.545993, acc 0.9375\n",
      "2017-11-06T00:48:02.772605: step 707, loss 0.333734, acc 0.96875\n",
      "2017-11-06T00:48:06.766600: step 708, loss 0.0498043, acc 1\n",
      "2017-11-06T00:48:10.742599: step 709, loss 0.125041, acc 0.96875\n",
      "2017-11-06T00:48:14.752159: step 710, loss 0.0947736, acc 0.9375\n",
      "2017-11-06T00:48:18.654900: step 711, loss 0.0249883, acc 1\n",
      "2017-11-06T00:48:22.650294: step 712, loss 0.338268, acc 0.9375\n",
      "2017-11-06T00:48:26.737007: step 713, loss 0.184505, acc 0.9375\n",
      "2017-11-06T00:48:30.668261: step 714, loss 0.34908, acc 0.90625\n",
      "2017-11-06T00:48:34.801049: step 715, loss 0.420023, acc 0.90625\n",
      "2017-11-06T00:48:38.795328: step 716, loss 0.0336358, acc 1\n",
      "2017-11-06T00:48:42.736523: step 717, loss 0.0423946, acc 0.96875\n",
      "2017-11-06T00:48:46.671869: step 718, loss 0.0979243, acc 0.9375\n",
      "2017-11-06T00:48:50.652178: step 719, loss 0.353949, acc 0.90625\n",
      "2017-11-06T00:48:53.213846: step 720, loss 0.126321, acc 0.95\n",
      "2017-11-06T00:48:57.227617: step 721, loss 0.0315271, acc 0.96875\n",
      "2017-11-06T00:49:01.206801: step 722, loss 0.201226, acc 0.9375\n",
      "2017-11-06T00:49:05.145915: step 723, loss 0.16171, acc 0.96875\n",
      "2017-11-06T00:49:09.154708: step 724, loss 0.28307, acc 0.9375\n",
      "2017-11-06T00:49:13.456909: step 725, loss 0.34026, acc 0.9375\n",
      "2017-11-06T00:49:17.369104: step 726, loss 0.30109, acc 0.90625\n",
      "2017-11-06T00:49:21.321838: step 727, loss 0.14925, acc 0.96875\n",
      "2017-11-06T00:49:25.480671: step 728, loss 0.210538, acc 0.9375\n",
      "2017-11-06T00:49:29.517245: step 729, loss 0.136123, acc 0.96875\n",
      "2017-11-06T00:49:33.578597: step 730, loss 0.529817, acc 0.90625\n",
      "2017-11-06T00:49:37.528939: step 731, loss 0.13269, acc 0.96875\n",
      "2017-11-06T00:49:41.498345: step 732, loss 0.541069, acc 0.875\n",
      "2017-11-06T00:49:45.458879: step 733, loss 0.232965, acc 0.96875\n",
      "2017-11-06T00:49:49.396305: step 734, loss 0.132675, acc 0.9375\n",
      "2017-11-06T00:49:53.372458: step 735, loss 0.269334, acc 0.96875\n",
      "2017-11-06T00:49:57.386164: step 736, loss 0.23853, acc 0.9375\n",
      "2017-11-06T00:50:01.615218: step 737, loss 0.11753, acc 0.96875\n",
      "2017-11-06T00:50:05.591721: step 738, loss 0.275684, acc 0.9375\n",
      "2017-11-06T00:50:09.519503: step 739, loss 0.141431, acc 0.90625\n",
      "2017-11-06T00:50:13.529684: step 740, loss 0.254442, acc 0.96875\n",
      "2017-11-06T00:50:17.501325: step 741, loss 0.323333, acc 0.875\n",
      "2017-11-06T00:50:21.430832: step 742, loss 0.406342, acc 0.90625\n",
      "2017-11-06T00:50:25.425232: step 743, loss 0.276133, acc 0.9375\n",
      "2017-11-06T00:50:29.422333: step 744, loss 0.287945, acc 0.875\n",
      "2017-11-06T00:50:33.462055: step 745, loss 0.370552, acc 0.90625\n",
      "2017-11-06T00:50:37.504182: step 746, loss 0.123811, acc 0.96875\n",
      "2017-11-06T00:50:41.442428: step 747, loss 0.695695, acc 0.84375\n",
      "2017-11-06T00:50:45.436988: step 748, loss 0.592767, acc 0.90625\n",
      "2017-11-06T00:50:49.405892: step 749, loss 0.246671, acc 0.90625\n",
      "2017-11-06T00:50:53.401145: step 750, loss 0.328644, acc 0.9375\n",
      "2017-11-06T00:50:57.393680: step 751, loss 0.344589, acc 0.90625\n",
      "2017-11-06T00:51:01.358852: step 752, loss 0.261362, acc 0.90625\n",
      "2017-11-06T00:51:05.259479: step 753, loss 0.47681, acc 0.90625\n",
      "2017-11-06T00:51:09.241910: step 754, loss 0.28471, acc 0.90625\n",
      "2017-11-06T00:51:13.178657: step 755, loss 0.357508, acc 0.875\n",
      "2017-11-06T00:51:15.777549: step 756, loss 0.333723, acc 0.9\n",
      "2017-11-06T00:51:19.748710: step 757, loss 0.297514, acc 0.90625\n",
      "2017-11-06T00:51:23.711130: step 758, loss 0.340882, acc 0.90625\n",
      "2017-11-06T00:51:27.645597: step 759, loss 0.488893, acc 0.875\n",
      "2017-11-06T00:51:31.547734: step 760, loss 0.273961, acc 0.875\n",
      "2017-11-06T00:51:35.572058: step 761, loss 0.0728109, acc 0.96875\n",
      "2017-11-06T00:51:39.531997: step 762, loss 0.355557, acc 0.90625\n",
      "2017-11-06T00:51:43.488526: step 763, loss 0.141454, acc 0.9375\n",
      "2017-11-06T00:51:47.487611: step 764, loss 0.889541, acc 0.8125\n",
      "2017-11-06T00:51:51.421461: step 765, loss 0.40305, acc 0.90625\n",
      "2017-11-06T00:51:55.349517: step 766, loss 0.0557314, acc 0.9375\n",
      "2017-11-06T00:51:59.357985: step 767, loss 0.186359, acc 0.96875\n",
      "2017-11-06T00:52:03.336520: step 768, loss 0.32895, acc 0.90625\n",
      "2017-11-06T00:52:07.311274: step 769, loss 0.412311, acc 0.90625\n",
      "2017-11-06T00:52:11.276508: step 770, loss 0.0471051, acc 0.96875\n",
      "2017-11-06T00:52:15.259241: step 771, loss 0.319393, acc 0.90625\n",
      "2017-11-06T00:52:19.171093: step 772, loss 0.2582, acc 0.9375\n",
      "2017-11-06T00:52:23.225797: step 773, loss 0.177443, acc 0.96875\n",
      "2017-11-06T00:52:27.274052: step 774, loss 0.283651, acc 0.84375\n",
      "2017-11-06T00:52:31.253437: step 775, loss 0.290243, acc 0.90625\n",
      "2017-11-06T00:52:35.453974: step 776, loss 0.331146, acc 0.90625\n",
      "2017-11-06T00:52:39.435322: step 777, loss 0.0547578, acc 0.96875\n",
      "2017-11-06T00:52:43.439208: step 778, loss 0.20645, acc 0.875\n",
      "2017-11-06T00:52:47.400712: step 779, loss 0.599173, acc 0.90625\n",
      "2017-11-06T00:52:51.372766: step 780, loss 0.303594, acc 0.90625\n",
      "2017-11-06T00:52:55.388263: step 781, loss 0.122021, acc 0.9375\n",
      "2017-11-06T00:52:59.332565: step 782, loss 0.279712, acc 0.9375\n",
      "2017-11-06T00:53:03.422831: step 783, loss 0.198621, acc 0.90625\n",
      "2017-11-06T00:53:07.437706: step 784, loss 0.0569615, acc 0.96875\n",
      "2017-11-06T00:53:11.496488: step 785, loss 0.0756328, acc 0.96875\n",
      "2017-11-06T00:53:15.600432: step 786, loss 0.132067, acc 0.96875\n",
      "2017-11-06T00:53:19.570661: step 787, loss 0.343038, acc 0.9375\n",
      "2017-11-06T00:53:23.749228: step 788, loss 0.280395, acc 0.90625\n",
      "2017-11-06T00:53:27.938415: step 789, loss 0.124882, acc 0.9375\n",
      "2017-11-06T00:53:31.972210: step 790, loss 0.9146, acc 0.8125\n",
      "2017-11-06T00:53:35.956696: step 791, loss 0.0340898, acc 0.96875\n",
      "2017-11-06T00:53:38.519005: step 792, loss 0.821922, acc 0.85\n",
      "2017-11-06T00:53:42.481527: step 793, loss 0.343604, acc 0.90625\n",
      "2017-11-06T00:53:46.536534: step 794, loss 0.221076, acc 0.9375\n",
      "2017-11-06T00:53:50.602310: step 795, loss 0.436458, acc 0.84375\n",
      "2017-11-06T00:53:54.585345: step 796, loss 0.263145, acc 0.9375\n",
      "2017-11-06T00:53:58.580280: step 797, loss 0.2659, acc 0.9375\n",
      "2017-11-06T00:54:02.659907: step 798, loss 0.160333, acc 0.9375\n",
      "2017-11-06T00:54:06.680294: step 799, loss 0.0908852, acc 0.96875\n",
      "2017-11-06T00:54:10.711011: step 800, loss 0.308514, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T00:54:13.496304: step 800, loss 0.761693, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-800\n",
      "\n",
      "2017-11-06T00:54:18.899724: step 801, loss 0.344044, acc 0.90625\n",
      "2017-11-06T00:54:22.888422: step 802, loss 0.202935, acc 0.96875\n",
      "2017-11-06T00:54:26.904492: step 803, loss 0.0641494, acc 0.96875\n",
      "2017-11-06T00:54:30.922243: step 804, loss 0.344943, acc 0.90625\n",
      "2017-11-06T00:54:35.023792: step 805, loss 0.0626826, acc 0.96875\n",
      "2017-11-06T00:54:38.972059: step 806, loss 0.0928837, acc 0.96875\n",
      "2017-11-06T00:54:42.946729: step 807, loss 0.135015, acc 0.9375\n",
      "2017-11-06T00:54:46.898262: step 808, loss 0.161551, acc 0.90625\n",
      "2017-11-06T00:54:50.904022: step 809, loss 0.0967013, acc 0.96875\n",
      "2017-11-06T00:54:54.922992: step 810, loss 0.0982998, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T00:54:58.865239: step 811, loss 0.119933, acc 0.9375\n",
      "2017-11-06T00:55:02.846048: step 812, loss 0.0910497, acc 0.9375\n",
      "2017-11-06T00:55:06.847124: step 813, loss 0.0108196, acc 1\n",
      "2017-11-06T00:55:10.837680: step 814, loss 0.816941, acc 0.84375\n",
      "2017-11-06T00:55:14.829128: step 815, loss 0.06639, acc 0.96875\n",
      "2017-11-06T00:55:18.918097: step 816, loss 0.0704034, acc 0.96875\n",
      "2017-11-06T00:55:22.960505: step 817, loss 0.157775, acc 0.90625\n",
      "2017-11-06T00:55:27.062760: step 818, loss 0.369807, acc 0.90625\n",
      "2017-11-06T00:55:31.033877: step 819, loss 0.0860849, acc 0.96875\n",
      "2017-11-06T00:55:35.019524: step 820, loss 0.0811626, acc 0.96875\n",
      "2017-11-06T00:55:39.070669: step 821, loss 0.301776, acc 0.90625\n",
      "2017-11-06T00:55:43.038501: step 822, loss 0.33255, acc 0.875\n",
      "2017-11-06T00:55:47.089963: step 823, loss 0.601734, acc 0.84375\n",
      "2017-11-06T00:55:51.119721: step 824, loss 0.0313494, acc 0.96875\n",
      "2017-11-06T00:55:55.156546: step 825, loss 0.266303, acc 0.9375\n",
      "2017-11-06T00:55:59.192395: step 826, loss 0.331861, acc 0.90625\n",
      "2017-11-06T00:56:03.229333: step 827, loss 0.180225, acc 0.9375\n",
      "2017-11-06T00:56:05.813223: step 828, loss 0.195317, acc 0.85\n",
      "2017-11-06T00:56:09.806747: step 829, loss 0.260606, acc 0.9375\n",
      "2017-11-06T00:56:13.808654: step 830, loss 0.144678, acc 0.96875\n",
      "2017-11-06T00:56:17.803092: step 831, loss 0.380479, acc 0.90625\n",
      "2017-11-06T00:56:21.764402: step 832, loss 0.265934, acc 0.9375\n",
      "2017-11-06T00:56:25.790410: step 833, loss 0.194385, acc 0.9375\n",
      "2017-11-06T00:56:29.757370: step 834, loss 0.0657516, acc 0.96875\n",
      "2017-11-06T00:56:33.861639: step 835, loss 0.395968, acc 0.90625\n",
      "2017-11-06T00:56:37.944470: step 836, loss 0.13, acc 0.9375\n",
      "2017-11-06T00:56:41.930853: step 837, loss 0.948584, acc 0.8125\n",
      "2017-11-06T00:56:45.903567: step 838, loss 0.227376, acc 0.9375\n",
      "2017-11-06T00:56:49.903661: step 839, loss 0.181263, acc 0.96875\n",
      "2017-11-06T00:56:53.919143: step 840, loss 0.182738, acc 0.9375\n",
      "2017-11-06T00:56:57.866026: step 841, loss 0.0472179, acc 0.96875\n",
      "2017-11-06T00:57:01.803664: step 842, loss 0.303707, acc 0.90625\n",
      "2017-11-06T00:57:05.783492: step 843, loss 0.257772, acc 0.875\n",
      "2017-11-06T00:57:09.838109: step 844, loss 0.3436, acc 0.90625\n",
      "2017-11-06T00:57:13.816355: step 845, loss 0.0232919, acc 1\n",
      "2017-11-06T00:57:17.779445: step 846, loss 0.276018, acc 0.9375\n",
      "2017-11-06T00:57:21.816079: step 847, loss 0.37412, acc 0.90625\n",
      "2017-11-06T00:57:25.793476: step 848, loss 0.31919, acc 0.96875\n",
      "2017-11-06T00:57:29.756243: step 849, loss 0.142401, acc 0.96875\n",
      "2017-11-06T00:57:33.707175: step 850, loss 0.355435, acc 0.90625\n",
      "2017-11-06T00:57:37.655901: step 851, loss 0.077908, acc 0.96875\n",
      "2017-11-06T00:57:41.655019: step 852, loss 0.259316, acc 0.9375\n",
      "2017-11-06T00:57:45.633441: step 853, loss 0.131551, acc 0.9375\n",
      "2017-11-06T00:57:49.645614: step 854, loss 0.1643, acc 0.875\n",
      "2017-11-06T00:57:53.647474: step 855, loss 0.201889, acc 0.90625\n",
      "2017-11-06T00:57:57.612514: step 856, loss 0.216186, acc 0.90625\n",
      "2017-11-06T00:58:01.601746: step 857, loss 0.0465384, acc 1\n",
      "2017-11-06T00:58:05.613603: step 858, loss 0.167272, acc 0.9375\n",
      "2017-11-06T00:58:09.680003: step 859, loss 0.168254, acc 0.9375\n",
      "2017-11-06T00:58:13.688998: step 860, loss 0.227685, acc 0.875\n",
      "2017-11-06T00:58:17.666517: step 861, loss 0.188331, acc 0.9375\n",
      "2017-11-06T00:58:21.681725: step 862, loss 0.131566, acc 0.9375\n",
      "2017-11-06T00:58:25.834756: step 863, loss 0.122842, acc 0.96875\n",
      "2017-11-06T00:58:28.490234: step 864, loss 0.206431, acc 0.95\n",
      "2017-11-06T00:58:32.483543: step 865, loss 0.348171, acc 0.9375\n",
      "2017-11-06T00:58:36.622776: step 866, loss 0.0349538, acc 0.96875\n",
      "2017-11-06T00:58:40.662848: step 867, loss 0.149527, acc 0.96875\n",
      "2017-11-06T00:58:44.604552: step 868, loss 0.648143, acc 0.84375\n",
      "2017-11-06T00:58:48.637013: step 869, loss 0.204606, acc 0.96875\n",
      "2017-11-06T00:58:52.609313: step 870, loss 0.225087, acc 0.90625\n",
      "2017-11-06T00:58:56.594508: step 871, loss 0.123589, acc 0.9375\n",
      "2017-11-06T00:59:00.526634: step 872, loss 0.224826, acc 0.96875\n",
      "2017-11-06T00:59:04.575458: step 873, loss 0.0849402, acc 0.9375\n",
      "2017-11-06T00:59:08.551543: step 874, loss 0.0822157, acc 0.9375\n",
      "2017-11-06T00:59:12.536630: step 875, loss 0.240532, acc 0.9375\n",
      "2017-11-06T00:59:16.539498: step 876, loss 0.127387, acc 0.9375\n",
      "2017-11-06T00:59:20.483186: step 877, loss 0.121301, acc 0.96875\n",
      "2017-11-06T00:59:24.479866: step 878, loss 0.340926, acc 0.875\n",
      "2017-11-06T00:59:28.485736: step 879, loss 0.151061, acc 0.9375\n",
      "2017-11-06T00:59:32.412101: step 880, loss 0.319167, acc 0.875\n",
      "2017-11-06T00:59:36.363830: step 881, loss 0.224935, acc 0.9375\n",
      "2017-11-06T00:59:40.380965: step 882, loss 0.241309, acc 0.9375\n",
      "2017-11-06T00:59:44.388117: step 883, loss 0.621153, acc 0.875\n",
      "2017-11-06T00:59:48.330597: step 884, loss 0.10016, acc 0.96875\n",
      "2017-11-06T00:59:52.318367: step 885, loss 0.129394, acc 0.9375\n",
      "2017-11-06T00:59:56.336519: step 886, loss 0.39151, acc 0.84375\n",
      "2017-11-06T01:00:00.275252: step 887, loss 0.0411155, acc 1\n",
      "2017-11-06T01:00:04.541154: step 888, loss 0.128529, acc 0.96875\n",
      "2017-11-06T01:00:08.547404: step 889, loss 0.13823, acc 0.90625\n",
      "2017-11-06T01:00:12.553741: step 890, loss 0.106691, acc 0.96875\n",
      "2017-11-06T01:00:16.509206: step 891, loss 0.0154479, acc 1\n",
      "2017-11-06T01:00:20.555070: step 892, loss 0.0456079, acc 0.96875\n",
      "2017-11-06T01:00:24.663159: step 893, loss 0.275751, acc 0.90625\n",
      "2017-11-06T01:00:28.752549: step 894, loss 0.236219, acc 0.875\n",
      "2017-11-06T01:00:32.792617: step 895, loss 0.282403, acc 0.96875\n",
      "2017-11-06T01:00:36.890999: step 896, loss 0.217266, acc 0.9375\n",
      "2017-11-06T01:00:40.882010: step 897, loss 0.110341, acc 0.9375\n",
      "2017-11-06T01:00:44.848018: step 898, loss 0.326504, acc 0.875\n",
      "2017-11-06T01:00:48.858533: step 899, loss 0.186126, acc 0.96875\n",
      "2017-11-06T01:00:51.372172: step 900, loss 0.0011331, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:00:53.958546: step 900, loss 0.946218, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-900\n",
      "\n",
      "2017-11-06T01:00:59.406957: step 901, loss 0.221919, acc 0.9375\n",
      "2017-11-06T01:01:03.402722: step 902, loss 0.00179906, acc 1\n",
      "2017-11-06T01:01:07.392386: step 903, loss 0.58879, acc 0.90625\n",
      "2017-11-06T01:01:11.527089: step 904, loss 0.135193, acc 0.96875\n",
      "2017-11-06T01:01:15.584714: step 905, loss 0.0108245, acc 1\n",
      "2017-11-06T01:01:19.552594: step 906, loss 0.377754, acc 0.90625\n",
      "2017-11-06T01:01:23.545073: step 907, loss 0.0880757, acc 0.9375\n",
      "2017-11-06T01:01:27.599138: step 908, loss 0.140504, acc 0.96875\n",
      "2017-11-06T01:01:31.637811: step 909, loss 0.299248, acc 0.90625\n",
      "2017-11-06T01:01:35.763611: step 910, loss 0.047176, acc 0.96875\n",
      "2017-11-06T01:01:39.815015: step 911, loss 0.380381, acc 0.875\n",
      "2017-11-06T01:01:43.856650: step 912, loss 0.418808, acc 0.90625\n",
      "2017-11-06T01:01:47.856804: step 913, loss 0.545261, acc 0.84375\n",
      "2017-11-06T01:01:51.848276: step 914, loss 0.336643, acc 0.90625\n",
      "2017-11-06T01:01:55.803799: step 915, loss 0.0623806, acc 1\n",
      "2017-11-06T01:01:59.768260: step 916, loss 0.201325, acc 0.96875\n",
      "2017-11-06T01:02:03.806674: step 917, loss 0.0157504, acc 1\n",
      "2017-11-06T01:02:07.826055: step 918, loss 0.0358889, acc 0.96875\n",
      "2017-11-06T01:02:11.903891: step 919, loss 0.187069, acc 0.9375\n",
      "2017-11-06T01:02:15.911028: step 920, loss 0.382613, acc 0.90625\n",
      "2017-11-06T01:02:19.829267: step 921, loss 0.0168354, acc 1\n",
      "2017-11-06T01:02:23.835852: step 922, loss 0.189465, acc 0.9375\n",
      "2017-11-06T01:02:27.802499: step 923, loss 0.222861, acc 0.9375\n",
      "2017-11-06T01:02:31.680263: step 924, loss 0.00507735, acc 1\n",
      "2017-11-06T01:02:35.839643: step 925, loss 0.0780243, acc 0.9375\n",
      "2017-11-06T01:02:39.748200: step 926, loss 0.560747, acc 0.90625\n",
      "2017-11-06T01:02:43.723067: step 927, loss 0.09011, acc 0.96875\n",
      "2017-11-06T01:02:47.708543: step 928, loss 0.37206, acc 0.9375\n",
      "2017-11-06T01:02:51.688380: step 929, loss 0.174289, acc 0.9375\n",
      "2017-11-06T01:02:55.606751: step 930, loss 0.249206, acc 0.90625\n",
      "2017-11-06T01:02:59.579096: step 931, loss 0.105304, acc 0.96875\n",
      "2017-11-06T01:03:03.547721: step 932, loss 0.575908, acc 0.90625\n",
      "2017-11-06T01:03:07.538122: step 933, loss 0.188069, acc 0.9375\n",
      "2017-11-06T01:03:11.501821: step 934, loss 0.292623, acc 0.9375\n",
      "2017-11-06T01:03:15.461144: step 935, loss 0.220751, acc 0.96875\n",
      "2017-11-06T01:03:17.951036: step 936, loss 0.18588, acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T01:03:21.949494: step 937, loss 0.0637389, acc 0.96875\n",
      "2017-11-06T01:03:26.017904: step 938, loss 0.523738, acc 0.90625\n",
      "2017-11-06T01:03:30.082918: step 939, loss 0.281228, acc 0.9375\n",
      "2017-11-06T01:03:33.973523: step 940, loss 0.0361463, acc 1\n",
      "2017-11-06T01:03:37.887616: step 941, loss 0.0845451, acc 0.9375\n",
      "2017-11-06T01:03:41.900021: step 942, loss 0.454762, acc 0.875\n",
      "2017-11-06T01:03:45.890277: step 943, loss 0.0327554, acc 0.96875\n",
      "2017-11-06T01:03:50.034070: step 944, loss 0.228251, acc 0.96875\n",
      "2017-11-06T01:03:54.362144: step 945, loss 0.106931, acc 0.9375\n",
      "2017-11-06T01:03:58.533108: step 946, loss 0.25639, acc 0.90625\n",
      "2017-11-06T01:04:02.468905: step 947, loss 0.276581, acc 0.90625\n",
      "2017-11-06T01:04:06.596838: step 948, loss 0.225259, acc 0.9375\n",
      "2017-11-06T01:04:10.630704: step 949, loss 0.284479, acc 0.9375\n",
      "2017-11-06T01:04:14.832690: step 950, loss 0.206146, acc 0.9375\n",
      "2017-11-06T01:04:18.917593: step 951, loss 0.0532993, acc 1\n",
      "2017-11-06T01:04:23.024510: step 952, loss 0.178152, acc 0.9375\n",
      "2017-11-06T01:04:27.102408: step 953, loss 0.729852, acc 0.875\n",
      "2017-11-06T01:04:31.255358: step 954, loss 0.0925982, acc 0.90625\n",
      "2017-11-06T01:04:35.445336: step 955, loss 0.375163, acc 0.9375\n",
      "2017-11-06T01:04:39.633311: step 956, loss 0.534357, acc 0.90625\n",
      "2017-11-06T01:04:43.641159: step 957, loss 0.305216, acc 0.90625\n",
      "2017-11-06T01:04:47.557942: step 958, loss 0.00975545, acc 1\n",
      "2017-11-06T01:04:51.463719: step 959, loss 0.114421, acc 0.9375\n",
      "2017-11-06T01:04:55.626675: step 960, loss 0.00647472, acc 1\n",
      "2017-11-06T01:04:59.909719: step 961, loss 0.13669, acc 0.9375\n",
      "2017-11-06T01:05:03.808489: step 962, loss 0.408884, acc 0.84375\n",
      "2017-11-06T01:05:07.751292: step 963, loss 0.168759, acc 0.9375\n",
      "2017-11-06T01:05:11.728118: step 964, loss 0.0553129, acc 0.96875\n",
      "2017-11-06T01:05:15.699940: step 965, loss 0.0629118, acc 0.96875\n",
      "2017-11-06T01:05:19.639738: step 966, loss 0.293366, acc 0.9375\n",
      "2017-11-06T01:05:23.632575: step 967, loss 0.445485, acc 0.875\n",
      "2017-11-06T01:05:27.582381: step 968, loss 0.00975598, acc 1\n",
      "2017-11-06T01:05:31.642266: step 969, loss 0.22454, acc 0.90625\n",
      "2017-11-06T01:05:35.631100: step 970, loss 0.329469, acc 0.9375\n",
      "2017-11-06T01:05:39.583909: step 971, loss 0.0273594, acc 1\n",
      "2017-11-06T01:05:42.151733: step 972, loss 0.802399, acc 0.9\n",
      "2017-11-06T01:05:46.125557: step 973, loss 0.372155, acc 0.9375\n",
      "2017-11-06T01:05:50.103384: step 974, loss 0.107038, acc 0.9375\n",
      "2017-11-06T01:05:54.105227: step 975, loss 0.14505, acc 0.96875\n",
      "2017-11-06T01:05:58.058802: step 976, loss 0.189159, acc 0.9375\n",
      "2017-11-06T01:06:02.248779: step 977, loss 0.562282, acc 0.84375\n",
      "2017-11-06T01:06:06.442759: step 978, loss 0.249424, acc 0.9375\n",
      "2017-11-06T01:06:10.412581: step 979, loss 0.22236, acc 0.9375\n",
      "2017-11-06T01:06:14.324360: step 980, loss 0.419889, acc 0.90625\n",
      "2017-11-06T01:06:18.272165: step 981, loss 0.151577, acc 0.9375\n",
      "2017-11-06T01:06:22.251994: step 982, loss 0.0132973, acc 1\n",
      "2017-11-06T01:06:26.184787: step 983, loss 0.173163, acc 0.9375\n",
      "2017-11-06T01:06:30.128589: step 984, loss 0.00226477, acc 1\n",
      "2017-11-06T01:06:34.220497: step 985, loss 0.135188, acc 0.9375\n",
      "2017-11-06T01:06:38.167302: step 986, loss 0.270749, acc 0.9375\n",
      "2017-11-06T01:06:42.081082: step 987, loss 0.0345413, acc 1\n",
      "2017-11-06T01:06:46.010875: step 988, loss 0.0640409, acc 0.96875\n",
      "2017-11-06T01:06:49.905643: step 989, loss 0.108302, acc 0.9375\n",
      "2017-11-06T01:06:53.863454: step 990, loss 0.0311539, acc 0.96875\n",
      "2017-11-06T01:06:57.748214: step 991, loss 0.303202, acc 0.9375\n",
      "2017-11-06T01:07:01.653989: step 992, loss 0.323452, acc 0.90625\n",
      "2017-11-06T01:07:05.783924: step 993, loss 0.303333, acc 0.90625\n",
      "2017-11-06T01:07:10.107996: step 994, loss 0.160648, acc 0.90625\n",
      "2017-11-06T01:07:14.101834: step 995, loss 0.206112, acc 0.96875\n",
      "2017-11-06T01:07:18.112684: step 996, loss 0.161854, acc 0.9375\n",
      "2017-11-06T01:07:22.116529: step 997, loss 0.23715, acc 0.875\n",
      "2017-11-06T01:07:26.077343: step 998, loss 0.0200292, acc 1\n",
      "2017-11-06T01:07:30.012139: step 999, loss 0.356454, acc 0.90625\n",
      "2017-11-06T01:07:33.971987: step 1000, loss 0.271191, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:07:36.514794: step 1000, loss 0.795798, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-06T01:07:41.805605: step 1001, loss 0.100003, acc 0.9375\n",
      "2017-11-06T01:07:45.792437: step 1002, loss 0.227043, acc 0.90625\n",
      "2017-11-06T01:07:49.767261: step 1003, loss 0.0347691, acc 1\n",
      "2017-11-06T01:07:53.776110: step 1004, loss 0.0142925, acc 1\n",
      "2017-11-06T01:07:57.698897: step 1005, loss 0.173002, acc 0.96875\n",
      "2017-11-06T01:08:01.682728: step 1006, loss 0.220876, acc 0.9375\n",
      "2017-11-06T01:08:05.670561: step 1007, loss 0.167262, acc 0.90625\n",
      "2017-11-06T01:08:08.226378: step 1008, loss 0.659244, acc 0.9\n",
      "2017-11-06T01:08:12.385332: step 1009, loss 0.448795, acc 0.875\n",
      "2017-11-06T01:08:16.733422: step 1010, loss 0.078862, acc 0.96875\n",
      "2017-11-06T01:08:20.737267: step 1011, loss 0.279825, acc 0.9375\n",
      "2017-11-06T01:08:24.913236: step 1012, loss 0.0724594, acc 0.96875\n",
      "2017-11-06T01:08:29.121226: step 1013, loss 0.0684205, acc 0.96875\n",
      "2017-11-06T01:08:33.182109: step 1014, loss 0.421571, acc 0.84375\n",
      "2017-11-06T01:08:37.291028: step 1015, loss 0.0899156, acc 0.96875\n",
      "2017-11-06T01:08:41.260850: step 1016, loss 0.0908542, acc 0.96875\n",
      "2017-11-06T01:08:45.247682: step 1017, loss 0.212269, acc 0.90625\n",
      "2017-11-06T01:08:49.164467: step 1018, loss 0.119602, acc 0.9375\n",
      "2017-11-06T01:08:53.155301: step 1019, loss 0.466571, acc 0.90625\n",
      "2017-11-06T01:08:57.114115: step 1020, loss 0.0971992, acc 0.96875\n",
      "2017-11-06T01:09:01.087717: step 1021, loss 0.0883078, acc 0.96875\n",
      "2017-11-06T01:09:05.046530: step 1022, loss 0.421994, acc 0.90625\n",
      "2017-11-06T01:09:09.058380: step 1023, loss 0.0840683, acc 0.96875\n",
      "2017-11-06T01:09:13.072232: step 1024, loss 0.270334, acc 0.96875\n",
      "2017-11-06T01:09:17.146127: step 1025, loss 0.401849, acc 0.875\n",
      "2017-11-06T01:09:21.596289: step 1026, loss 0.0833305, acc 0.9375\n",
      "2017-11-06T01:09:25.597131: step 1027, loss 0.204803, acc 0.9375\n",
      "2017-11-06T01:09:29.631999: step 1028, loss 0.0344669, acc 0.96875\n",
      "2017-11-06T01:09:33.627837: step 1029, loss 0.200475, acc 0.90625\n",
      "2017-11-06T01:09:37.648696: step 1030, loss 0.321265, acc 0.90625\n",
      "2017-11-06T01:09:41.653541: step 1031, loss 0.0967696, acc 0.9375\n",
      "2017-11-06T01:09:45.585336: step 1032, loss 0.0735845, acc 0.96875\n",
      "2017-11-06T01:09:49.590180: step 1033, loss 0.00456102, acc 1\n",
      "2017-11-06T01:09:53.580017: step 1034, loss 0.113351, acc 0.96875\n",
      "2017-11-06T01:09:57.564846: step 1035, loss 0.12441, acc 0.9375\n",
      "2017-11-06T01:10:01.866903: step 1036, loss 0.200904, acc 0.9375\n",
      "2017-11-06T01:10:05.861741: step 1037, loss 0.262756, acc 0.96875\n",
      "2017-11-06T01:10:09.799539: step 1038, loss 0.155534, acc 0.9375\n",
      "2017-11-06T01:10:13.805386: step 1039, loss 0.291139, acc 0.9375\n",
      "2017-11-06T01:10:17.757195: step 1040, loss 0.0938787, acc 0.96875\n",
      "2017-11-06T01:10:21.755034: step 1041, loss 0.786858, acc 0.84375\n",
      "2017-11-06T01:10:26.224210: step 1042, loss 0.242269, acc 0.9375\n",
      "2017-11-06T01:10:30.201036: step 1043, loss 0.396785, acc 0.90625\n",
      "2017-11-06T01:10:32.787874: step 1044, loss 1.21153, acc 0.8\n",
      "2017-11-06T01:10:36.940824: step 1045, loss 0.0916012, acc 0.96875\n",
      "2017-11-06T01:10:40.928657: step 1046, loss 0.184783, acc 0.90625\n",
      "2017-11-06T01:10:44.922496: step 1047, loss 0.389042, acc 0.9375\n",
      "2017-11-06T01:10:48.864297: step 1048, loss 0.0501475, acc 0.9375\n",
      "2017-11-06T01:10:52.775077: step 1049, loss 0.197771, acc 0.96875\n",
      "2017-11-06T01:10:56.753902: step 1050, loss 0.0214881, acc 1\n",
      "2017-11-06T01:11:00.647670: step 1051, loss 0.363422, acc 0.9375\n",
      "2017-11-06T01:11:04.612486: step 1052, loss 0.484368, acc 0.84375\n",
      "2017-11-06T01:11:08.509257: step 1053, loss 0.259961, acc 0.90625\n",
      "2017-11-06T01:11:12.492086: step 1054, loss 0.0233104, acc 1\n",
      "2017-11-06T01:11:16.412870: step 1055, loss 0.100138, acc 0.9375\n",
      "2017-11-06T01:11:20.340663: step 1056, loss 0.269381, acc 0.90625\n",
      "2017-11-06T01:11:24.316487: step 1057, loss 0.0366426, acc 1\n",
      "2017-11-06T01:11:28.360360: step 1058, loss 0.133872, acc 0.9375\n",
      "2017-11-06T01:11:32.670424: step 1059, loss 0.311678, acc 0.875\n",
      "2017-11-06T01:11:36.590207: step 1060, loss 0.158491, acc 0.9375\n",
      "2017-11-06T01:11:40.558110: step 1061, loss 0.0429131, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T01:11:44.530932: step 1062, loss 0.0134761, acc 1\n",
      "2017-11-06T01:11:48.448716: step 1063, loss 0.303273, acc 0.90625\n",
      "2017-11-06T01:11:52.386514: step 1064, loss 0.27858, acc 0.90625\n",
      "2017-11-06T01:11:56.358336: step 1065, loss 0.101094, acc 0.9375\n",
      "2017-11-06T01:12:00.270979: step 1066, loss 0.00381773, acc 1\n",
      "2017-11-06T01:12:04.258812: step 1067, loss 0.124112, acc 0.9375\n",
      "2017-11-06T01:12:08.319698: step 1068, loss 0.169096, acc 0.96875\n",
      "2017-11-06T01:12:12.214465: step 1069, loss 0.524615, acc 0.8125\n",
      "2017-11-06T01:12:16.166275: step 1070, loss 0.162887, acc 0.90625\n",
      "2017-11-06T01:12:20.115078: step 1071, loss 0.338792, acc 0.90625\n",
      "2017-11-06T01:12:24.124927: step 1072, loss 0.188323, acc 0.9375\n",
      "2017-11-06T01:12:28.102756: step 1073, loss 0.542734, acc 0.90625\n",
      "2017-11-06T01:12:32.009530: step 1074, loss 0.0721795, acc 0.96875\n",
      "2017-11-06T01:12:36.501723: step 1075, loss 0.0678111, acc 0.96875\n",
      "2017-11-06T01:12:40.566612: step 1076, loss 0.139575, acc 0.9375\n",
      "2017-11-06T01:12:44.493400: step 1077, loss 0.247334, acc 0.875\n",
      "2017-11-06T01:12:48.389168: step 1078, loss 0.238497, acc 0.90625\n",
      "2017-11-06T01:12:52.294943: step 1079, loss 0.786125, acc 0.875\n",
      "2017-11-06T01:12:54.802726: step 1080, loss 0.136917, acc 0.95\n",
      "2017-11-06T01:12:58.717509: step 1081, loss 0.146868, acc 0.9375\n",
      "2017-11-06T01:13:02.601267: step 1082, loss 0.167619, acc 0.9375\n",
      "2017-11-06T01:13:06.552074: step 1083, loss 0.354467, acc 0.90625\n",
      "2017-11-06T01:13:10.442838: step 1084, loss 0.302258, acc 0.9375\n",
      "2017-11-06T01:13:14.344611: step 1085, loss 0.148038, acc 0.96875\n",
      "2017-11-06T01:13:18.262395: step 1086, loss 0.0738439, acc 0.96875\n",
      "2017-11-06T01:13:22.362308: step 1087, loss 0.0827566, acc 0.96875\n",
      "2017-11-06T01:13:26.547282: step 1088, loss 0.197223, acc 0.9375\n",
      "2017-11-06T01:13:30.468069: step 1089, loss 0.142048, acc 0.9375\n",
      "2017-11-06T01:13:34.371841: step 1090, loss 0.311409, acc 0.875\n",
      "2017-11-06T01:13:38.323649: step 1091, loss 0.0799316, acc 0.96875\n",
      "2017-11-06T01:13:42.696816: step 1092, loss 0.256913, acc 0.9375\n",
      "2017-11-06T01:13:46.777716: step 1093, loss 0.0818158, acc 0.9375\n",
      "2017-11-06T01:13:50.764549: step 1094, loss 0.283321, acc 0.90625\n",
      "2017-11-06T01:13:54.673326: step 1095, loss 0.11801, acc 0.9375\n",
      "2017-11-06T01:13:58.586106: step 1096, loss 0.441072, acc 0.875\n",
      "2017-11-06T01:14:02.500888: step 1097, loss 0.126253, acc 0.96875\n",
      "2017-11-06T01:14:06.437685: step 1098, loss 0.074142, acc 0.96875\n",
      "2017-11-06T01:14:10.337456: step 1099, loss 0.421667, acc 0.84375\n",
      "2017-11-06T01:14:14.280258: step 1100, loss 0.201768, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:14:16.951156: step 1100, loss 0.724713, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-06T01:14:22.161851: step 1101, loss 0.0483086, acc 0.96875\n",
      "2017-11-06T01:14:26.094647: step 1102, loss 0.313317, acc 0.90625\n",
      "2017-11-06T01:14:30.089484: step 1103, loss 0.13997, acc 0.9375\n",
      "2017-11-06T01:14:34.198404: step 1104, loss 0.0486492, acc 1\n",
      "2017-11-06T01:14:38.111184: step 1105, loss 0.210886, acc 0.96875\n",
      "2017-11-06T01:14:42.000948: step 1106, loss 0.285292, acc 0.90625\n",
      "2017-11-06T01:14:46.131883: step 1107, loss 0.00576978, acc 1\n",
      "2017-11-06T01:14:50.343876: step 1108, loss 0.0912009, acc 0.96875\n",
      "2017-11-06T01:14:54.278671: step 1109, loss 0.159752, acc 0.90625\n",
      "2017-11-06T01:14:58.202289: step 1110, loss 0.0822063, acc 0.96875\n",
      "2017-11-06T01:15:02.179116: step 1111, loss 0.17058, acc 0.9375\n",
      "2017-11-06T01:15:06.144934: step 1112, loss 0.163383, acc 0.9375\n",
      "2017-11-06T01:15:10.089736: step 1113, loss 0.474257, acc 0.90625\n",
      "2017-11-06T01:15:14.075570: step 1114, loss 0.172775, acc 0.875\n",
      "2017-11-06T01:15:18.003359: step 1115, loss 0.371184, acc 0.90625\n",
      "2017-11-06T01:15:20.554172: step 1116, loss 0.0720011, acc 0.95\n",
      "2017-11-06T01:15:24.588038: step 1117, loss 0.0665867, acc 0.96875\n",
      "2017-11-06T01:15:28.586880: step 1118, loss 0.109487, acc 0.96875\n",
      "2017-11-06T01:15:32.569709: step 1119, loss 0.0574827, acc 0.96875\n",
      "2017-11-06T01:15:36.506506: step 1120, loss 0.0732742, acc 0.96875\n",
      "2017-11-06T01:15:40.453311: step 1121, loss 0.265976, acc 0.875\n",
      "2017-11-06T01:15:44.477170: step 1122, loss 0.098506, acc 0.9375\n",
      "2017-11-06T01:15:48.404961: step 1123, loss 0.267335, acc 0.90625\n",
      "2017-11-06T01:15:52.653980: step 1124, loss 0.288907, acc 0.90625\n",
      "2017-11-06T01:15:56.851962: step 1125, loss 0.258399, acc 0.875\n",
      "2017-11-06T01:16:00.855809: step 1126, loss 0.152022, acc 0.9375\n",
      "2017-11-06T01:16:04.826629: step 1127, loss 0.325477, acc 0.875\n",
      "2017-11-06T01:16:08.741410: step 1128, loss 0.0806869, acc 0.96875\n",
      "2017-11-06T01:16:12.714233: step 1129, loss 0.0967294, acc 0.9375\n",
      "2017-11-06T01:16:16.678050: step 1130, loss 0.0473779, acc 0.96875\n",
      "2017-11-06T01:16:20.583825: step 1131, loss 0.138033, acc 0.96875\n",
      "2017-11-06T01:16:24.555648: step 1132, loss 0.0419328, acc 1\n",
      "2017-11-06T01:16:28.515461: step 1133, loss 0.169828, acc 0.9375\n",
      "2017-11-06T01:16:32.534317: step 1134, loss 0.18524, acc 0.90625\n",
      "2017-11-06T01:16:36.629226: step 1135, loss 0.275084, acc 0.875\n",
      "2017-11-06T01:16:40.583035: step 1136, loss 0.263555, acc 0.9375\n",
      "2017-11-06T01:16:44.518852: step 1137, loss 0.0915396, acc 0.96875\n",
      "2017-11-06T01:16:48.453628: step 1138, loss 0.166208, acc 0.96875\n",
      "2017-11-06T01:16:52.353399: step 1139, loss 0.165199, acc 0.9375\n",
      "2017-11-06T01:16:56.480331: step 1140, loss 0.339625, acc 0.90625\n",
      "2017-11-06T01:17:00.870450: step 1141, loss 0.274427, acc 0.90625\n",
      "2017-11-06T01:17:04.800261: step 1142, loss 0.143046, acc 0.9375\n",
      "2017-11-06T01:17:08.783074: step 1143, loss 0.0824886, acc 0.96875\n",
      "2017-11-06T01:17:12.698856: step 1144, loss 0.221683, acc 0.9375\n",
      "2017-11-06T01:17:16.627648: step 1145, loss 0.226439, acc 0.90625\n",
      "2017-11-06T01:17:20.554437: step 1146, loss 0.232218, acc 0.9375\n",
      "2017-11-06T01:17:24.493235: step 1147, loss 0.340197, acc 0.9375\n",
      "2017-11-06T01:17:28.459055: step 1148, loss 0.274317, acc 0.90625\n",
      "2017-11-06T01:17:32.459896: step 1149, loss 0.140319, acc 0.96875\n",
      "2017-11-06T01:17:36.441726: step 1150, loss 0.187891, acc 0.9375\n",
      "2017-11-06T01:17:40.395534: step 1151, loss 0.571667, acc 0.875\n",
      "2017-11-06T01:17:42.896311: step 1152, loss 0.00975528, acc 1\n",
      "2017-11-06T01:17:46.882358: step 1153, loss 0.197232, acc 0.9375\n",
      "2017-11-06T01:17:50.819158: step 1154, loss 0.136775, acc 0.96875\n",
      "2017-11-06T01:17:54.798984: step 1155, loss 0.181792, acc 0.9375\n",
      "2017-11-06T01:17:58.700540: step 1156, loss 0.101375, acc 0.96875\n",
      "2017-11-06T01:18:02.886513: step 1157, loss 0.0323281, acc 1\n",
      "2017-11-06T01:18:07.126526: step 1158, loss 0.228122, acc 0.9375\n",
      "2017-11-06T01:18:11.126368: step 1159, loss 0.200478, acc 0.875\n",
      "2017-11-06T01:18:15.088183: step 1160, loss 0.23384, acc 0.9375\n",
      "2017-11-06T01:18:19.008969: step 1161, loss 0.11748, acc 0.9375\n",
      "2017-11-06T01:18:23.116888: step 1162, loss 0.0127176, acc 1\n",
      "2017-11-06T01:18:27.200790: step 1163, loss 0.0867678, acc 0.9375\n",
      "2017-11-06T01:18:31.186621: step 1164, loss 0.470671, acc 0.875\n",
      "2017-11-06T01:18:35.365591: step 1165, loss 0.504676, acc 0.875\n",
      "2017-11-06T01:18:39.309395: step 1166, loss 0.0570413, acc 0.96875\n",
      "2017-11-06T01:18:43.268206: step 1167, loss 0.250303, acc 0.9375\n",
      "2017-11-06T01:18:47.246033: step 1168, loss 0.145596, acc 0.96875\n",
      "2017-11-06T01:18:51.180829: step 1169, loss 0.294001, acc 0.90625\n",
      "2017-11-06T01:18:55.206690: step 1170, loss 0.307229, acc 0.9375\n",
      "2017-11-06T01:18:59.156495: step 1171, loss 0.119402, acc 0.96875\n",
      "2017-11-06T01:19:03.057267: step 1172, loss 0.03556, acc 1\n",
      "2017-11-06T01:19:07.111147: step 1173, loss 0.18934, acc 0.90625\n",
      "2017-11-06T01:19:11.517280: step 1174, loss 0.0326809, acc 1\n",
      "2017-11-06T01:19:15.508116: step 1175, loss 0.12915, acc 0.9375\n",
      "2017-11-06T01:19:19.489945: step 1176, loss 0.0807024, acc 0.96875\n",
      "2017-11-06T01:19:23.480778: step 1177, loss 0.498808, acc 0.84375\n",
      "2017-11-06T01:19:27.395561: step 1178, loss 0.147477, acc 0.96875\n",
      "2017-11-06T01:19:31.424423: step 1179, loss 0.300619, acc 0.875\n",
      "2017-11-06T01:19:35.433272: step 1180, loss 0.013963, acc 1\n",
      "2017-11-06T01:19:39.469139: step 1181, loss 0.658147, acc 0.875\n",
      "2017-11-06T01:19:43.401933: step 1182, loss 0.0196207, acc 1\n",
      "2017-11-06T01:19:47.449810: step 1183, loss 0.357147, acc 0.875\n",
      "2017-11-06T01:19:51.390610: step 1184, loss 0.191545, acc 0.90625\n",
      "2017-11-06T01:19:55.371438: step 1185, loss 0.0307415, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T01:19:59.325248: step 1186, loss 0.214641, acc 0.9375\n",
      "2017-11-06T01:20:03.512222: step 1187, loss 0.100958, acc 0.96875\n",
      "2017-11-06T01:20:06.115074: step 1188, loss 0.0668306, acc 0.95\n",
      "2017-11-06T01:20:10.070883: step 1189, loss 0.354064, acc 0.875\n",
      "2017-11-06T01:20:14.214827: step 1190, loss 0.0192419, acc 1\n",
      "2017-11-06T01:20:18.480859: step 1191, loss 0.0550297, acc 0.96875\n",
      "2017-11-06T01:20:22.414655: step 1192, loss 0.0433872, acc 0.96875\n",
      "2017-11-06T01:20:26.361460: step 1193, loss 0.0390252, acc 1\n",
      "2017-11-06T01:20:30.355296: step 1194, loss 0.176259, acc 0.90625\n",
      "2017-11-06T01:20:34.436196: step 1195, loss 0.116768, acc 0.96875\n",
      "2017-11-06T01:20:38.468062: step 1196, loss 0.179152, acc 0.9375\n",
      "2017-11-06T01:20:42.410862: step 1197, loss 0.136807, acc 0.96875\n",
      "2017-11-06T01:20:46.310633: step 1198, loss 0.117698, acc 0.9375\n",
      "2017-11-06T01:20:50.317481: step 1199, loss 0.112096, acc 0.96875\n",
      "2017-11-06T01:20:54.252276: step 1200, loss 0.355975, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:20:56.869136: step 1200, loss 0.810969, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-06T01:21:02.057750: step 1201, loss 0.154791, acc 0.90625\n",
      "2017-11-06T01:21:05.995550: step 1202, loss 0.393106, acc 0.875\n",
      "2017-11-06T01:21:10.020408: step 1203, loss 0.193026, acc 0.90625\n",
      "2017-11-06T01:21:13.976219: step 1204, loss 0.124059, acc 0.96875\n",
      "2017-11-06T01:21:17.978062: step 1205, loss 0.22042, acc 0.9375\n",
      "2017-11-06T01:21:22.411212: step 1206, loss 0.301467, acc 0.9375\n",
      "2017-11-06T01:21:26.399046: step 1207, loss 0.0481795, acc 0.96875\n",
      "2017-11-06T01:21:30.411897: step 1208, loss 0.031818, acc 1\n",
      "2017-11-06T01:21:34.370711: step 1209, loss 0.333712, acc 0.875\n",
      "2017-11-06T01:21:38.407578: step 1210, loss 0.158598, acc 0.90625\n",
      "2017-11-06T01:21:42.434440: step 1211, loss 0.109744, acc 0.96875\n",
      "2017-11-06T01:21:46.434282: step 1212, loss 0.347976, acc 0.90625\n",
      "2017-11-06T01:21:50.522186: step 1213, loss 0.185954, acc 0.90625\n",
      "2017-11-06T01:21:54.534037: step 1214, loss 0.227619, acc 0.9375\n",
      "2017-11-06T01:21:58.569904: step 1215, loss 0.128831, acc 0.9375\n",
      "2017-11-06T01:22:02.555737: step 1216, loss 0.151932, acc 0.9375\n",
      "2017-11-06T01:22:06.605614: step 1217, loss 0.0780457, acc 0.96875\n",
      "2017-11-06T01:22:10.623470: step 1218, loss 0.0745579, acc 0.9375\n",
      "2017-11-06T01:22:14.692361: step 1219, loss 0.460882, acc 0.90625\n",
      "2017-11-06T01:22:18.677192: step 1220, loss 0.262519, acc 0.84375\n",
      "2017-11-06T01:22:22.759092: step 1221, loss 0.152436, acc 0.9375\n",
      "2017-11-06T01:22:27.192242: step 1222, loss 0.0166199, acc 1\n",
      "2017-11-06T01:22:31.337187: step 1223, loss 0.202444, acc 0.9375\n",
      "2017-11-06T01:22:34.130172: step 1224, loss 0.12147, acc 0.95\n",
      "2017-11-06T01:22:38.273115: step 1225, loss 0.155485, acc 0.9375\n",
      "2017-11-06T01:22:42.250941: step 1226, loss 0.123002, acc 0.96875\n",
      "2017-11-06T01:22:46.215759: step 1227, loss 0.0673759, acc 0.9375\n",
      "2017-11-06T01:22:50.232614: step 1228, loss 0.200876, acc 0.90625\n",
      "2017-11-06T01:22:54.211440: step 1229, loss 0.036028, acc 1\n",
      "2017-11-06T01:22:58.182262: step 1230, loss 0.00677197, acc 1\n",
      "2017-11-06T01:23:02.152083: step 1231, loss 0.128488, acc 0.9375\n",
      "2017-11-06T01:23:06.163933: step 1232, loss 0.448027, acc 0.8125\n",
      "2017-11-06T01:23:10.131752: step 1233, loss 0.224463, acc 0.96875\n",
      "2017-11-06T01:23:14.143603: step 1234, loss 0.320529, acc 0.90625\n",
      "2017-11-06T01:23:18.116426: step 1235, loss 0.125073, acc 0.9375\n",
      "2017-11-06T01:23:22.090249: step 1236, loss 0.321425, acc 0.84375\n",
      "2017-11-06T01:23:26.128118: step 1237, loss 0.211856, acc 0.9375\n",
      "2017-11-06T01:23:30.316094: step 1238, loss 0.172614, acc 0.9375\n",
      "2017-11-06T01:23:34.733233: step 1239, loss 0.0131207, acc 1\n",
      "2017-11-06T01:23:38.703053: step 1240, loss 0.211056, acc 0.875\n",
      "2017-11-06T01:23:42.720908: step 1241, loss 0.0716209, acc 0.96875\n",
      "2017-11-06T01:23:46.675718: step 1242, loss 0.28634, acc 0.84375\n",
      "2017-11-06T01:23:50.747614: step 1243, loss 0.227236, acc 0.9375\n",
      "2017-11-06T01:23:54.771470: step 1244, loss 0.0731897, acc 0.96875\n",
      "2017-11-06T01:23:58.777317: step 1245, loss 0.087317, acc 0.96875\n",
      "2017-11-06T01:24:02.731886: step 1246, loss 0.0641281, acc 0.96875\n",
      "2017-11-06T01:24:06.680693: step 1247, loss 0.0560255, acc 0.96875\n",
      "2017-11-06T01:24:10.759590: step 1248, loss 0.0958079, acc 0.96875\n",
      "2017-11-06T01:24:14.783449: step 1249, loss 0.300456, acc 0.9375\n",
      "2017-11-06T01:24:18.840331: step 1250, loss 0.191847, acc 0.90625\n",
      "2017-11-06T01:24:22.961259: step 1251, loss 0.311948, acc 0.875\n",
      "2017-11-06T01:24:27.251307: step 1252, loss 0.189583, acc 0.90625\n",
      "2017-11-06T01:24:31.221128: step 1253, loss 0.146412, acc 0.96875\n",
      "2017-11-06T01:24:35.713320: step 1254, loss 0.430487, acc 0.875\n",
      "2017-11-06T01:24:40.136463: step 1255, loss 0.0267117, acc 1\n",
      "2017-11-06T01:24:44.190344: step 1256, loss 0.136639, acc 0.9375\n",
      "2017-11-06T01:24:48.190186: step 1257, loss 0.0814417, acc 0.96875\n",
      "2017-11-06T01:24:52.222052: step 1258, loss 0.282385, acc 0.875\n",
      "2017-11-06T01:24:56.233901: step 1259, loss 0.0902044, acc 0.9375\n",
      "2017-11-06T01:24:58.835749: step 1260, loss 0.350374, acc 0.85\n",
      "2017-11-06T01:25:02.847601: step 1261, loss 0.415739, acc 0.84375\n",
      "2017-11-06T01:25:06.886470: step 1262, loss 0.341895, acc 0.90625\n",
      "2017-11-06T01:25:10.882309: step 1263, loss 0.100306, acc 0.96875\n",
      "2017-11-06T01:25:14.893159: step 1264, loss 0.0374383, acc 1\n",
      "2017-11-06T01:25:18.866982: step 1265, loss 0.143471, acc 0.90625\n",
      "2017-11-06T01:25:22.851815: step 1266, loss 0.18465, acc 0.90625\n",
      "2017-11-06T01:25:26.819634: step 1267, loss 0.326293, acc 0.875\n",
      "2017-11-06T01:25:30.845493: step 1268, loss 0.0132473, acc 1\n",
      "2017-11-06T01:25:34.852341: step 1269, loss 0.201331, acc 0.96875\n",
      "2017-11-06T01:25:38.812155: step 1270, loss 0.1112, acc 0.9375\n",
      "2017-11-06T01:25:43.148236: step 1271, loss 0.231141, acc 0.9375\n",
      "2017-11-06T01:25:47.359228: step 1272, loss 0.501491, acc 0.875\n",
      "2017-11-06T01:25:51.386090: step 1273, loss 0.161939, acc 0.90625\n",
      "2017-11-06T01:25:55.450977: step 1274, loss 0.134686, acc 0.96875\n",
      "2017-11-06T01:25:59.420798: step 1275, loss 0.0298919, acc 1\n",
      "2017-11-06T01:26:03.599767: step 1276, loss 0.173247, acc 0.9375\n",
      "2017-11-06T01:26:07.682668: step 1277, loss 0.252773, acc 0.9375\n",
      "2017-11-06T01:26:11.712532: step 1278, loss 0.105579, acc 0.9375\n",
      "2017-11-06T01:26:15.724382: step 1279, loss 0.0916806, acc 0.96875\n",
      "2017-11-06T01:26:19.755246: step 1280, loss 0.0792832, acc 0.9375\n",
      "2017-11-06T01:26:23.741079: step 1281, loss 0.206144, acc 0.90625\n",
      "2017-11-06T01:26:27.768941: step 1282, loss 0.0756902, acc 0.96875\n",
      "2017-11-06T01:26:31.761778: step 1283, loss 0.184632, acc 0.96875\n",
      "2017-11-06T01:26:36.105865: step 1284, loss 0.0658583, acc 0.96875\n",
      "2017-11-06T01:26:40.177759: step 1285, loss 0.208927, acc 0.9375\n",
      "2017-11-06T01:26:44.163589: step 1286, loss 0.00770184, acc 1\n",
      "2017-11-06T01:26:48.512680: step 1287, loss 0.168587, acc 0.96875\n",
      "2017-11-06T01:26:52.717668: step 1288, loss 0.137169, acc 0.9375\n",
      "2017-11-06T01:26:56.752535: step 1289, loss 0.180369, acc 0.90625\n",
      "2017-11-06T01:27:00.751136: step 1290, loss 0.31495, acc 0.9375\n",
      "2017-11-06T01:27:04.716956: step 1291, loss 0.25038, acc 0.90625\n",
      "2017-11-06T01:27:08.711794: step 1292, loss 0.0870482, acc 0.96875\n",
      "2017-11-06T01:27:12.748661: step 1293, loss 0.0102196, acc 1\n",
      "2017-11-06T01:27:16.748503: step 1294, loss 0.462085, acc 0.84375\n",
      "2017-11-06T01:27:20.717323: step 1295, loss 0.148018, acc 0.90625\n",
      "2017-11-06T01:27:23.304162: step 1296, loss 0.120895, acc 0.95\n",
      "2017-11-06T01:27:27.319014: step 1297, loss 0.228809, acc 0.90625\n",
      "2017-11-06T01:27:31.364888: step 1298, loss 0.0811424, acc 0.96875\n",
      "2017-11-06T01:27:35.325703: step 1299, loss 0.142334, acc 0.9375\n",
      "2017-11-06T01:27:39.328547: step 1300, loss 0.198905, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:27:41.932397: step 1300, loss 0.792745, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-06T01:27:47.290713: step 1301, loss 0.0455853, acc 0.96875\n",
      "2017-11-06T01:27:51.388625: step 1302, loss 0.0836558, acc 0.96875\n",
      "2017-11-06T01:27:55.881817: step 1303, loss 0.0972834, acc 0.96875\n",
      "2017-11-06T01:28:00.005747: step 1304, loss 0.412971, acc 0.875\n",
      "2017-11-06T01:28:04.012594: step 1305, loss 0.575857, acc 0.84375\n",
      "2017-11-06T01:28:08.028449: step 1306, loss 0.074462, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T01:28:12.012279: step 1307, loss 0.162058, acc 0.9375\n",
      "2017-11-06T01:28:16.032134: step 1308, loss 0.085697, acc 0.96875\n",
      "2017-11-06T01:28:20.036980: step 1309, loss 0.19839, acc 0.9375\n",
      "2017-11-06T01:28:23.975798: step 1310, loss 0.333705, acc 0.875\n",
      "2017-11-06T01:28:27.949602: step 1311, loss 0.287539, acc 0.875\n",
      "2017-11-06T01:28:31.940438: step 1312, loss 0.0538205, acc 1\n",
      "2017-11-06T01:28:36.108399: step 1313, loss 0.141086, acc 0.96875\n",
      "2017-11-06T01:28:40.023181: step 1314, loss 0.182555, acc 0.9375\n",
      "2017-11-06T01:28:43.976990: step 1315, loss 0.293978, acc 0.9375\n",
      "2017-11-06T01:28:47.996847: step 1316, loss 0.0470834, acc 1\n",
      "2017-11-06T01:28:51.996689: step 1317, loss 0.349244, acc 0.9375\n",
      "2017-11-06T01:28:55.942493: step 1318, loss 0.0917022, acc 0.9375\n",
      "2017-11-06T01:29:00.316600: step 1319, loss 0.334522, acc 0.875\n",
      "2017-11-06T01:29:04.471554: step 1320, loss 0.123228, acc 0.9375\n",
      "2017-11-06T01:29:08.431366: step 1321, loss 0.205564, acc 0.90625\n",
      "2017-11-06T01:29:12.431209: step 1322, loss 0.103778, acc 0.96875\n",
      "2017-11-06T01:29:16.449063: step 1323, loss 0.244941, acc 0.90625\n",
      "2017-11-06T01:29:20.425889: step 1324, loss 0.170254, acc 0.90625\n",
      "2017-11-06T01:29:24.626874: step 1325, loss 0.0247868, acc 1\n",
      "2017-11-06T01:29:28.800840: step 1326, loss 0.00464748, acc 1\n",
      "2017-11-06T01:29:32.775664: step 1327, loss 0.524408, acc 0.875\n",
      "2017-11-06T01:29:36.769503: step 1328, loss 0.0447883, acc 0.96875\n",
      "2017-11-06T01:29:40.733319: step 1329, loss 0.338506, acc 0.875\n",
      "2017-11-06T01:29:44.716148: step 1330, loss 0.265932, acc 0.9375\n",
      "2017-11-06T01:29:48.762023: step 1331, loss 0.144854, acc 0.90625\n",
      "2017-11-06T01:29:51.290820: step 1332, loss 0.0512372, acc 1\n",
      "2017-11-06T01:29:55.211605: step 1333, loss 0.0988814, acc 0.96875\n",
      "2017-11-06T01:29:59.203444: step 1334, loss 0.119427, acc 0.9375\n",
      "2017-11-06T01:30:03.623380: step 1335, loss 0.224959, acc 0.96875\n",
      "2017-11-06T01:30:08.030513: step 1336, loss 0.134869, acc 0.9375\n",
      "2017-11-06T01:30:11.987323: step 1337, loss 0.0443726, acc 1\n",
      "2017-11-06T01:30:15.952140: step 1338, loss 0.197387, acc 0.875\n",
      "2017-11-06T01:30:19.942976: step 1339, loss 0.0098352, acc 1\n",
      "2017-11-06T01:30:23.959830: step 1340, loss 0.138737, acc 0.9375\n",
      "2017-11-06T01:30:27.910637: step 1341, loss 0.203773, acc 0.9375\n",
      "2017-11-06T01:30:31.891466: step 1342, loss 0.327186, acc 0.96875\n",
      "2017-11-06T01:30:36.142486: step 1343, loss 0.0549075, acc 0.96875\n",
      "2017-11-06T01:30:40.085288: step 1344, loss 0.107343, acc 0.9375\n",
      "2017-11-06T01:30:44.019083: step 1345, loss 0.201863, acc 0.9375\n",
      "2017-11-06T01:30:47.996912: step 1346, loss 0.144448, acc 0.9375\n",
      "2017-11-06T01:30:52.034778: step 1347, loss 0.2535, acc 0.90625\n",
      "2017-11-06T01:30:56.022612: step 1348, loss 0.00803109, acc 1\n",
      "2017-11-06T01:30:59.931390: step 1349, loss 0.0700245, acc 0.9375\n",
      "2017-11-06T01:31:03.908216: step 1350, loss 0.0499074, acc 0.96875\n",
      "2017-11-06T01:31:07.913061: step 1351, loss 0.135438, acc 0.9375\n",
      "2017-11-06T01:31:12.354216: step 1352, loss 0.186937, acc 0.90625\n",
      "2017-11-06T01:31:16.476145: step 1353, loss 0.0964064, acc 0.96875\n",
      "2017-11-06T01:31:20.462978: step 1354, loss 0.0797718, acc 0.96875\n",
      "2017-11-06T01:31:24.462822: step 1355, loss 0.0551238, acc 0.96875\n",
      "2017-11-06T01:31:28.447651: step 1356, loss 0.272499, acc 0.9375\n",
      "2017-11-06T01:31:32.376443: step 1357, loss 0.0814669, acc 0.96875\n",
      "2017-11-06T01:31:36.586435: step 1358, loss 0.146483, acc 0.90625\n",
      "2017-11-06T01:31:40.670336: step 1359, loss 0.120925, acc 0.96875\n",
      "2017-11-06T01:31:44.629149: step 1360, loss 0.0487693, acc 0.96875\n",
      "2017-11-06T01:31:48.599970: step 1361, loss 0.130945, acc 0.9375\n",
      "2017-11-06T01:31:52.663857: step 1362, loss 0.0272881, acc 1\n",
      "2017-11-06T01:31:56.632678: step 1363, loss 0.183702, acc 0.96875\n",
      "2017-11-06T01:32:00.600497: step 1364, loss 0.441647, acc 0.875\n",
      "2017-11-06T01:32:04.663384: step 1365, loss 0.568652, acc 0.875\n",
      "2017-11-06T01:32:08.659223: step 1366, loss 0.362458, acc 0.90625\n",
      "2017-11-06T01:32:12.607047: step 1367, loss 0.064555, acc 0.96875\n",
      "2017-11-06T01:32:15.417024: step 1368, loss 0.0832167, acc 0.95\n",
      "2017-11-06T01:32:19.715079: step 1369, loss 0.24547, acc 0.90625\n",
      "2017-11-06T01:32:23.818996: step 1370, loss 0.0646887, acc 1\n",
      "2017-11-06T01:32:27.787815: step 1371, loss 0.0671513, acc 0.96875\n",
      "2017-11-06T01:32:31.801669: step 1372, loss 0.319464, acc 0.875\n",
      "2017-11-06T01:32:36.011658: step 1373, loss 0.162369, acc 0.90625\n",
      "2017-11-06T01:32:39.937448: step 1374, loss 0.160356, acc 0.9375\n",
      "2017-11-06T01:32:43.953301: step 1375, loss 0.149721, acc 0.9375\n",
      "2017-11-06T01:32:47.915116: step 1376, loss 0.385701, acc 0.875\n",
      "2017-11-06T01:32:51.889940: step 1377, loss 0.203764, acc 0.9375\n",
      "2017-11-06T01:32:55.879777: step 1378, loss 0.178475, acc 0.9375\n",
      "2017-11-06T01:32:59.887623: step 1379, loss 0.0345084, acc 1\n",
      "2017-11-06T01:33:03.861266: step 1380, loss 0.140777, acc 0.9375\n",
      "2017-11-06T01:33:07.913146: step 1381, loss 0.0847676, acc 0.96875\n",
      "2017-11-06T01:33:11.895975: step 1382, loss 0.0971351, acc 0.96875\n",
      "2017-11-06T01:33:15.862796: step 1383, loss 0.0405579, acc 0.96875\n",
      "2017-11-06T01:33:19.996732: step 1384, loss 0.32147, acc 0.90625\n",
      "2017-11-06T01:33:24.386851: step 1385, loss 0.234697, acc 0.9375\n",
      "2017-11-06T01:33:28.389694: step 1386, loss 0.059591, acc 1\n",
      "2017-11-06T01:33:32.386535: step 1387, loss 0.0105855, acc 1\n",
      "2017-11-06T01:33:36.357356: step 1388, loss 0.213122, acc 0.9375\n",
      "2017-11-06T01:33:40.516311: step 1389, loss 0.172657, acc 0.9375\n",
      "2017-11-06T01:33:44.547176: step 1390, loss 0.356088, acc 0.84375\n",
      "2017-11-06T01:33:48.704129: step 1391, loss 0.0725615, acc 0.96875\n",
      "2017-11-06T01:33:52.921126: step 1392, loss 0.259118, acc 0.90625\n",
      "2017-11-06T01:33:57.175148: step 1393, loss 0.110693, acc 0.9375\n",
      "2017-11-06T01:34:01.211015: step 1394, loss 0.0267992, acc 1\n",
      "2017-11-06T01:34:05.354961: step 1395, loss 0.108212, acc 0.9375\n",
      "2017-11-06T01:34:09.397833: step 1396, loss 0.272936, acc 0.90625\n",
      "2017-11-06T01:34:13.536774: step 1397, loss 0.0584215, acc 0.96875\n",
      "2017-11-06T01:34:17.650696: step 1398, loss 0.104764, acc 0.96875\n",
      "2017-11-06T01:34:21.990781: step 1399, loss 0.0613648, acc 0.96875\n",
      "2017-11-06T01:34:26.656096: step 1400, loss 0.0312835, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:34:29.751295: step 1400, loss 0.801651, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-06T01:34:35.925239: step 1401, loss 0.141206, acc 0.90625\n",
      "2017-11-06T01:34:39.936089: step 1402, loss 0.0479831, acc 0.96875\n",
      "2017-11-06T01:34:44.065023: step 1403, loss 0.144561, acc 0.9375\n",
      "2017-11-06T01:34:46.574806: step 1404, loss 0.280992, acc 0.85\n",
      "2017-11-06T01:34:50.568644: step 1405, loss 0.0966536, acc 0.9375\n",
      "2017-11-06T01:34:54.579494: step 1406, loss 0.0136108, acc 1\n",
      "2017-11-06T01:34:58.553317: step 1407, loss 0.273298, acc 0.875\n",
      "2017-11-06T01:35:02.571173: step 1408, loss 0.145345, acc 0.9375\n",
      "2017-11-06T01:35:06.556003: step 1409, loss 0.0339258, acc 1\n",
      "2017-11-06T01:35:10.494802: step 1410, loss 0.0608293, acc 0.96875\n",
      "2017-11-06T01:35:14.483636: step 1411, loss 0.15016, acc 0.96875\n",
      "2017-11-06T01:35:18.488482: step 1412, loss 0.014895, acc 1\n",
      "2017-11-06T01:35:22.477316: step 1413, loss 0.366024, acc 0.875\n",
      "2017-11-06T01:35:26.434128: step 1414, loss 0.0531483, acc 0.96875\n",
      "2017-11-06T01:35:30.449981: step 1415, loss 0.0419271, acc 1\n",
      "2017-11-06T01:35:34.983202: step 1416, loss 0.0163666, acc 1\n",
      "2017-11-06T01:35:39.003058: step 1417, loss 0.0997705, acc 0.9375\n",
      "2017-11-06T01:35:42.954866: step 1418, loss 0.12845, acc 0.96875\n",
      "2017-11-06T01:35:46.946704: step 1419, loss 0.209192, acc 0.9375\n",
      "2017-11-06T01:35:50.984572: step 1420, loss 0.0761008, acc 0.96875\n",
      "2017-11-06T01:35:54.988417: step 1421, loss 0.0423346, acc 1\n",
      "2017-11-06T01:35:58.925214: step 1422, loss 0.256886, acc 0.90625\n",
      "2017-11-06T01:36:02.919839: step 1423, loss 0.116422, acc 0.9375\n",
      "2017-11-06T01:36:06.975722: step 1424, loss 0.0680337, acc 1\n",
      "2017-11-06T01:36:11.030603: step 1425, loss 0.227535, acc 0.875\n",
      "2017-11-06T01:36:15.136520: step 1426, loss 0.136302, acc 0.90625\n",
      "2017-11-06T01:36:19.169386: step 1427, loss 0.094505, acc 0.96875\n",
      "2017-11-06T01:36:23.186240: step 1428, loss 0.0889755, acc 0.96875\n",
      "2017-11-06T01:36:27.142052: step 1429, loss 0.312076, acc 0.84375\n",
      "2017-11-06T01:36:31.102866: step 1430, loss 0.0152434, acc 1\n",
      "2017-11-06T01:36:35.294844: step 1431, loss 0.00399866, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T01:36:39.595900: step 1432, loss 0.0478681, acc 0.96875\n",
      "2017-11-06T01:36:43.685807: step 1433, loss 0.44455, acc 0.90625\n",
      "2017-11-06T01:36:47.579572: step 1434, loss 0.326699, acc 0.875\n",
      "2017-11-06T01:36:51.519372: step 1435, loss 0.373078, acc 0.90625\n",
      "2017-11-06T01:36:55.472181: step 1436, loss 0.381117, acc 0.90625\n",
      "2017-11-06T01:36:59.494038: step 1437, loss 0.0928585, acc 0.96875\n",
      "2017-11-06T01:37:03.395810: step 1438, loss 0.316764, acc 0.90625\n",
      "2017-11-06T01:37:07.314594: step 1439, loss 0.0424548, acc 0.96875\n",
      "2017-11-06T01:37:09.838388: step 1440, loss 0.559332, acc 0.8\n",
      "2017-11-06T01:37:13.770181: step 1441, loss 0.255972, acc 0.90625\n",
      "2017-11-06T01:37:17.671954: step 1442, loss 0.0828899, acc 0.96875\n",
      "2017-11-06T01:37:21.599745: step 1443, loss 0.248087, acc 0.875\n",
      "2017-11-06T01:37:25.547551: step 1444, loss 0.249751, acc 0.90625\n",
      "2017-11-06T01:37:29.492353: step 1445, loss 0.11734, acc 0.96875\n",
      "2017-11-06T01:37:33.411138: step 1446, loss 0.123, acc 0.96875\n",
      "2017-11-06T01:37:37.418986: step 1447, loss 0.258187, acc 0.875\n",
      "2017-11-06T01:37:41.342774: step 1448, loss 0.0304134, acc 1\n",
      "2017-11-06T01:37:45.721885: step 1449, loss 0.207858, acc 0.90625\n",
      "2017-11-06T01:37:49.779768: step 1450, loss 0.298385, acc 0.875\n",
      "2017-11-06T01:37:53.760596: step 1451, loss 0.136708, acc 0.9375\n",
      "2017-11-06T01:37:57.721413: step 1452, loss 0.140689, acc 0.9375\n",
      "2017-11-06T01:38:01.741267: step 1453, loss 0.353204, acc 0.875\n",
      "2017-11-06T01:38:05.732103: step 1454, loss 0.092433, acc 0.96875\n",
      "2017-11-06T01:38:09.725941: step 1455, loss 0.201693, acc 0.90625\n",
      "2017-11-06T01:38:13.696762: step 1456, loss 0.191357, acc 0.90625\n",
      "2017-11-06T01:38:17.682594: step 1457, loss 0.332544, acc 0.90625\n",
      "2017-11-06T01:38:21.621392: step 1458, loss 0.00785285, acc 1\n",
      "2017-11-06T01:38:25.816373: step 1459, loss 0.0153301, acc 1\n",
      "2017-11-06T01:38:29.857245: step 1460, loss 0.0207371, acc 1\n",
      "2017-11-06T01:38:34.000188: step 1461, loss 0.0487157, acc 1\n",
      "2017-11-06T01:38:38.040059: step 1462, loss 0.166571, acc 0.9375\n",
      "2017-11-06T01:38:42.002875: step 1463, loss 0.166275, acc 0.9375\n",
      "2017-11-06T01:38:45.968692: step 1464, loss 0.12528, acc 0.96875\n",
      "2017-11-06T01:38:50.186690: step 1465, loss 0.358376, acc 0.90625\n",
      "2017-11-06T01:38:54.495752: step 1466, loss 0.0366898, acc 1\n",
      "2017-11-06T01:38:58.500597: step 1467, loss 0.222398, acc 0.9375\n",
      "2017-11-06T01:39:02.454158: step 1468, loss 0.0728734, acc 0.96875\n",
      "2017-11-06T01:39:06.380948: step 1469, loss 0.42773, acc 0.8125\n",
      "2017-11-06T01:39:10.306738: step 1470, loss 0.185865, acc 0.96875\n",
      "2017-11-06T01:39:14.308581: step 1471, loss 0.245274, acc 0.90625\n",
      "2017-11-06T01:39:18.260389: step 1472, loss 0.101105, acc 0.9375\n",
      "2017-11-06T01:39:22.215199: step 1473, loss 0.273114, acc 0.875\n",
      "2017-11-06T01:39:26.100960: step 1474, loss 0.0298214, acc 1\n",
      "2017-11-06T01:39:30.156842: step 1475, loss 0.135676, acc 0.96875\n",
      "2017-11-06T01:39:32.744681: step 1476, loss 0.151419, acc 0.95\n",
      "2017-11-06T01:39:36.703495: step 1477, loss 0.0771156, acc 0.9375\n",
      "2017-11-06T01:39:40.688326: step 1478, loss 0.154958, acc 0.9375\n",
      "2017-11-06T01:39:44.661148: step 1479, loss 0.0446712, acc 0.96875\n",
      "2017-11-06T01:39:48.560919: step 1480, loss 0.146199, acc 0.96875\n",
      "2017-11-06T01:39:52.531740: step 1481, loss 0.0759671, acc 0.96875\n",
      "2017-11-06T01:39:56.963890: step 1482, loss 0.188301, acc 0.90625\n",
      "2017-11-06T01:40:01.229920: step 1483, loss 0.121045, acc 0.96875\n",
      "2017-11-06T01:40:05.295809: step 1484, loss 0.181761, acc 0.875\n",
      "2017-11-06T01:40:09.299889: step 1485, loss 0.191818, acc 0.9375\n",
      "2017-11-06T01:40:13.200659: step 1486, loss 0.058949, acc 0.96875\n",
      "2017-11-06T01:40:17.116443: step 1487, loss 0.0366438, acc 1\n",
      "2017-11-06T01:40:21.094269: step 1488, loss 0.148966, acc 0.9375\n",
      "2017-11-06T01:40:25.068092: step 1489, loss 0.0124358, acc 1\n",
      "2017-11-06T01:40:28.990879: step 1490, loss 0.0817537, acc 0.9375\n",
      "2017-11-06T01:40:32.995724: step 1491, loss 0.32443, acc 0.84375\n",
      "2017-11-06T01:40:37.121656: step 1492, loss 0.130893, acc 0.90625\n",
      "2017-11-06T01:40:41.039440: step 1493, loss 0.0791117, acc 0.96875\n",
      "2017-11-06T01:40:44.988247: step 1494, loss 0.163378, acc 0.90625\n",
      "2017-11-06T01:40:48.906029: step 1495, loss 0.118368, acc 0.9375\n",
      "2017-11-06T01:40:52.837823: step 1496, loss 0.105979, acc 0.96875\n",
      "2017-11-06T01:40:56.790632: step 1497, loss 0.255096, acc 0.9375\n",
      "2017-11-06T01:41:01.002625: step 1498, loss 0.217219, acc 0.90625\n",
      "2017-11-06T01:41:05.208612: step 1499, loss 0.183683, acc 0.875\n",
      "2017-11-06T01:41:09.214459: step 1500, loss 0.265315, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:41:12.644897: step 1500, loss 0.743295, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-06T01:41:18.196842: step 1501, loss 0.0494425, acc 0.96875\n",
      "2017-11-06T01:41:22.280743: step 1502, loss 0.148565, acc 0.9375\n",
      "2017-11-06T01:41:26.230550: step 1503, loss 0.0621702, acc 0.9375\n",
      "2017-11-06T01:41:30.253408: step 1504, loss 0.610671, acc 0.84375\n",
      "2017-11-06T01:41:34.225231: step 1505, loss 0.270144, acc 0.96875\n",
      "2017-11-06T01:41:38.343156: step 1506, loss 0.0164488, acc 1\n",
      "2017-11-06T01:41:42.444070: step 1507, loss 0.0991715, acc 0.9375\n",
      "2017-11-06T01:41:46.473934: step 1508, loss 0.0649725, acc 0.96875\n",
      "2017-11-06T01:41:50.463770: step 1509, loss 0.127175, acc 0.9375\n",
      "2017-11-06T01:41:54.592702: step 1510, loss 0.0443416, acc 1\n",
      "2017-11-06T01:41:58.620564: step 1511, loss 0.0768697, acc 0.96875\n",
      "2017-11-06T01:42:01.210405: step 1512, loss 0.0819658, acc 0.95\n",
      "2017-11-06T01:42:05.379365: step 1513, loss 0.0842424, acc 0.96875\n",
      "2017-11-06T01:42:09.858548: step 1514, loss 0.187124, acc 0.9375\n",
      "2017-11-06T01:42:13.867397: step 1515, loss 0.294179, acc 0.84375\n",
      "2017-11-06T01:42:17.910269: step 1516, loss 0.142681, acc 0.96875\n",
      "2017-11-06T01:42:22.027195: step 1517, loss 0.140411, acc 0.9375\n",
      "2017-11-06T01:42:26.098087: step 1518, loss 0.237022, acc 0.875\n",
      "2017-11-06T01:42:30.168980: step 1519, loss 0.112186, acc 0.9375\n",
      "2017-11-06T01:42:34.329937: step 1520, loss 0.0353237, acc 1\n",
      "2017-11-06T01:42:38.360800: step 1521, loss 0.0484489, acc 0.96875\n",
      "2017-11-06T01:42:42.369649: step 1522, loss 0.0247523, acc 1\n",
      "2017-11-06T01:42:46.355481: step 1523, loss 0.102692, acc 0.9375\n",
      "2017-11-06T01:42:50.339312: step 1524, loss 0.0220443, acc 1\n",
      "2017-11-06T01:42:54.374180: step 1525, loss 0.183604, acc 0.9375\n",
      "2017-11-06T01:42:58.442069: step 1526, loss 0.119274, acc 0.9375\n",
      "2017-11-06T01:43:02.510961: step 1527, loss 0.155091, acc 0.90625\n",
      "2017-11-06T01:43:06.499794: step 1528, loss 0.132702, acc 0.90625\n",
      "2017-11-06T01:43:10.662752: step 1529, loss 0.0396226, acc 0.96875\n",
      "2017-11-06T01:43:15.057875: step 1530, loss 0.163349, acc 0.90625\n",
      "2017-11-06T01:43:19.160790: step 1531, loss 0.0557126, acc 0.96875\n",
      "2017-11-06T01:43:23.371783: step 1532, loss 0.182673, acc 0.875\n",
      "2017-11-06T01:43:27.533740: step 1533, loss 0.261028, acc 0.90625\n",
      "2017-11-06T01:43:31.512567: step 1534, loss 0.0332861, acc 1\n",
      "2017-11-06T01:43:35.687535: step 1535, loss 0.3206, acc 0.90625\n",
      "2017-11-06T01:43:39.761428: step 1536, loss 0.402521, acc 0.84375\n",
      "2017-11-06T01:43:43.849332: step 1537, loss 0.191161, acc 0.9375\n",
      "2017-11-06T01:43:47.934235: step 1538, loss 0.124941, acc 0.96875\n",
      "2017-11-06T01:43:52.000124: step 1539, loss 0.481665, acc 0.84375\n",
      "2017-11-06T01:43:56.059008: step 1540, loss 0.196541, acc 0.90625\n",
      "2017-11-06T01:44:00.120894: step 1541, loss 0.178419, acc 0.96875\n",
      "2017-11-06T01:44:04.076706: step 1542, loss 0.133522, acc 0.9375\n",
      "2017-11-06T01:44:08.071544: step 1543, loss 0.0977145, acc 0.96875\n",
      "2017-11-06T01:44:12.092401: step 1544, loss 0.0649801, acc 0.96875\n",
      "2017-11-06T01:44:16.217331: step 1545, loss 0.087259, acc 0.96875\n",
      "2017-11-06T01:44:20.729538: step 1546, loss 0.193097, acc 0.9375\n",
      "2017-11-06T01:44:24.794426: step 1547, loss 0.072896, acc 0.96875\n",
      "2017-11-06T01:44:27.371257: step 1548, loss 0.10681, acc 0.95\n",
      "2017-11-06T01:44:31.342078: step 1549, loss 0.187645, acc 0.90625\n",
      "2017-11-06T01:44:35.531055: step 1550, loss 0.0939412, acc 0.9375\n",
      "2017-11-06T01:44:39.571926: step 1551, loss 0.248007, acc 0.90625\n",
      "2017-11-06T01:44:43.546750: step 1552, loss 0.0586948, acc 0.96875\n",
      "2017-11-06T01:44:47.490552: step 1553, loss 0.128588, acc 0.9375\n",
      "2017-11-06T01:44:51.504404: step 1554, loss 0.0395155, acc 1\n",
      "2017-11-06T01:44:55.468221: step 1555, loss 0.260127, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T01:44:59.528106: step 1556, loss 0.172117, acc 0.90625\n",
      "2017-11-06T01:45:03.455660: step 1557, loss 0.186667, acc 0.9375\n",
      "2017-11-06T01:45:07.435488: step 1558, loss 0.182771, acc 0.9375\n",
      "2017-11-06T01:45:11.425324: step 1559, loss 0.179609, acc 0.90625\n",
      "2017-11-06T01:45:15.437173: step 1560, loss 0.0366241, acc 0.96875\n",
      "2017-11-06T01:45:19.441018: step 1561, loss 0.176604, acc 0.90625\n",
      "2017-11-06T01:45:23.764090: step 1562, loss 0.174387, acc 0.96875\n",
      "2017-11-06T01:45:27.916040: step 1563, loss 0.150762, acc 0.90625\n",
      "2017-11-06T01:45:31.957912: step 1564, loss 0.128409, acc 0.96875\n",
      "2017-11-06T01:45:35.974766: step 1565, loss 0.221998, acc 0.875\n",
      "2017-11-06T01:45:39.999627: step 1566, loss 0.00494798, acc 1\n",
      "2017-11-06T01:45:43.929419: step 1567, loss 0.0465688, acc 1\n",
      "2017-11-06T01:45:47.984299: step 1568, loss 0.0348699, acc 1\n",
      "2017-11-06T01:45:51.984141: step 1569, loss 0.128233, acc 0.96875\n",
      "2017-11-06T01:45:55.971975: step 1570, loss 0.0095272, acc 1\n",
      "2017-11-06T01:46:00.059879: step 1571, loss 0.113843, acc 0.96875\n",
      "2017-11-06T01:46:04.075733: step 1572, loss 0.0663031, acc 0.96875\n",
      "2017-11-06T01:46:08.154631: step 1573, loss 0.237426, acc 0.90625\n",
      "2017-11-06T01:46:12.175488: step 1574, loss 0.153422, acc 0.9375\n",
      "2017-11-06T01:46:16.235373: step 1575, loss 0.0841828, acc 0.9375\n",
      "2017-11-06T01:46:20.281248: step 1576, loss 0.206153, acc 0.9375\n",
      "2017-11-06T01:46:24.350139: step 1577, loss 0.122676, acc 0.9375\n",
      "2017-11-06T01:46:28.662203: step 1578, loss 0.289475, acc 0.9375\n",
      "2017-11-06T01:46:33.049320: step 1579, loss 0.117772, acc 0.9375\n",
      "2017-11-06T01:46:37.213279: step 1580, loss 0.0245492, acc 1\n",
      "2017-11-06T01:46:41.189104: step 1581, loss 0.268116, acc 0.875\n",
      "2017-11-06T01:46:45.204957: step 1582, loss 0.323171, acc 0.84375\n",
      "2017-11-06T01:46:49.171775: step 1583, loss 0.0854337, acc 0.96875\n",
      "2017-11-06T01:46:51.790637: step 1584, loss 0.0981887, acc 0.95\n",
      "2017-11-06T01:46:55.753452: step 1585, loss 0.132119, acc 0.9375\n",
      "2017-11-06T01:46:59.761300: step 1586, loss 0.405775, acc 0.84375\n",
      "2017-11-06T01:47:03.696096: step 1587, loss 0.231264, acc 0.90625\n",
      "2017-11-06T01:47:07.691935: step 1588, loss 0.160602, acc 0.90625\n",
      "2017-11-06T01:47:11.757824: step 1589, loss 0.225627, acc 0.9375\n",
      "2017-11-06T01:47:15.718638: step 1590, loss 0.577666, acc 0.90625\n",
      "2017-11-06T01:47:19.698466: step 1591, loss 0.141184, acc 0.9375\n",
      "2017-11-06T01:47:23.723326: step 1592, loss 0.448816, acc 0.8125\n",
      "2017-11-06T01:47:27.697149: step 1593, loss 0.0814463, acc 0.96875\n",
      "2017-11-06T01:47:31.669972: step 1594, loss 0.100011, acc 0.9375\n",
      "2017-11-06T01:47:36.098119: step 1595, loss 0.158121, acc 0.96875\n",
      "2017-11-06T01:47:40.206039: step 1596, loss 0.0852601, acc 0.9375\n",
      "2017-11-06T01:47:44.175858: step 1597, loss 0.138843, acc 0.96875\n",
      "2017-11-06T01:47:48.184707: step 1598, loss 0.195594, acc 0.90625\n",
      "2017-11-06T01:47:52.174542: step 1599, loss 0.0673512, acc 0.96875\n",
      "2017-11-06T01:47:56.161374: step 1600, loss 0.21258, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:47:58.869298: step 1600, loss 0.478933, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-06T01:48:04.189806: step 1601, loss 0.393381, acc 0.90625\n",
      "2017-11-06T01:48:08.243687: step 1602, loss 0.0681439, acc 0.96875\n",
      "2017-11-06T01:48:12.235523: step 1603, loss 0.0493648, acc 0.96875\n",
      "2017-11-06T01:48:16.284400: step 1604, loss 0.250757, acc 0.90625\n",
      "2017-11-06T01:48:20.314264: step 1605, loss 0.0816572, acc 0.96875\n",
      "2017-11-06T01:48:24.478222: step 1606, loss 0.0357615, acc 1\n",
      "2017-11-06T01:48:28.924382: step 1607, loss 0.238009, acc 0.90625\n",
      "2017-11-06T01:48:33.377546: step 1608, loss 0.00207579, acc 1\n",
      "2017-11-06T01:48:37.516487: step 1609, loss 0.0181757, acc 1\n",
      "2017-11-06T01:48:42.013682: step 1610, loss 0.111756, acc 0.96875\n",
      "2017-11-06T01:48:46.057556: step 1611, loss 0.187451, acc 0.9375\n",
      "2017-11-06T01:48:49.996373: step 1612, loss 0.102768, acc 0.96875\n",
      "2017-11-06T01:48:54.023215: step 1613, loss 0.0574976, acc 0.96875\n",
      "2017-11-06T01:48:58.038067: step 1614, loss 0.301241, acc 0.90625\n",
      "2017-11-06T01:49:02.007888: step 1615, loss 0.216213, acc 0.9375\n",
      "2017-11-06T01:49:05.921671: step 1616, loss 0.0525809, acc 0.96875\n",
      "2017-11-06T01:49:09.907502: step 1617, loss 0.0499555, acc 0.96875\n",
      "2017-11-06T01:49:13.958379: step 1618, loss 0.122364, acc 0.9375\n",
      "2017-11-06T01:49:17.996248: step 1619, loss 0.073813, acc 0.96875\n",
      "2017-11-06T01:49:20.575081: step 1620, loss 0.0692279, acc 0.95\n",
      "2017-11-06T01:49:24.589934: step 1621, loss 0.112303, acc 0.9375\n",
      "2017-11-06T01:49:28.509719: step 1622, loss 0.167083, acc 0.875\n",
      "2017-11-06T01:49:32.449519: step 1623, loss 0.127029, acc 0.9375\n",
      "2017-11-06T01:49:36.436351: step 1624, loss 0.125752, acc 0.96875\n",
      "2017-11-06T01:49:40.474221: step 1625, loss 0.165648, acc 0.90625\n",
      "2017-11-06T01:49:44.580137: step 1626, loss 0.289048, acc 0.875\n",
      "2017-11-06T01:49:48.905211: step 1627, loss 0.274965, acc 0.9375\n",
      "2017-11-06T01:49:52.901050: step 1628, loss 0.0676827, acc 1\n",
      "2017-11-06T01:49:56.959934: step 1629, loss 0.112599, acc 0.9375\n",
      "2017-11-06T01:50:01.212956: step 1630, loss 0.129725, acc 0.9375\n",
      "2017-11-06T01:50:05.335886: step 1631, loss 0.196805, acc 0.875\n",
      "2017-11-06T01:50:09.297700: step 1632, loss 0.113668, acc 0.96875\n",
      "2017-11-06T01:50:13.242503: step 1633, loss 0.0440491, acc 1\n",
      "2017-11-06T01:50:17.225335: step 1634, loss 0.119756, acc 0.96875\n",
      "2017-11-06T01:50:21.234182: step 1635, loss 0.0366106, acc 1\n",
      "2017-11-06T01:50:25.187991: step 1636, loss 0.0775608, acc 0.96875\n",
      "2017-11-06T01:50:29.230864: step 1637, loss 0.130622, acc 0.9375\n",
      "2017-11-06T01:50:33.327775: step 1638, loss 0.161696, acc 0.9375\n",
      "2017-11-06T01:50:37.384659: step 1639, loss 0.0399956, acc 1\n",
      "2017-11-06T01:50:41.420527: step 1640, loss 0.00745896, acc 1\n",
      "2017-11-06T01:50:45.371332: step 1641, loss 0.121562, acc 0.9375\n",
      "2017-11-06T01:50:49.412204: step 1642, loss 0.174925, acc 0.96875\n",
      "2017-11-06T01:50:53.914402: step 1643, loss 0.325862, acc 0.9375\n",
      "2017-11-06T01:50:57.918247: step 1644, loss 0.0353016, acc 1\n",
      "2017-11-06T01:51:01.896074: step 1645, loss 0.187023, acc 0.9375\n",
      "2017-11-06T01:51:05.988746: step 1646, loss 0.0289765, acc 1\n",
      "2017-11-06T01:51:09.945556: step 1647, loss 0.117858, acc 0.96875\n",
      "2017-11-06T01:51:13.977421: step 1648, loss 0.14781, acc 0.9375\n",
      "2017-11-06T01:51:17.937234: step 1649, loss 0.252383, acc 0.84375\n",
      "2017-11-06T01:51:21.952087: step 1650, loss 0.108888, acc 0.9375\n",
      "2017-11-06T01:51:25.930915: step 1651, loss 0.0359876, acc 0.96875\n",
      "2017-11-06T01:51:29.915746: step 1652, loss 0.103615, acc 0.96875\n",
      "2017-11-06T01:51:33.872558: step 1653, loss 0.218566, acc 0.90625\n",
      "2017-11-06T01:51:37.920434: step 1654, loss 0.0758077, acc 0.96875\n",
      "2017-11-06T01:51:41.893257: step 1655, loss 0.233652, acc 0.9375\n",
      "2017-11-06T01:51:44.472090: step 1656, loss 0.0581389, acc 1\n",
      "2017-11-06T01:51:48.424897: step 1657, loss 0.114714, acc 0.9375\n",
      "2017-11-06T01:51:52.405726: step 1658, loss 0.105821, acc 0.9375\n",
      "2017-11-06T01:51:56.676760: step 1659, loss 0.0971358, acc 0.9375\n",
      "2017-11-06T01:52:00.898760: step 1660, loss 0.255747, acc 0.84375\n",
      "2017-11-06T01:52:04.897603: step 1661, loss 0.227279, acc 0.90625\n",
      "2017-11-06T01:52:08.857415: step 1662, loss 0.123113, acc 0.90625\n",
      "2017-11-06T01:52:12.845250: step 1663, loss 0.0184111, acc 1\n",
      "2017-11-06T01:52:16.843089: step 1664, loss 0.266394, acc 0.90625\n",
      "2017-11-06T01:52:20.902974: step 1665, loss 0.0699428, acc 0.9375\n",
      "2017-11-06T01:52:24.974867: step 1666, loss 0.159494, acc 0.9375\n",
      "2017-11-06T01:52:28.994724: step 1667, loss 0.0331282, acc 0.96875\n",
      "2017-11-06T01:52:33.159684: step 1668, loss 0.174505, acc 0.9375\n",
      "2017-11-06T01:52:37.213564: step 1669, loss 0.0330409, acc 1\n",
      "2017-11-06T01:52:41.238424: step 1670, loss 0.199787, acc 0.9375\n",
      "2017-11-06T01:52:45.225258: step 1671, loss 0.116794, acc 0.9375\n",
      "2017-11-06T01:52:49.186071: step 1672, loss 0.230795, acc 0.875\n",
      "2017-11-06T01:52:53.225941: step 1673, loss 0.0653622, acc 0.96875\n",
      "2017-11-06T01:52:57.195762: step 1674, loss 0.285215, acc 0.875\n",
      "2017-11-06T01:53:01.354717: step 1675, loss 0.0667924, acc 0.96875\n",
      "2017-11-06T01:53:05.673786: step 1676, loss 0.122319, acc 0.9375\n",
      "2017-11-06T01:53:09.687639: step 1677, loss 0.1835, acc 0.9375\n",
      "2017-11-06T01:53:13.716501: step 1678, loss 0.141408, acc 0.9375\n",
      "2017-11-06T01:53:17.655299: step 1679, loss 0.0697849, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T01:53:21.729194: step 1680, loss 0.170936, acc 0.9375\n",
      "2017-11-06T01:53:25.707024: step 1681, loss 0.0130834, acc 1\n",
      "2017-11-06T01:53:29.697856: step 1682, loss 0.157601, acc 0.875\n",
      "2017-11-06T01:53:33.688693: step 1683, loss 0.0900681, acc 0.9375\n",
      "2017-11-06T01:53:37.631493: step 1684, loss 0.138537, acc 0.96875\n",
      "2017-11-06T01:53:41.601315: step 1685, loss 0.18734, acc 0.96875\n",
      "2017-11-06T01:53:45.621172: step 1686, loss 0.24481, acc 0.875\n",
      "2017-11-06T01:53:49.571978: step 1687, loss 0.0378653, acc 0.96875\n",
      "2017-11-06T01:53:53.553807: step 1688, loss 0.115785, acc 0.90625\n",
      "2017-11-06T01:53:57.572663: step 1689, loss 0.235942, acc 0.90625\n",
      "2017-11-06T01:54:01.523469: step 1690, loss 0.0201575, acc 1\n",
      "2017-11-06T01:54:05.554127: step 1691, loss 0.33115, acc 0.90625\n",
      "2017-11-06T01:54:08.468197: step 1692, loss 0.335763, acc 0.85\n",
      "2017-11-06T01:54:12.731228: step 1693, loss 0.127677, acc 0.9375\n",
      "2017-11-06T01:54:16.750083: step 1694, loss 0.0132525, acc 1\n",
      "2017-11-06T01:54:20.827980: step 1695, loss 0.22188, acc 0.9375\n",
      "2017-11-06T01:54:25.085004: step 1696, loss 0.165623, acc 0.96875\n",
      "2017-11-06T01:54:29.262974: step 1697, loss 0.0314451, acc 0.96875\n",
      "2017-11-06T01:54:33.352880: step 1698, loss 0.00606323, acc 1\n",
      "2017-11-06T01:54:37.442786: step 1699, loss 0.135137, acc 0.90625\n",
      "2017-11-06T01:54:41.525687: step 1700, loss 0.123233, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T01:54:44.137543: step 1700, loss 0.679072, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-06T01:54:49.579788: step 1701, loss 0.113015, acc 0.96875\n",
      "2017-11-06T01:54:53.609652: step 1702, loss 0.21253, acc 0.875\n",
      "2017-11-06T01:54:57.571467: step 1703, loss 0.0534076, acc 0.96875\n",
      "2017-11-06T01:55:01.658371: step 1704, loss 0.16852, acc 0.96875\n",
      "2017-11-06T01:55:05.610179: step 1705, loss 0.102038, acc 0.96875\n",
      "2017-11-06T01:55:09.636039: step 1706, loss 0.188453, acc 0.96875\n",
      "2017-11-06T01:55:13.868046: step 1707, loss 0.502796, acc 0.84375\n",
      "2017-11-06T01:55:17.955951: step 1708, loss 0.217937, acc 0.90625\n",
      "2017-11-06T01:55:21.939782: step 1709, loss 0.0398982, acc 1\n",
      "2017-11-06T01:55:25.873578: step 1710, loss 0.220485, acc 0.90625\n",
      "2017-11-06T01:55:29.786357: step 1711, loss 0.02396, acc 1\n",
      "2017-11-06T01:55:33.707143: step 1712, loss 0.127153, acc 0.96875\n",
      "2017-11-06T01:55:37.616921: step 1713, loss 0.120825, acc 0.90625\n",
      "2017-11-06T01:55:41.579738: step 1714, loss 0.0884202, acc 0.96875\n",
      "2017-11-06T01:55:45.482510: step 1715, loss 0.376449, acc 0.90625\n",
      "2017-11-06T01:55:49.424310: step 1716, loss 0.174507, acc 0.90625\n",
      "2017-11-06T01:55:53.405140: step 1717, loss 0.0574508, acc 0.96875\n",
      "2017-11-06T01:55:57.303910: step 1718, loss 0.0138519, acc 1\n",
      "2017-11-06T01:56:01.213688: step 1719, loss 0.249374, acc 0.9375\n",
      "2017-11-06T01:56:05.131475: step 1720, loss 0.0907978, acc 0.96875\n",
      "2017-11-06T01:56:09.046253: step 1721, loss 0.136691, acc 0.90625\n",
      "2017-11-06T01:56:12.994058: step 1722, loss 0.120215, acc 0.96875\n",
      "2017-11-06T01:56:16.965880: step 1723, loss 0.0595504, acc 0.96875\n",
      "2017-11-06T01:56:21.451188: step 1724, loss 0.010621, acc 1\n",
      "2017-11-06T01:56:25.408000: step 1725, loss 0.113835, acc 0.9375\n",
      "2017-11-06T01:56:29.345798: step 1726, loss 0.18567, acc 0.96875\n",
      "2017-11-06T01:56:33.395675: step 1727, loss 0.226614, acc 0.90625\n",
      "2017-11-06T01:56:35.989518: step 1728, loss 0.488021, acc 0.8\n",
      "2017-11-06T01:56:39.982355: step 1729, loss 0.0996782, acc 0.96875\n",
      "2017-11-06T01:56:43.978195: step 1730, loss 0.394462, acc 0.875\n",
      "2017-11-06T01:56:47.945013: step 1731, loss 0.307197, acc 0.875\n",
      "2017-11-06T01:56:51.950859: step 1732, loss 0.152498, acc 0.9375\n",
      "2017-11-06T01:56:55.909672: step 1733, loss 0.0320133, acc 1\n",
      "2017-11-06T01:56:59.859478: step 1734, loss 0.0828163, acc 0.96875\n",
      "2017-11-06T01:57:03.843104: step 1735, loss 0.0222773, acc 1\n",
      "2017-11-06T01:57:07.765892: step 1736, loss 0.140627, acc 0.9375\n",
      "2017-11-06T01:57:11.768736: step 1737, loss 0.0502373, acc 0.96875\n",
      "2017-11-06T01:57:15.717542: step 1738, loss 0.105652, acc 0.9375\n",
      "2017-11-06T01:57:19.674353: step 1739, loss 0.25882, acc 0.9375\n",
      "2017-11-06T01:57:23.778269: step 1740, loss 0.158518, acc 0.96875\n",
      "2017-11-06T01:57:28.147375: step 1741, loss 0.248581, acc 0.90625\n",
      "2017-11-06T01:57:32.070162: step 1742, loss 0.113509, acc 0.96875\n",
      "2017-11-06T01:57:36.030976: step 1743, loss 0.237039, acc 0.875\n",
      "2017-11-06T01:57:39.950760: step 1744, loss 0.20992, acc 0.9375\n",
      "2017-11-06T01:57:43.895564: step 1745, loss 0.146692, acc 0.96875\n",
      "2017-11-06T01:57:47.752305: step 1746, loss 0.0464609, acc 1\n",
      "2017-11-06T01:57:51.724126: step 1747, loss 0.0591859, acc 0.96875\n",
      "2017-11-06T01:57:55.689944: step 1748, loss 0.0590076, acc 0.96875\n",
      "2017-11-06T01:57:59.611731: step 1749, loss 0.389558, acc 0.90625\n",
      "2017-11-06T01:58:03.562537: step 1750, loss 0.158455, acc 0.90625\n",
      "2017-11-06T01:58:07.501337: step 1751, loss 0.347423, acc 0.875\n",
      "2017-11-06T01:58:11.413116: step 1752, loss 0.281704, acc 0.875\n",
      "2017-11-06T01:58:15.358919: step 1753, loss 0.135759, acc 0.96875\n",
      "2017-11-06T01:58:19.300720: step 1754, loss 0.110608, acc 0.9375\n",
      "2017-11-06T01:58:23.360649: step 1755, loss 0.176019, acc 0.90625\n",
      "2017-11-06T01:58:27.480575: step 1756, loss 0.172914, acc 0.90625\n",
      "2017-11-06T01:58:31.763618: step 1757, loss 0.192461, acc 0.96875\n",
      "2017-11-06T01:58:36.137726: step 1758, loss 0.0639752, acc 0.96875\n",
      "2017-11-06T01:58:40.136568: step 1759, loss 0.228461, acc 0.875\n",
      "2017-11-06T01:58:44.131406: step 1760, loss 0.17451, acc 0.9375\n",
      "2017-11-06T01:58:48.118238: step 1761, loss 0.192211, acc 0.90625\n",
      "2017-11-06T01:58:52.047030: step 1762, loss 0.0142424, acc 1\n",
      "2017-11-06T01:58:55.964815: step 1763, loss 0.0160457, acc 1\n",
      "2017-11-06T01:58:58.475598: step 1764, loss 0.0734665, acc 0.95\n",
      "2017-11-06T01:59:02.477442: step 1765, loss 0.0551136, acc 0.96875\n",
      "2017-11-06T01:59:06.464274: step 1766, loss 0.230897, acc 0.90625\n",
      "2017-11-06T01:59:10.413080: step 1767, loss 0.0237429, acc 1\n",
      "2017-11-06T01:59:14.362906: step 1768, loss 0.0625296, acc 0.96875\n",
      "2017-11-06T01:59:18.273665: step 1769, loss 0.264827, acc 0.90625\n",
      "2017-11-06T01:59:22.260498: step 1770, loss 0.0890973, acc 0.9375\n",
      "2017-11-06T01:59:26.175280: step 1771, loss 0.105237, acc 0.96875\n",
      "2017-11-06T01:59:30.104072: step 1772, loss 0.0672851, acc 0.96875\n",
      "2017-11-06T01:59:34.141942: step 1773, loss 0.0647097, acc 0.96875\n",
      "2017-11-06T01:59:38.510047: step 1774, loss 0.254765, acc 0.9375\n",
      "2017-11-06T01:59:42.513889: step 1775, loss 0.0608156, acc 0.96875\n",
      "2017-11-06T01:59:46.418664: step 1776, loss 0.326037, acc 0.90625\n",
      "2017-11-06T01:59:50.319435: step 1777, loss 0.0249702, acc 1\n",
      "2017-11-06T01:59:54.259235: step 1778, loss 0.151064, acc 0.96875\n",
      "2017-11-06T01:59:58.203036: step 1779, loss 0.0337493, acc 1\n",
      "2017-11-06T02:00:02.484081: step 1780, loss 0.0536875, acc 0.96875\n",
      "2017-11-06T02:00:06.522726: step 1781, loss 0.0184489, acc 1\n",
      "2017-11-06T02:00:10.481539: step 1782, loss 0.159572, acc 0.9375\n",
      "2017-11-06T02:00:14.432346: step 1783, loss 0.344455, acc 0.8125\n",
      "2017-11-06T02:00:18.349128: step 1784, loss 0.0967353, acc 0.96875\n",
      "2017-11-06T02:00:22.278921: step 1785, loss 0.0481973, acc 0.96875\n",
      "2017-11-06T02:00:26.428585: step 1786, loss 0.109093, acc 0.9375\n",
      "2017-11-06T02:00:30.424423: step 1787, loss 0.0982658, acc 0.96875\n",
      "2017-11-06T02:00:34.537346: step 1788, loss 0.120315, acc 0.9375\n",
      "2017-11-06T02:00:38.519176: step 1789, loss 0.0107214, acc 1\n",
      "2017-11-06T02:00:42.898287: step 1790, loss 0.252575, acc 0.90625\n",
      "2017-11-06T02:00:47.034225: step 1791, loss 0.0121066, acc 1\n",
      "2017-11-06T02:00:50.943005: step 1792, loss 0.19809, acc 0.9375\n",
      "2017-11-06T02:00:54.898814: step 1793, loss 0.0433262, acc 0.96875\n",
      "2017-11-06T02:00:58.861629: step 1794, loss 0.1516, acc 0.9375\n",
      "2017-11-06T02:01:02.820442: step 1795, loss 0.0976667, acc 0.90625\n",
      "2017-11-06T02:01:06.794267: step 1796, loss 0.239846, acc 0.9375\n",
      "2017-11-06T02:01:10.824130: step 1797, loss 0.152322, acc 0.9375\n",
      "2017-11-06T02:01:14.860999: step 1798, loss 0.406123, acc 0.875\n",
      "2017-11-06T02:01:18.830819: step 1799, loss 0.257071, acc 0.90625\n",
      "2017-11-06T02:01:21.409651: step 1800, loss 0.216352, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:01:24.036518: step 1800, loss 0.750233, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-06T02:01:29.789266: step 1801, loss 0.112265, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:01:33.740074: step 1802, loss 0.057169, acc 0.96875\n",
      "2017-11-06T02:01:37.861001: step 1803, loss 0.025544, acc 0.96875\n",
      "2017-11-06T02:01:41.952909: step 1804, loss 0.203779, acc 0.90625\n",
      "2017-11-06T02:01:46.039813: step 1805, loss 0.155135, acc 0.9375\n",
      "2017-11-06T02:01:50.424929: step 1806, loss 0.19583, acc 0.84375\n",
      "2017-11-06T02:01:54.351720: step 1807, loss 0.106403, acc 0.9375\n",
      "2017-11-06T02:01:58.325545: step 1808, loss 0.00804893, acc 1\n",
      "2017-11-06T02:02:02.318380: step 1809, loss 0.219201, acc 0.90625\n",
      "2017-11-06T02:02:06.375262: step 1810, loss 0.098105, acc 0.9375\n",
      "2017-11-06T02:02:10.444153: step 1811, loss 0.0998444, acc 0.9375\n",
      "2017-11-06T02:02:14.470014: step 1812, loss 0.0512295, acc 0.96875\n",
      "2017-11-06T02:02:18.455846: step 1813, loss 0.0767139, acc 1\n",
      "2017-11-06T02:02:22.490713: step 1814, loss 0.0258502, acc 1\n",
      "2017-11-06T02:02:26.632657: step 1815, loss 0.0679474, acc 0.96875\n",
      "2017-11-06T02:02:30.558445: step 1816, loss 0.103241, acc 0.96875\n",
      "2017-11-06T02:02:34.716399: step 1817, loss 0.0498188, acc 0.96875\n",
      "2017-11-06T02:02:38.722246: step 1818, loss 0.0600257, acc 0.96875\n",
      "2017-11-06T02:02:42.648036: step 1819, loss 0.0190758, acc 1\n",
      "2017-11-06T02:02:46.647878: step 1820, loss 0.182782, acc 0.9375\n",
      "2017-11-06T02:02:50.630707: step 1821, loss 0.392833, acc 0.90625\n",
      "2017-11-06T02:02:55.042842: step 1822, loss 0.280975, acc 0.84375\n",
      "2017-11-06T02:02:59.048689: step 1823, loss 0.0683976, acc 0.96875\n",
      "2017-11-06T02:03:03.019510: step 1824, loss 0.0878834, acc 0.96875\n",
      "2017-11-06T02:03:07.006098: step 1825, loss 0.207602, acc 0.90625\n",
      "2017-11-06T02:03:10.960911: step 1826, loss 0.461143, acc 0.84375\n",
      "2017-11-06T02:03:14.953746: step 1827, loss 0.0790322, acc 0.96875\n",
      "2017-11-06T02:03:18.849514: step 1828, loss 0.25443, acc 0.9375\n",
      "2017-11-06T02:03:22.990456: step 1829, loss 0.102096, acc 0.9375\n",
      "2017-11-06T02:03:27.263492: step 1830, loss 0.15994, acc 0.9375\n",
      "2017-11-06T02:03:31.223306: step 1831, loss 0.0813447, acc 0.96875\n",
      "2017-11-06T02:03:35.177115: step 1832, loss 0.0753751, acc 0.96875\n",
      "2017-11-06T02:03:39.169953: step 1833, loss 0.386827, acc 0.84375\n",
      "2017-11-06T02:03:43.280873: step 1834, loss 0.0532902, acc 0.96875\n",
      "2017-11-06T02:03:47.400800: step 1835, loss 0.0840255, acc 0.96875\n",
      "2017-11-06T02:03:50.059690: step 1836, loss 0.200369, acc 0.9\n",
      "2017-11-06T02:03:54.101561: step 1837, loss 0.0758392, acc 0.9375\n",
      "2017-11-06T02:03:58.422632: step 1838, loss 0.130857, acc 0.9375\n",
      "2017-11-06T02:04:02.865789: step 1839, loss 0.377596, acc 0.84375\n",
      "2017-11-06T02:04:06.981713: step 1840, loss 0.127368, acc 0.90625\n",
      "2017-11-06T02:04:10.990562: step 1841, loss 0.0943922, acc 0.9375\n",
      "2017-11-06T02:04:15.105486: step 1842, loss 0.135812, acc 0.9375\n",
      "2017-11-06T02:04:19.086314: step 1843, loss 0.11814, acc 0.9375\n",
      "2017-11-06T02:04:23.268286: step 1844, loss 0.180703, acc 0.9375\n",
      "2017-11-06T02:04:27.548328: step 1845, loss 0.116097, acc 0.96875\n",
      "2017-11-06T02:04:31.555174: step 1846, loss 0.0690606, acc 0.96875\n",
      "2017-11-06T02:04:35.819204: step 1847, loss 0.23459, acc 0.90625\n",
      "2017-11-06T02:04:39.967151: step 1848, loss 0.0573231, acc 0.96875\n",
      "2017-11-06T02:04:44.046049: step 1849, loss 0.279017, acc 0.90625\n",
      "2017-11-06T02:04:48.168979: step 1850, loss 0.0907772, acc 0.96875\n",
      "2017-11-06T02:04:52.270893: step 1851, loss 0.0233813, acc 1\n",
      "2017-11-06T02:04:56.400828: step 1852, loss 0.178904, acc 0.9375\n",
      "2017-11-06T02:05:00.510750: step 1853, loss 0.0560913, acc 0.96875\n",
      "2017-11-06T02:05:04.915878: step 1854, loss 0.0198507, acc 1\n",
      "2017-11-06T02:05:09.072833: step 1855, loss 0.0269343, acc 1\n",
      "2017-11-06T02:05:13.014632: step 1856, loss 0.122048, acc 0.9375\n",
      "2017-11-06T02:05:17.022481: step 1857, loss 0.259839, acc 0.875\n",
      "2017-11-06T02:05:20.968284: step 1858, loss 0.160641, acc 0.96875\n",
      "2017-11-06T02:05:25.008154: step 1859, loss 0.335661, acc 0.875\n",
      "2017-11-06T02:05:28.924937: step 1860, loss 0.187144, acc 0.875\n",
      "2017-11-06T02:05:32.885752: step 1861, loss 0.0392321, acc 0.96875\n",
      "2017-11-06T02:05:36.930628: step 1862, loss 0.152911, acc 0.9375\n",
      "2017-11-06T02:05:40.897445: step 1863, loss 0.137301, acc 0.90625\n",
      "2017-11-06T02:05:44.917301: step 1864, loss 0.271698, acc 0.90625\n",
      "2017-11-06T02:05:48.840088: step 1865, loss 0.158655, acc 0.9375\n",
      "2017-11-06T02:05:52.812912: step 1866, loss 0.224938, acc 0.90625\n",
      "2017-11-06T02:05:56.733697: step 1867, loss 0.0205686, acc 1\n",
      "2017-11-06T02:06:00.707521: step 1868, loss 0.258924, acc 0.90625\n",
      "2017-11-06T02:06:04.673246: step 1869, loss 0.170264, acc 0.90625\n",
      "2017-11-06T02:06:08.803180: step 1870, loss 0.346172, acc 0.875\n",
      "2017-11-06T02:06:13.180291: step 1871, loss 0.335722, acc 0.9375\n",
      "2017-11-06T02:06:15.641039: step 1872, loss 0.203395, acc 0.95\n",
      "2017-11-06T02:06:19.622868: step 1873, loss 0.0672084, acc 0.96875\n",
      "2017-11-06T02:06:23.599693: step 1874, loss 0.179239, acc 0.90625\n",
      "2017-11-06T02:06:27.582524: step 1875, loss 0.0807616, acc 0.96875\n",
      "2017-11-06T02:06:31.517319: step 1876, loss 0.201291, acc 0.875\n",
      "2017-11-06T02:06:35.647253: step 1877, loss 0.219402, acc 0.9375\n",
      "2017-11-06T02:06:39.623079: step 1878, loss 0.0811209, acc 0.96875\n",
      "2017-11-06T02:06:43.651942: step 1879, loss 0.101682, acc 0.96875\n",
      "2017-11-06T02:06:47.646780: step 1880, loss 0.144149, acc 0.96875\n",
      "2017-11-06T02:06:51.620605: step 1881, loss 0.26348, acc 0.875\n",
      "2017-11-06T02:06:55.612440: step 1882, loss 0.17179, acc 0.9375\n",
      "2017-11-06T02:06:59.564248: step 1883, loss 0.14235, acc 0.9375\n",
      "2017-11-06T02:07:03.566092: step 1884, loss 0.170868, acc 0.9375\n",
      "2017-11-06T02:07:07.617970: step 1885, loss 0.266556, acc 0.9375\n",
      "2017-11-06T02:07:11.643831: step 1886, loss 0.116027, acc 0.9375\n",
      "2017-11-06T02:07:16.044958: step 1887, loss 0.0165155, acc 1\n",
      "2017-11-06T02:07:20.213923: step 1888, loss 0.181205, acc 0.9375\n",
      "2017-11-06T02:07:24.191747: step 1889, loss 0.0500671, acc 0.96875\n",
      "2017-11-06T02:07:28.199596: step 1890, loss 0.0841468, acc 0.96875\n",
      "2017-11-06T02:07:32.258479: step 1891, loss 0.23836, acc 0.9375\n",
      "2017-11-06T02:07:36.263324: step 1892, loss 0.213464, acc 0.875\n",
      "2017-11-06T02:07:40.266169: step 1893, loss 0.0318858, acc 0.96875\n",
      "2017-11-06T02:07:44.224981: step 1894, loss 0.128572, acc 0.90625\n",
      "2017-11-06T02:07:48.165783: step 1895, loss 0.166406, acc 0.9375\n",
      "2017-11-06T02:07:52.150613: step 1896, loss 0.118483, acc 0.9375\n",
      "2017-11-06T02:07:56.187482: step 1897, loss 0.127516, acc 0.96875\n",
      "2017-11-06T02:08:00.126279: step 1898, loss 0.349772, acc 0.9375\n",
      "2017-11-06T02:08:04.130125: step 1899, loss 0.24984, acc 0.9375\n",
      "2017-11-06T02:08:08.054915: step 1900, loss 0.136985, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:08:10.650757: step 1900, loss 0.51126, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-06T02:08:15.848863: step 1901, loss 0.225343, acc 0.90625\n",
      "2017-11-06T02:08:20.043844: step 1902, loss 0.185156, acc 0.90625\n",
      "2017-11-06T02:08:24.449975: step 1903, loss 0.271256, acc 0.90625\n",
      "2017-11-06T02:08:28.792061: step 1904, loss 0.181116, acc 0.875\n",
      "2017-11-06T02:08:32.877964: step 1905, loss 0.154288, acc 0.96875\n",
      "2017-11-06T02:08:37.254072: step 1906, loss 0.160417, acc 0.9375\n",
      "2017-11-06T02:08:41.296945: step 1907, loss 0.0478484, acc 0.96875\n",
      "2017-11-06T02:08:43.857765: step 1908, loss 0.0129207, acc 1\n",
      "2017-11-06T02:08:47.910646: step 1909, loss 0.0826571, acc 0.96875\n",
      "2017-11-06T02:08:51.895476: step 1910, loss 0.0709761, acc 0.9375\n",
      "2017-11-06T02:08:55.919335: step 1911, loss 0.206321, acc 0.90625\n",
      "2017-11-06T02:08:59.874145: step 1912, loss 0.0768552, acc 0.9375\n",
      "2017-11-06T02:09:03.824952: step 1913, loss 0.0929175, acc 0.9375\n",
      "2017-11-06T02:09:07.792533: step 1914, loss 0.161786, acc 0.96875\n",
      "2017-11-06T02:09:11.759354: step 1915, loss 0.263517, acc 0.90625\n",
      "2017-11-06T02:09:15.725170: step 1916, loss 0.236405, acc 0.90625\n",
      "2017-11-06T02:09:19.673976: step 1917, loss 0.189638, acc 0.9375\n",
      "2017-11-06T02:09:23.688828: step 1918, loss 0.0323735, acc 1\n",
      "2017-11-06T02:09:28.164008: step 1919, loss 0.0223328, acc 1\n",
      "2017-11-06T02:09:32.239904: step 1920, loss 0.208129, acc 0.875\n",
      "2017-11-06T02:09:36.195716: step 1921, loss 0.238115, acc 0.90625\n",
      "2017-11-06T02:09:40.241591: step 1922, loss 0.215465, acc 0.9375\n",
      "2017-11-06T02:09:44.198401: step 1923, loss 0.181138, acc 0.9375\n",
      "2017-11-06T02:09:48.205250: step 1924, loss 0.170998, acc 0.875\n",
      "2017-11-06T02:09:52.233110: step 1925, loss 0.0281183, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:09:56.202931: step 1926, loss 0.214714, acc 0.90625\n",
      "2017-11-06T02:10:00.251808: step 1927, loss 0.200194, acc 0.9375\n",
      "2017-11-06T02:10:04.446789: step 1928, loss 0.0116674, acc 1\n",
      "2017-11-06T02:10:08.385587: step 1929, loss 0.175717, acc 0.9375\n",
      "2017-11-06T02:10:12.326387: step 1930, loss 0.167276, acc 0.90625\n",
      "2017-11-06T02:10:16.290204: step 1931, loss 0.266919, acc 0.875\n",
      "2017-11-06T02:10:20.325071: step 1932, loss 0.447853, acc 0.875\n",
      "2017-11-06T02:10:24.400967: step 1933, loss 0.193046, acc 0.90625\n",
      "2017-11-06T02:10:28.390803: step 1934, loss 0.125238, acc 0.96875\n",
      "2017-11-06T02:10:32.707870: step 1935, loss 0.0174956, acc 1\n",
      "2017-11-06T02:10:37.091984: step 1936, loss 0.233069, acc 0.9375\n",
      "2017-11-06T02:10:41.154872: step 1937, loss 0.272525, acc 0.875\n",
      "2017-11-06T02:10:45.083664: step 1938, loss 0.120387, acc 0.9375\n",
      "2017-11-06T02:10:49.096514: step 1939, loss 0.0808675, acc 0.96875\n",
      "2017-11-06T02:10:53.118372: step 1940, loss 0.200904, acc 0.90625\n",
      "2017-11-06T02:10:57.032153: step 1941, loss 0.272125, acc 0.90625\n",
      "2017-11-06T02:11:01.004975: step 1942, loss 0.123241, acc 0.9375\n",
      "2017-11-06T02:11:04.977798: step 1943, loss 0.0610544, acc 0.96875\n",
      "2017-11-06T02:11:07.570641: step 1944, loss 0.11009, acc 0.95\n",
      "2017-11-06T02:11:11.574486: step 1945, loss 0.174127, acc 0.90625\n",
      "2017-11-06T02:11:15.570325: step 1946, loss 0.112432, acc 0.90625\n",
      "2017-11-06T02:11:19.563162: step 1947, loss 0.0119762, acc 1\n",
      "2017-11-06T02:11:23.536985: step 1948, loss 0.0886598, acc 0.96875\n",
      "2017-11-06T02:11:27.488794: step 1949, loss 0.148749, acc 0.90625\n",
      "2017-11-06T02:11:31.473626: step 1950, loss 0.291127, acc 0.9375\n",
      "2017-11-06T02:11:35.534510: step 1951, loss 0.300628, acc 0.90625\n",
      "2017-11-06T02:11:39.964658: step 1952, loss 0.029004, acc 1\n",
      "2017-11-06T02:11:44.117609: step 1953, loss 0.350912, acc 0.875\n",
      "2017-11-06T02:11:48.176493: step 1954, loss 0.142175, acc 0.9375\n",
      "2017-11-06T02:11:52.169349: step 1955, loss 0.194739, acc 0.90625\n",
      "2017-11-06T02:11:56.174175: step 1956, loss 0.316836, acc 0.8125\n",
      "2017-11-06T02:12:00.094961: step 1957, loss 0.110993, acc 0.96875\n",
      "2017-11-06T02:12:04.074789: step 1958, loss 0.0730702, acc 0.96875\n",
      "2017-11-06T02:12:08.074403: step 1959, loss 0.0420287, acc 0.96875\n",
      "2017-11-06T02:12:12.070242: step 1960, loss 0.10351, acc 0.96875\n",
      "2017-11-06T02:12:16.052072: step 1961, loss 0.308978, acc 0.875\n",
      "2017-11-06T02:12:20.013886: step 1962, loss 0.299808, acc 0.90625\n",
      "2017-11-06T02:12:24.084779: step 1963, loss 0.238109, acc 0.875\n",
      "2017-11-06T02:12:28.178688: step 1964, loss 0.0749993, acc 1\n",
      "2017-11-06T02:12:32.157515: step 1965, loss 0.090798, acc 0.9375\n",
      "2017-11-06T02:12:36.292454: step 1966, loss 0.214791, acc 0.9375\n",
      "2017-11-06T02:12:40.291294: step 1967, loss 0.219742, acc 0.875\n",
      "2017-11-06T02:12:44.704430: step 1968, loss 0.0798613, acc 0.96875\n",
      "2017-11-06T02:12:48.902413: step 1969, loss 0.12734, acc 0.9375\n",
      "2017-11-06T02:12:52.872234: step 1970, loss 0.00776987, acc 1\n",
      "2017-11-06T02:12:56.846057: step 1971, loss 0.0555954, acc 0.96875\n",
      "2017-11-06T02:13:00.815878: step 1972, loss 0.00270851, acc 1\n",
      "2017-11-06T02:13:04.776692: step 1973, loss 0.116795, acc 0.9375\n",
      "2017-11-06T02:13:08.785541: step 1974, loss 0.0100249, acc 1\n",
      "2017-11-06T02:13:12.803396: step 1975, loss 0.0800145, acc 0.96875\n",
      "2017-11-06T02:13:16.808241: step 1976, loss 0.114834, acc 0.96875\n",
      "2017-11-06T02:13:20.797076: step 1977, loss 0.141526, acc 0.96875\n",
      "2017-11-06T02:13:24.981048: step 1978, loss 0.186496, acc 0.90625\n",
      "2017-11-06T02:13:29.211054: step 1979, loss 0.350052, acc 0.90625\n",
      "2017-11-06T02:13:31.778878: step 1980, loss 0.0738688, acc 0.95\n",
      "2017-11-06T02:13:35.896805: step 1981, loss 0.145132, acc 0.96875\n",
      "2017-11-06T02:13:39.925667: step 1982, loss 0.0113524, acc 1\n",
      "2017-11-06T02:13:43.927510: step 1983, loss 0.0765765, acc 0.96875\n",
      "2017-11-06T02:13:48.176529: step 1984, loss 0.270298, acc 0.90625\n",
      "2017-11-06T02:13:52.567649: step 1985, loss 0.128425, acc 0.96875\n",
      "2017-11-06T02:13:56.570494: step 1986, loss 0.0421163, acc 1\n",
      "2017-11-06T02:14:00.561329: step 1987, loss 0.308724, acc 0.875\n",
      "2017-11-06T02:14:04.578183: step 1988, loss 0.320939, acc 0.875\n",
      "2017-11-06T02:14:08.584030: step 1989, loss 0.474152, acc 0.875\n",
      "2017-11-06T02:14:12.559856: step 1990, loss 0.125909, acc 0.90625\n",
      "2017-11-06T02:14:16.567702: step 1991, loss 0.13352, acc 0.9375\n",
      "2017-11-06T02:14:20.507502: step 1992, loss 0.0885582, acc 0.9375\n",
      "2017-11-06T02:14:24.546372: step 1993, loss 0.069245, acc 0.96875\n",
      "2017-11-06T02:14:28.676307: step 1994, loss 0.101225, acc 0.96875\n",
      "2017-11-06T02:14:32.750201: step 1995, loss 0.097041, acc 0.96875\n",
      "2017-11-06T02:14:36.798077: step 1996, loss 0.178489, acc 0.90625\n",
      "2017-11-06T02:14:40.801923: step 1997, loss 0.207213, acc 0.90625\n",
      "2017-11-06T02:14:44.760734: step 1998, loss 0.277222, acc 0.9375\n",
      "2017-11-06T02:14:48.793600: step 1999, loss 0.117053, acc 0.9375\n",
      "2017-11-06T02:14:52.930540: step 2000, loss 0.0731997, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:14:55.833602: step 2000, loss 0.638254, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-06T02:15:01.338281: step 2001, loss 0.166147, acc 0.9375\n",
      "2017-11-06T02:15:05.333119: step 2002, loss 0.0591196, acc 0.96875\n",
      "2017-11-06T02:15:09.438799: step 2003, loss 0.202225, acc 0.9375\n",
      "2017-11-06T02:15:13.378599: step 2004, loss 0.0509746, acc 0.96875\n",
      "2017-11-06T02:15:17.363430: step 2005, loss 0.128451, acc 0.96875\n",
      "2017-11-06T02:15:21.330248: step 2006, loss 0.118866, acc 0.9375\n",
      "2017-11-06T02:15:25.333093: step 2007, loss 0.382746, acc 0.90625\n",
      "2017-11-06T02:15:29.358955: step 2008, loss 0.526577, acc 0.84375\n",
      "2017-11-06T02:15:33.330777: step 2009, loss 0.20601, acc 0.90625\n",
      "2017-11-06T02:15:37.289589: step 2010, loss 0.27634, acc 0.9375\n",
      "2017-11-06T02:15:41.284429: step 2011, loss 0.124868, acc 0.9375\n",
      "2017-11-06T02:15:45.294277: step 2012, loss 0.150825, acc 0.9375\n",
      "2017-11-06T02:15:49.321137: step 2013, loss 0.182438, acc 0.90625\n",
      "2017-11-06T02:15:53.318978: step 2014, loss 0.31652, acc 0.875\n",
      "2017-11-06T02:15:57.265782: step 2015, loss 0.0797509, acc 0.96875\n",
      "2017-11-06T02:16:00.119811: step 2016, loss 0.0432352, acc 1\n",
      "2017-11-06T02:16:04.374834: step 2017, loss 0.0444909, acc 0.96875\n",
      "2017-11-06T02:16:08.513776: step 2018, loss 0.0480675, acc 1\n",
      "2017-11-06T02:16:12.530631: step 2019, loss 0.0892951, acc 0.96875\n",
      "2017-11-06T02:16:16.631542: step 2020, loss 0.19307, acc 0.96875\n",
      "2017-11-06T02:16:20.608368: step 2021, loss 0.327455, acc 0.84375\n",
      "2017-11-06T02:16:24.688267: step 2022, loss 0.235324, acc 0.9375\n",
      "2017-11-06T02:16:28.754156: step 2023, loss 0.0688518, acc 0.9375\n",
      "2017-11-06T02:16:32.709967: step 2024, loss 0.0896338, acc 0.96875\n",
      "2017-11-06T02:16:36.803876: step 2025, loss 0.0272773, acc 1\n",
      "2017-11-06T02:16:40.818375: step 2026, loss 0.260635, acc 0.90625\n",
      "2017-11-06T02:16:44.799202: step 2027, loss 0.191827, acc 0.90625\n",
      "2017-11-06T02:16:48.759016: step 2028, loss 0.222414, acc 0.9375\n",
      "2017-11-06T02:16:52.699816: step 2029, loss 0.236704, acc 0.9375\n",
      "2017-11-06T02:16:56.620604: step 2030, loss 0.150003, acc 0.9375\n",
      "2017-11-06T02:17:00.571410: step 2031, loss 0.11134, acc 0.96875\n",
      "2017-11-06T02:17:04.629293: step 2032, loss 0.0291963, acc 1\n",
      "2017-11-06T02:17:08.901329: step 2033, loss 0.184635, acc 0.90625\n",
      "2017-11-06T02:17:12.856138: step 2034, loss 0.14818, acc 0.96875\n",
      "2017-11-06T02:17:16.849976: step 2035, loss 0.0420514, acc 0.96875\n",
      "2017-11-06T02:17:20.806788: step 2036, loss 0.037869, acc 0.96875\n",
      "2017-11-06T02:17:24.837651: step 2037, loss 0.29558, acc 0.8125\n",
      "2017-11-06T02:17:28.791461: step 2038, loss 0.0236922, acc 1\n",
      "2017-11-06T02:17:32.821326: step 2039, loss 0.0324256, acc 1\n",
      "2017-11-06T02:17:36.796148: step 2040, loss 0.179812, acc 0.96875\n",
      "2017-11-06T02:17:40.770973: step 2041, loss 0.193391, acc 0.9375\n",
      "2017-11-06T02:17:44.753804: step 2042, loss 0.178292, acc 0.9375\n",
      "2017-11-06T02:17:48.736633: step 2043, loss 0.11771, acc 0.96875\n",
      "2017-11-06T02:17:52.750485: step 2044, loss 0.217728, acc 0.90625\n",
      "2017-11-06T02:17:56.679277: step 2045, loss 0.130866, acc 0.9375\n",
      "2017-11-06T02:18:00.618077: step 2046, loss 0.0750366, acc 0.96875\n",
      "2017-11-06T02:18:04.583912: step 2047, loss 0.138456, acc 0.875\n",
      "2017-11-06T02:18:08.583504: step 2048, loss 0.389939, acc 0.84375\n",
      "2017-11-06T02:18:12.975624: step 2049, loss 0.255616, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:18:16.992478: step 2050, loss 0.180453, acc 0.90625\n",
      "2017-11-06T02:18:20.920270: step 2051, loss 0.0664058, acc 0.9375\n",
      "2017-11-06T02:18:23.669223: step 2052, loss 0.337444, acc 0.85\n",
      "2017-11-06T02:18:27.722103: step 2053, loss 0.0475782, acc 0.96875\n",
      "2017-11-06T02:18:31.670908: step 2054, loss 0.0275946, acc 1\n",
      "2017-11-06T02:18:35.826863: step 2055, loss 0.144404, acc 0.9375\n",
      "2017-11-06T02:18:39.745645: step 2056, loss 0.316583, acc 0.84375\n",
      "2017-11-06T02:18:43.726474: step 2057, loss 0.263945, acc 0.875\n",
      "2017-11-06T02:18:47.673279: step 2058, loss 0.0612357, acc 0.96875\n",
      "2017-11-06T02:18:51.608074: step 2059, loss 0.26205, acc 0.84375\n",
      "2017-11-06T02:18:55.606916: step 2060, loss 0.170894, acc 0.90625\n",
      "2017-11-06T02:18:59.553720: step 2061, loss 0.18036, acc 0.9375\n",
      "2017-11-06T02:19:03.508532: step 2062, loss 0.099532, acc 0.96875\n",
      "2017-11-06T02:19:07.413305: step 2063, loss 0.0779634, acc 0.96875\n",
      "2017-11-06T02:19:11.399137: step 2064, loss 0.0296897, acc 1\n",
      "2017-11-06T02:19:15.462024: step 2065, loss 0.163556, acc 0.90625\n",
      "2017-11-06T02:19:19.829127: step 2066, loss 0.131015, acc 0.9375\n",
      "2017-11-06T02:19:23.769928: step 2067, loss 0.120386, acc 0.96875\n",
      "2017-11-06T02:19:27.748754: step 2068, loss 0.116249, acc 0.9375\n",
      "2017-11-06T02:19:31.688553: step 2069, loss 0.160676, acc 0.90625\n",
      "2017-11-06T02:19:35.623349: step 2070, loss 0.190688, acc 0.9375\n",
      "2017-11-06T02:19:39.570153: step 2071, loss 0.0165623, acc 1\n",
      "2017-11-06T02:19:43.518959: step 2072, loss 0.0401825, acc 0.96875\n",
      "2017-11-06T02:19:47.405721: step 2073, loss 0.37335, acc 0.84375\n",
      "2017-11-06T02:19:51.379546: step 2074, loss 0.105101, acc 0.9375\n",
      "2017-11-06T02:19:55.359373: step 2075, loss 0.0633081, acc 0.96875\n",
      "2017-11-06T02:19:59.300173: step 2076, loss 0.0239129, acc 1\n",
      "2017-11-06T02:20:03.549192: step 2077, loss 0.209912, acc 0.90625\n",
      "2017-11-06T02:20:07.440957: step 2078, loss 0.163896, acc 0.96875\n",
      "2017-11-06T02:20:11.404773: step 2079, loss 0.120023, acc 0.90625\n",
      "2017-11-06T02:20:15.373593: step 2080, loss 0.378812, acc 0.84375\n",
      "2017-11-06T02:20:19.280371: step 2081, loss 0.0552679, acc 0.96875\n",
      "2017-11-06T02:20:23.652475: step 2082, loss 0.25379, acc 0.875\n",
      "2017-11-06T02:20:27.787415: step 2083, loss 0.124398, acc 0.9375\n",
      "2017-11-06T02:20:31.723210: step 2084, loss 0.0213508, acc 1\n",
      "2017-11-06T02:20:35.881165: step 2085, loss 0.36894, acc 0.875\n",
      "2017-11-06T02:20:39.824967: step 2086, loss 0.444602, acc 0.84375\n",
      "2017-11-06T02:20:43.774815: step 2087, loss 0.00540577, acc 1\n",
      "2017-11-06T02:20:46.266585: step 2088, loss 0.335738, acc 0.85\n",
      "2017-11-06T02:20:50.206385: step 2089, loss 0.0602636, acc 0.96875\n",
      "2017-11-06T02:20:54.136178: step 2090, loss 0.129056, acc 0.9375\n",
      "2017-11-06T02:20:58.131015: step 2091, loss 0.143669, acc 0.875\n",
      "2017-11-06T02:21:02.066813: step 2092, loss 0.263117, acc 0.84375\n",
      "2017-11-06T02:21:06.018620: step 2093, loss 0.0167347, acc 1\n",
      "2017-11-06T02:21:09.967250: step 2094, loss 0.0355958, acc 1\n",
      "2017-11-06T02:21:13.913053: step 2095, loss 0.0292307, acc 1\n",
      "2017-11-06T02:21:17.869863: step 2096, loss 0.0310189, acc 0.96875\n",
      "2017-11-06T02:21:21.800658: step 2097, loss 0.124234, acc 0.9375\n",
      "2017-11-06T02:21:25.810506: step 2098, loss 0.076028, acc 0.96875\n",
      "2017-11-06T02:21:30.237651: step 2099, loss 0.0142542, acc 1\n",
      "2017-11-06T02:21:34.257509: step 2100, loss 0.304802, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:21:36.792309: step 2100, loss 0.603728, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-06T02:21:42.044795: step 2101, loss 0.0244754, acc 1\n",
      "2017-11-06T02:21:45.964580: step 2102, loss 0.3025, acc 0.90625\n",
      "2017-11-06T02:21:49.935402: step 2103, loss 0.314244, acc 0.90625\n",
      "2017-11-06T02:21:53.865194: step 2104, loss 0.130369, acc 0.9375\n",
      "2017-11-06T02:21:57.778975: step 2105, loss 0.288608, acc 0.875\n",
      "2017-11-06T02:22:01.747797: step 2106, loss 0.0362277, acc 0.96875\n",
      "2017-11-06T02:22:05.667580: step 2107, loss 0.319569, acc 0.875\n",
      "2017-11-06T02:22:09.623390: step 2108, loss 0.229669, acc 0.9375\n",
      "2017-11-06T02:22:13.637243: step 2109, loss 0.108666, acc 0.96875\n",
      "2017-11-06T02:22:17.521002: step 2110, loss 0.133536, acc 0.9375\n",
      "2017-11-06T02:22:21.501830: step 2111, loss 0.324005, acc 0.9375\n",
      "2017-11-06T02:22:25.492666: step 2112, loss 0.204372, acc 0.84375\n",
      "2017-11-06T02:22:29.486523: step 2113, loss 0.171223, acc 0.96875\n",
      "2017-11-06T02:22:33.832592: step 2114, loss 0.65643, acc 0.875\n",
      "2017-11-06T02:22:38.094620: step 2115, loss 0.117117, acc 0.96875\n",
      "2017-11-06T02:22:41.983385: step 2116, loss 0.272402, acc 0.90625\n",
      "2017-11-06T02:22:45.936192: step 2117, loss 0.242247, acc 0.90625\n",
      "2017-11-06T02:22:49.839966: step 2118, loss 0.0757463, acc 0.96875\n",
      "2017-11-06T02:22:53.795778: step 2119, loss 0.118478, acc 0.9375\n",
      "2017-11-06T02:22:57.709558: step 2120, loss 0.0671931, acc 0.96875\n",
      "2017-11-06T02:23:01.650358: step 2121, loss 0.0204655, acc 1\n",
      "2017-11-06T02:23:05.637190: step 2122, loss 0.0820412, acc 0.9375\n",
      "2017-11-06T02:23:09.636033: step 2123, loss 0.119794, acc 0.96875\n",
      "2017-11-06T02:23:12.197853: step 2124, loss 0.281909, acc 0.85\n",
      "2017-11-06T02:23:16.153663: step 2125, loss 0.115569, acc 0.96875\n",
      "2017-11-06T02:23:20.108473: step 2126, loss 0.423331, acc 0.84375\n",
      "2017-11-06T02:23:24.188373: step 2127, loss 0.0678896, acc 0.96875\n",
      "2017-11-06T02:23:28.394362: step 2128, loss 0.127794, acc 0.9375\n",
      "2017-11-06T02:23:32.402208: step 2129, loss 0.17828, acc 0.9375\n",
      "2017-11-06T02:23:36.346010: step 2130, loss 0.056438, acc 0.96875\n",
      "2017-11-06T02:23:40.716116: step 2131, loss 0.189379, acc 0.9375\n",
      "2017-11-06T02:23:44.833041: step 2132, loss 0.183824, acc 0.875\n",
      "2017-11-06T02:23:48.764835: step 2133, loss 0.127971, acc 0.96875\n",
      "2017-11-06T02:23:52.797700: step 2134, loss 0.0660523, acc 0.96875\n",
      "2017-11-06T02:23:56.726493: step 2135, loss 0.187835, acc 0.90625\n",
      "2017-11-06T02:24:00.744349: step 2136, loss 0.067322, acc 0.96875\n",
      "2017-11-06T02:24:04.670136: step 2137, loss 0.191549, acc 0.9375\n",
      "2017-11-06T02:24:08.608730: step 2138, loss 0.0292222, acc 1\n",
      "2017-11-06T02:24:12.588555: step 2139, loss 0.198727, acc 0.90625\n",
      "2017-11-06T02:24:16.495333: step 2140, loss 0.00349724, acc 1\n",
      "2017-11-06T02:24:20.432129: step 2141, loss 0.121039, acc 0.96875\n",
      "2017-11-06T02:24:24.401949: step 2142, loss 0.152577, acc 0.90625\n",
      "2017-11-06T02:24:28.454830: step 2143, loss 0.382781, acc 0.875\n",
      "2017-11-06T02:24:32.394629: step 2144, loss 0.16895, acc 0.875\n",
      "2017-11-06T02:24:36.539574: step 2145, loss 0.0120767, acc 1\n",
      "2017-11-06T02:24:40.449352: step 2146, loss 0.0274838, acc 1\n",
      "2017-11-06T02:24:44.619315: step 2147, loss 0.0812782, acc 0.9375\n",
      "2017-11-06T02:24:48.839313: step 2148, loss 0.151239, acc 0.9375\n",
      "2017-11-06T02:24:52.820142: step 2149, loss 0.110111, acc 0.9375\n",
      "2017-11-06T02:24:56.766946: step 2150, loss 0.214215, acc 0.9375\n",
      "2017-11-06T02:25:00.764787: step 2151, loss 0.228787, acc 0.875\n",
      "2017-11-06T02:25:04.720599: step 2152, loss 0.134913, acc 0.9375\n",
      "2017-11-06T02:25:08.699425: step 2153, loss 0.0287818, acc 1\n",
      "2017-11-06T02:25:12.724284: step 2154, loss 0.163933, acc 0.96875\n",
      "2017-11-06T02:25:16.666085: step 2155, loss 0.149361, acc 0.9375\n",
      "2017-11-06T02:25:20.581867: step 2156, loss 0.0361905, acc 0.96875\n",
      "2017-11-06T02:25:24.554691: step 2157, loss 0.261044, acc 0.875\n",
      "2017-11-06T02:25:28.532517: step 2158, loss 0.11919, acc 0.96875\n",
      "2017-11-06T02:25:32.471315: step 2159, loss 0.0918859, acc 0.96875\n",
      "2017-11-06T02:25:35.049148: step 2160, loss 0.191858, acc 0.85\n",
      "2017-11-06T02:25:38.999955: step 2161, loss 0.0602389, acc 0.96875\n",
      "2017-11-06T02:25:42.938753: step 2162, loss 0.363383, acc 0.875\n",
      "2017-11-06T02:25:46.852534: step 2163, loss 0.194383, acc 0.90625\n",
      "2017-11-06T02:25:51.123569: step 2164, loss 0.0935831, acc 0.96875\n",
      "2017-11-06T02:25:55.280525: step 2165, loss 0.0735263, acc 0.96875\n",
      "2017-11-06T02:25:59.248344: step 2166, loss 0.0973211, acc 0.9375\n",
      "2017-11-06T02:26:03.326240: step 2167, loss 0.0138501, acc 1\n",
      "2017-11-06T02:26:07.357104: step 2168, loss 0.143275, acc 0.96875\n",
      "2017-11-06T02:26:11.364951: step 2169, loss 0.195864, acc 0.90625\n",
      "2017-11-06T02:26:15.375801: step 2170, loss 0.0965184, acc 0.96875\n",
      "2017-11-06T02:26:19.345623: step 2171, loss 0.226049, acc 0.9375\n",
      "2017-11-06T02:26:23.316443: step 2172, loss 0.128011, acc 0.9375\n",
      "2017-11-06T02:26:27.323292: step 2173, loss 0.191396, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:26:31.252101: step 2174, loss 0.279713, acc 0.90625\n",
      "2017-11-06T02:26:35.445062: step 2175, loss 0.0379408, acc 0.96875\n",
      "2017-11-06T02:26:39.357842: step 2176, loss 0.238887, acc 0.84375\n",
      "2017-11-06T02:26:43.311651: step 2177, loss 0.0211966, acc 1\n",
      "2017-11-06T02:26:47.242446: step 2178, loss 0.0162232, acc 1\n",
      "2017-11-06T02:26:51.205259: step 2179, loss 0.040906, acc 1\n",
      "2017-11-06T02:26:55.333193: step 2180, loss 0.0968988, acc 0.9375\n",
      "2017-11-06T02:26:59.701296: step 2181, loss 0.0711277, acc 0.96875\n",
      "2017-11-06T02:27:03.670118: step 2182, loss 0.0833103, acc 0.9375\n",
      "2017-11-06T02:27:07.624688: step 2183, loss 0.106133, acc 0.96875\n",
      "2017-11-06T02:27:11.562487: step 2184, loss 0.1622, acc 0.9375\n",
      "2017-11-06T02:27:15.583343: step 2185, loss 0.226713, acc 0.875\n",
      "2017-11-06T02:27:19.540154: step 2186, loss 0.0257065, acc 1\n",
      "2017-11-06T02:27:23.555007: step 2187, loss 0.200588, acc 0.90625\n",
      "2017-11-06T02:27:27.483799: step 2188, loss 0.209829, acc 0.875\n",
      "2017-11-06T02:27:31.433605: step 2189, loss 0.18024, acc 0.9375\n",
      "2017-11-06T02:27:35.347386: step 2190, loss 0.0368897, acc 1\n",
      "2017-11-06T02:27:39.350230: step 2191, loss 0.0240923, acc 1\n",
      "2017-11-06T02:27:43.321051: step 2192, loss 0.115539, acc 0.9375\n",
      "2017-11-06T02:27:47.366926: step 2193, loss 0.321498, acc 0.90625\n",
      "2017-11-06T02:27:51.352758: step 2194, loss 0.157682, acc 0.875\n",
      "2017-11-06T02:27:55.408640: step 2195, loss 0.24142, acc 0.9375\n",
      "2017-11-06T02:27:57.938438: step 2196, loss 0.660563, acc 0.9\n",
      "2017-11-06T02:28:02.269515: step 2197, loss 0.0486317, acc 0.96875\n",
      "2017-11-06T02:28:06.500522: step 2198, loss 0.168521, acc 0.9375\n",
      "2017-11-06T02:28:10.506368: step 2199, loss 0.0158107, acc 1\n",
      "2017-11-06T02:28:14.454173: step 2200, loss 0.0943581, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:28:17.053019: step 2200, loss 0.507015, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-06T02:28:22.417743: step 2201, loss 0.0486311, acc 1\n",
      "2017-11-06T02:28:26.672766: step 2202, loss 0.149389, acc 0.9375\n",
      "2017-11-06T02:28:30.658597: step 2203, loss 0.129891, acc 0.9375\n",
      "2017-11-06T02:28:34.837567: step 2204, loss 0.0826505, acc 0.96875\n",
      "2017-11-06T02:28:38.881440: step 2205, loss 0.0865529, acc 0.9375\n",
      "2017-11-06T02:28:42.875278: step 2206, loss 0.166241, acc 0.96875\n",
      "2017-11-06T02:28:46.791060: step 2207, loss 0.0840414, acc 0.96875\n",
      "2017-11-06T02:28:50.754877: step 2208, loss 0.15016, acc 0.9375\n",
      "2017-11-06T02:28:54.825853: step 2209, loss 0.280852, acc 0.90625\n",
      "2017-11-06T02:28:58.801678: step 2210, loss 0.15746, acc 0.90625\n",
      "2017-11-06T02:29:02.870571: step 2211, loss 0.0284066, acc 1\n",
      "2017-11-06T02:29:07.154613: step 2212, loss 0.0708879, acc 0.96875\n",
      "2017-11-06T02:29:11.398630: step 2213, loss 0.187774, acc 0.90625\n",
      "2017-11-06T02:29:15.371452: step 2214, loss 0.189465, acc 0.9375\n",
      "2017-11-06T02:29:19.349278: step 2215, loss 0.10834, acc 0.90625\n",
      "2017-11-06T02:29:23.329106: step 2216, loss 0.330322, acc 0.90625\n",
      "2017-11-06T02:29:27.330950: step 2217, loss 0.193808, acc 0.9375\n",
      "2017-11-06T02:29:31.381828: step 2218, loss 0.226604, acc 0.90625\n",
      "2017-11-06T02:29:35.333637: step 2219, loss 0.219112, acc 0.90625\n",
      "2017-11-06T02:29:39.318469: step 2220, loss 0.0311051, acc 0.96875\n",
      "2017-11-06T02:29:43.330318: step 2221, loss 0.236684, acc 0.9375\n",
      "2017-11-06T02:29:47.288130: step 2222, loss 0.124804, acc 0.96875\n",
      "2017-11-06T02:29:51.255949: step 2223, loss 0.318524, acc 0.84375\n",
      "2017-11-06T02:29:55.218765: step 2224, loss 0.0774353, acc 0.96875\n",
      "2017-11-06T02:29:59.193591: step 2225, loss 0.159007, acc 0.9375\n",
      "2017-11-06T02:30:03.484640: step 2226, loss 0.0650468, acc 0.96875\n",
      "2017-11-06T02:30:07.471229: step 2227, loss 0.224232, acc 0.90625\n",
      "2017-11-06T02:30:11.597162: step 2228, loss 0.347404, acc 0.8125\n",
      "2017-11-06T02:30:16.007296: step 2229, loss 0.233648, acc 0.875\n",
      "2017-11-06T02:30:20.008138: step 2230, loss 0.0154892, acc 1\n",
      "2017-11-06T02:30:24.010983: step 2231, loss 0.0232743, acc 1\n",
      "2017-11-06T02:30:26.602824: step 2232, loss 0.267919, acc 0.9\n",
      "2017-11-06T02:30:30.666711: step 2233, loss 0.034374, acc 0.96875\n",
      "2017-11-06T02:30:34.830670: step 2234, loss 0.0707705, acc 0.96875\n",
      "2017-11-06T02:30:38.891556: step 2235, loss 0.0123116, acc 1\n",
      "2017-11-06T02:30:42.853370: step 2236, loss 0.0685312, acc 0.96875\n",
      "2017-11-06T02:30:46.843205: step 2237, loss 0.0165246, acc 1\n",
      "2017-11-06T02:30:50.835042: step 2238, loss 0.165049, acc 0.9375\n",
      "2017-11-06T02:30:54.884919: step 2239, loss 0.153204, acc 0.9375\n",
      "2017-11-06T02:30:58.894768: step 2240, loss 0.0664785, acc 0.96875\n",
      "2017-11-06T02:31:02.834568: step 2241, loss 0.157799, acc 0.90625\n",
      "2017-11-06T02:31:06.843417: step 2242, loss 0.260542, acc 0.90625\n",
      "2017-11-06T02:31:10.915309: step 2243, loss 0.0736191, acc 0.9375\n",
      "2017-11-06T02:31:14.975194: step 2244, loss 0.116444, acc 0.9375\n",
      "2017-11-06T02:31:19.282255: step 2245, loss 0.0844766, acc 0.96875\n",
      "2017-11-06T02:31:23.495248: step 2246, loss 0.0281119, acc 1\n",
      "2017-11-06T02:31:27.441052: step 2247, loss 0.294717, acc 0.9375\n",
      "2017-11-06T02:31:31.451903: step 2248, loss 0.0410091, acc 1\n",
      "2017-11-06T02:31:35.568827: step 2249, loss 0.0340264, acc 1\n",
      "2017-11-06T02:31:39.727782: step 2250, loss 0.194099, acc 0.9375\n",
      "2017-11-06T02:31:43.732628: step 2251, loss 0.167244, acc 0.9375\n",
      "2017-11-06T02:31:47.730468: step 2252, loss 0.21832, acc 0.875\n",
      "2017-11-06T02:31:51.740319: step 2253, loss 0.197236, acc 0.875\n",
      "2017-11-06T02:31:55.694127: step 2254, loss 0.133619, acc 0.90625\n",
      "2017-11-06T02:31:59.684962: step 2255, loss 0.0423762, acc 0.96875\n",
      "2017-11-06T02:32:03.717828: step 2256, loss 0.310403, acc 0.875\n",
      "2017-11-06T02:32:07.749693: step 2257, loss 0.0797473, acc 0.96875\n",
      "2017-11-06T02:32:11.756539: step 2258, loss 0.0693834, acc 0.9375\n",
      "2017-11-06T02:32:15.746375: step 2259, loss 0.199433, acc 0.9375\n",
      "2017-11-06T02:32:19.692179: step 2260, loss 0.115296, acc 0.96875\n",
      "2017-11-06T02:32:24.037266: step 2261, loss 0.0958165, acc 0.9375\n",
      "2017-11-06T02:32:28.315305: step 2262, loss 0.0728777, acc 0.96875\n",
      "2017-11-06T02:32:32.411216: step 2263, loss 0.0657762, acc 1\n",
      "2017-11-06T02:32:36.571172: step 2264, loss 0.297302, acc 0.90625\n",
      "2017-11-06T02:32:40.679091: step 2265, loss 0.138352, acc 0.9375\n",
      "2017-11-06T02:32:44.585866: step 2266, loss 0.0252954, acc 1\n",
      "2017-11-06T02:32:48.566694: step 2267, loss 0.0778324, acc 0.96875\n",
      "2017-11-06T02:32:51.113506: step 2268, loss 0.427835, acc 0.8\n",
      "2017-11-06T02:32:55.196809: step 2269, loss 0.111452, acc 0.96875\n",
      "2017-11-06T02:32:59.233676: step 2270, loss 0.122603, acc 0.9375\n",
      "2017-11-06T02:33:03.173475: step 2271, loss 0.130068, acc 0.90625\n",
      "2017-11-06T02:33:07.153302: step 2272, loss 0.41857, acc 0.8125\n",
      "2017-11-06T02:33:11.187932: step 2273, loss 0.0273946, acc 1\n",
      "2017-11-06T02:33:15.208789: step 2274, loss 0.0303639, acc 1\n",
      "2017-11-06T02:33:19.199625: step 2275, loss 0.0155602, acc 1\n",
      "2017-11-06T02:33:23.247501: step 2276, loss 0.113357, acc 0.96875\n",
      "2017-11-06T02:33:27.496521: step 2277, loss 0.0267963, acc 1\n",
      "2017-11-06T02:33:31.918663: step 2278, loss 0.161595, acc 0.9375\n",
      "2017-11-06T02:33:36.014573: step 2279, loss 0.444701, acc 0.875\n",
      "2017-11-06T02:33:40.101477: step 2280, loss 0.0389046, acc 0.96875\n",
      "2017-11-06T02:33:44.093313: step 2281, loss 0.138747, acc 0.96875\n",
      "2017-11-06T02:33:48.241260: step 2282, loss 0.170789, acc 0.9375\n",
      "2017-11-06T02:33:52.386206: step 2283, loss 0.0532769, acc 0.96875\n",
      "2017-11-06T02:33:56.509135: step 2284, loss 0.206338, acc 0.90625\n",
      "2017-11-06T02:34:00.767161: step 2285, loss 0.0141805, acc 1\n",
      "2017-11-06T02:34:04.882085: step 2286, loss 0.137341, acc 0.96875\n",
      "2017-11-06T02:34:09.024027: step 2287, loss 0.236668, acc 0.90625\n",
      "2017-11-06T02:34:13.264040: step 2288, loss 0.0315996, acc 1\n",
      "2017-11-06T02:34:17.362953: step 2289, loss 0.176086, acc 0.9375\n",
      "2017-11-06T02:34:21.558934: step 2290, loss 0.0577492, acc 0.96875\n",
      "2017-11-06T02:34:25.687868: step 2291, loss 0.131575, acc 0.9375\n",
      "2017-11-06T02:34:29.914872: step 2292, loss 0.289376, acc 0.90625\n",
      "2017-11-06T02:34:34.320001: step 2293, loss 0.00951453, acc 1\n",
      "2017-11-06T02:34:38.809191: step 2294, loss 0.0721279, acc 1\n",
      "2017-11-06T02:34:43.028189: step 2295, loss 0.135673, acc 0.9375\n",
      "2017-11-06T02:34:47.207158: step 2296, loss 0.139236, acc 0.9375\n",
      "2017-11-06T02:34:51.255035: step 2297, loss 0.222804, acc 0.875\n",
      "2017-11-06T02:34:55.288900: step 2298, loss 0.0704494, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:34:59.495890: step 2299, loss 0.0208908, acc 1\n",
      "2017-11-06T02:35:03.508741: step 2300, loss 0.255919, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:35:06.162627: step 2300, loss 0.613729, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-06T02:35:11.439024: step 2301, loss 0.048215, acc 1\n",
      "2017-11-06T02:35:15.485898: step 2302, loss 0.230001, acc 0.9375\n",
      "2017-11-06T02:35:19.558795: step 2303, loss 0.0841323, acc 0.96875\n",
      "2017-11-06T02:35:22.191664: step 2304, loss 0.162984, acc 0.9\n",
      "2017-11-06T02:35:26.285573: step 2305, loss 0.14484, acc 0.96875\n",
      "2017-11-06T02:35:30.315436: step 2306, loss 0.094172, acc 0.96875\n",
      "2017-11-06T02:35:34.323283: step 2307, loss 0.105066, acc 0.9375\n",
      "2017-11-06T02:35:38.348144: step 2308, loss 0.0550737, acc 0.96875\n",
      "2017-11-06T02:35:42.803309: step 2309, loss 0.28483, acc 0.875\n",
      "2017-11-06T02:35:46.916231: step 2310, loss 0.130281, acc 0.90625\n",
      "2017-11-06T02:35:51.004136: step 2311, loss 0.0449885, acc 1\n",
      "2017-11-06T02:35:55.080032: step 2312, loss 0.327484, acc 0.875\n",
      "2017-11-06T02:35:59.111896: step 2313, loss 0.215296, acc 0.90625\n",
      "2017-11-06T02:36:03.168779: step 2314, loss 0.183413, acc 0.875\n",
      "2017-11-06T02:36:07.139600: step 2315, loss 0.359699, acc 0.90625\n",
      "2017-11-06T02:36:11.169230: step 2316, loss 0.123231, acc 0.9375\n",
      "2017-11-06T02:36:15.203077: step 2317, loss 0.150811, acc 0.9375\n",
      "2017-11-06T02:36:19.245950: step 2318, loss 0.0913842, acc 0.9375\n",
      "2017-11-06T02:36:23.249794: step 2319, loss 0.101258, acc 0.9375\n",
      "2017-11-06T02:36:27.305676: step 2320, loss 0.0562372, acc 0.96875\n",
      "2017-11-06T02:36:31.307520: step 2321, loss 0.0455327, acc 0.96875\n",
      "2017-11-06T02:36:35.518511: step 2322, loss 0.217292, acc 0.90625\n",
      "2017-11-06T02:36:39.496338: step 2323, loss 0.0550588, acc 0.96875\n",
      "2017-11-06T02:36:43.594250: step 2324, loss 0.37602, acc 0.8125\n",
      "2017-11-06T02:36:48.003383: step 2325, loss 0.203414, acc 0.9375\n",
      "2017-11-06T02:36:52.155332: step 2326, loss 0.212913, acc 0.9375\n",
      "2017-11-06T02:36:56.213216: step 2327, loss 0.0729667, acc 0.96875\n",
      "2017-11-06T02:37:00.293116: step 2328, loss 0.0249398, acc 1\n",
      "2017-11-06T02:37:04.352999: step 2329, loss 0.137659, acc 0.96875\n",
      "2017-11-06T02:37:08.381862: step 2330, loss 0.12158, acc 0.96875\n",
      "2017-11-06T02:37:12.366696: step 2331, loss 0.0593599, acc 1\n",
      "2017-11-06T02:37:16.447593: step 2332, loss 0.262269, acc 0.875\n",
      "2017-11-06T02:37:20.485463: step 2333, loss 0.192028, acc 0.9375\n",
      "2017-11-06T02:37:24.554354: step 2334, loss 0.132822, acc 0.90625\n",
      "2017-11-06T02:37:28.571207: step 2335, loss 0.101223, acc 0.9375\n",
      "2017-11-06T02:37:32.555039: step 2336, loss 0.0930243, acc 0.96875\n",
      "2017-11-06T02:37:36.637940: step 2337, loss 0.0611862, acc 0.96875\n",
      "2017-11-06T02:37:40.671805: step 2338, loss 0.106182, acc 0.9375\n",
      "2017-11-06T02:37:44.785729: step 2339, loss 0.0541345, acc 0.96875\n",
      "2017-11-06T02:37:47.338542: step 2340, loss 0.0616125, acc 0.95\n",
      "2017-11-06T02:37:51.553538: step 2341, loss 0.0854311, acc 0.96875\n",
      "2017-11-06T02:37:55.914636: step 2342, loss 0.146553, acc 0.96875\n",
      "2017-11-06T02:37:59.953506: step 2343, loss 0.0783895, acc 0.9375\n",
      "2017-11-06T02:38:04.065428: step 2344, loss 0.236476, acc 0.90625\n",
      "2017-11-06T02:38:08.108300: step 2345, loss 0.199299, acc 0.875\n",
      "2017-11-06T02:38:12.199207: step 2346, loss 0.0903243, acc 0.9375\n",
      "2017-11-06T02:38:16.203052: step 2347, loss 0.0125196, acc 1\n",
      "2017-11-06T02:38:20.348998: step 2348, loss 0.254973, acc 0.96875\n",
      "2017-11-06T02:38:24.394872: step 2349, loss 0.00888688, acc 1\n",
      "2017-11-06T02:38:28.444750: step 2350, loss 0.127549, acc 0.9375\n",
      "2017-11-06T02:38:32.525650: step 2351, loss 0.0343129, acc 1\n",
      "2017-11-06T02:38:36.734640: step 2352, loss 0.0430616, acc 0.96875\n",
      "2017-11-06T02:38:40.768507: step 2353, loss 0.0299794, acc 1\n",
      "2017-11-06T02:38:44.796369: step 2354, loss 0.09956, acc 0.96875\n",
      "2017-11-06T02:38:48.806218: step 2355, loss 0.292023, acc 0.90625\n",
      "2017-11-06T02:38:52.929147: step 2356, loss 0.212976, acc 0.875\n",
      "2017-11-06T02:38:57.123127: step 2357, loss 0.110203, acc 0.96875\n",
      "2017-11-06T02:39:01.350133: step 2358, loss 0.0300789, acc 1\n",
      "2017-11-06T02:39:05.397006: step 2359, loss 0.0323032, acc 1\n",
      "2017-11-06T02:39:09.433714: step 2360, loss 0.0496423, acc 0.96875\n",
      "2017-11-06T02:39:13.461577: step 2361, loss 0.150071, acc 0.90625\n",
      "2017-11-06T02:39:17.555484: step 2362, loss 0.175535, acc 0.96875\n",
      "2017-11-06T02:39:21.734454: step 2363, loss 0.138084, acc 0.9375\n",
      "2017-11-06T02:39:25.881400: step 2364, loss 0.0544275, acc 1\n",
      "2017-11-06T02:39:30.197467: step 2365, loss 0.13668, acc 0.9375\n",
      "2017-11-06T02:39:34.230332: step 2366, loss 0.139185, acc 0.9375\n",
      "2017-11-06T02:39:38.314235: step 2367, loss 0.0574598, acc 0.9375\n",
      "2017-11-06T02:39:42.325084: step 2368, loss 0.0822042, acc 0.9375\n",
      "2017-11-06T02:39:46.400980: step 2369, loss 0.293676, acc 0.90625\n",
      "2017-11-06T02:39:50.495890: step 2370, loss 0.0198212, acc 1\n",
      "2017-11-06T02:39:54.506740: step 2371, loss 0.0133996, acc 1\n",
      "2017-11-06T02:39:58.529598: step 2372, loss 0.0627845, acc 0.96875\n",
      "2017-11-06T02:40:03.061819: step 2373, loss 0.124425, acc 0.9375\n",
      "2017-11-06T02:40:07.448936: step 2374, loss 0.048288, acc 0.96875\n",
      "2017-11-06T02:40:11.555854: step 2375, loss 0.193312, acc 0.90625\n",
      "2017-11-06T02:40:14.142692: step 2376, loss 0.414355, acc 0.85\n",
      "2017-11-06T02:40:18.254614: step 2377, loss 0.0582197, acc 0.96875\n",
      "2017-11-06T02:40:22.405563: step 2378, loss 0.152815, acc 0.9375\n",
      "2017-11-06T02:40:26.467449: step 2379, loss 0.116635, acc 0.9375\n",
      "2017-11-06T02:40:30.528335: step 2380, loss 0.165481, acc 0.9375\n",
      "2017-11-06T02:40:34.806375: step 2381, loss 0.0734225, acc 0.96875\n",
      "2017-11-06T02:40:38.799211: step 2382, loss 0.0860412, acc 0.9375\n",
      "2017-11-06T02:40:42.854094: step 2383, loss 0.114853, acc 0.9375\n",
      "2017-11-06T02:40:46.925985: step 2384, loss 0.103247, acc 0.96875\n",
      "2017-11-06T02:40:50.908816: step 2385, loss 0.139809, acc 0.90625\n",
      "2017-11-06T02:40:54.951689: step 2386, loss 0.0199919, acc 1\n",
      "2017-11-06T02:40:59.022581: step 2387, loss 0.125918, acc 0.96875\n",
      "2017-11-06T02:41:03.050443: step 2388, loss 0.233028, acc 0.875\n",
      "2017-11-06T02:41:07.210399: step 2389, loss 0.0901019, acc 0.9375\n",
      "2017-11-06T02:41:12.350051: step 2390, loss 0.12576, acc 0.9375\n",
      "2017-11-06T02:41:17.124443: step 2391, loss 0.356651, acc 0.875\n",
      "2017-11-06T02:41:21.201339: step 2392, loss 0.169012, acc 0.875\n",
      "2017-11-06T02:41:25.354291: step 2393, loss 0.0103579, acc 1\n",
      "2017-11-06T02:41:29.291088: step 2394, loss 0.0634381, acc 0.96875\n",
      "2017-11-06T02:41:33.201867: step 2395, loss 0.0674055, acc 0.96875\n",
      "2017-11-06T02:41:37.123653: step 2396, loss 0.126371, acc 0.9375\n",
      "2017-11-06T02:41:41.053446: step 2397, loss 0.0359044, acc 0.96875\n",
      "2017-11-06T02:41:44.967227: step 2398, loss 0.205721, acc 0.875\n",
      "2017-11-06T02:41:48.907026: step 2399, loss 0.11495, acc 0.9375\n",
      "2017-11-06T02:41:52.817806: step 2400, loss 0.115532, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:41:55.416651: step 2400, loss 0.526819, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-06T02:42:00.769519: step 2401, loss 0.428414, acc 0.875\n",
      "2017-11-06T02:42:04.670291: step 2402, loss 0.236369, acc 0.90625\n",
      "2017-11-06T02:42:08.610090: step 2403, loss 0.0617075, acc 0.96875\n",
      "2017-11-06T02:42:12.756862: step 2404, loss 0.281627, acc 0.90625\n",
      "2017-11-06T02:42:17.126966: step 2405, loss 0.186665, acc 0.875\n",
      "2017-11-06T02:42:21.045751: step 2406, loss 0.255502, acc 0.90625\n",
      "2017-11-06T02:42:25.147665: step 2407, loss 0.299489, acc 0.90625\n",
      "2017-11-06T02:42:29.102476: step 2408, loss 0.184708, acc 0.9375\n",
      "2017-11-06T02:42:33.053283: step 2409, loss 0.105207, acc 0.96875\n",
      "2017-11-06T02:42:37.054125: step 2410, loss 0.236672, acc 0.9375\n",
      "2017-11-06T02:42:41.015941: step 2411, loss 0.113953, acc 0.9375\n",
      "2017-11-06T02:42:43.555745: step 2412, loss 0.27792, acc 0.9\n",
      "2017-11-06T02:42:47.450514: step 2413, loss 0.273305, acc 0.875\n",
      "2017-11-06T02:42:51.376302: step 2414, loss 0.121039, acc 0.96875\n",
      "2017-11-06T02:42:55.342120: step 2415, loss 0.0807763, acc 0.96875\n",
      "2017-11-06T02:42:59.350413: step 2416, loss 0.0237457, acc 1\n",
      "2017-11-06T02:43:03.278204: step 2417, loss 0.12386, acc 0.96875\n",
      "2017-11-06T02:43:07.233014: step 2418, loss 0.582904, acc 0.90625\n",
      "2017-11-06T02:43:11.157804: step 2419, loss 0.362893, acc 0.90625\n",
      "2017-11-06T02:43:15.107609: step 2420, loss 0.30628, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:43:19.179502: step 2421, loss 0.191042, acc 0.90625\n",
      "2017-11-06T02:43:23.586634: step 2422, loss 0.231513, acc 0.90625\n",
      "2017-11-06T02:43:27.829649: step 2423, loss 0.234788, acc 0.8125\n",
      "2017-11-06T02:43:31.736425: step 2424, loss 0.0519695, acc 0.96875\n",
      "2017-11-06T02:43:35.735266: step 2425, loss 0.334774, acc 0.875\n",
      "2017-11-06T02:43:39.701084: step 2426, loss 0.0661334, acc 0.96875\n",
      "2017-11-06T02:43:43.694922: step 2427, loss 0.329967, acc 0.90625\n",
      "2017-11-06T02:43:47.696765: step 2428, loss 0.16477, acc 0.9375\n",
      "2017-11-06T02:43:51.701611: step 2429, loss 0.525068, acc 0.90625\n",
      "2017-11-06T02:43:55.625399: step 2430, loss 0.0590786, acc 1\n",
      "2017-11-06T02:43:59.563197: step 2431, loss 0.242802, acc 0.9375\n",
      "2017-11-06T02:44:03.577050: step 2432, loss 0.202902, acc 0.9375\n",
      "2017-11-06T02:44:07.571887: step 2433, loss 0.0638055, acc 0.96875\n",
      "2017-11-06T02:44:11.554717: step 2434, loss 0.00879437, acc 1\n",
      "2017-11-06T02:44:15.498519: step 2435, loss 0.0939385, acc 0.96875\n",
      "2017-11-06T02:44:19.440320: step 2436, loss 0.0888487, acc 0.96875\n",
      "2017-11-06T02:44:23.499204: step 2437, loss 0.450249, acc 0.84375\n",
      "2017-11-06T02:44:27.858301: step 2438, loss 0.099592, acc 0.96875\n",
      "2017-11-06T02:44:32.003247: step 2439, loss 0.205535, acc 0.9375\n",
      "2017-11-06T02:44:36.126179: step 2440, loss 0.0945187, acc 0.96875\n",
      "2017-11-06T02:44:40.087992: step 2441, loss 0.312349, acc 0.90625\n",
      "2017-11-06T02:44:43.998772: step 2442, loss 0.142498, acc 0.9375\n",
      "2017-11-06T02:44:47.968591: step 2443, loss 0.0745752, acc 0.96875\n",
      "2017-11-06T02:44:51.912392: step 2444, loss 0.122129, acc 0.96875\n",
      "2017-11-06T02:44:55.918239: step 2445, loss 0.320976, acc 0.84375\n",
      "2017-11-06T02:44:59.969799: step 2446, loss 0.0983079, acc 0.96875\n",
      "2017-11-06T02:45:03.899592: step 2447, loss 0.0789427, acc 0.96875\n",
      "2017-11-06T02:45:06.459412: step 2448, loss 0.574661, acc 0.85\n",
      "2017-11-06T02:45:10.432996: step 2449, loss 0.458061, acc 0.8125\n",
      "2017-11-06T02:45:14.348778: step 2450, loss 0.256748, acc 0.9375\n",
      "2017-11-06T02:45:18.329607: step 2451, loss 0.095753, acc 0.9375\n",
      "2017-11-06T02:45:22.277412: step 2452, loss 0.125981, acc 0.90625\n",
      "2017-11-06T02:45:26.219214: step 2453, loss 0.00677739, acc 1\n",
      "2017-11-06T02:45:30.331135: step 2454, loss 0.0441721, acc 0.96875\n",
      "2017-11-06T02:45:34.609175: step 2455, loss 0.167928, acc 0.9375\n",
      "2017-11-06T02:45:38.567987: step 2456, loss 0.0381835, acc 0.96875\n",
      "2017-11-06T02:45:42.480768: step 2457, loss 0.02037, acc 1\n",
      "2017-11-06T02:45:46.402554: step 2458, loss 0.120546, acc 0.90625\n",
      "2017-11-06T02:45:50.376378: step 2459, loss 0.481287, acc 0.875\n",
      "2017-11-06T02:45:54.315176: step 2460, loss 0.119266, acc 0.9375\n",
      "2017-11-06T02:45:58.311017: step 2461, loss 0.25559, acc 0.90625\n",
      "2017-11-06T02:46:02.273831: step 2462, loss 0.300294, acc 0.90625\n",
      "2017-11-06T02:46:06.290686: step 2463, loss 0.125211, acc 0.9375\n",
      "2017-11-06T02:46:10.276518: step 2464, loss 0.131815, acc 0.9375\n",
      "2017-11-06T02:46:14.239333: step 2465, loss 0.145368, acc 0.96875\n",
      "2017-11-06T02:46:18.150113: step 2466, loss 0.216246, acc 0.90625\n",
      "2017-11-06T02:46:22.097919: step 2467, loss 0.17975, acc 0.9375\n",
      "2017-11-06T02:46:26.036715: step 2468, loss 0.0212233, acc 1\n",
      "2017-11-06T02:46:30.069583: step 2469, loss 0.383386, acc 0.90625\n",
      "2017-11-06T02:46:34.168494: step 2470, loss 0.076112, acc 0.96875\n",
      "2017-11-06T02:46:38.661687: step 2471, loss 0.172349, acc 0.90625\n",
      "2017-11-06T02:46:42.746589: step 2472, loss 0.149494, acc 0.9375\n",
      "2017-11-06T02:46:46.743431: step 2473, loss 0.0561452, acc 0.96875\n",
      "2017-11-06T02:46:50.655208: step 2474, loss 0.068741, acc 0.9375\n",
      "2017-11-06T02:46:54.617023: step 2475, loss 0.0723483, acc 0.9375\n",
      "2017-11-06T02:46:58.574836: step 2476, loss 0.0380436, acc 0.96875\n",
      "2017-11-06T02:47:02.554663: step 2477, loss 0.0134251, acc 1\n",
      "2017-11-06T02:47:06.555507: step 2478, loss 0.610984, acc 0.875\n",
      "2017-11-06T02:47:10.473290: step 2479, loss 0.189463, acc 0.9375\n",
      "2017-11-06T02:47:14.438107: step 2480, loss 0.249142, acc 0.90625\n",
      "2017-11-06T02:47:18.403925: step 2481, loss 0.137261, acc 0.9375\n",
      "2017-11-06T02:47:22.333717: step 2482, loss 0.356768, acc 0.875\n",
      "2017-11-06T02:47:26.289528: step 2483, loss 0.0725192, acc 0.96875\n",
      "2017-11-06T02:47:28.862356: step 2484, loss 0.0179367, acc 1\n",
      "2017-11-06T02:47:32.881211: step 2485, loss 0.185295, acc 0.9375\n",
      "2017-11-06T02:47:36.819010: step 2486, loss 0.189424, acc 0.9375\n",
      "2017-11-06T02:47:40.871889: step 2487, loss 0.433236, acc 0.90625\n",
      "2017-11-06T02:47:45.282024: step 2488, loss 0.208706, acc 0.9375\n",
      "2017-11-06T02:47:49.306884: step 2489, loss 0.22824, acc 0.96875\n",
      "2017-11-06T02:47:53.264696: step 2490, loss 0.0723875, acc 0.96875\n",
      "2017-11-06T02:47:57.186482: step 2491, loss 0.173791, acc 0.96875\n",
      "2017-11-06T02:48:01.136288: step 2492, loss 0.0379837, acc 0.96875\n",
      "2017-11-06T02:48:05.106109: step 2493, loss 0.0642758, acc 0.96875\n",
      "2017-11-06T02:48:09.099951: step 2494, loss 0.0835498, acc 0.96875\n",
      "2017-11-06T02:48:13.062537: step 2495, loss 0.136008, acc 0.96875\n",
      "2017-11-06T02:48:17.016348: step 2496, loss 0.0691648, acc 0.96875\n",
      "2017-11-06T02:48:21.001178: step 2497, loss 0.158116, acc 0.9375\n",
      "2017-11-06T02:48:25.142120: step 2498, loss 0.116941, acc 0.9375\n",
      "2017-11-06T02:48:29.252041: step 2499, loss 0.23172, acc 0.9375\n",
      "2017-11-06T02:48:33.407993: step 2500, loss 0.155919, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:48:36.106912: step 2500, loss 0.746171, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-06T02:48:41.582792: step 2501, loss 0.125342, acc 0.96875\n",
      "2017-11-06T02:48:45.572627: step 2502, loss 0.232696, acc 0.875\n",
      "2017-11-06T02:48:49.992768: step 2503, loss 0.0804495, acc 0.96875\n",
      "2017-11-06T02:48:54.038642: step 2504, loss 0.0120277, acc 1\n",
      "2017-11-06T02:48:58.037484: step 2505, loss 0.0203643, acc 1\n",
      "2017-11-06T02:49:01.982287: step 2506, loss 0.266852, acc 0.90625\n",
      "2017-11-06T02:49:05.959113: step 2507, loss 0.0279604, acc 1\n",
      "2017-11-06T02:49:09.982972: step 2508, loss 0.35659, acc 0.90625\n",
      "2017-11-06T02:49:13.955795: step 2509, loss 0.261096, acc 0.9375\n",
      "2017-11-06T02:49:17.915610: step 2510, loss 0.0779766, acc 0.96875\n",
      "2017-11-06T02:49:21.854407: step 2511, loss 0.00793064, acc 1\n",
      "2017-11-06T02:49:25.875264: step 2512, loss 0.0707343, acc 0.96875\n",
      "2017-11-06T02:49:29.831075: step 2513, loss 0.153769, acc 0.9375\n",
      "2017-11-06T02:49:33.857940: step 2514, loss 0.142526, acc 0.9375\n",
      "2017-11-06T02:49:37.855777: step 2515, loss 0.103644, acc 0.9375\n",
      "2017-11-06T02:49:41.892645: step 2516, loss 0.103112, acc 0.96875\n",
      "2017-11-06T02:49:45.849456: step 2517, loss 0.24119, acc 0.875\n",
      "2017-11-06T02:49:49.857305: step 2518, loss 0.0470739, acc 0.96875\n",
      "2017-11-06T02:49:54.116330: step 2519, loss 0.27094, acc 0.9375\n",
      "2017-11-06T02:49:56.973361: step 2520, loss 0.0423631, acc 1\n",
      "2017-11-06T02:50:01.199365: step 2521, loss 0.0389522, acc 0.96875\n",
      "2017-11-06T02:50:05.333301: step 2522, loss 0.279978, acc 0.90625\n",
      "2017-11-06T02:50:09.336147: step 2523, loss 0.0955158, acc 0.9375\n",
      "2017-11-06T02:50:13.388024: step 2524, loss 0.042285, acc 0.96875\n",
      "2017-11-06T02:50:17.358845: step 2525, loss 0.229557, acc 0.90625\n",
      "2017-11-06T02:50:21.402718: step 2526, loss 0.0026152, acc 1\n",
      "2017-11-06T02:50:25.421574: step 2527, loss 0.046428, acc 0.96875\n",
      "2017-11-06T02:50:29.461445: step 2528, loss 0.1474, acc 0.9375\n",
      "2017-11-06T02:50:33.594381: step 2529, loss 0.164533, acc 0.9375\n",
      "2017-11-06T02:50:37.735323: step 2530, loss 0.426969, acc 0.8125\n",
      "2017-11-06T02:50:41.742170: step 2531, loss 0.148662, acc 0.9375\n",
      "2017-11-06T02:50:45.755022: step 2532, loss 0.275424, acc 0.84375\n",
      "2017-11-06T02:50:49.773877: step 2533, loss 0.0571776, acc 0.96875\n",
      "2017-11-06T02:50:53.757709: step 2534, loss 0.188391, acc 0.90625\n",
      "2017-11-06T02:50:57.831602: step 2535, loss 0.0763156, acc 0.96875\n",
      "2017-11-06T02:51:02.267759: step 2536, loss 0.110687, acc 0.96875\n",
      "2017-11-06T02:51:06.320082: step 2537, loss 0.373049, acc 0.875\n",
      "2017-11-06T02:51:10.332702: step 2538, loss 0.0568905, acc 0.96875\n",
      "2017-11-06T02:51:14.314531: step 2539, loss 0.360192, acc 0.875\n",
      "2017-11-06T02:51:18.320377: step 2540, loss 0.0184754, acc 1\n",
      "2017-11-06T02:51:22.324222: step 2541, loss 0.118604, acc 0.96875\n",
      "2017-11-06T02:51:26.270025: step 2542, loss 0.0331974, acc 0.96875\n",
      "2017-11-06T02:51:30.271869: step 2543, loss 0.257421, acc 0.875\n",
      "2017-11-06T02:51:34.286722: step 2544, loss 0.114783, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:51:38.313583: step 2545, loss 0.0803884, acc 0.96875\n",
      "2017-11-06T02:51:42.339444: step 2546, loss 0.280225, acc 0.875\n",
      "2017-11-06T02:51:46.287248: step 2547, loss 0.15338, acc 0.9375\n",
      "2017-11-06T02:51:50.340129: step 2548, loss 0.0307814, acc 1\n",
      "2017-11-06T02:51:54.347976: step 2549, loss 0.211056, acc 0.9375\n",
      "2017-11-06T02:51:58.325803: step 2550, loss 0.195787, acc 0.84375\n",
      "2017-11-06T02:52:02.284615: step 2551, loss 0.0715923, acc 0.96875\n",
      "2017-11-06T02:52:06.733776: step 2552, loss 0.106244, acc 0.96875\n",
      "2017-11-06T02:52:10.828686: step 2553, loss 0.0265422, acc 1\n",
      "2017-11-06T02:52:14.833532: step 2554, loss 0.0821654, acc 0.96875\n",
      "2017-11-06T02:52:18.797348: step 2555, loss 0.139433, acc 0.96875\n",
      "2017-11-06T02:52:21.343158: step 2556, loss 0.106869, acc 0.95\n",
      "2017-11-06T02:52:25.452077: step 2557, loss 0.159263, acc 0.96875\n",
      "2017-11-06T02:52:29.481940: step 2558, loss 0.0735368, acc 0.96875\n",
      "2017-11-06T02:52:33.604870: step 2559, loss 0.15031, acc 0.96875\n",
      "2017-11-06T02:52:37.675764: step 2560, loss 0.255226, acc 0.90625\n",
      "2017-11-06T02:52:41.635576: step 2561, loss 0.098863, acc 0.96875\n",
      "2017-11-06T02:52:45.659435: step 2562, loss 0.12432, acc 0.9375\n",
      "2017-11-06T02:52:49.605240: step 2563, loss 0.206616, acc 0.90625\n",
      "2017-11-06T02:52:53.595073: step 2564, loss 0.176386, acc 0.9375\n",
      "2017-11-06T02:52:57.559891: step 2565, loss 0.141575, acc 0.96875\n",
      "2017-11-06T02:53:01.547724: step 2566, loss 0.132346, acc 0.9375\n",
      "2017-11-06T02:53:05.784735: step 2567, loss 0.0518475, acc 1\n",
      "2017-11-06T02:53:10.061774: step 2568, loss 0.237747, acc 0.875\n",
      "2017-11-06T02:53:14.434883: step 2569, loss 0.109399, acc 0.96875\n",
      "2017-11-06T02:53:18.503772: step 2570, loss 0.131289, acc 0.9375\n",
      "2017-11-06T02:53:22.598682: step 2571, loss 0.0721697, acc 0.96875\n",
      "2017-11-06T02:53:26.698597: step 2572, loss 0.208993, acc 0.9375\n",
      "2017-11-06T02:53:30.703442: step 2573, loss 0.0817377, acc 0.9375\n",
      "2017-11-06T02:53:34.641238: step 2574, loss 0.182147, acc 0.90625\n",
      "2017-11-06T02:53:38.655090: step 2575, loss 0.122271, acc 0.96875\n",
      "2017-11-06T02:53:42.661937: step 2576, loss 0.0953596, acc 0.96875\n",
      "2017-11-06T02:53:46.674790: step 2577, loss 0.0955126, acc 0.9375\n",
      "2017-11-06T02:53:50.731671: step 2578, loss 0.0160113, acc 1\n",
      "2017-11-06T02:53:54.720506: step 2579, loss 0.249976, acc 0.90625\n",
      "2017-11-06T02:53:58.684322: step 2580, loss 0.00735888, acc 1\n",
      "2017-11-06T02:54:02.650140: step 2581, loss 0.0654412, acc 0.96875\n",
      "2017-11-06T02:54:06.648983: step 2582, loss 0.278018, acc 0.90625\n",
      "2017-11-06T02:54:10.670598: step 2583, loss 0.0369143, acc 0.96875\n",
      "2017-11-06T02:54:14.760505: step 2584, loss 0.131472, acc 0.9375\n",
      "2017-11-06T02:54:19.209666: step 2585, loss 0.218507, acc 0.9375\n",
      "2017-11-06T02:54:23.217513: step 2586, loss 0.180457, acc 0.90625\n",
      "2017-11-06T02:54:27.193340: step 2587, loss 0.227566, acc 0.96875\n",
      "2017-11-06T02:54:31.335284: step 2588, loss 0.0298513, acc 1\n",
      "2017-11-06T02:54:35.564286: step 2589, loss 0.131573, acc 0.9375\n",
      "2017-11-06T02:54:39.603158: step 2590, loss 0.180568, acc 0.90625\n",
      "2017-11-06T02:54:43.578981: step 2591, loss 0.17577, acc 0.9375\n",
      "2017-11-06T02:54:46.159815: step 2592, loss 0.127049, acc 0.95\n",
      "2017-11-06T02:54:50.157655: step 2593, loss 0.0384371, acc 1\n",
      "2017-11-06T02:54:54.118472: step 2594, loss 0.0795452, acc 0.96875\n",
      "2017-11-06T02:54:58.096297: step 2595, loss 0.00532496, acc 1\n",
      "2017-11-06T02:55:02.038097: step 2596, loss 0.154961, acc 0.9375\n",
      "2017-11-06T02:55:05.997911: step 2597, loss 0.0511857, acc 0.96875\n",
      "2017-11-06T02:55:10.061799: step 2598, loss 0.312144, acc 0.90625\n",
      "2017-11-06T02:55:14.073649: step 2599, loss 0.0824866, acc 0.96875\n",
      "2017-11-06T02:55:18.034463: step 2600, loss 0.159413, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T02:55:20.826448: step 2600, loss 0.685775, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-06T02:55:26.475072: step 2601, loss 0.36297, acc 0.90625\n",
      "2017-11-06T02:55:30.470912: step 2602, loss 0.101047, acc 0.96875\n",
      "2017-11-06T02:55:34.462747: step 2603, loss 0.163278, acc 0.9375\n",
      "2017-11-06T02:55:38.445577: step 2604, loss 0.204613, acc 0.9375\n",
      "2017-11-06T02:55:42.434413: step 2605, loss 0.041073, acc 1\n",
      "2017-11-06T02:55:46.421247: step 2606, loss 0.119856, acc 0.9375\n",
      "2017-11-06T02:55:50.497140: step 2607, loss 0.142859, acc 0.96875\n",
      "2017-11-06T02:55:54.510992: step 2608, loss 0.114185, acc 0.96875\n",
      "2017-11-06T02:55:58.493823: step 2609, loss 0.0914579, acc 0.96875\n",
      "2017-11-06T02:56:02.484660: step 2610, loss 0.0750642, acc 1\n",
      "2017-11-06T02:56:06.512520: step 2611, loss 0.357849, acc 0.84375\n",
      "2017-11-06T02:56:10.516367: step 2612, loss 0.292599, acc 0.875\n",
      "2017-11-06T02:56:14.556236: step 2613, loss 0.118215, acc 0.9375\n",
      "2017-11-06T02:56:18.595105: step 2614, loss 0.289461, acc 0.90625\n",
      "2017-11-06T02:56:22.561924: step 2615, loss 0.185191, acc 0.96875\n",
      "2017-11-06T02:56:26.675847: step 2616, loss 0.304043, acc 0.9375\n",
      "2017-11-06T02:56:31.065966: step 2617, loss 0.475046, acc 0.8125\n",
      "2017-11-06T02:56:35.217917: step 2618, loss 0.175396, acc 0.9375\n",
      "2017-11-06T02:56:39.238773: step 2619, loss 0.0623953, acc 0.96875\n",
      "2017-11-06T02:56:43.263634: step 2620, loss 0.159637, acc 0.9375\n",
      "2017-11-06T02:56:47.252469: step 2621, loss 0.20112, acc 0.9375\n",
      "2017-11-06T02:56:51.264318: step 2622, loss 0.291119, acc 0.875\n",
      "2017-11-06T02:56:55.304188: step 2623, loss 0.0442076, acc 0.96875\n",
      "2017-11-06T02:56:59.254998: step 2624, loss 0.265586, acc 0.875\n",
      "2017-11-06T02:57:03.254838: step 2625, loss 0.10375, acc 0.96875\n",
      "2017-11-06T02:57:07.242673: step 2626, loss 0.14463, acc 0.90625\n",
      "2017-11-06T02:57:11.249269: step 2627, loss 0.0225993, acc 1\n",
      "2017-11-06T02:57:13.920166: step 2628, loss 0.0579648, acc 1\n",
      "2017-11-06T02:57:17.896992: step 2629, loss 0.0608817, acc 0.96875\n",
      "2017-11-06T02:57:21.902840: step 2630, loss 0.317773, acc 0.84375\n",
      "2017-11-06T02:57:25.871658: step 2631, loss 0.158003, acc 0.96875\n",
      "2017-11-06T02:57:29.840477: step 2632, loss 0.0309971, acc 1\n",
      "2017-11-06T02:57:34.206580: step 2633, loss 0.0429984, acc 0.96875\n",
      "2017-11-06T02:57:38.417573: step 2634, loss 0.0959636, acc 0.9375\n",
      "2017-11-06T02:57:42.370381: step 2635, loss 0.0886872, acc 0.96875\n",
      "2017-11-06T02:57:46.411253: step 2636, loss 0.259188, acc 0.96875\n",
      "2017-11-06T02:57:50.387077: step 2637, loss 0.0117828, acc 1\n",
      "2017-11-06T02:57:54.408935: step 2638, loss 0.0151745, acc 1\n",
      "2017-11-06T02:57:58.383759: step 2639, loss 0.0951899, acc 0.9375\n",
      "2017-11-06T02:58:02.409620: step 2640, loss 0.0861404, acc 0.96875\n",
      "2017-11-06T02:58:06.382443: step 2641, loss 0.155692, acc 0.9375\n",
      "2017-11-06T02:58:10.380284: step 2642, loss 0.0148355, acc 1\n",
      "2017-11-06T02:58:14.410146: step 2643, loss 0.301593, acc 0.90625\n",
      "2017-11-06T02:58:18.426000: step 2644, loss 0.0872904, acc 0.96875\n",
      "2017-11-06T02:58:22.599966: step 2645, loss 0.168285, acc 0.90625\n",
      "2017-11-06T02:58:26.770931: step 2646, loss 0.154315, acc 0.90625\n",
      "2017-11-06T02:58:30.759764: step 2647, loss 0.204406, acc 0.90625\n",
      "2017-11-06T02:58:34.908714: step 2648, loss 0.129536, acc 0.90625\n",
      "2017-11-06T02:58:39.317844: step 2649, loss 0.154025, acc 0.9375\n",
      "2017-11-06T02:58:43.555856: step 2650, loss 0.127986, acc 0.96875\n",
      "2017-11-06T02:58:47.624747: step 2651, loss 0.17762, acc 0.9375\n",
      "2017-11-06T02:58:51.684633: step 2652, loss 0.101741, acc 0.9375\n",
      "2017-11-06T02:58:55.676468: step 2653, loss 0.319713, acc 0.875\n",
      "2017-11-06T02:58:59.671308: step 2654, loss 0.0748233, acc 0.96875\n",
      "2017-11-06T02:59:03.652136: step 2655, loss 0.140495, acc 0.90625\n",
      "2017-11-06T02:59:07.666987: step 2656, loss 0.0699964, acc 0.96875\n",
      "2017-11-06T02:59:11.700854: step 2657, loss 0.150076, acc 0.9375\n",
      "2017-11-06T02:59:15.708702: step 2658, loss 0.0374873, acc 0.96875\n",
      "2017-11-06T02:59:19.756578: step 2659, loss 0.246212, acc 0.875\n",
      "2017-11-06T02:59:23.816463: step 2660, loss 0.151014, acc 0.90625\n",
      "2017-11-06T02:59:27.840321: step 2661, loss 0.136814, acc 0.96875\n",
      "2017-11-06T02:59:31.853173: step 2662, loss 0.00624437, acc 1\n",
      "2017-11-06T02:59:35.860021: step 2663, loss 0.274409, acc 0.84375\n",
      "2017-11-06T02:59:38.440854: step 2664, loss 0.0870155, acc 0.95\n",
      "2017-11-06T02:59:42.467715: step 2665, loss 0.103634, acc 0.96875\n",
      "2017-11-06T02:59:46.870843: step 2666, loss 0.165148, acc 0.96875\n",
      "2017-11-06T02:59:50.932730: step 2667, loss 0.00280236, acc 1\n",
      "2017-11-06T02:59:55.006624: step 2668, loss 0.0107206, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T02:59:59.033486: step 2669, loss 0.147559, acc 0.9375\n",
      "2017-11-06T03:00:03.251483: step 2670, loss 0.110066, acc 0.9375\n",
      "2017-11-06T03:00:07.277343: step 2671, loss 0.359956, acc 0.90625\n",
      "2017-11-06T03:00:11.327983: step 2672, loss 0.17102, acc 0.90625\n",
      "2017-11-06T03:00:15.380861: step 2673, loss 0.047639, acc 0.9375\n",
      "2017-11-06T03:00:19.388709: step 2674, loss 0.389205, acc 0.78125\n",
      "2017-11-06T03:00:23.439589: step 2675, loss 0.195777, acc 0.9375\n",
      "2017-11-06T03:00:27.436427: step 2676, loss 0.0237388, acc 1\n",
      "2017-11-06T03:00:31.391237: step 2677, loss 0.240047, acc 0.84375\n",
      "2017-11-06T03:00:35.604250: step 2678, loss 0.140173, acc 0.9375\n",
      "2017-11-06T03:00:39.541029: step 2679, loss 0.310042, acc 0.875\n",
      "2017-11-06T03:00:43.525864: step 2680, loss 0.211988, acc 0.9375\n",
      "2017-11-06T03:00:47.532707: step 2681, loss 0.0896362, acc 0.96875\n",
      "2017-11-06T03:00:51.960854: step 2682, loss 0.0469444, acc 1\n",
      "2017-11-06T03:00:56.088786: step 2683, loss 0.214038, acc 0.9375\n",
      "2017-11-06T03:01:00.081624: step 2684, loss 0.102163, acc 0.96875\n",
      "2017-11-06T03:01:04.040437: step 2685, loss 0.0358741, acc 1\n",
      "2017-11-06T03:01:08.066298: step 2686, loss 0.0217466, acc 1\n",
      "2017-11-06T03:01:12.061136: step 2687, loss 0.0625342, acc 0.96875\n",
      "2017-11-06T03:01:16.048969: step 2688, loss 0.145971, acc 0.96875\n",
      "2017-11-06T03:01:20.066823: step 2689, loss 0.29812, acc 0.90625\n",
      "2017-11-06T03:01:24.079675: step 2690, loss 0.0378778, acc 1\n",
      "2017-11-06T03:01:28.033486: step 2691, loss 0.413003, acc 0.90625\n",
      "2017-11-06T03:01:32.009310: step 2692, loss 0.154516, acc 0.96875\n",
      "2017-11-06T03:01:36.241316: step 2693, loss 0.132595, acc 0.96875\n",
      "2017-11-06T03:01:40.397270: step 2694, loss 0.135492, acc 0.9375\n",
      "2017-11-06T03:01:44.386103: step 2695, loss 0.193061, acc 0.9375\n",
      "2017-11-06T03:01:48.384945: step 2696, loss 0.153683, acc 0.9375\n",
      "2017-11-06T03:01:52.496866: step 2697, loss 0.286476, acc 0.90625\n",
      "2017-11-06T03:01:56.841954: step 2698, loss 0.146379, acc 0.9375\n",
      "2017-11-06T03:02:01.073961: step 2699, loss 0.138112, acc 0.9375\n",
      "2017-11-06T03:02:03.664801: step 2700, loss 0.00573964, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:02:06.312684: step 2700, loss 0.567626, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-06T03:02:11.690534: step 2701, loss 0.162865, acc 0.9375\n",
      "2017-11-06T03:02:15.674365: step 2702, loss 0.107754, acc 0.96875\n",
      "2017-11-06T03:02:19.649189: step 2703, loss 0.113095, acc 0.9375\n",
      "2017-11-06T03:02:23.725085: step 2704, loss 0.0750222, acc 0.96875\n",
      "2017-11-06T03:02:27.798980: step 2705, loss 0.146319, acc 0.9375\n",
      "2017-11-06T03:02:31.848858: step 2706, loss 0.086813, acc 0.9375\n",
      "2017-11-06T03:02:36.042838: step 2707, loss 0.0538457, acc 0.96875\n",
      "2017-11-06T03:02:40.006655: step 2708, loss 0.123598, acc 0.96875\n",
      "2017-11-06T03:02:44.017504: step 2709, loss 0.0397345, acc 0.96875\n",
      "2017-11-06T03:02:47.988325: step 2710, loss 0.118452, acc 0.96875\n",
      "2017-11-06T03:02:51.988167: step 2711, loss 0.0187933, acc 1\n",
      "2017-11-06T03:02:56.001019: step 2712, loss 0.253384, acc 0.875\n",
      "2017-11-06T03:03:00.147965: step 2713, loss 0.4018, acc 0.8125\n",
      "2017-11-06T03:03:04.507063: step 2714, loss 0.0774308, acc 0.9375\n",
      "2017-11-06T03:03:08.530922: step 2715, loss 0.0421181, acc 0.96875\n",
      "2017-11-06T03:03:12.528525: step 2716, loss 0.211636, acc 0.9375\n",
      "2017-11-06T03:03:16.582405: step 2717, loss 0.144502, acc 0.875\n",
      "2017-11-06T03:03:20.628280: step 2718, loss 0.133333, acc 0.9375\n",
      "2017-11-06T03:03:24.868293: step 2719, loss 0.0168322, acc 1\n",
      "2017-11-06T03:03:29.021243: step 2720, loss 0.235984, acc 0.9375\n",
      "2017-11-06T03:03:33.040098: step 2721, loss 0.0799845, acc 0.96875\n",
      "2017-11-06T03:03:37.025931: step 2722, loss 0.238555, acc 0.90625\n",
      "2017-11-06T03:03:41.185887: step 2723, loss 0.293402, acc 0.90625\n",
      "2017-11-06T03:03:45.229760: step 2724, loss 0.13584, acc 0.90625\n",
      "2017-11-06T03:03:49.364698: step 2725, loss 0.126371, acc 0.9375\n",
      "2017-11-06T03:03:53.486627: step 2726, loss 0.201484, acc 0.96875\n",
      "2017-11-06T03:03:57.486469: step 2727, loss 0.409646, acc 0.84375\n",
      "2017-11-06T03:04:01.600393: step 2728, loss 0.0884794, acc 0.9375\n",
      "2017-11-06T03:04:05.835402: step 2729, loss 0.241733, acc 0.875\n",
      "2017-11-06T03:04:10.271553: step 2730, loss 0.0784454, acc 0.96875\n",
      "2017-11-06T03:04:14.393482: step 2731, loss 0.0335642, acc 0.96875\n",
      "2017-11-06T03:04:18.410336: step 2732, loss 0.114068, acc 0.96875\n",
      "2017-11-06T03:04:22.614324: step 2733, loss 0.0206428, acc 1\n",
      "2017-11-06T03:04:26.643186: step 2734, loss 0.268663, acc 0.90625\n",
      "2017-11-06T03:04:30.745101: step 2735, loss 0.068203, acc 1\n",
      "2017-11-06T03:04:33.686191: step 2736, loss 0.584564, acc 0.9\n",
      "2017-11-06T03:04:38.039302: step 2737, loss 0.0456148, acc 0.96875\n",
      "2017-11-06T03:04:42.130209: step 2738, loss 0.043934, acc 1\n",
      "2017-11-06T03:04:46.236127: step 2739, loss 0.0167284, acc 1\n",
      "2017-11-06T03:04:50.549192: step 2740, loss 0.116225, acc 0.9375\n",
      "2017-11-06T03:04:54.753180: step 2741, loss 0.163514, acc 0.9375\n",
      "2017-11-06T03:04:58.964170: step 2742, loss 0.176863, acc 0.9375\n",
      "2017-11-06T03:05:03.204183: step 2743, loss 0.0707633, acc 0.9375\n",
      "2017-11-06T03:05:07.286083: step 2744, loss 0.107321, acc 0.96875\n",
      "2017-11-06T03:05:11.426026: step 2745, loss 0.0926287, acc 0.96875\n",
      "2017-11-06T03:05:15.869182: step 2746, loss 0.114904, acc 0.9375\n",
      "2017-11-06T03:05:19.871025: step 2747, loss 0.253205, acc 0.875\n",
      "2017-11-06T03:05:23.948923: step 2748, loss 0.297745, acc 0.875\n",
      "2017-11-06T03:05:27.911739: step 2749, loss 0.118452, acc 0.9375\n",
      "2017-11-06T03:05:31.900574: step 2750, loss 0.0321267, acc 0.96875\n",
      "2017-11-06T03:05:35.920430: step 2751, loss 0.0480388, acc 0.96875\n",
      "2017-11-06T03:05:39.995325: step 2752, loss 0.318162, acc 0.875\n",
      "2017-11-06T03:05:43.994166: step 2753, loss 0.0304942, acc 1\n",
      "2017-11-06T03:05:47.994009: step 2754, loss 0.0456682, acc 1\n",
      "2017-11-06T03:05:51.987846: step 2755, loss 0.193051, acc 0.90625\n",
      "2017-11-06T03:05:56.031719: step 2756, loss 0.0930774, acc 0.96875\n",
      "2017-11-06T03:06:00.063584: step 2757, loss 0.0628624, acc 0.96875\n",
      "2017-11-06T03:06:04.008390: step 2758, loss 0.217599, acc 0.90625\n",
      "2017-11-06T03:06:07.951190: step 2759, loss 0.0905177, acc 0.9375\n",
      "2017-11-06T03:06:11.991832: step 2760, loss 0.128077, acc 0.9375\n",
      "2017-11-06T03:06:16.094746: step 2761, loss 0.0582896, acc 0.96875\n",
      "2017-11-06T03:06:20.578933: step 2762, loss 0.0807015, acc 0.96875\n",
      "2017-11-06T03:06:24.646824: step 2763, loss 0.049845, acc 0.96875\n",
      "2017-11-06T03:06:28.641663: step 2764, loss 0.291594, acc 0.90625\n",
      "2017-11-06T03:06:32.703548: step 2765, loss 0.218585, acc 0.90625\n",
      "2017-11-06T03:06:36.803462: step 2766, loss 0.0851516, acc 0.96875\n",
      "2017-11-06T03:06:40.793298: step 2767, loss 0.0633872, acc 0.96875\n",
      "2017-11-06T03:06:44.805146: step 2768, loss 0.462251, acc 0.90625\n",
      "2017-11-06T03:06:48.751951: step 2769, loss 0.217184, acc 0.96875\n",
      "2017-11-06T03:06:52.782816: step 2770, loss 0.227744, acc 0.9375\n",
      "2017-11-06T03:06:56.862714: step 2771, loss 0.151849, acc 0.90625\n",
      "2017-11-06T03:06:59.372498: step 2772, loss 0.0778418, acc 0.95\n",
      "2017-11-06T03:07:03.298286: step 2773, loss 0.0102259, acc 1\n",
      "2017-11-06T03:07:07.206064: step 2774, loss 0.0380102, acc 1\n",
      "2017-11-06T03:07:11.124848: step 2775, loss 0.385827, acc 0.8125\n",
      "2017-11-06T03:07:15.053640: step 2776, loss 0.0187654, acc 1\n",
      "2017-11-06T03:07:19.014512: step 2777, loss 0.0581135, acc 0.96875\n",
      "2017-11-06T03:07:23.118428: step 2778, loss 0.124043, acc 0.90625\n",
      "2017-11-06T03:07:27.383458: step 2779, loss 0.195623, acc 0.9375\n",
      "2017-11-06T03:07:31.328260: step 2780, loss 0.279345, acc 0.90625\n",
      "2017-11-06T03:07:35.304085: step 2781, loss 0.217082, acc 0.90625\n",
      "2017-11-06T03:07:39.259897: step 2782, loss 0.13607, acc 0.96875\n",
      "2017-11-06T03:07:43.168675: step 2783, loss 0.0485334, acc 0.96875\n",
      "2017-11-06T03:07:47.059439: step 2784, loss 0.0341972, acc 0.96875\n",
      "2017-11-06T03:07:51.018252: step 2785, loss 0.0807318, acc 0.9375\n",
      "2017-11-06T03:07:54.952047: step 2786, loss 0.0357036, acc 1\n",
      "2017-11-06T03:07:58.957892: step 2787, loss 0.0576077, acc 0.96875\n",
      "2017-11-06T03:08:02.883682: step 2788, loss 0.0104173, acc 1\n",
      "2017-11-06T03:08:06.758435: step 2789, loss 0.130447, acc 0.96875\n",
      "2017-11-06T03:08:10.655206: step 2790, loss 0.0953483, acc 0.9375\n",
      "2017-11-06T03:08:14.648042: step 2791, loss 0.0765284, acc 0.96875\n",
      "2017-11-06T03:08:18.562824: step 2792, loss 0.0458806, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T03:08:22.492615: step 2793, loss 0.234847, acc 0.96875\n",
      "2017-11-06T03:08:26.410399: step 2794, loss 0.254471, acc 0.875\n",
      "2017-11-06T03:08:30.674429: step 2795, loss 0.00425581, acc 1\n",
      "2017-11-06T03:08:34.948466: step 2796, loss 0.0260446, acc 1\n",
      "2017-11-06T03:08:38.906280: step 2797, loss 0.0125942, acc 1\n",
      "2017-11-06T03:08:42.797043: step 2798, loss 0.437953, acc 0.84375\n",
      "2017-11-06T03:08:46.737843: step 2799, loss 0.204815, acc 0.9375\n",
      "2017-11-06T03:08:50.628607: step 2800, loss 0.0294895, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:08:53.181421: step 2800, loss 0.500056, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-06T03:08:58.608276: step 2801, loss 0.173822, acc 0.9375\n",
      "2017-11-06T03:09:02.571093: step 2802, loss 0.351211, acc 0.84375\n",
      "2017-11-06T03:09:06.522900: step 2803, loss 0.141087, acc 0.9375\n",
      "2017-11-06T03:09:10.471707: step 2804, loss 0.193346, acc 0.90625\n",
      "2017-11-06T03:09:14.435609: step 2805, loss 0.0441943, acc 0.96875\n",
      "2017-11-06T03:09:18.412435: step 2806, loss 0.0754756, acc 0.9375\n",
      "2017-11-06T03:09:22.487331: step 2807, loss 0.275093, acc 0.8125\n",
      "2017-11-06T03:09:25.228277: step 2808, loss 0.19795, acc 0.95\n",
      "2017-11-06T03:09:29.320186: step 2809, loss 0.269957, acc 0.875\n",
      "2017-11-06T03:09:33.286002: step 2810, loss 0.124705, acc 0.96875\n",
      "2017-11-06T03:09:37.772191: step 2811, loss 0.181746, acc 0.9375\n",
      "2017-11-06T03:09:41.739009: step 2812, loss 0.0712032, acc 0.96875\n",
      "2017-11-06T03:09:45.725842: step 2813, loss 0.0682474, acc 0.96875\n",
      "2017-11-06T03:09:49.670644: step 2814, loss 0.0309341, acc 1\n",
      "2017-11-06T03:09:53.652474: step 2815, loss 0.255429, acc 0.875\n",
      "2017-11-06T03:09:57.602282: step 2816, loss 0.172226, acc 0.9375\n",
      "2017-11-06T03:10:01.882321: step 2817, loss 0.0225624, acc 1\n",
      "2017-11-06T03:10:05.823121: step 2818, loss 0.320021, acc 0.84375\n",
      "2017-11-06T03:10:09.770927: step 2819, loss 0.0280015, acc 0.96875\n",
      "2017-11-06T03:10:13.748755: step 2820, loss 0.0875813, acc 0.96875\n",
      "2017-11-06T03:10:17.698560: step 2821, loss 0.0853854, acc 0.9375\n",
      "2017-11-06T03:10:21.692398: step 2822, loss 0.187993, acc 0.9375\n",
      "2017-11-06T03:10:25.632198: step 2823, loss 0.0309718, acc 0.96875\n",
      "2017-11-06T03:10:29.591010: step 2824, loss 0.189308, acc 0.90625\n",
      "2017-11-06T03:10:33.682918: step 2825, loss 0.0107862, acc 1\n",
      "2017-11-06T03:10:37.788837: step 2826, loss 0.128108, acc 0.9375\n",
      "2017-11-06T03:10:42.145931: step 2827, loss 0.168041, acc 0.90625\n",
      "2017-11-06T03:10:46.183799: step 2828, loss 0.258479, acc 0.875\n",
      "2017-11-06T03:10:50.157623: step 2829, loss 0.0859505, acc 0.96875\n",
      "2017-11-06T03:10:54.137451: step 2830, loss 0.276916, acc 0.875\n",
      "2017-11-06T03:10:58.128287: step 2831, loss 0.0514276, acc 0.96875\n",
      "2017-11-06T03:11:02.072089: step 2832, loss 0.127365, acc 0.9375\n",
      "2017-11-06T03:11:06.042910: step 2833, loss 0.0923062, acc 0.9375\n",
      "2017-11-06T03:11:09.984711: step 2834, loss 0.24789, acc 0.90625\n",
      "2017-11-06T03:11:13.902495: step 2835, loss 0.17937, acc 0.90625\n",
      "2017-11-06T03:11:17.844295: step 2836, loss 0.229975, acc 0.90625\n",
      "2017-11-06T03:11:21.808112: step 2837, loss 0.0290301, acc 0.96875\n",
      "2017-11-06T03:11:25.841979: step 2838, loss 0.0515086, acc 0.96875\n",
      "2017-11-06T03:11:29.722736: step 2839, loss 0.0468428, acc 0.96875\n",
      "2017-11-06T03:11:33.624510: step 2840, loss 0.136714, acc 0.9375\n",
      "2017-11-06T03:11:37.598332: step 2841, loss 0.448561, acc 0.875\n",
      "2017-11-06T03:11:41.623191: step 2842, loss 0.0086983, acc 1\n",
      "2017-11-06T03:11:45.708094: step 2843, loss 0.231883, acc 0.96875\n",
      "2017-11-06T03:11:48.527097: step 2844, loss 0.140166, acc 0.9\n",
      "2017-11-06T03:11:52.591985: step 2845, loss 0.198184, acc 0.9375\n",
      "2017-11-06T03:11:56.573814: step 2846, loss 0.0840491, acc 0.96875\n",
      "2017-11-06T03:12:00.510612: step 2847, loss 0.0497742, acc 0.96875\n",
      "2017-11-06T03:12:04.461421: step 2848, loss 0.25171, acc 0.9375\n",
      "2017-11-06T03:12:08.411225: step 2849, loss 0.177222, acc 0.90625\n",
      "2017-11-06T03:12:12.324006: step 2850, loss 0.0157248, acc 1\n",
      "2017-11-06T03:12:16.285568: step 2851, loss 0.218048, acc 0.90625\n",
      "2017-11-06T03:12:20.227369: step 2852, loss 0.0835752, acc 0.9375\n",
      "2017-11-06T03:12:24.198192: step 2853, loss 0.110833, acc 0.9375\n",
      "2017-11-06T03:12:28.373157: step 2854, loss 0.129438, acc 0.90625\n",
      "2017-11-06T03:12:32.335973: step 2855, loss 0.0201236, acc 1\n",
      "2017-11-06T03:12:36.448895: step 2856, loss 0.0894754, acc 0.96875\n",
      "2017-11-06T03:12:40.469752: step 2857, loss 0.118898, acc 0.90625\n",
      "2017-11-06T03:12:44.354512: step 2858, loss 0.114939, acc 0.9375\n",
      "2017-11-06T03:12:48.297314: step 2859, loss 0.261443, acc 0.875\n",
      "2017-11-06T03:12:52.544331: step 2860, loss 0.278093, acc 0.96875\n",
      "2017-11-06T03:12:56.694279: step 2861, loss 0.0903511, acc 0.96875\n",
      "2017-11-06T03:13:00.682113: step 2862, loss 0.101203, acc 0.96875\n",
      "2017-11-06T03:13:04.658941: step 2863, loss 0.18155, acc 0.90625\n",
      "2017-11-06T03:13:08.593735: step 2864, loss 0.0273354, acc 0.96875\n",
      "2017-11-06T03:13:12.510518: step 2865, loss 0.31316, acc 0.8125\n",
      "2017-11-06T03:13:16.454320: step 2866, loss 0.141151, acc 0.9375\n",
      "2017-11-06T03:13:20.483184: step 2867, loss 0.129621, acc 0.9375\n",
      "2017-11-06T03:13:24.625126: step 2868, loss 0.459388, acc 0.84375\n",
      "2017-11-06T03:13:28.599950: step 2869, loss 0.0808317, acc 0.96875\n",
      "2017-11-06T03:13:32.533745: step 2870, loss 0.0830758, acc 0.9375\n",
      "2017-11-06T03:13:36.545595: step 2871, loss 0.0376373, acc 0.96875\n",
      "2017-11-06T03:13:40.580464: step 2872, loss 0.041232, acc 1\n",
      "2017-11-06T03:13:44.552285: step 2873, loss 0.20608, acc 0.9375\n",
      "2017-11-06T03:13:48.494086: step 2874, loss 0.141236, acc 0.9375\n",
      "2017-11-06T03:13:52.429884: step 2875, loss 0.101257, acc 0.9375\n",
      "2017-11-06T03:13:56.574827: step 2876, loss 0.049395, acc 0.96875\n",
      "2017-11-06T03:14:00.901902: step 2877, loss 0.0429619, acc 0.96875\n",
      "2017-11-06T03:14:04.821687: step 2878, loss 0.0300063, acc 1\n",
      "2017-11-06T03:14:08.831537: step 2879, loss 0.283064, acc 0.9375\n",
      "2017-11-06T03:14:11.410369: step 2880, loss 0.256905, acc 0.9\n",
      "2017-11-06T03:14:15.353171: step 2881, loss 0.0100727, acc 1\n",
      "2017-11-06T03:14:19.340003: step 2882, loss 0.0620125, acc 0.96875\n",
      "2017-11-06T03:14:23.307823: step 2883, loss 0.261002, acc 0.875\n",
      "2017-11-06T03:14:27.259632: step 2884, loss 0.102432, acc 0.9375\n",
      "2017-11-06T03:14:31.227451: step 2885, loss 0.0513479, acc 0.96875\n",
      "2017-11-06T03:14:35.455455: step 2886, loss 0.13226, acc 0.96875\n",
      "2017-11-06T03:14:39.504330: step 2887, loss 0.0989714, acc 0.96875\n",
      "2017-11-06T03:14:43.490164: step 2888, loss 0.146013, acc 0.9375\n",
      "2017-11-06T03:14:47.380928: step 2889, loss 0.15911, acc 0.9375\n",
      "2017-11-06T03:14:51.349747: step 2890, loss 0.0182366, acc 1\n",
      "2017-11-06T03:14:55.287545: step 2891, loss 0.22269, acc 0.90625\n",
      "2017-11-06T03:14:59.217338: step 2892, loss 0.412499, acc 0.78125\n",
      "2017-11-06T03:15:03.365285: step 2893, loss 0.237086, acc 0.875\n",
      "2017-11-06T03:15:07.604316: step 2894, loss 0.104569, acc 0.96875\n",
      "2017-11-06T03:15:11.600136: step 2895, loss 0.270981, acc 0.90625\n",
      "2017-11-06T03:15:15.533728: step 2896, loss 0.00734289, acc 1\n",
      "2017-11-06T03:15:19.519561: step 2897, loss 0.122163, acc 0.96875\n",
      "2017-11-06T03:15:23.476372: step 2898, loss 0.0297571, acc 0.96875\n",
      "2017-11-06T03:15:27.425179: step 2899, loss 0.220904, acc 0.9375\n",
      "2017-11-06T03:15:31.407007: step 2900, loss 0.065497, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:15:33.952816: step 2900, loss 0.733815, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-06T03:15:39.396953: step 2901, loss 0.0693988, acc 0.96875\n",
      "2017-11-06T03:15:43.475851: step 2902, loss 0.234227, acc 0.84375\n",
      "2017-11-06T03:15:47.404642: step 2903, loss 0.166553, acc 0.9375\n",
      "2017-11-06T03:15:51.370460: step 2904, loss 0.0309286, acc 1\n",
      "2017-11-06T03:15:55.326271: step 2905, loss 0.270648, acc 0.90625\n",
      "2017-11-06T03:15:59.378149: step 2906, loss 0.13922, acc 0.90625\n",
      "2017-11-06T03:16:03.399006: step 2907, loss 0.0250648, acc 1\n",
      "2017-11-06T03:16:07.665038: step 2908, loss 0.124287, acc 0.96875\n",
      "2017-11-06T03:16:12.066165: step 2909, loss 0.126906, acc 0.9375\n",
      "2017-11-06T03:16:16.111040: step 2910, loss 0.225808, acc 0.96875\n",
      "2017-11-06T03:16:20.095871: step 2911, loss 0.199214, acc 0.90625\n",
      "2017-11-06T03:16:24.073697: step 2912, loss 0.38391, acc 0.875\n",
      "2017-11-06T03:16:28.042517: step 2913, loss 0.106362, acc 0.9375\n",
      "2017-11-06T03:16:32.011338: step 2914, loss 0.0974263, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T03:16:36.233337: step 2915, loss 0.0823714, acc 0.9375\n",
      "2017-11-06T03:16:38.820176: step 2916, loss 0.233092, acc 0.9\n",
      "2017-11-06T03:16:42.849038: step 2917, loss 0.337905, acc 0.84375\n",
      "2017-11-06T03:16:46.781832: step 2918, loss 0.166115, acc 0.9375\n",
      "2017-11-06T03:16:50.744649: step 2919, loss 0.112452, acc 0.96875\n",
      "2017-11-06T03:16:54.663433: step 2920, loss 0.142866, acc 0.9375\n",
      "2017-11-06T03:16:58.607235: step 2921, loss 0.0992436, acc 0.96875\n",
      "2017-11-06T03:17:02.608077: step 2922, loss 0.135757, acc 0.9375\n",
      "2017-11-06T03:17:06.536870: step 2923, loss 0.0367009, acc 0.96875\n",
      "2017-11-06T03:17:10.485675: step 2924, loss 0.144299, acc 0.9375\n",
      "2017-11-06T03:17:14.730692: step 2925, loss 0.26788, acc 0.9375\n",
      "2017-11-06T03:17:18.987718: step 2926, loss 0.0225286, acc 1\n",
      "2017-11-06T03:17:22.970546: step 2927, loss 0.261851, acc 0.9375\n",
      "2017-11-06T03:17:26.937364: step 2928, loss 0.248661, acc 0.875\n",
      "2017-11-06T03:17:30.886170: step 2929, loss 0.347427, acc 0.90625\n",
      "2017-11-06T03:17:34.831974: step 2930, loss 0.141746, acc 0.96875\n",
      "2017-11-06T03:17:38.880850: step 2931, loss 0.243632, acc 0.90625\n",
      "2017-11-06T03:17:42.838663: step 2932, loss 0.114433, acc 0.96875\n",
      "2017-11-06T03:17:46.772458: step 2933, loss 0.0851637, acc 0.96875\n",
      "2017-11-06T03:17:50.743279: step 2934, loss 0.157863, acc 0.9375\n",
      "2017-11-06T03:17:54.658061: step 2935, loss 0.183711, acc 0.90625\n",
      "2017-11-06T03:17:58.656902: step 2936, loss 0.121065, acc 0.9375\n",
      "2017-11-06T03:18:02.661748: step 2937, loss 0.0805515, acc 0.96875\n",
      "2017-11-06T03:18:06.611555: step 2938, loss 0.101053, acc 0.9375\n",
      "2017-11-06T03:18:10.652426: step 2939, loss 0.121899, acc 0.90625\n",
      "2017-11-06T03:18:14.596027: step 2940, loss 0.138761, acc 0.875\n",
      "2017-11-06T03:18:18.748978: step 2941, loss 0.0301636, acc 1\n",
      "2017-11-06T03:18:23.225158: step 2942, loss 0.116476, acc 0.9375\n",
      "2017-11-06T03:18:27.525214: step 2943, loss 0.202806, acc 0.9375\n",
      "2017-11-06T03:18:31.448001: step 2944, loss 0.136494, acc 0.96875\n",
      "2017-11-06T03:18:35.649986: step 2945, loss 0.0610163, acc 0.96875\n",
      "2017-11-06T03:18:39.656833: step 2946, loss 0.142022, acc 0.9375\n",
      "2017-11-06T03:18:43.657676: step 2947, loss 0.201599, acc 0.90625\n",
      "2017-11-06T03:18:47.588469: step 2948, loss 0.0871894, acc 0.96875\n",
      "2017-11-06T03:18:51.565294: step 2949, loss 0.0753765, acc 0.96875\n",
      "2017-11-06T03:18:55.530112: step 2950, loss 0.0999282, acc 0.96875\n",
      "2017-11-06T03:18:59.597002: step 2951, loss 0.102645, acc 0.9375\n",
      "2017-11-06T03:19:02.121796: step 2952, loss 0.0741717, acc 0.95\n",
      "2017-11-06T03:19:06.155663: step 2953, loss 0.082554, acc 0.96875\n",
      "2017-11-06T03:19:10.122481: step 2954, loss 0.0156042, acc 1\n",
      "2017-11-06T03:19:14.122323: step 2955, loss 0.124512, acc 0.9375\n",
      "2017-11-06T03:19:18.152187: step 2956, loss 0.220528, acc 0.875\n",
      "2017-11-06T03:19:22.136017: step 2957, loss 0.22176, acc 0.90625\n",
      "2017-11-06T03:19:26.464111: step 2958, loss 0.0905858, acc 0.9375\n",
      "2017-11-06T03:19:30.662079: step 2959, loss 0.147988, acc 0.90625\n",
      "2017-11-06T03:19:34.660916: step 2960, loss 0.0691597, acc 0.96875\n",
      "2017-11-06T03:19:38.595712: step 2961, loss 0.0696815, acc 0.96875\n",
      "2017-11-06T03:19:42.605561: step 2962, loss 0.258088, acc 0.875\n",
      "2017-11-06T03:19:46.579385: step 2963, loss 0.0467698, acc 0.96875\n",
      "2017-11-06T03:19:50.567219: step 2964, loss 0.0654885, acc 0.9375\n",
      "2017-11-06T03:19:54.537039: step 2965, loss 0.340546, acc 0.875\n",
      "2017-11-06T03:19:58.539883: step 2966, loss 0.318135, acc 0.90625\n",
      "2017-11-06T03:20:02.861956: step 2967, loss 0.159415, acc 0.9375\n",
      "2017-11-06T03:20:06.833776: step 2968, loss 0.0332729, acc 1\n",
      "2017-11-06T03:20:10.834620: step 2969, loss 0.219478, acc 0.90625\n",
      "2017-11-06T03:20:14.911516: step 2970, loss 0.19047, acc 0.90625\n",
      "2017-11-06T03:20:18.926370: step 2971, loss 0.175019, acc 0.96875\n",
      "2017-11-06T03:20:22.921207: step 2972, loss 0.0485586, acc 0.96875\n",
      "2017-11-06T03:20:26.906038: step 2973, loss 0.120497, acc 0.96875\n",
      "2017-11-06T03:20:31.104021: step 2974, loss 0.123074, acc 0.9375\n",
      "2017-11-06T03:20:35.537171: step 2975, loss 0.132841, acc 0.96875\n",
      "2017-11-06T03:20:39.580046: step 2976, loss 0.0617341, acc 0.96875\n",
      "2017-11-06T03:20:43.638928: step 2977, loss 0.365879, acc 0.875\n",
      "2017-11-06T03:20:47.542702: step 2978, loss 0.124731, acc 0.96875\n",
      "2017-11-06T03:20:51.443474: step 2979, loss 0.232812, acc 0.9375\n",
      "2017-11-06T03:20:55.433308: step 2980, loss 0.0878967, acc 0.9375\n",
      "2017-11-06T03:20:59.376112: step 2981, loss 0.32246, acc 0.875\n",
      "2017-11-06T03:21:03.412978: step 2982, loss 0.087587, acc 0.9375\n",
      "2017-11-06T03:21:07.454850: step 2983, loss 0.130957, acc 0.90625\n",
      "2017-11-06T03:21:11.436680: step 2984, loss 0.213505, acc 0.90625\n",
      "2017-11-06T03:21:15.439281: step 2985, loss 0.142247, acc 0.9375\n",
      "2017-11-06T03:21:19.394091: step 2986, loss 0.33677, acc 0.875\n",
      "2017-11-06T03:21:23.405941: step 2987, loss 0.100651, acc 0.96875\n",
      "2017-11-06T03:21:26.015797: step 2988, loss 0.043367, acc 1\n",
      "2017-11-06T03:21:30.048661: step 2989, loss 0.168421, acc 0.9375\n",
      "2017-11-06T03:21:34.026488: step 2990, loss 0.216666, acc 0.90625\n",
      "2017-11-06T03:21:38.333548: step 2991, loss 0.0207779, acc 1\n",
      "2017-11-06T03:21:42.520524: step 2992, loss 0.129951, acc 0.9375\n",
      "2017-11-06T03:21:46.524368: step 2993, loss 0.0180019, acc 1\n",
      "2017-11-06T03:21:50.545224: step 2994, loss 0.131656, acc 0.9375\n",
      "2017-11-06T03:21:54.519049: step 2995, loss 0.0879405, acc 0.96875\n",
      "2017-11-06T03:21:58.516889: step 2996, loss 0.037767, acc 1\n",
      "2017-11-06T03:22:02.541749: step 2997, loss 0.0291969, acc 0.96875\n",
      "2017-11-06T03:22:06.448525: step 2998, loss 0.0584906, acc 0.9375\n",
      "2017-11-06T03:22:10.413342: step 2999, loss 0.177835, acc 0.90625\n",
      "2017-11-06T03:22:14.357144: step 3000, loss 0.28126, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:22:16.948986: step 3000, loss 0.827907, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-06T03:22:22.271015: step 3001, loss 0.0898551, acc 0.9375\n",
      "2017-11-06T03:22:26.350912: step 3002, loss 0.207394, acc 0.9375\n",
      "2017-11-06T03:22:30.253686: step 3003, loss 0.366376, acc 0.90625\n",
      "2017-11-06T03:22:34.398631: step 3004, loss 0.0202869, acc 1\n",
      "2017-11-06T03:22:38.384463: step 3005, loss 0.154199, acc 0.9375\n",
      "2017-11-06T03:22:42.633482: step 3006, loss 0.0154321, acc 1\n",
      "2017-11-06T03:22:46.950552: step 3007, loss 0.0222776, acc 1\n",
      "2017-11-06T03:22:50.998427: step 3008, loss 0.143977, acc 0.9375\n",
      "2017-11-06T03:22:55.006274: step 3009, loss 0.0625959, acc 0.96875\n",
      "2017-11-06T03:22:58.961084: step 3010, loss 0.208776, acc 0.9375\n",
      "2017-11-06T03:23:02.968932: step 3011, loss 0.469038, acc 0.84375\n",
      "2017-11-06T03:23:06.956765: step 3012, loss 0.0821815, acc 0.96875\n",
      "2017-11-06T03:23:10.941596: step 3013, loss 0.128534, acc 0.96875\n",
      "2017-11-06T03:23:14.967457: step 3014, loss 0.459866, acc 0.8125\n",
      "2017-11-06T03:23:18.975304: step 3015, loss 0.0834621, acc 0.96875\n",
      "2017-11-06T03:23:23.182295: step 3016, loss 0.092241, acc 0.96875\n",
      "2017-11-06T03:23:27.396289: step 3017, loss 0.276561, acc 0.875\n",
      "2017-11-06T03:23:31.389125: step 3018, loss 0.00745034, acc 1\n",
      "2017-11-06T03:23:35.386965: step 3019, loss 0.203855, acc 0.90625\n",
      "2017-11-06T03:23:39.374799: step 3020, loss 0.2555, acc 0.875\n",
      "2017-11-06T03:23:43.546462: step 3021, loss 0.0631946, acc 0.96875\n",
      "2017-11-06T03:23:47.695410: step 3022, loss 0.316019, acc 0.875\n",
      "2017-11-06T03:23:51.986459: step 3023, loss 0.0911125, acc 0.9375\n",
      "2017-11-06T03:23:54.554283: step 3024, loss 0.195734, acc 0.95\n",
      "2017-11-06T03:23:58.556128: step 3025, loss 0.212173, acc 0.90625\n",
      "2017-11-06T03:24:02.565976: step 3026, loss 0.138349, acc 0.9375\n",
      "2017-11-06T03:24:06.594840: step 3027, loss 0.0214512, acc 1\n",
      "2017-11-06T03:24:10.554653: step 3028, loss 0.0327258, acc 0.96875\n",
      "2017-11-06T03:24:14.588336: step 3029, loss 0.162008, acc 0.9375\n",
      "2017-11-06T03:24:18.640216: step 3030, loss 0.154564, acc 0.9375\n",
      "2017-11-06T03:24:22.702103: step 3031, loss 0.108677, acc 0.96875\n",
      "2017-11-06T03:24:26.761986: step 3032, loss 0.0154379, acc 1\n",
      "2017-11-06T03:24:30.752822: step 3033, loss 0.156926, acc 0.9375\n",
      "2017-11-06T03:24:35.046874: step 3034, loss 0.125908, acc 0.96875\n",
      "2017-11-06T03:24:39.176808: step 3035, loss 0.0757428, acc 0.96875\n",
      "2017-11-06T03:24:43.174649: step 3036, loss 0.0537181, acc 0.96875\n",
      "2017-11-06T03:24:47.177492: step 3037, loss 0.00965989, acc 1\n",
      "2017-11-06T03:24:51.172333: step 3038, loss 0.147308, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T03:24:55.518419: step 3039, loss 0.314316, acc 0.90625\n",
      "2017-11-06T03:24:59.683378: step 3040, loss 0.0475403, acc 1\n",
      "2017-11-06T03:25:03.641191: step 3041, loss 0.178795, acc 0.90625\n",
      "2017-11-06T03:25:07.616015: step 3042, loss 0.0824038, acc 0.96875\n",
      "2017-11-06T03:25:11.676900: step 3043, loss 0.109795, acc 0.9375\n",
      "2017-11-06T03:25:15.706764: step 3044, loss 0.170078, acc 0.90625\n",
      "2017-11-06T03:25:19.692597: step 3045, loss 0.142675, acc 0.9375\n",
      "2017-11-06T03:25:23.748478: step 3046, loss 0.111183, acc 0.96875\n",
      "2017-11-06T03:25:27.734310: step 3047, loss 0.0449114, acc 1\n",
      "2017-11-06T03:25:31.748162: step 3048, loss 0.230547, acc 0.9375\n",
      "2017-11-06T03:25:35.695967: step 3049, loss 0.0721817, acc 0.96875\n",
      "2017-11-06T03:25:39.672792: step 3050, loss 0.321533, acc 0.90625\n",
      "2017-11-06T03:25:43.700655: step 3051, loss 0.128019, acc 0.96875\n",
      "2017-11-06T03:25:47.709503: step 3052, loss 0.18579, acc 0.90625\n",
      "2017-11-06T03:25:51.697336: step 3053, loss 0.209835, acc 0.90625\n",
      "2017-11-06T03:25:55.657151: step 3054, loss 0.33224, acc 0.90625\n",
      "2017-11-06T03:25:59.881151: step 3055, loss 0.157854, acc 0.9375\n",
      "2017-11-06T03:26:04.221236: step 3056, loss 0.124432, acc 0.9375\n",
      "2017-11-06T03:26:08.219076: step 3057, loss 0.098488, acc 0.9375\n",
      "2017-11-06T03:26:12.187896: step 3058, loss 0.147024, acc 0.90625\n",
      "2017-11-06T03:26:16.180733: step 3059, loss 0.427324, acc 0.875\n",
      "2017-11-06T03:26:18.782582: step 3060, loss 0.411319, acc 0.9\n",
      "2017-11-06T03:26:22.845470: step 3061, loss 0.0184455, acc 1\n",
      "2017-11-06T03:26:26.862323: step 3062, loss 0.255134, acc 0.90625\n",
      "2017-11-06T03:26:30.791115: step 3063, loss 0.0495415, acc 0.96875\n",
      "2017-11-06T03:26:34.996102: step 3064, loss 0.0193759, acc 1\n",
      "2017-11-06T03:26:38.979933: step 3065, loss 0.16997, acc 0.90625\n",
      "2017-11-06T03:26:42.978776: step 3066, loss 0.0940357, acc 0.9375\n",
      "2017-11-06T03:26:47.000633: step 3067, loss 0.146808, acc 0.9375\n",
      "2017-11-06T03:26:51.001474: step 3068, loss 0.0114197, acc 1\n",
      "2017-11-06T03:26:55.027336: step 3069, loss 0.0114994, acc 1\n",
      "2017-11-06T03:26:58.967136: step 3070, loss 0.231965, acc 0.90625\n",
      "2017-11-06T03:27:03.002002: step 3071, loss 0.067044, acc 0.9375\n",
      "2017-11-06T03:27:07.443157: step 3072, loss 0.0739454, acc 0.9375\n",
      "2017-11-06T03:27:11.553078: step 3073, loss 0.0613107, acc 0.9375\n",
      "2017-11-06T03:27:15.533732: step 3074, loss 0.191404, acc 0.9375\n",
      "2017-11-06T03:27:19.524569: step 3075, loss 0.299374, acc 0.875\n",
      "2017-11-06T03:27:23.582451: step 3076, loss 0.0737607, acc 0.96875\n",
      "2017-11-06T03:27:27.573288: step 3077, loss 0.148335, acc 0.90625\n",
      "2017-11-06T03:27:31.515089: step 3078, loss 0.0977825, acc 0.96875\n",
      "2017-11-06T03:27:35.455888: step 3079, loss 0.0577726, acc 0.9375\n",
      "2017-11-06T03:27:39.440719: step 3080, loss 0.0637736, acc 0.96875\n",
      "2017-11-06T03:27:43.423549: step 3081, loss 0.255091, acc 0.90625\n",
      "2017-11-06T03:27:47.436401: step 3082, loss 0.0964351, acc 0.90625\n",
      "2017-11-06T03:27:51.440245: step 3083, loss 0.172542, acc 0.96875\n",
      "2017-11-06T03:27:55.430099: step 3084, loss 0.0176089, acc 1\n",
      "2017-11-06T03:27:59.421916: step 3085, loss 0.118893, acc 0.9375\n",
      "2017-11-06T03:28:03.392740: step 3086, loss 0.212221, acc 0.90625\n",
      "2017-11-06T03:28:07.446618: step 3087, loss 0.284419, acc 0.90625\n",
      "2017-11-06T03:28:11.737667: step 3088, loss 0.10248, acc 0.96875\n",
      "2017-11-06T03:28:15.889617: step 3089, loss 0.329054, acc 0.84375\n",
      "2017-11-06T03:28:19.941497: step 3090, loss 0.322704, acc 0.84375\n",
      "2017-11-06T03:28:23.967357: step 3091, loss 0.192274, acc 0.9375\n",
      "2017-11-06T03:28:27.929172: step 3092, loss 0.141556, acc 0.90625\n",
      "2017-11-06T03:28:31.906998: step 3093, loss 0.0513817, acc 0.96875\n",
      "2017-11-06T03:28:36.174031: step 3094, loss 0.298346, acc 0.875\n",
      "2017-11-06T03:28:40.154859: step 3095, loss 0.0886296, acc 0.96875\n",
      "2017-11-06T03:28:42.733693: step 3096, loss 0.0104868, acc 1\n",
      "2017-11-06T03:28:46.699509: step 3097, loss 0.0628175, acc 0.9375\n",
      "2017-11-06T03:28:50.681338: step 3098, loss 0.226563, acc 0.90625\n",
      "2017-11-06T03:28:54.672174: step 3099, loss 0.148617, acc 0.96875\n",
      "2017-11-06T03:28:58.665011: step 3100, loss 0.262735, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:29:01.331906: step 3100, loss 0.583185, acc 0.866667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-06T03:29:06.577954: step 3101, loss 0.163234, acc 0.90625\n",
      "2017-11-06T03:29:10.599812: step 3102, loss 0.134305, acc 0.9375\n",
      "2017-11-06T03:29:14.727745: step 3103, loss 0.155331, acc 0.90625\n",
      "2017-11-06T03:29:19.162896: step 3104, loss 0.116491, acc 0.96875\n",
      "2017-11-06T03:29:23.335861: step 3105, loss 0.164722, acc 0.90625\n",
      "2017-11-06T03:29:27.631914: step 3106, loss 0.13842, acc 0.90625\n",
      "2017-11-06T03:29:31.636759: step 3107, loss 0.214261, acc 0.875\n",
      "2017-11-06T03:29:35.617588: step 3108, loss 0.0749999, acc 0.9375\n",
      "2017-11-06T03:29:39.676471: step 3109, loss 0.00678595, acc 1\n",
      "2017-11-06T03:29:43.691325: step 3110, loss 0.12744, acc 0.96875\n",
      "2017-11-06T03:29:47.990223: step 3111, loss 0.251221, acc 0.90625\n",
      "2017-11-06T03:29:51.976057: step 3112, loss 0.0471663, acc 0.96875\n",
      "2017-11-06T03:29:55.985907: step 3113, loss 0.167302, acc 0.9375\n",
      "2017-11-06T03:29:59.975740: step 3114, loss 0.0942231, acc 0.96875\n",
      "2017-11-06T03:30:04.156710: step 3115, loss 0.126128, acc 0.9375\n",
      "2017-11-06T03:30:08.118526: step 3116, loss 0.241974, acc 0.875\n",
      "2017-11-06T03:30:12.203428: step 3117, loss 0.141934, acc 0.96875\n",
      "2017-11-06T03:30:16.210081: step 3118, loss 0.0310333, acc 1\n",
      "2017-11-06T03:30:20.234941: step 3119, loss 0.0128758, acc 1\n",
      "2017-11-06T03:30:24.715124: step 3120, loss 0.1206, acc 0.90625\n",
      "2017-11-06T03:30:28.750992: step 3121, loss 0.0653138, acc 0.96875\n",
      "2017-11-06T03:30:32.824887: step 3122, loss 0.293659, acc 0.875\n",
      "2017-11-06T03:30:36.870762: step 3123, loss 0.0134421, acc 1\n",
      "2017-11-06T03:30:40.907629: step 3124, loss 0.221326, acc 0.9375\n",
      "2017-11-06T03:30:44.937493: step 3125, loss 0.0790247, acc 0.9375\n",
      "2017-11-06T03:30:48.901310: step 3126, loss 0.110795, acc 0.9375\n",
      "2017-11-06T03:30:52.906155: step 3127, loss 0.248108, acc 0.90625\n",
      "2017-11-06T03:30:56.853960: step 3128, loss 0.182797, acc 0.875\n",
      "2017-11-06T03:31:00.811773: step 3129, loss 0.118817, acc 0.96875\n",
      "2017-11-06T03:31:04.816618: step 3130, loss 0.211784, acc 0.96875\n",
      "2017-11-06T03:31:08.911527: step 3131, loss 0.146768, acc 0.9375\n",
      "2017-11-06T03:31:11.503369: step 3132, loss 0.534775, acc 0.9\n",
      "2017-11-06T03:31:15.577264: step 3133, loss 0.0772496, acc 0.96875\n",
      "2017-11-06T03:31:19.541081: step 3134, loss 0.0805893, acc 0.96875\n",
      "2017-11-06T03:31:23.589957: step 3135, loss 0.0642358, acc 0.96875\n",
      "2017-11-06T03:31:27.846982: step 3136, loss 0.089898, acc 0.96875\n",
      "2017-11-06T03:31:32.042963: step 3137, loss 0.0875055, acc 0.96875\n",
      "2017-11-06T03:31:36.205922: step 3138, loss 0.0763796, acc 0.96875\n",
      "2017-11-06T03:31:40.351868: step 3139, loss 0.152925, acc 0.96875\n",
      "2017-11-06T03:31:44.351709: step 3140, loss 0.412961, acc 0.875\n",
      "2017-11-06T03:31:48.336540: step 3141, loss 0.123962, acc 0.9375\n",
      "2017-11-06T03:31:52.339385: step 3142, loss 0.109738, acc 0.96875\n",
      "2017-11-06T03:31:56.358240: step 3143, loss 0.0123833, acc 1\n",
      "2017-11-06T03:32:00.339069: step 3144, loss 0.0173162, acc 1\n",
      "2017-11-06T03:32:04.415967: step 3145, loss 0.125415, acc 0.96875\n",
      "2017-11-06T03:32:08.412806: step 3146, loss 0.0273518, acc 1\n",
      "2017-11-06T03:32:12.409645: step 3147, loss 0.0838954, acc 0.9375\n",
      "2017-11-06T03:32:16.430503: step 3148, loss 0.168622, acc 0.96875\n",
      "2017-11-06T03:32:20.393320: step 3149, loss 0.0723056, acc 0.9375\n",
      "2017-11-06T03:32:24.418178: step 3150, loss 0.0554723, acc 0.96875\n",
      "2017-11-06T03:32:28.403009: step 3151, loss 0.0396857, acc 0.96875\n",
      "2017-11-06T03:32:32.635017: step 3152, loss 0.115634, acc 0.96875\n",
      "2017-11-06T03:32:37.152226: step 3153, loss 0.0141493, acc 1\n",
      "2017-11-06T03:32:41.165079: step 3154, loss 0.335967, acc 0.875\n",
      "2017-11-06T03:32:45.175927: step 3155, loss 0.37345, acc 0.875\n",
      "2017-11-06T03:32:49.151753: step 3156, loss 0.152551, acc 0.9375\n",
      "2017-11-06T03:32:53.176612: step 3157, loss 0.0605778, acc 0.96875\n",
      "2017-11-06T03:32:57.155440: step 3158, loss 0.388049, acc 0.84375\n",
      "2017-11-06T03:33:01.142272: step 3159, loss 0.0461203, acc 1\n",
      "2017-11-06T03:33:05.180142: step 3160, loss 0.0960079, acc 0.9375\n",
      "2017-11-06T03:33:09.136952: step 3161, loss 0.0278009, acc 1\n",
      "2017-11-06T03:33:13.194836: step 3162, loss 0.00619955, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T03:33:17.166431: step 3163, loss 0.17451, acc 0.9375\n",
      "2017-11-06T03:33:21.142256: step 3164, loss 0.319653, acc 0.875\n",
      "2017-11-06T03:33:25.240168: step 3165, loss 0.251283, acc 0.875\n",
      "2017-11-06T03:33:29.238008: step 3166, loss 0.125301, acc 0.9375\n",
      "2017-11-06T03:33:33.239852: step 3167, loss 0.352367, acc 0.84375\n",
      "2017-11-06T03:33:35.819685: step 3168, loss 0.0689038, acc 0.95\n",
      "2017-11-06T03:33:40.316881: step 3169, loss 0.0424669, acc 0.96875\n",
      "2017-11-06T03:33:44.610931: step 3170, loss 0.11498, acc 0.90625\n",
      "2017-11-06T03:33:48.700839: step 3171, loss 0.118408, acc 0.9375\n",
      "2017-11-06T03:33:52.838778: step 3172, loss 0.0558986, acc 0.96875\n",
      "2017-11-06T03:33:56.982722: step 3173, loss 0.193295, acc 0.875\n",
      "2017-11-06T03:34:01.141678: step 3174, loss 0.119923, acc 0.96875\n",
      "2017-11-06T03:34:05.139518: step 3175, loss 0.176566, acc 0.90625\n",
      "2017-11-06T03:34:09.209410: step 3176, loss 0.157582, acc 0.90625\n",
      "2017-11-06T03:34:13.296314: step 3177, loss 0.196878, acc 0.96875\n",
      "2017-11-06T03:34:17.401230: step 3178, loss 0.271241, acc 0.875\n",
      "2017-11-06T03:34:21.477126: step 3179, loss 0.177297, acc 0.90625\n",
      "2017-11-06T03:34:25.785188: step 3180, loss 0.0896356, acc 0.96875\n",
      "2017-11-06T03:34:29.989174: step 3181, loss 0.171, acc 0.90625\n",
      "2017-11-06T03:34:34.248201: step 3182, loss 0.227367, acc 0.90625\n",
      "2017-11-06T03:34:38.557262: step 3183, loss 0.350717, acc 0.84375\n",
      "2017-11-06T03:34:42.782265: step 3184, loss 0.418543, acc 0.90625\n",
      "2017-11-06T03:34:47.336501: step 3185, loss 0.33229, acc 0.84375\n",
      "2017-11-06T03:34:51.434412: step 3186, loss 0.0361461, acc 1\n",
      "2017-11-06T03:34:55.578357: step 3187, loss 0.326392, acc 0.875\n",
      "2017-11-06T03:34:59.617226: step 3188, loss 0.0596042, acc 0.96875\n",
      "2017-11-06T03:35:03.802200: step 3189, loss 0.00820144, acc 1\n",
      "2017-11-06T03:35:07.784029: step 3190, loss 0.101426, acc 0.96875\n",
      "2017-11-06T03:35:11.795880: step 3191, loss 0.230664, acc 0.90625\n",
      "2017-11-06T03:35:15.773707: step 3192, loss 0.389978, acc 0.9375\n",
      "2017-11-06T03:35:19.765543: step 3193, loss 0.230165, acc 0.90625\n",
      "2017-11-06T03:35:23.736364: step 3194, loss 0.131956, acc 0.9375\n",
      "2017-11-06T03:35:27.765227: step 3195, loss 0.313517, acc 0.90625\n",
      "2017-11-06T03:35:31.788085: step 3196, loss 0.183351, acc 0.9375\n",
      "2017-11-06T03:35:35.752902: step 3197, loss 0.0263729, acc 1\n",
      "2017-11-06T03:35:39.738734: step 3198, loss 0.054208, acc 0.96875\n",
      "2017-11-06T03:35:43.761612: step 3199, loss 0.223881, acc 0.90625\n",
      "2017-11-06T03:35:47.824480: step 3200, loss 0.277818, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:35:50.743554: step 3200, loss 0.464689, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-06T03:35:56.199211: step 3201, loss 0.0720778, acc 0.96875\n",
      "2017-11-06T03:36:00.187046: step 3202, loss 0.0590661, acc 0.96875\n",
      "2017-11-06T03:36:04.185886: step 3203, loss 0.19561, acc 0.96875\n",
      "2017-11-06T03:36:06.730695: step 3204, loss 0.217974, acc 0.9\n",
      "2017-11-06T03:36:10.685505: step 3205, loss 0.0322128, acc 1\n",
      "2017-11-06T03:36:14.644318: step 3206, loss 0.0293552, acc 1\n",
      "2017-11-06T03:36:18.612918: step 3207, loss 0.169876, acc 0.9375\n",
      "2017-11-06T03:36:22.602733: step 3208, loss 0.0167356, acc 1\n",
      "2017-11-06T03:36:26.603576: step 3209, loss 0.00580005, acc 1\n",
      "2017-11-06T03:36:30.579401: step 3210, loss 0.0512287, acc 0.96875\n",
      "2017-11-06T03:36:34.755368: step 3211, loss 0.249571, acc 0.90625\n",
      "2017-11-06T03:36:38.842273: step 3212, loss 0.348812, acc 0.84375\n",
      "2017-11-06T03:36:42.794080: step 3213, loss 0.289432, acc 0.9375\n",
      "2017-11-06T03:36:46.742886: step 3214, loss 0.311741, acc 0.875\n",
      "2017-11-06T03:36:50.789762: step 3215, loss 0.0619955, acc 0.96875\n",
      "2017-11-06T03:36:54.920697: step 3216, loss 0.136396, acc 0.9375\n",
      "2017-11-06T03:36:59.239767: step 3217, loss 0.0875365, acc 0.9375\n",
      "2017-11-06T03:37:03.313661: step 3218, loss 0.0283672, acc 1\n",
      "2017-11-06T03:37:07.264468: step 3219, loss 0.193446, acc 0.875\n",
      "2017-11-06T03:37:11.329356: step 3220, loss 0.210894, acc 0.96875\n",
      "2017-11-06T03:37:15.430270: step 3221, loss 0.204115, acc 0.90625\n",
      "2017-11-06T03:37:19.516173: step 3222, loss 0.151808, acc 0.9375\n",
      "2017-11-06T03:37:23.530027: step 3223, loss 0.136412, acc 0.96875\n",
      "2017-11-06T03:37:27.605923: step 3224, loss 0.069596, acc 0.96875\n",
      "2017-11-06T03:37:31.557729: step 3225, loss 0.0275276, acc 1\n",
      "2017-11-06T03:37:35.613611: step 3226, loss 0.257211, acc 0.90625\n",
      "2017-11-06T03:37:39.658485: step 3227, loss 0.0842198, acc 0.9375\n",
      "2017-11-06T03:37:43.631308: step 3228, loss 0.138552, acc 0.96875\n",
      "2017-11-06T03:37:47.691192: step 3229, loss 0.147651, acc 0.9375\n",
      "2017-11-06T03:37:51.681027: step 3230, loss 0.0651011, acc 0.96875\n",
      "2017-11-06T03:37:55.705887: step 3231, loss 0.113265, acc 0.9375\n",
      "2017-11-06T03:37:59.851833: step 3232, loss 0.190909, acc 0.90625\n",
      "2017-11-06T03:38:04.250960: step 3233, loss 0.10223, acc 0.96875\n",
      "2017-11-06T03:38:08.325854: step 3234, loss 0.186011, acc 0.875\n",
      "2017-11-06T03:38:12.341709: step 3235, loss 0.0674301, acc 0.96875\n",
      "2017-11-06T03:38:16.341550: step 3236, loss 0.290668, acc 0.875\n",
      "2017-11-06T03:38:20.374415: step 3237, loss 0.0508592, acc 0.96875\n",
      "2017-11-06T03:38:24.539375: step 3238, loss 0.0340295, acc 1\n",
      "2017-11-06T03:38:28.548224: step 3239, loss 0.151315, acc 0.9375\n",
      "2017-11-06T03:38:31.108044: step 3240, loss 0.0221911, acc 1\n",
      "2017-11-06T03:38:35.306025: step 3241, loss 0.0732432, acc 0.96875\n",
      "2017-11-06T03:38:39.309870: step 3242, loss 0.145012, acc 0.96875\n",
      "2017-11-06T03:38:43.338732: step 3243, loss 0.17918, acc 0.9375\n",
      "2017-11-06T03:38:47.386608: step 3244, loss 0.0666714, acc 0.96875\n",
      "2017-11-06T03:38:51.367437: step 3245, loss 0.197544, acc 0.9375\n",
      "2017-11-06T03:38:55.383292: step 3246, loss 0.115089, acc 0.96875\n",
      "2017-11-06T03:38:59.366120: step 3247, loss 0.0565579, acc 0.96875\n",
      "2017-11-06T03:39:03.327937: step 3248, loss 0.169023, acc 0.9375\n",
      "2017-11-06T03:39:07.600972: step 3249, loss 0.145809, acc 0.90625\n",
      "2017-11-06T03:39:11.862002: step 3250, loss 0.0965164, acc 0.9375\n",
      "2017-11-06T03:39:15.892645: step 3251, loss 0.191877, acc 0.9375\n",
      "2017-11-06T03:39:19.882482: step 3252, loss 0.0942255, acc 0.96875\n",
      "2017-11-06T03:39:23.910343: step 3253, loss 0.0504966, acc 0.96875\n",
      "2017-11-06T03:39:27.985238: step 3254, loss 0.00760398, acc 1\n",
      "2017-11-06T03:39:31.973071: step 3255, loss 0.0904296, acc 0.96875\n",
      "2017-11-06T03:39:36.001935: step 3256, loss 0.181645, acc 0.90625\n",
      "2017-11-06T03:39:40.049811: step 3257, loss 0.0609137, acc 0.9375\n",
      "2017-11-06T03:39:44.122705: step 3258, loss 0.174514, acc 0.9375\n",
      "2017-11-06T03:39:48.129551: step 3259, loss 0.0504877, acc 0.96875\n",
      "2017-11-06T03:39:52.233467: step 3260, loss 0.0435378, acc 0.96875\n",
      "2017-11-06T03:39:56.247320: step 3261, loss 0.112585, acc 0.96875\n",
      "2017-11-06T03:40:00.374254: step 3262, loss 0.27105, acc 0.875\n",
      "2017-11-06T03:40:04.546216: step 3263, loss 0.135937, acc 0.9375\n",
      "2017-11-06T03:40:08.598097: step 3264, loss 0.0354921, acc 1\n",
      "2017-11-06T03:40:12.891146: step 3265, loss 0.278641, acc 0.875\n",
      "2017-11-06T03:40:17.124154: step 3266, loss 0.0908611, acc 0.9375\n",
      "2017-11-06T03:40:21.137004: step 3267, loss 0.243278, acc 0.90625\n",
      "2017-11-06T03:40:25.255934: step 3268, loss 0.0628562, acc 0.96875\n",
      "2017-11-06T03:40:29.306809: step 3269, loss 0.358791, acc 0.90625\n",
      "2017-11-06T03:40:33.438747: step 3270, loss 0.159694, acc 0.96875\n",
      "2017-11-06T03:40:37.542662: step 3271, loss 0.0995889, acc 0.96875\n",
      "2017-11-06T03:40:41.596542: step 3272, loss 0.299426, acc 0.875\n",
      "2017-11-06T03:40:45.643419: step 3273, loss 0.0598685, acc 1\n",
      "2017-11-06T03:40:49.641258: step 3274, loss 0.289953, acc 0.84375\n",
      "2017-11-06T03:40:53.640100: step 3275, loss 0.258295, acc 0.875\n",
      "2017-11-06T03:40:56.211926: step 3276, loss 0.141394, acc 0.9\n",
      "2017-11-06T03:41:00.245793: step 3277, loss 0.0372, acc 0.96875\n",
      "2017-11-06T03:41:04.231625: step 3278, loss 0.131564, acc 0.96875\n",
      "2017-11-06T03:41:08.219459: step 3279, loss 0.11789, acc 0.96875\n",
      "2017-11-06T03:41:12.973837: step 3280, loss 0.767145, acc 0.84375\n",
      "2017-11-06T03:41:17.677178: step 3281, loss 0.171837, acc 0.9375\n",
      "2017-11-06T03:41:22.028271: step 3282, loss 0.341649, acc 0.9375\n",
      "2017-11-06T03:41:26.041122: step 3283, loss 0.34269, acc 0.875\n",
      "2017-11-06T03:41:29.972915: step 3284, loss 0.291759, acc 0.90625\n",
      "2017-11-06T03:41:33.902707: step 3285, loss 0.11287, acc 0.9375\n",
      "2017-11-06T03:41:37.834502: step 3286, loss 0.0454959, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T03:41:41.808325: step 3287, loss 0.0237594, acc 1\n",
      "2017-11-06T03:41:45.743122: step 3288, loss 0.11808, acc 0.96875\n",
      "2017-11-06T03:41:49.734957: step 3289, loss 0.148094, acc 0.9375\n",
      "2017-11-06T03:41:53.675757: step 3290, loss 0.570966, acc 0.90625\n",
      "2017-11-06T03:41:57.588800: step 3291, loss 0.386791, acc 0.90625\n",
      "2017-11-06T03:42:01.502579: step 3292, loss 0.254357, acc 0.9375\n",
      "2017-11-06T03:42:05.404352: step 3293, loss 0.0188094, acc 1\n",
      "2017-11-06T03:42:09.425209: step 3294, loss 0.080883, acc 0.96875\n",
      "2017-11-06T03:42:13.459094: step 3295, loss 0.235528, acc 0.9375\n",
      "2017-11-06T03:42:17.424665: step 3296, loss 0.0787974, acc 0.96875\n",
      "2017-11-06T03:42:21.380476: step 3297, loss 0.323716, acc 0.9375\n",
      "2017-11-06T03:42:25.816628: step 3298, loss 0.227502, acc 0.96875\n",
      "2017-11-06T03:42:29.947563: step 3299, loss 0.241207, acc 0.90625\n",
      "2017-11-06T03:42:33.998441: step 3300, loss 0.0734421, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:42:36.640320: step 3300, loss 0.778303, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-06T03:42:42.403577: step 3301, loss 0.207647, acc 0.90625\n",
      "2017-11-06T03:42:46.334370: step 3302, loss 0.076858, acc 0.9375\n",
      "2017-11-06T03:42:50.303190: step 3303, loss 0.0412982, acc 0.96875\n",
      "2017-11-06T03:42:54.230980: step 3304, loss 0.011169, acc 1\n",
      "2017-11-06T03:42:58.143762: step 3305, loss 0.161688, acc 0.90625\n",
      "2017-11-06T03:43:02.168621: step 3306, loss 0.110062, acc 0.9375\n",
      "2017-11-06T03:43:06.094410: step 3307, loss 0.135495, acc 0.9375\n",
      "2017-11-06T03:43:10.077239: step 3308, loss 0.199995, acc 0.9375\n",
      "2017-11-06T03:43:14.029048: step 3309, loss 0.0336268, acc 1\n",
      "2017-11-06T03:43:18.003872: step 3310, loss 0.407348, acc 0.84375\n",
      "2017-11-06T03:43:22.048748: step 3311, loss 0.0670028, acc 0.9375\n",
      "2017-11-06T03:43:24.874754: step 3312, loss 0.0955068, acc 0.95\n",
      "2017-11-06T03:43:29.204831: step 3313, loss 0.0559521, acc 0.9375\n",
      "2017-11-06T03:43:33.453850: step 3314, loss 0.201818, acc 0.9375\n",
      "2017-11-06T03:43:37.430676: step 3315, loss 0.0820129, acc 0.96875\n",
      "2017-11-06T03:43:41.437524: step 3316, loss 0.135759, acc 0.96875\n",
      "2017-11-06T03:43:45.387329: step 3317, loss 0.0355234, acc 1\n",
      "2017-11-06T03:43:49.391174: step 3318, loss 0.324225, acc 0.9375\n",
      "2017-11-06T03:43:53.388014: step 3319, loss 0.0661465, acc 0.96875\n",
      "2017-11-06T03:43:57.352832: step 3320, loss 0.118736, acc 0.9375\n",
      "2017-11-06T03:44:01.421723: step 3321, loss 0.0571547, acc 0.96875\n",
      "2017-11-06T03:44:05.396547: step 3322, loss 0.0880034, acc 0.96875\n",
      "2017-11-06T03:44:09.367369: step 3323, loss 0.0317551, acc 1\n",
      "2017-11-06T03:44:13.328183: step 3324, loss 0.147687, acc 0.9375\n",
      "2017-11-06T03:44:17.275987: step 3325, loss 0.150292, acc 0.9375\n",
      "2017-11-06T03:44:21.273828: step 3326, loss 0.0773021, acc 0.96875\n",
      "2017-11-06T03:44:25.245650: step 3327, loss 0.0682155, acc 0.96875\n",
      "2017-11-06T03:44:29.287522: step 3328, loss 0.0831563, acc 0.9375\n",
      "2017-11-06T03:44:33.457484: step 3329, loss 0.308128, acc 0.90625\n",
      "2017-11-06T03:44:37.983701: step 3330, loss 0.137523, acc 0.96875\n",
      "2017-11-06T03:44:42.000555: step 3331, loss 0.0920308, acc 0.9375\n",
      "2017-11-06T03:44:45.993392: step 3332, loss 0.141651, acc 0.96875\n",
      "2017-11-06T03:44:49.961212: step 3333, loss 0.17069, acc 0.9375\n",
      "2017-11-06T03:44:53.913020: step 3334, loss 0.175917, acc 0.9375\n",
      "2017-11-06T03:44:57.854820: step 3335, loss 0.369034, acc 0.875\n",
      "2017-11-06T03:45:01.895693: step 3336, loss 0.120341, acc 0.9375\n",
      "2017-11-06T03:45:05.870515: step 3337, loss 0.222453, acc 0.9375\n",
      "2017-11-06T03:45:09.864354: step 3338, loss 0.251923, acc 0.90625\n",
      "2017-11-06T03:45:13.845182: step 3339, loss 0.110635, acc 0.96875\n",
      "2017-11-06T03:45:17.952900: step 3340, loss 0.128756, acc 0.96875\n",
      "2017-11-06T03:45:22.036802: step 3341, loss 0.244634, acc 0.90625\n",
      "2017-11-06T03:45:26.160732: step 3342, loss 0.255966, acc 0.90625\n",
      "2017-11-06T03:45:30.186595: step 3343, loss 0.054742, acc 0.96875\n",
      "2017-11-06T03:45:34.142403: step 3344, loss 0.133422, acc 0.9375\n",
      "2017-11-06T03:45:38.166262: step 3345, loss 0.0934642, acc 0.96875\n",
      "2017-11-06T03:45:42.555381: step 3346, loss 0.0328562, acc 1\n",
      "2017-11-06T03:45:46.574236: step 3347, loss 0.222007, acc 0.9375\n",
      "2017-11-06T03:45:49.217115: step 3348, loss 0.0400389, acc 1\n",
      "2017-11-06T03:45:53.181932: step 3349, loss 0.13393, acc 0.9375\n",
      "2017-11-06T03:45:57.192782: step 3350, loss 0.0842379, acc 0.96875\n",
      "2017-11-06T03:46:01.246662: step 3351, loss 0.272152, acc 0.875\n",
      "2017-11-06T03:46:05.278527: step 3352, loss 0.00666551, acc 1\n",
      "2017-11-06T03:46:09.277369: step 3353, loss 0.2407, acc 0.90625\n",
      "2017-11-06T03:46:13.261199: step 3354, loss 0.224991, acc 0.90625\n",
      "2017-11-06T03:46:17.242027: step 3355, loss 0.124523, acc 0.9375\n",
      "2017-11-06T03:46:21.263885: step 3356, loss 0.226043, acc 0.9375\n",
      "2017-11-06T03:46:25.251718: step 3357, loss 0.0346904, acc 0.96875\n",
      "2017-11-06T03:46:29.210532: step 3358, loss 0.129379, acc 0.9375\n",
      "2017-11-06T03:46:33.342469: step 3359, loss 0.00226569, acc 1\n",
      "2017-11-06T03:46:37.463395: step 3360, loss 0.160604, acc 0.9375\n",
      "2017-11-06T03:46:41.400193: step 3361, loss 0.330556, acc 0.90625\n",
      "2017-11-06T03:46:45.599176: step 3362, loss 0.266118, acc 0.90625\n",
      "2017-11-06T03:46:49.868211: step 3363, loss 0.230872, acc 0.96875\n",
      "2017-11-06T03:46:53.868051: step 3364, loss 0.0108256, acc 1\n",
      "2017-11-06T03:46:57.844878: step 3365, loss 0.0231492, acc 1\n",
      "2017-11-06T03:47:01.825706: step 3366, loss 0.118166, acc 0.96875\n",
      "2017-11-06T03:47:05.774511: step 3367, loss 0.0624895, acc 0.96875\n",
      "2017-11-06T03:47:09.725319: step 3368, loss 0.110623, acc 0.9375\n",
      "2017-11-06T03:47:13.728163: step 3369, loss 0.146002, acc 0.9375\n",
      "2017-11-06T03:47:17.723001: step 3370, loss 0.155964, acc 0.90625\n",
      "2017-11-06T03:47:21.701830: step 3371, loss 0.00783033, acc 1\n",
      "2017-11-06T03:47:25.685659: step 3372, loss 0.0478775, acc 1\n",
      "2017-11-06T03:47:29.772564: step 3373, loss 0.105632, acc 0.96875\n",
      "2017-11-06T03:47:33.713364: step 3374, loss 0.0483389, acc 0.96875\n",
      "2017-11-06T03:47:37.669174: step 3375, loss 0.237537, acc 0.875\n",
      "2017-11-06T03:47:41.627987: step 3376, loss 0.253548, acc 0.90625\n",
      "2017-11-06T03:47:45.580797: step 3377, loss 0.243782, acc 0.90625\n",
      "2017-11-06T03:47:49.647685: step 3378, loss 0.276112, acc 0.875\n",
      "2017-11-06T03:47:54.122866: step 3379, loss 0.322609, acc 0.84375\n",
      "2017-11-06T03:47:58.105696: step 3380, loss 0.237073, acc 0.90625\n",
      "2017-11-06T03:48:02.108539: step 3381, loss 0.232941, acc 0.875\n",
      "2017-11-06T03:48:06.118388: step 3382, loss 0.0586416, acc 0.96875\n",
      "2017-11-06T03:48:10.085207: step 3383, loss 0.300287, acc 0.90625\n",
      "2017-11-06T03:48:12.650029: step 3384, loss 0.2062, acc 0.95\n",
      "2017-11-06T03:48:16.629857: step 3385, loss 0.23198, acc 0.84375\n",
      "2017-11-06T03:48:20.603464: step 3386, loss 0.00872189, acc 1\n",
      "2017-11-06T03:48:24.596302: step 3387, loss 0.141801, acc 0.90625\n",
      "2017-11-06T03:48:28.583134: step 3388, loss 0.055077, acc 0.96875\n",
      "2017-11-06T03:48:32.644020: step 3389, loss 0.0220744, acc 1\n",
      "2017-11-06T03:48:36.697901: step 3390, loss 0.0223791, acc 1\n",
      "2017-11-06T03:48:40.672724: step 3391, loss 0.23733, acc 0.875\n",
      "2017-11-06T03:48:44.623533: step 3392, loss 0.188975, acc 0.875\n",
      "2017-11-06T03:48:48.574340: step 3393, loss 0.0054524, acc 1\n",
      "2017-11-06T03:48:52.576184: step 3394, loss 0.0106957, acc 1\n",
      "2017-11-06T03:48:56.796180: step 3395, loss 0.0293487, acc 1\n",
      "2017-11-06T03:49:01.094234: step 3396, loss 0.163616, acc 0.90625\n",
      "2017-11-06T03:49:05.041039: step 3397, loss 0.298407, acc 0.84375\n",
      "2017-11-06T03:49:09.015863: step 3398, loss 0.0238084, acc 1\n",
      "2017-11-06T03:49:12.935648: step 3399, loss 0.13414, acc 0.96875\n",
      "2017-11-06T03:49:16.933489: step 3400, loss 0.146775, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:49:19.517326: step 3400, loss 0.624743, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-06T03:49:25.171129: step 3401, loss 0.131737, acc 0.90625\n",
      "2017-11-06T03:49:29.330085: step 3402, loss 0.109494, acc 0.9375\n",
      "2017-11-06T03:49:33.312914: step 3403, loss 0.193799, acc 0.90625\n",
      "2017-11-06T03:49:37.327768: step 3404, loss 0.302748, acc 0.875\n",
      "2017-11-06T03:49:41.321605: step 3405, loss 0.280361, acc 0.90625\n",
      "2017-11-06T03:49:45.276416: step 3406, loss 0.0339647, acc 1\n",
      "2017-11-06T03:49:49.253241: step 3407, loss 0.0367629, acc 1\n",
      "2017-11-06T03:49:53.278101: step 3408, loss 0.0418009, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T03:49:57.298958: step 3409, loss 0.280698, acc 0.9375\n",
      "2017-11-06T03:50:01.703087: step 3410, loss 0.0533409, acc 0.96875\n",
      "2017-11-06T03:50:06.111219: step 3411, loss 0.17583, acc 0.9375\n",
      "2017-11-06T03:50:10.414355: step 3412, loss 0.0257608, acc 1\n",
      "2017-11-06T03:50:14.466234: step 3413, loss 0.127687, acc 0.9375\n",
      "2017-11-06T03:50:18.482088: step 3414, loss 0.116104, acc 0.96875\n",
      "2017-11-06T03:50:22.501944: step 3415, loss 0.41963, acc 0.90625\n",
      "2017-11-06T03:50:26.497783: step 3416, loss 0.254936, acc 0.90625\n",
      "2017-11-06T03:50:30.475610: step 3417, loss 0.0709101, acc 0.96875\n",
      "2017-11-06T03:50:34.672592: step 3418, loss 0.1213, acc 0.9375\n",
      "2017-11-06T03:50:38.683443: step 3419, loss 0.0840645, acc 0.9375\n",
      "2017-11-06T03:50:41.316312: step 3420, loss 0.0367361, acc 1\n",
      "2017-11-06T03:50:45.367191: step 3421, loss 0.0189361, acc 1\n",
      "2017-11-06T03:50:49.331007: step 3422, loss 0.0075123, acc 1\n",
      "2017-11-06T03:50:53.384888: step 3423, loss 0.228899, acc 0.90625\n",
      "2017-11-06T03:50:57.370719: step 3424, loss 0.117261, acc 0.9375\n",
      "2017-11-06T03:51:01.357552: step 3425, loss 0.0953666, acc 0.9375\n",
      "2017-11-06T03:51:05.334379: step 3426, loss 0.0993631, acc 0.96875\n",
      "2017-11-06T03:51:09.636434: step 3427, loss 0.023943, acc 1\n",
      "2017-11-06T03:51:13.811401: step 3428, loss 0.0231165, acc 1\n",
      "2017-11-06T03:51:17.799002: step 3429, loss 0.266404, acc 0.84375\n",
      "2017-11-06T03:51:21.768822: step 3430, loss 0.228587, acc 0.90625\n",
      "2017-11-06T03:51:25.777670: step 3431, loss 0.0769509, acc 0.96875\n",
      "2017-11-06T03:51:29.790521: step 3432, loss 0.0720474, acc 0.96875\n",
      "2017-11-06T03:51:33.850406: step 3433, loss 0.190916, acc 0.90625\n",
      "2017-11-06T03:51:37.798212: step 3434, loss 0.0320588, acc 0.96875\n",
      "2017-11-06T03:51:41.798053: step 3435, loss 0.145972, acc 0.9375\n",
      "2017-11-06T03:51:45.781884: step 3436, loss 0.0957685, acc 0.96875\n",
      "2017-11-06T03:51:49.779724: step 3437, loss 0.0648761, acc 0.96875\n",
      "2017-11-06T03:51:53.815594: step 3438, loss 0.145192, acc 0.96875\n",
      "2017-11-06T03:51:57.849458: step 3439, loss 0.280601, acc 0.9375\n",
      "2017-11-06T03:52:01.882324: step 3440, loss 0.0471585, acc 0.96875\n",
      "2017-11-06T03:52:05.859149: step 3441, loss 0.229297, acc 0.90625\n",
      "2017-11-06T03:52:09.884009: step 3442, loss 0.032013, acc 0.96875\n",
      "2017-11-06T03:52:14.144036: step 3443, loss 0.15643, acc 0.90625\n",
      "2017-11-06T03:52:18.399059: step 3444, loss 0.103058, acc 0.9375\n",
      "2017-11-06T03:52:22.443934: step 3445, loss 0.119616, acc 0.9375\n",
      "2017-11-06T03:52:26.539844: step 3446, loss 0.198629, acc 0.90625\n",
      "2017-11-06T03:52:30.513669: step 3447, loss 0.165012, acc 0.9375\n",
      "2017-11-06T03:52:34.678627: step 3448, loss 0.10367, acc 0.9375\n",
      "2017-11-06T03:52:38.695481: step 3449, loss 0.144599, acc 0.9375\n",
      "2017-11-06T03:52:42.647289: step 3450, loss 0.217703, acc 0.875\n",
      "2017-11-06T03:52:46.700169: step 3451, loss 0.2645, acc 0.90625\n",
      "2017-11-06T03:52:50.614950: step 3452, loss 0.122436, acc 0.9375\n",
      "2017-11-06T03:52:54.648816: step 3453, loss 0.0762242, acc 0.96875\n",
      "2017-11-06T03:52:58.651662: step 3454, loss 0.184562, acc 0.90625\n",
      "2017-11-06T03:53:02.649501: step 3455, loss 0.1325, acc 0.96875\n",
      "2017-11-06T03:53:05.260356: step 3456, loss 0.137526, acc 0.95\n",
      "2017-11-06T03:53:09.217167: step 3457, loss 0.0406178, acc 0.96875\n",
      "2017-11-06T03:53:13.253035: step 3458, loss 0.158441, acc 0.96875\n",
      "2017-11-06T03:53:17.252878: step 3459, loss 0.128544, acc 0.9375\n",
      "2017-11-06T03:53:21.588958: step 3460, loss 0.0367583, acc 1\n",
      "2017-11-06T03:53:25.696877: step 3461, loss 0.147496, acc 0.90625\n",
      "2017-11-06T03:53:29.719737: step 3462, loss 0.170164, acc 0.90625\n",
      "2017-11-06T03:53:33.678550: step 3463, loss 0.0436013, acc 0.96875\n",
      "2017-11-06T03:53:37.671386: step 3464, loss 0.117855, acc 0.9375\n",
      "2017-11-06T03:53:41.652214: step 3465, loss 0.297983, acc 0.84375\n",
      "2017-11-06T03:53:45.690083: step 3466, loss 0.0869693, acc 0.96875\n",
      "2017-11-06T03:53:49.718945: step 3467, loss 0.0228427, acc 1\n",
      "2017-11-06T03:53:53.772826: step 3468, loss 0.0609821, acc 0.96875\n",
      "2017-11-06T03:53:57.783676: step 3469, loss 0.0989582, acc 0.96875\n",
      "2017-11-06T03:54:01.849565: step 3470, loss 0.0863788, acc 0.96875\n",
      "2017-11-06T03:54:05.837400: step 3471, loss 0.0350272, acc 1\n",
      "2017-11-06T03:54:09.928307: step 3472, loss 0.0832407, acc 0.9375\n",
      "2017-11-06T03:54:13.986189: step 3473, loss 0.0741814, acc 0.96875\n",
      "2017-11-06T03:54:17.984875: step 3474, loss 0.124708, acc 0.96875\n",
      "2017-11-06T03:54:22.075781: step 3475, loss 0.22921, acc 0.9375\n",
      "2017-11-06T03:54:26.645028: step 3476, loss 0.0327621, acc 1\n",
      "2017-11-06T03:54:30.790973: step 3477, loss 0.140495, acc 0.90625\n",
      "2017-11-06T03:54:34.951930: step 3478, loss 0.0898946, acc 0.96875\n",
      "2017-11-06T03:54:39.098876: step 3479, loss 0.14471, acc 0.9375\n",
      "2017-11-06T03:54:43.120735: step 3480, loss 0.212789, acc 0.90625\n",
      "2017-11-06T03:54:47.125580: step 3481, loss 0.129578, acc 0.90625\n",
      "2017-11-06T03:54:51.072384: step 3482, loss 0.0110279, acc 1\n",
      "2017-11-06T03:54:55.059218: step 3483, loss 0.259057, acc 0.90625\n",
      "2017-11-06T03:54:59.048052: step 3484, loss 0.0724571, acc 0.96875\n",
      "2017-11-06T03:55:03.043890: step 3485, loss 0.058782, acc 0.9375\n",
      "2017-11-06T03:55:07.013712: step 3486, loss 0.125686, acc 0.9375\n",
      "2017-11-06T03:55:11.047577: step 3487, loss 0.170805, acc 0.875\n",
      "2017-11-06T03:55:15.056426: step 3488, loss 0.0294405, acc 0.96875\n",
      "2017-11-06T03:55:19.009236: step 3489, loss 0.160522, acc 0.9375\n",
      "2017-11-06T03:55:22.980056: step 3490, loss 0.150687, acc 0.9375\n",
      "2017-11-06T03:55:26.985902: step 3491, loss 0.0179738, acc 1\n",
      "2017-11-06T03:55:29.715842: step 3492, loss 0.117584, acc 0.95\n",
      "2017-11-06T03:55:34.050924: step 3493, loss 0.167393, acc 0.9375\n",
      "2017-11-06T03:55:38.131822: step 3494, loss 0.190056, acc 0.90625\n",
      "2017-11-06T03:55:42.160684: step 3495, loss 0.196206, acc 0.9375\n",
      "2017-11-06T03:55:46.155523: step 3496, loss 0.170358, acc 0.90625\n",
      "2017-11-06T03:55:50.138354: step 3497, loss 0.29034, acc 0.90625\n",
      "2017-11-06T03:55:54.159211: step 3498, loss 0.0813905, acc 0.96875\n",
      "2017-11-06T03:55:58.176064: step 3499, loss 0.121873, acc 0.96875\n",
      "2017-11-06T03:56:02.194922: step 3500, loss 0.0572365, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T03:56:04.769749: step 3500, loss 0.514362, acc 0.85\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-06T03:56:10.037630: step 3501, loss 0.204496, acc 0.90625\n",
      "2017-11-06T03:56:14.071497: step 3502, loss 0.237493, acc 0.90625\n",
      "2017-11-06T03:56:18.042318: step 3503, loss 0.0138759, acc 1\n",
      "2017-11-06T03:56:22.029150: step 3504, loss 0.125224, acc 0.96875\n",
      "2017-11-06T03:56:26.036999: step 3505, loss 0.0266563, acc 1\n",
      "2017-11-06T03:56:30.012823: step 3506, loss 0.0255739, acc 1\n",
      "2017-11-06T03:56:34.167777: step 3507, loss 0.132546, acc 0.9375\n",
      "2017-11-06T03:56:38.651962: step 3508, loss 0.0836106, acc 0.9375\n",
      "2017-11-06T03:56:42.659810: step 3509, loss 0.397084, acc 0.875\n",
      "2017-11-06T03:56:46.602611: step 3510, loss 0.152691, acc 0.9375\n",
      "2017-11-06T03:56:50.607477: step 3511, loss 0.161679, acc 0.90625\n",
      "2017-11-06T03:56:54.683353: step 3512, loss 0.130467, acc 0.96875\n",
      "2017-11-06T03:56:58.653173: step 3513, loss 0.0830621, acc 0.96875\n",
      "2017-11-06T03:57:02.692043: step 3514, loss 0.035687, acc 1\n",
      "2017-11-06T03:57:06.681878: step 3515, loss 0.274063, acc 0.875\n",
      "2017-11-06T03:57:10.608668: step 3516, loss 0.0615253, acc 0.96875\n",
      "2017-11-06T03:57:14.626523: step 3517, loss 0.161446, acc 0.90625\n",
      "2017-11-06T03:57:18.575091: step 3518, loss 0.107339, acc 0.96875\n",
      "2017-11-06T03:57:22.648987: step 3519, loss 0.0226752, acc 1\n",
      "2017-11-06T03:57:26.648828: step 3520, loss 0.407334, acc 0.90625\n",
      "2017-11-06T03:57:30.560608: step 3521, loss 0.316724, acc 0.875\n",
      "2017-11-06T03:57:34.630500: step 3522, loss 0.162291, acc 0.9375\n",
      "2017-11-06T03:57:38.649355: step 3523, loss 0.22384, acc 0.9375\n",
      "2017-11-06T03:57:42.936401: step 3524, loss 0.168018, acc 0.96875\n",
      "2017-11-06T03:57:47.105363: step 3525, loss 0.0949199, acc 0.9375\n",
      "2017-11-06T03:57:51.118215: step 3526, loss 0.330179, acc 0.90625\n",
      "2017-11-06T03:57:55.143075: step 3527, loss 0.114357, acc 0.96875\n",
      "2017-11-06T03:57:57.758936: step 3528, loss 0.0179248, acc 1\n",
      "2017-11-06T03:58:01.738762: step 3529, loss 0.244243, acc 0.9375\n",
      "2017-11-06T03:58:05.785637: step 3530, loss 0.0836412, acc 0.96875\n",
      "2017-11-06T03:58:09.784479: step 3531, loss 0.0867186, acc 0.9375\n",
      "2017-11-06T03:58:13.855370: step 3532, loss 0.207633, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T03:58:17.884233: step 3533, loss 0.107215, acc 0.9375\n",
      "2017-11-06T03:58:21.849052: step 3534, loss 0.175577, acc 0.90625\n",
      "2017-11-06T03:58:25.850894: step 3535, loss 0.0868683, acc 0.9375\n",
      "2017-11-06T03:58:29.794696: step 3536, loss 0.234235, acc 0.875\n",
      "2017-11-06T03:58:34.009692: step 3537, loss 0.14494, acc 0.9375\n",
      "2017-11-06T03:58:38.040557: step 3538, loss 0.144728, acc 0.9375\n",
      "2017-11-06T03:58:42.063415: step 3539, loss 0.226249, acc 0.84375\n",
      "2017-11-06T03:58:46.143312: step 3540, loss 0.0606164, acc 0.9375\n",
      "2017-11-06T03:58:50.493403: step 3541, loss 0.0746888, acc 0.96875\n",
      "2017-11-06T03:58:54.529272: step 3542, loss 0.292552, acc 0.84375\n",
      "2017-11-06T03:58:58.603166: step 3543, loss 0.151387, acc 0.9375\n",
      "2017-11-06T03:59:02.548969: step 3544, loss 0.0772919, acc 0.9375\n",
      "2017-11-06T03:59:06.539805: step 3545, loss 0.129433, acc 0.90625\n",
      "2017-11-06T03:59:10.551656: step 3546, loss 0.087966, acc 0.9375\n",
      "2017-11-06T03:59:14.539490: step 3547, loss 0.0388993, acc 0.96875\n",
      "2017-11-06T03:59:18.487294: step 3548, loss 0.297309, acc 0.90625\n",
      "2017-11-06T03:59:22.572197: step 3549, loss 0.0095122, acc 1\n",
      "2017-11-06T03:59:26.821216: step 3550, loss 0.0416809, acc 0.96875\n",
      "2017-11-06T03:59:30.863087: step 3551, loss 0.165955, acc 0.9375\n",
      "2017-11-06T03:59:34.862930: step 3552, loss 0.0697297, acc 0.9375\n",
      "2017-11-06T03:59:38.872779: step 3553, loss 0.085655, acc 0.96875\n",
      "2017-11-06T03:59:42.843600: step 3554, loss 0.0947733, acc 0.96875\n",
      "2017-11-06T03:59:46.850447: step 3555, loss 0.0192779, acc 1\n",
      "2017-11-06T03:59:50.847287: step 3556, loss 0.0200834, acc 1\n",
      "2017-11-06T03:59:55.328472: step 3557, loss 0.119674, acc 0.90625\n",
      "2017-11-06T03:59:59.399364: step 3558, loss 0.119627, acc 0.9375\n",
      "2017-11-06T04:00:03.694416: step 3559, loss 0.00677793, acc 1\n",
      "2017-11-06T04:00:07.723278: step 3560, loss 0.143401, acc 0.9375\n",
      "2017-11-06T04:00:11.757145: step 3561, loss 0.348922, acc 0.90625\n",
      "2017-11-06T04:00:15.781004: step 3562, loss 0.138228, acc 0.90625\n",
      "2017-11-06T04:00:19.870697: step 3563, loss 0.0504922, acc 1\n",
      "2017-11-06T04:00:22.406499: step 3564, loss 0.250489, acc 0.95\n",
      "2017-11-06T04:00:26.374318: step 3565, loss 0.181973, acc 0.9375\n",
      "2017-11-06T04:00:30.434202: step 3566, loss 0.0498339, acc 0.96875\n",
      "2017-11-06T04:00:34.515103: step 3567, loss 0.0662935, acc 0.96875\n",
      "2017-11-06T04:00:38.563981: step 3568, loss 0.0734023, acc 0.96875\n",
      "2017-11-06T04:00:42.514786: step 3569, loss 0.0200412, acc 1\n",
      "2017-11-06T04:00:46.529639: step 3570, loss 0.189365, acc 0.9375\n",
      "2017-11-06T04:00:50.483450: step 3571, loss 0.303812, acc 0.875\n",
      "2017-11-06T04:00:54.461276: step 3572, loss 0.0166689, acc 1\n",
      "2017-11-06T04:00:58.688279: step 3573, loss 0.120764, acc 0.9375\n",
      "2017-11-06T04:01:02.942303: step 3574, loss 0.226757, acc 0.90625\n",
      "2017-11-06T04:01:06.971163: step 3575, loss 0.180586, acc 0.9375\n",
      "2017-11-06T04:01:11.007050: step 3576, loss 0.191001, acc 0.875\n",
      "2017-11-06T04:01:15.092934: step 3577, loss 0.201993, acc 0.90625\n",
      "2017-11-06T04:01:19.107789: step 3578, loss 0.0453859, acc 0.96875\n",
      "2017-11-06T04:01:23.071604: step 3579, loss 0.163186, acc 0.96875\n",
      "2017-11-06T04:01:27.078451: step 3580, loss 0.113023, acc 0.9375\n",
      "2017-11-06T04:01:31.090302: step 3581, loss 0.0800354, acc 0.96875\n",
      "2017-11-06T04:01:35.156190: step 3582, loss 0.0798426, acc 0.96875\n",
      "2017-11-06T04:01:39.357176: step 3583, loss 0.0470774, acc 0.96875\n",
      "2017-11-06T04:01:43.445080: step 3584, loss 0.204015, acc 0.9375\n",
      "2017-11-06T04:01:47.355860: step 3585, loss 0.145221, acc 0.875\n",
      "2017-11-06T04:01:51.370711: step 3586, loss 0.0357045, acc 0.96875\n",
      "2017-11-06T04:01:55.372555: step 3587, loss 0.0993438, acc 0.9375\n",
      "2017-11-06T04:01:59.358387: step 3588, loss 0.112844, acc 0.96875\n",
      "2017-11-06T04:02:03.482317: step 3589, loss 0.151422, acc 0.9375\n",
      "2017-11-06T04:02:07.787376: step 3590, loss 0.0370455, acc 1\n",
      "2017-11-06T04:02:11.808233: step 3591, loss 0.16799, acc 0.9375\n",
      "2017-11-06T04:02:15.868119: step 3592, loss 0.411432, acc 0.84375\n",
      "2017-11-06T04:02:20.294867: step 3593, loss 0.13822, acc 0.9375\n",
      "2017-11-06T04:02:24.309720: step 3594, loss 0.0725007, acc 0.96875\n",
      "2017-11-06T04:02:28.374608: step 3595, loss 0.151345, acc 0.9375\n",
      "2017-11-06T04:02:32.364443: step 3596, loss 0.252315, acc 0.90625\n",
      "2017-11-06T04:02:36.543413: step 3597, loss 0.154825, acc 0.90625\n",
      "2017-11-06T04:02:40.607300: step 3598, loss 0.0991158, acc 0.9375\n",
      "2017-11-06T04:02:44.602138: step 3599, loss 0.0946748, acc 0.96875\n",
      "2017-11-06T04:02:47.174967: step 3600, loss 0.0200274, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:02:49.798831: step 3600, loss 0.500292, acc 0.866667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\4\\1509901228\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C3730CF8>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\n",
      "\n",
      "2017-11-06T04:02:59.971646: step 1, loss 9.4651, acc 0.09375\n",
      "2017-11-06T04:03:04.011517: step 2, loss 6.05743, acc 0.09375\n",
      "2017-11-06T04:03:08.084411: step 3, loss 3.77321, acc 0.25\n",
      "2017-11-06T04:03:12.536575: step 4, loss 2.04812, acc 0.5\n",
      "2017-11-06T04:03:16.585453: step 5, loss 1.51, acc 0.6875\n",
      "2017-11-06T04:03:20.555072: step 6, loss 1.51956, acc 0.78125\n",
      "2017-11-06T04:03:24.689010: step 7, loss 0.321916, acc 0.96875\n",
      "2017-11-06T04:03:28.775915: step 8, loss 1.53315, acc 0.78125\n",
      "2017-11-06T04:03:32.758743: step 9, loss 1.76543, acc 0.875\n",
      "2017-11-06T04:03:36.802616: step 10, loss 2.40926, acc 0.8125\n",
      "2017-11-06T04:03:40.933552: step 11, loss 0.606974, acc 0.96875\n",
      "2017-11-06T04:03:44.986431: step 12, loss 4.1092, acc 0.8125\n",
      "2017-11-06T04:03:49.066330: step 13, loss 1.18488, acc 0.9375\n",
      "2017-11-06T04:03:53.216279: step 14, loss 3.33428, acc 0.8125\n",
      "2017-11-06T04:03:57.393247: step 15, loss 1.54167, acc 0.90625\n",
      "2017-11-06T04:04:01.491159: step 16, loss 2.86534, acc 0.84375\n",
      "2017-11-06T04:04:05.643109: step 17, loss 2.27796, acc 0.90625\n",
      "2017-11-06T04:04:09.638948: step 18, loss 3.62798, acc 0.8125\n",
      "2017-11-06T04:04:13.961019: step 19, loss 0.513178, acc 0.96875\n",
      "2017-11-06T04:04:18.501245: step 20, loss 2.66196, acc 0.84375\n",
      "2017-11-06T04:04:22.686218: step 21, loss 1.37387, acc 0.9375\n",
      "2017-11-06T04:04:26.839169: step 22, loss 2.01934, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T04:04:31.027145: step 23, loss 2.01803, acc 0.90625\n",
      "2017-11-06T04:04:35.348215: step 24, loss 1.33202, acc 0.90625\n",
      "2017-11-06T04:04:39.564212: step 25, loss 0.285823, acc 0.96875\n",
      "2017-11-06T04:04:43.889284: step 26, loss 1.67149, acc 0.875\n",
      "2017-11-06T04:04:47.840091: step 27, loss 0.688304, acc 0.90625\n",
      "2017-11-06T04:04:51.828925: step 28, loss 1.55026, acc 0.90625\n",
      "2017-11-06T04:04:55.802749: step 29, loss 1.21315, acc 0.9375\n",
      "2017-11-06T04:04:59.822605: step 30, loss 2.56552, acc 0.84375\n",
      "2017-11-06T04:05:03.817444: step 31, loss 1.84938, acc 0.84375\n",
      "2017-11-06T04:05:07.761248: step 32, loss 0.868124, acc 0.90625\n",
      "2017-11-06T04:05:11.728065: step 33, loss 0.0481287, acc 0.96875\n",
      "2017-11-06T04:05:15.787950: step 34, loss 0.437969, acc 0.9375\n",
      "2017-11-06T04:05:19.921887: step 35, loss 2.85791, acc 0.78125\n",
      "2017-11-06T04:05:22.783920: step 36, loss 2.16747, acc 0.8\n",
      "2017-11-06T04:05:26.961889: step 37, loss 1.17097, acc 0.8125\n",
      "2017-11-06T04:05:30.929708: step 38, loss 0.455046, acc 0.90625\n",
      "2017-11-06T04:05:34.896526: step 39, loss 1.50444, acc 0.71875\n",
      "2017-11-06T04:05:38.888363: step 40, loss 1.16473, acc 0.78125\n",
      "2017-11-06T04:05:42.894212: step 41, loss 0.930602, acc 0.78125\n",
      "2017-11-06T04:05:46.860029: step 42, loss 1.02969, acc 0.78125\n",
      "2017-11-06T04:05:50.900899: step 43, loss 0.408456, acc 0.875\n",
      "2017-11-06T04:05:54.924758: step 44, loss 0.839767, acc 0.875\n",
      "2017-11-06T04:05:58.924600: step 45, loss 1.46301, acc 0.71875\n",
      "2017-11-06T04:06:02.914435: step 46, loss 0.510022, acc 0.84375\n",
      "2017-11-06T04:06:06.880252: step 47, loss 0.261723, acc 0.875\n",
      "2017-11-06T04:06:10.853075: step 48, loss 1.06238, acc 0.75\n",
      "2017-11-06T04:06:14.880937: step 49, loss 1.19506, acc 0.8125\n",
      "2017-11-06T04:06:18.948571: step 50, loss 1.04947, acc 0.8125\n",
      "2017-11-06T04:06:22.907384: step 51, loss 0.750821, acc 0.875\n",
      "2017-11-06T04:06:27.182421: step 52, loss 1.30932, acc 0.75\n",
      "2017-11-06T04:06:31.433442: step 53, loss 1.00352, acc 0.84375\n",
      "2017-11-06T04:06:35.622418: step 54, loss 1.19836, acc 0.78125\n",
      "2017-11-06T04:06:39.740344: step 55, loss 2.17676, acc 0.875\n",
      "2017-11-06T04:06:43.659131: step 56, loss 0.288252, acc 0.9375\n",
      "2017-11-06T04:06:47.666976: step 57, loss 0.67896, acc 0.875\n",
      "2017-11-06T04:06:51.646806: step 58, loss 0.224841, acc 0.96875\n",
      "2017-11-06T04:06:55.712707: step 59, loss 0.087364, acc 0.96875\n",
      "2017-11-06T04:06:59.729578: step 60, loss 1.23246, acc 0.84375\n",
      "2017-11-06T04:07:03.832149: step 61, loss 1.85365, acc 0.75\n",
      "2017-11-06T04:07:08.183241: step 62, loss 1.98634, acc 0.75\n",
      "2017-11-06T04:07:12.175077: step 63, loss 0.407353, acc 0.90625\n",
      "2017-11-06T04:07:16.253975: step 64, loss 2.04492, acc 0.84375\n",
      "2017-11-06T04:07:20.232802: step 65, loss 0.53012, acc 0.90625\n",
      "2017-11-06T04:07:24.246654: step 66, loss 0.690164, acc 0.84375\n",
      "2017-11-06T04:07:28.237489: step 67, loss 1.29079, acc 0.8125\n",
      "2017-11-06T04:07:32.449482: step 68, loss 1.15866, acc 0.84375\n",
      "2017-11-06T04:07:36.704506: step 69, loss 2.16119, acc 0.78125\n",
      "2017-11-06T04:07:40.689338: step 70, loss 1.47084, acc 0.875\n",
      "2017-11-06T04:07:44.618129: step 71, loss 0.882695, acc 0.875\n",
      "2017-11-06T04:07:47.121908: step 72, loss 2.82487, acc 0.7\n",
      "2017-11-06T04:07:51.094731: step 73, loss 1.40736, acc 0.84375\n",
      "2017-11-06T04:07:55.047540: step 74, loss 0.89734, acc 0.84375\n",
      "2017-11-06T04:07:58.977331: step 75, loss 0.958289, acc 0.78125\n",
      "2017-11-06T04:08:02.935144: step 76, loss 1.272, acc 0.78125\n",
      "2017-11-06T04:08:06.914971: step 77, loss 0.851433, acc 0.84375\n",
      "2017-11-06T04:08:10.853770: step 78, loss 1.21484, acc 0.78125\n",
      "2017-11-06T04:08:14.772556: step 79, loss 0.917694, acc 0.875\n",
      "2017-11-06T04:08:18.802418: step 80, loss 0.140024, acc 0.90625\n",
      "2017-11-06T04:08:22.849294: step 81, loss 0.362943, acc 0.90625\n",
      "2017-11-06T04:08:26.953210: step 82, loss 0.308422, acc 0.875\n",
      "2017-11-06T04:08:30.878999: step 83, loss 0.18802, acc 0.875\n",
      "2017-11-06T04:08:34.995924: step 84, loss 1.04464, acc 0.84375\n",
      "2017-11-06T04:08:39.344014: step 85, loss 1.56628, acc 0.78125\n",
      "2017-11-06T04:08:43.445930: step 86, loss 1.77928, acc 0.78125\n",
      "2017-11-06T04:08:47.381725: step 87, loss 1.20249, acc 0.90625\n",
      "2017-11-06T04:08:51.305513: step 88, loss 0.235555, acc 0.9375\n",
      "2017-11-06T04:08:55.272332: step 89, loss 0.430992, acc 0.90625\n",
      "2017-11-06T04:08:59.205126: step 90, loss 0.607984, acc 0.84375\n",
      "2017-11-06T04:09:03.128914: step 91, loss 1.17713, acc 0.8125\n",
      "2017-11-06T04:09:07.119814: step 92, loss 0.395101, acc 0.90625\n",
      "2017-11-06T04:09:11.056611: step 93, loss 1.7401, acc 0.71875\n",
      "2017-11-06T04:09:15.034438: step 94, loss 1.19643, acc 0.8125\n",
      "2017-11-06T04:09:19.031277: step 95, loss 0.164074, acc 0.9375\n",
      "2017-11-06T04:09:22.995874: step 96, loss 0.307818, acc 0.96875\n",
      "2017-11-06T04:09:26.939676: step 97, loss 0.221215, acc 0.96875\n",
      "2017-11-06T04:09:30.851456: step 98, loss 1.7672, acc 0.75\n",
      "2017-11-06T04:09:34.833285: step 99, loss 0.861381, acc 0.84375\n",
      "2017-11-06T04:09:38.755071: step 100, loss 1.63956, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:09:41.307885: step 100, loss 1.4086, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-100\n",
      "\n",
      "2017-11-06T04:09:47.081205: step 101, loss 0.469106, acc 0.84375\n",
      "2017-11-06T04:09:51.031012: step 102, loss 0.531792, acc 0.84375\n",
      "2017-11-06T04:09:54.969810: step 103, loss 0.412411, acc 0.875\n",
      "2017-11-06T04:09:58.910610: step 104, loss 1.4641, acc 0.84375\n",
      "2017-11-06T04:10:03.076570: step 105, loss 0.777733, acc 0.90625\n",
      "2017-11-06T04:10:07.028378: step 106, loss 1.3162, acc 0.8125\n",
      "2017-11-06T04:10:10.933153: step 107, loss 0.838848, acc 0.90625\n",
      "2017-11-06T04:10:13.459949: step 108, loss 1.41078, acc 0.85\n",
      "2017-11-06T04:10:17.404752: step 109, loss 0.177947, acc 0.9375\n",
      "2017-11-06T04:10:21.414602: step 110, loss 1.05797, acc 0.84375\n",
      "2017-11-06T04:10:25.450468: step 111, loss 0.800765, acc 0.8125\n",
      "2017-11-06T04:10:29.390268: step 112, loss 0.87236, acc 0.8125\n",
      "2017-11-06T04:10:33.584247: step 113, loss 0.589875, acc 0.90625\n",
      "2017-11-06T04:10:37.605104: step 114, loss 0.804041, acc 0.875\n",
      "2017-11-06T04:10:41.612952: step 115, loss 1.23852, acc 0.875\n",
      "2017-11-06T04:10:45.538743: step 116, loss 1.2268, acc 0.75\n",
      "2017-11-06T04:10:49.730720: step 117, loss 0.363441, acc 0.875\n",
      "2017-11-06T04:10:53.940712: step 118, loss 1.06315, acc 0.75\n",
      "2017-11-06T04:10:57.886515: step 119, loss 0.513317, acc 0.875\n",
      "2017-11-06T04:11:01.842326: step 120, loss 0.149825, acc 0.96875\n",
      "2017-11-06T04:11:05.792133: step 121, loss 1.33673, acc 0.75\n",
      "2017-11-06T04:11:09.817138: step 122, loss 1.212, acc 0.8125\n",
      "2017-11-06T04:11:13.781955: step 123, loss 0.0225345, acc 1\n",
      "2017-11-06T04:11:17.706744: step 124, loss 0.481409, acc 0.875\n",
      "2017-11-06T04:11:21.667557: step 125, loss 0.880521, acc 0.84375\n",
      "2017-11-06T04:11:25.619365: step 126, loss 0.314869, acc 0.90625\n",
      "2017-11-06T04:11:29.561166: step 127, loss 1.02045, acc 0.875\n",
      "2017-11-06T04:11:33.500965: step 128, loss 1.10116, acc 0.875\n",
      "2017-11-06T04:11:37.454774: step 129, loss 0.791923, acc 0.875\n",
      "2017-11-06T04:11:41.435603: step 130, loss 0.351463, acc 0.9375\n",
      "2017-11-06T04:11:45.369398: step 131, loss 0.720501, acc 0.90625\n",
      "2017-11-06T04:11:49.332214: step 132, loss 0.390144, acc 0.90625\n",
      "2017-11-06T04:11:53.335058: step 133, loss 0.455393, acc 0.9375\n",
      "2017-11-06T04:11:57.768208: step 134, loss 0.896681, acc 0.84375\n",
      "2017-11-06T04:12:01.766050: step 135, loss 0.802104, acc 0.90625\n",
      "2017-11-06T04:12:05.695841: step 136, loss 1.06831, acc 0.75\n",
      "2017-11-06T04:12:09.669665: step 137, loss 0.668507, acc 0.8125\n",
      "2017-11-06T04:12:13.719542: step 138, loss 1.04587, acc 0.84375\n",
      "2017-11-06T04:12:17.658341: step 139, loss 0.774786, acc 0.875\n",
      "2017-11-06T04:12:21.624006: step 140, loss 0.594574, acc 0.9375\n",
      "2017-11-06T04:12:25.573813: step 141, loss 1.2685, acc 0.78125\n",
      "2017-11-06T04:12:29.580662: step 142, loss 0.332776, acc 0.96875\n",
      "2017-11-06T04:12:33.665562: step 143, loss 0.9763, acc 0.84375\n",
      "2017-11-06T04:12:36.227383: step 144, loss 1.72714, acc 0.8\n",
      "2017-11-06T04:12:40.191198: step 145, loss 1.31393, acc 0.75\n",
      "2017-11-06T04:12:44.109983: step 146, loss 0.568756, acc 0.90625\n",
      "2017-11-06T04:12:48.028768: step 147, loss 0.973128, acc 0.875\n",
      "2017-11-06T04:12:51.957559: step 148, loss 1.32277, acc 0.78125\n",
      "2017-11-06T04:12:55.919376: step 149, loss 0.295897, acc 0.90625\n",
      "2017-11-06T04:13:00.036300: step 150, loss 1.02465, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T04:13:04.289322: step 151, loss 0.470269, acc 0.84375\n",
      "2017-11-06T04:13:08.215111: step 152, loss 0.935873, acc 0.6875\n",
      "2017-11-06T04:13:12.229964: step 153, loss 0.576199, acc 0.90625\n",
      "2017-11-06T04:13:16.187775: step 154, loss 0.23035, acc 0.9375\n",
      "2017-11-06T04:13:20.148590: step 155, loss 0.355745, acc 0.9375\n",
      "2017-11-06T04:13:24.223488: step 156, loss 0.104481, acc 0.9375\n",
      "2017-11-06T04:13:28.598594: step 157, loss 0.148679, acc 0.96875\n",
      "2017-11-06T04:13:32.527385: step 158, loss 0.987534, acc 0.84375\n",
      "2017-11-06T04:13:36.475191: step 159, loss 0.529846, acc 0.90625\n",
      "2017-11-06T04:13:40.450015: step 160, loss 1.39923, acc 0.84375\n",
      "2017-11-06T04:13:44.369800: step 161, loss 1.06046, acc 0.8125\n",
      "2017-11-06T04:13:48.385656: step 162, loss 0.779512, acc 0.875\n",
      "2017-11-06T04:13:52.349470: step 163, loss 0.051319, acc 0.96875\n",
      "2017-11-06T04:13:56.319293: step 164, loss 1.68987, acc 0.78125\n",
      "2017-11-06T04:14:00.251085: step 165, loss 0.262173, acc 0.9375\n",
      "2017-11-06T04:14:04.263936: step 166, loss 0.370055, acc 0.875\n",
      "2017-11-06T04:14:08.648051: step 167, loss 1.19715, acc 0.84375\n",
      "2017-11-06T04:14:12.586851: step 168, loss 0.380359, acc 0.875\n",
      "2017-11-06T04:14:16.540659: step 169, loss 1.20124, acc 0.8125\n",
      "2017-11-06T04:14:20.484461: step 170, loss 0.923588, acc 0.875\n",
      "2017-11-06T04:14:24.514324: step 171, loss 0.583717, acc 0.875\n",
      "2017-11-06T04:14:28.602231: step 172, loss 0.43849, acc 0.875\n",
      "2017-11-06T04:14:32.629090: step 173, loss 0.436138, acc 0.90625\n",
      "2017-11-06T04:14:36.725001: step 174, loss 1.4162, acc 0.84375\n",
      "2017-11-06T04:14:40.657797: step 175, loss 0.803907, acc 0.875\n",
      "2017-11-06T04:14:44.693664: step 176, loss 0.712161, acc 0.875\n",
      "2017-11-06T04:14:48.602441: step 177, loss 0.849637, acc 0.84375\n",
      "2017-11-06T04:14:52.599280: step 178, loss 0.29391, acc 0.90625\n",
      "2017-11-06T04:14:56.538079: step 179, loss 0.526836, acc 0.84375\n",
      "2017-11-06T04:14:59.075882: step 180, loss 0.891643, acc 0.8\n",
      "2017-11-06T04:15:03.027690: step 181, loss 1.55782, acc 0.84375\n",
      "2017-11-06T04:15:07.002516: step 182, loss 0.921382, acc 0.90625\n",
      "2017-11-06T04:15:11.121442: step 183, loss 0.56777, acc 0.875\n",
      "2017-11-06T04:15:15.395478: step 184, loss 0.366086, acc 0.90625\n",
      "2017-11-06T04:15:19.420338: step 185, loss 0.532942, acc 0.875\n",
      "2017-11-06T04:15:23.424943: step 186, loss 0.329329, acc 0.9375\n",
      "2017-11-06T04:15:27.419781: step 187, loss 1.37134, acc 0.78125\n",
      "2017-11-06T04:15:31.449646: step 188, loss 0.246512, acc 0.9375\n",
      "2017-11-06T04:15:35.453489: step 189, loss 0.748535, acc 0.90625\n",
      "2017-11-06T04:15:39.410300: step 190, loss 0.819773, acc 0.90625\n",
      "2017-11-06T04:15:43.371115: step 191, loss 1.04044, acc 0.8125\n",
      "2017-11-06T04:15:47.323924: step 192, loss 1.21012, acc 0.75\n",
      "2017-11-06T04:15:51.258719: step 193, loss 0.772257, acc 0.9375\n",
      "2017-11-06T04:15:55.209528: step 194, loss 1.36129, acc 0.875\n",
      "2017-11-06T04:15:59.178347: step 195, loss 0.170376, acc 0.875\n",
      "2017-11-06T04:16:03.248239: step 196, loss 0.402673, acc 0.9375\n",
      "2017-11-06T04:16:07.259088: step 197, loss 0.996781, acc 0.8125\n",
      "2017-11-06T04:16:11.212898: step 198, loss 0.959124, acc 0.84375\n",
      "2017-11-06T04:16:15.215742: step 199, loss 0.936972, acc 0.8125\n",
      "2017-11-06T04:16:19.587848: step 200, loss 1.01219, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:16:22.209711: step 200, loss 1.65347, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-200\n",
      "\n",
      "2017-11-06T04:16:27.582492: step 201, loss 0.846903, acc 0.84375\n",
      "2017-11-06T04:16:31.535301: step 202, loss 0.511859, acc 0.90625\n",
      "2017-11-06T04:16:35.717272: step 203, loss 1.38951, acc 0.78125\n",
      "2017-11-06T04:16:39.644063: step 204, loss 0.726511, acc 0.90625\n",
      "2017-11-06T04:16:43.644905: step 205, loss 0.373571, acc 0.9375\n",
      "2017-11-06T04:16:47.535671: step 206, loss 0.80627, acc 0.90625\n",
      "2017-11-06T04:16:51.495483: step 207, loss 1.67358, acc 0.78125\n",
      "2017-11-06T04:16:55.429279: step 208, loss 0.825988, acc 0.75\n",
      "2017-11-06T04:16:59.368077: step 209, loss 0.00688504, acc 1\n",
      "2017-11-06T04:17:03.326890: step 210, loss 0.478962, acc 0.90625\n",
      "2017-11-06T04:17:07.238670: step 211, loss 1.16303, acc 0.875\n",
      "2017-11-06T04:17:11.184473: step 212, loss 0.87569, acc 0.84375\n",
      "2017-11-06T04:17:15.184363: step 213, loss 0.562666, acc 0.875\n",
      "2017-11-06T04:17:19.103149: step 214, loss 1.21042, acc 0.78125\n",
      "2017-11-06T04:17:23.359172: step 215, loss 0.979991, acc 0.875\n",
      "2017-11-06T04:17:26.136146: step 216, loss 0.281855, acc 0.9\n",
      "2017-11-06T04:17:30.088953: step 217, loss 0.842973, acc 0.875\n",
      "2017-11-06T04:17:34.059775: step 218, loss 0.627236, acc 0.84375\n",
      "2017-11-06T04:17:38.015585: step 219, loss 0.222162, acc 0.90625\n",
      "2017-11-06T04:17:41.981403: step 220, loss 0.661239, acc 0.875\n",
      "2017-11-06T04:17:45.973259: step 221, loss 0.498023, acc 0.875\n",
      "2017-11-06T04:17:49.965076: step 222, loss 0.122922, acc 0.96875\n",
      "2017-11-06T04:17:53.952910: step 223, loss 0.198278, acc 0.90625\n",
      "2017-11-06T04:17:57.867691: step 224, loss 0.816018, acc 0.90625\n",
      "2017-11-06T04:18:01.810494: step 225, loss 1.03546, acc 0.8125\n",
      "2017-11-06T04:18:05.848363: step 226, loss 0.782777, acc 0.84375\n",
      "2017-11-06T04:18:09.837196: step 227, loss 0.576043, acc 0.875\n",
      "2017-11-06T04:18:13.850047: step 228, loss 0.440185, acc 0.90625\n",
      "2017-11-06T04:18:17.834879: step 229, loss 1.74388, acc 0.75\n",
      "2017-11-06T04:18:21.920549: step 230, loss 0.499277, acc 0.8125\n",
      "2017-11-06T04:18:26.107525: step 231, loss 0.497858, acc 0.9375\n",
      "2017-11-06T04:18:30.468623: step 232, loss 0.837921, acc 0.78125\n",
      "2017-11-06T04:18:34.766676: step 233, loss 0.709026, acc 0.875\n",
      "2017-11-06T04:18:38.741501: step 234, loss 0.905114, acc 0.84375\n",
      "2017-11-06T04:18:42.660286: step 235, loss 0.682806, acc 0.90625\n",
      "2017-11-06T04:18:46.629105: step 236, loss 0.545194, acc 0.875\n",
      "2017-11-06T04:18:50.601930: step 237, loss 0.487597, acc 0.84375\n",
      "2017-11-06T04:18:54.588761: step 238, loss 0.518368, acc 0.90625\n",
      "2017-11-06T04:18:58.539569: step 239, loss 0.380406, acc 0.84375\n",
      "2017-11-06T04:19:02.596452: step 240, loss 0.225715, acc 0.9375\n",
      "2017-11-06T04:19:06.580281: step 241, loss 0.622202, acc 0.875\n",
      "2017-11-06T04:19:10.568115: step 242, loss 1.09649, acc 0.75\n",
      "2017-11-06T04:19:14.598979: step 243, loss 0.375286, acc 0.90625\n",
      "2017-11-06T04:19:18.588839: step 244, loss 0.59637, acc 0.875\n",
      "2017-11-06T04:19:22.574671: step 245, loss 0.395756, acc 0.9375\n",
      "2017-11-06T04:19:26.542490: step 246, loss 0.220665, acc 0.96875\n",
      "2017-11-06T04:19:30.530324: step 247, loss 0.629958, acc 0.84375\n",
      "2017-11-06T04:19:34.778342: step 248, loss 0.598219, acc 0.90625\n",
      "2017-11-06T04:19:39.088407: step 249, loss 0.229639, acc 0.90625\n",
      "2017-11-06T04:19:43.108261: step 250, loss 0.667324, acc 0.84375\n",
      "2017-11-06T04:19:47.064072: step 251, loss 0.389643, acc 0.875\n",
      "2017-11-06T04:19:49.637901: step 252, loss 0.53977, acc 0.9\n",
      "2017-11-06T04:19:53.663761: step 253, loss 0.644648, acc 0.90625\n",
      "2017-11-06T04:19:57.659601: step 254, loss 0.583827, acc 0.90625\n",
      "2017-11-06T04:20:01.916625: step 255, loss 1.46374, acc 0.78125\n",
      "2017-11-06T04:20:05.916468: step 256, loss 0.766587, acc 0.875\n",
      "2017-11-06T04:20:09.911305: step 257, loss 0.339808, acc 0.875\n",
      "2017-11-06T04:20:14.125300: step 258, loss 0.424306, acc 0.9375\n",
      "2017-11-06T04:20:18.239223: step 259, loss 1.24247, acc 0.875\n",
      "2017-11-06T04:20:22.435204: step 260, loss 0.67282, acc 0.875\n",
      "2017-11-06T04:20:26.520107: step 261, loss 0.397851, acc 0.9375\n",
      "2017-11-06T04:20:30.477919: step 262, loss 0.661244, acc 0.90625\n",
      "2017-11-06T04:20:34.808997: step 263, loss 1.25264, acc 0.84375\n",
      "2017-11-06T04:20:38.987966: step 264, loss 0.718024, acc 0.875\n",
      "2017-11-06T04:20:43.355069: step 265, loss 0.56823, acc 0.78125\n",
      "2017-11-06T04:20:47.389936: step 266, loss 0.431213, acc 0.84375\n",
      "2017-11-06T04:20:51.419800: step 267, loss 0.313559, acc 0.9375\n",
      "2017-11-06T04:20:55.452666: step 268, loss 0.480306, acc 0.90625\n",
      "2017-11-06T04:20:59.438497: step 269, loss 0.0327339, acc 1\n",
      "2017-11-06T04:21:03.484372: step 270, loss 1.17746, acc 0.78125\n",
      "2017-11-06T04:21:07.458195: step 271, loss 0.676089, acc 0.9375\n",
      "2017-11-06T04:21:11.442026: step 272, loss 0.685403, acc 0.9375\n",
      "2017-11-06T04:21:15.393835: step 273, loss 0.552513, acc 0.875\n",
      "2017-11-06T04:21:19.503633: step 274, loss 1.04468, acc 0.84375\n",
      "2017-11-06T04:21:23.495178: step 275, loss 0.233166, acc 0.9375\n",
      "2017-11-06T04:21:27.482011: step 276, loss 0.631942, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T04:21:31.408801: step 277, loss 0.235692, acc 0.9375\n",
      "2017-11-06T04:21:35.375621: step 278, loss 1.42675, acc 0.75\n",
      "2017-11-06T04:21:39.352446: step 279, loss 1.09128, acc 0.84375\n",
      "2017-11-06T04:21:43.414331: step 280, loss 1.09948, acc 0.84375\n",
      "2017-11-06T04:21:47.766424: step 281, loss 1.33952, acc 0.8125\n",
      "2017-11-06T04:21:51.858332: step 282, loss 0.166574, acc 0.96875\n",
      "2017-11-06T04:21:55.800132: step 283, loss 0.626023, acc 0.9375\n",
      "2017-11-06T04:21:59.727923: step 284, loss 0.59024, acc 0.875\n",
      "2017-11-06T04:22:03.728766: step 285, loss 0.524977, acc 0.875\n",
      "2017-11-06T04:22:07.696585: step 286, loss 0.431798, acc 0.875\n",
      "2017-11-06T04:22:11.655398: step 287, loss 0.363374, acc 0.84375\n",
      "2017-11-06T04:22:14.207211: step 288, loss 1.71347, acc 0.7\n",
      "2017-11-06T04:22:18.143007: step 289, loss 0.625901, acc 0.84375\n",
      "2017-11-06T04:22:22.086810: step 290, loss 0.433345, acc 0.90625\n",
      "2017-11-06T04:22:26.087652: step 291, loss 0.673144, acc 0.875\n",
      "2017-11-06T04:22:30.061476: step 292, loss 1.01876, acc 0.8125\n",
      "2017-11-06T04:22:34.206421: step 293, loss 0.0287339, acc 1\n",
      "2017-11-06T04:22:38.293326: step 294, loss 0.610549, acc 0.90625\n",
      "2017-11-06T04:22:42.277156: step 295, loss 0.537152, acc 0.90625\n",
      "2017-11-06T04:22:46.247977: step 296, loss 0.277881, acc 0.9375\n",
      "2017-11-06T04:22:50.344889: step 297, loss 1.65117, acc 0.78125\n",
      "2017-11-06T04:22:54.608918: step 298, loss 0.580602, acc 0.84375\n",
      "2017-11-06T04:22:58.579741: step 299, loss 0.0751633, acc 0.9375\n",
      "2017-11-06T04:23:02.505531: step 300, loss 0.554087, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:23:05.094369: step 300, loss 1.66022, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-300\n",
      "\n",
      "2017-11-06T04:23:10.523434: step 301, loss 0.166537, acc 0.96875\n",
      "2017-11-06T04:23:14.557300: step 302, loss 0.22126, acc 0.9375\n",
      "2017-11-06T04:23:18.569151: step 303, loss 0.567187, acc 0.875\n",
      "2017-11-06T04:23:22.673173: step 304, loss 0.304217, acc 0.96875\n",
      "2017-11-06T04:23:26.696031: step 305, loss 0.348266, acc 0.90625\n",
      "2017-11-06T04:23:30.702877: step 306, loss 0.354853, acc 0.90625\n",
      "2017-11-06T04:23:34.667694: step 307, loss 0.517153, acc 0.875\n",
      "2017-11-06T04:23:38.719573: step 308, loss 0.43396, acc 0.90625\n",
      "2017-11-06T04:23:42.710409: step 309, loss 0.685962, acc 0.875\n",
      "2017-11-06T04:23:46.742274: step 310, loss 0.85274, acc 0.90625\n",
      "2017-11-06T04:23:50.714096: step 311, loss 0.711502, acc 0.9375\n",
      "2017-11-06T04:23:54.792995: step 312, loss 0.838345, acc 0.84375\n",
      "2017-11-06T04:23:59.205130: step 313, loss 0.499155, acc 0.875\n",
      "2017-11-06T04:24:03.148933: step 314, loss 1.27242, acc 0.75\n",
      "2017-11-06T04:24:07.189805: step 315, loss 0.150323, acc 0.96875\n",
      "2017-11-06T04:24:11.147615: step 316, loss 0.585377, acc 0.84375\n",
      "2017-11-06T04:24:15.121438: step 317, loss 1.26751, acc 0.75\n",
      "2017-11-06T04:24:19.053233: step 318, loss 0.463592, acc 0.90625\n",
      "2017-11-06T04:24:23.211966: step 319, loss 0.333776, acc 0.875\n",
      "2017-11-06T04:24:27.441972: step 320, loss 0.37074, acc 0.875\n",
      "2017-11-06T04:24:31.398787: step 321, loss 0.99626, acc 0.875\n",
      "2017-11-06T04:24:35.557739: step 322, loss 0.0867965, acc 0.96875\n",
      "2017-11-06T04:24:39.581598: step 323, loss 1.23759, acc 0.8125\n",
      "2017-11-06T04:24:42.092382: step 324, loss 0.41606, acc 0.95\n",
      "2017-11-06T04:24:46.271351: step 325, loss 0.154502, acc 0.9375\n",
      "2017-11-06T04:24:50.244174: step 326, loss 0.101203, acc 0.9375\n",
      "2017-11-06T04:24:54.206990: step 327, loss 0.350571, acc 0.875\n",
      "2017-11-06T04:24:58.203830: step 328, loss 0.404832, acc 0.90625\n",
      "2017-11-06T04:25:02.447845: step 329, loss 0.443631, acc 0.9375\n",
      "2017-11-06T04:25:06.664841: step 330, loss 0.226825, acc 0.90625\n",
      "2017-11-06T04:25:10.640666: step 331, loss 0.183202, acc 0.96875\n",
      "2017-11-06T04:25:14.653518: step 332, loss 0.28492, acc 0.9375\n",
      "2017-11-06T04:25:18.604325: step 333, loss 0.271094, acc 0.96875\n",
      "2017-11-06T04:25:22.724157: step 334, loss 0.499809, acc 0.90625\n",
      "2017-11-06T04:25:26.797051: step 335, loss 0.293179, acc 0.90625\n",
      "2017-11-06T04:25:30.785885: step 336, loss 0.325135, acc 0.96875\n",
      "2017-11-06T04:25:34.736692: step 337, loss 0.476378, acc 0.875\n",
      "2017-11-06T04:25:38.695505: step 338, loss 0.0959151, acc 0.96875\n",
      "2017-11-06T04:25:42.680336: step 339, loss 0.983564, acc 0.875\n",
      "2017-11-06T04:25:46.688185: step 340, loss 0.104776, acc 0.9375\n",
      "2017-11-06T04:25:50.696032: step 341, loss 0.578103, acc 0.8125\n",
      "2017-11-06T04:25:54.647841: step 342, loss 0.522607, acc 0.9375\n",
      "2017-11-06T04:25:58.572628: step 343, loss 0.537765, acc 0.84375\n",
      "2017-11-06T04:26:02.663535: step 344, loss 0.722504, acc 0.875\n",
      "2017-11-06T04:26:06.779460: step 345, loss 0.795735, acc 0.875\n",
      "2017-11-06T04:26:11.185591: step 346, loss 1.25755, acc 0.84375\n",
      "2017-11-06T04:26:15.208449: step 347, loss 0.881527, acc 0.8125\n",
      "2017-11-06T04:26:19.127234: step 348, loss 0.676266, acc 0.78125\n",
      "2017-11-06T04:26:23.138084: step 349, loss 0.226483, acc 0.9375\n",
      "2017-11-06T04:26:27.092894: step 350, loss 0.864124, acc 0.78125\n",
      "2017-11-06T04:26:31.041700: step 351, loss 0.938865, acc 0.875\n",
      "2017-11-06T04:26:35.199654: step 352, loss 0.858729, acc 0.84375\n",
      "2017-11-06T04:26:39.218509: step 353, loss 0.507552, acc 0.84375\n",
      "2017-11-06T04:26:43.250374: step 354, loss 0.512247, acc 0.90625\n",
      "2017-11-06T04:26:47.260223: step 355, loss 0.586695, acc 0.875\n",
      "2017-11-06T04:26:51.210029: step 356, loss 0.484009, acc 0.9375\n",
      "2017-11-06T04:26:55.204868: step 357, loss 0.861118, acc 0.8125\n",
      "2017-11-06T04:26:59.198706: step 358, loss 0.192511, acc 0.90625\n",
      "2017-11-06T04:27:03.120494: step 359, loss 0.248312, acc 0.875\n",
      "2017-11-06T04:27:05.733350: step 360, loss 0.681003, acc 0.9\n",
      "2017-11-06T04:27:09.822471: step 361, loss 0.123036, acc 0.96875\n",
      "2017-11-06T04:27:14.136536: step 362, loss 0.538409, acc 0.875\n",
      "2017-11-06T04:27:18.294491: step 363, loss 0.525537, acc 0.9375\n",
      "2017-11-06T04:27:22.328106: step 364, loss 0.829937, acc 0.8125\n",
      "2017-11-06T04:27:26.390992: step 365, loss 0.527201, acc 0.90625\n",
      "2017-11-06T04:27:30.382829: step 366, loss 0.319015, acc 0.90625\n",
      "2017-11-06T04:27:34.376666: step 367, loss 0.676056, acc 0.875\n",
      "2017-11-06T04:27:38.468574: step 368, loss 0.913223, acc 0.875\n",
      "2017-11-06T04:27:42.530460: step 369, loss 1.18531, acc 0.8125\n",
      "2017-11-06T04:27:46.543312: step 370, loss 0.702576, acc 0.875\n",
      "2017-11-06T04:27:50.560165: step 371, loss 0.277857, acc 0.875\n",
      "2017-11-06T04:27:54.569014: step 372, loss 0.35077, acc 0.90625\n",
      "2017-11-06T04:27:58.549842: step 373, loss 0.859446, acc 0.84375\n",
      "2017-11-06T04:28:02.473631: step 374, loss 0.427999, acc 0.9375\n",
      "2017-11-06T04:28:06.444452: step 375, loss 1.35081, acc 0.8125\n",
      "2017-11-06T04:28:10.405270: step 376, loss 1.3327, acc 0.71875\n",
      "2017-11-06T04:28:14.444137: step 377, loss 0.775491, acc 0.84375\n",
      "2017-11-06T04:28:18.632112: step 378, loss 0.818818, acc 0.875\n",
      "2017-11-06T04:28:23.064262: step 379, loss 0.431665, acc 0.84375\n",
      "2017-11-06T04:28:27.210207: step 380, loss 0.00589788, acc 1\n",
      "2017-11-06T04:28:31.267089: step 381, loss 0.501182, acc 0.8125\n",
      "2017-11-06T04:28:35.440056: step 382, loss 0.172132, acc 0.90625\n",
      "2017-11-06T04:28:39.416880: step 383, loss 0.323244, acc 0.875\n",
      "2017-11-06T04:28:43.397709: step 384, loss 0.305896, acc 0.8125\n",
      "2017-11-06T04:28:47.491618: step 385, loss 1.01266, acc 0.78125\n",
      "2017-11-06T04:28:51.481453: step 386, loss 0.198471, acc 0.90625\n",
      "2017-11-06T04:28:55.497307: step 387, loss 0.259804, acc 0.9375\n",
      "2017-11-06T04:28:59.437105: step 388, loss 0.325755, acc 0.9375\n",
      "2017-11-06T04:29:03.417934: step 389, loss 0.551861, acc 0.875\n",
      "2017-11-06T04:29:07.380750: step 390, loss 0.632505, acc 0.875\n",
      "2017-11-06T04:29:11.296533: step 391, loss 1.04759, acc 0.84375\n",
      "2017-11-06T04:29:15.267353: step 392, loss 0.356389, acc 0.9375\n",
      "2017-11-06T04:29:19.211156: step 393, loss 0.0269706, acc 0.96875\n",
      "2017-11-06T04:29:23.278045: step 394, loss 1.13235, acc 0.8125\n",
      "2017-11-06T04:29:27.674306: step 395, loss 0.261294, acc 0.90625\n",
      "2017-11-06T04:29:30.250136: step 396, loss 0.170749, acc 0.9\n",
      "2017-11-06T04:29:34.286004: step 397, loss 0.678203, acc 0.8125\n",
      "2017-11-06T04:29:38.241814: step 398, loss 0.190934, acc 0.9375\n",
      "2017-11-06T04:29:42.182615: step 399, loss 0.234345, acc 0.875\n",
      "2017-11-06T04:29:46.148432: step 400, loss 0.538569, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:29:48.775298: step 400, loss 1.4624, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-400\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T04:29:54.500859: step 401, loss 0.378862, acc 0.90625\n",
      "2017-11-06T04:29:58.481688: step 402, loss 0.280818, acc 0.90625\n",
      "2017-11-06T04:30:02.754725: step 403, loss 0.62528, acc 0.84375\n",
      "2017-11-06T04:30:06.753567: step 404, loss 0.432803, acc 0.875\n",
      "2017-11-06T04:30:10.708376: step 405, loss 0.159338, acc 0.9375\n",
      "2017-11-06T04:30:14.672193: step 406, loss 0.66459, acc 0.875\n",
      "2017-11-06T04:30:18.634007: step 407, loss 0.224721, acc 0.90625\n",
      "2017-11-06T04:30:22.632609: step 408, loss 0.249527, acc 0.9375\n",
      "2017-11-06T04:30:26.601429: step 409, loss 0.471572, acc 0.875\n",
      "2017-11-06T04:30:30.813422: step 410, loss 0.0704763, acc 0.96875\n",
      "2017-11-06T04:30:35.135493: step 411, loss 0.435752, acc 0.875\n",
      "2017-11-06T04:30:39.156350: step 412, loss 0.477345, acc 0.875\n",
      "2017-11-06T04:30:43.096150: step 413, loss 0.164228, acc 0.90625\n",
      "2017-11-06T04:30:47.024941: step 414, loss 0.179382, acc 0.96875\n",
      "2017-11-06T04:30:50.987757: step 415, loss 0.383795, acc 0.9375\n",
      "2017-11-06T04:30:54.947571: step 416, loss 0.85513, acc 0.84375\n",
      "2017-11-06T04:30:58.925397: step 417, loss 0.226066, acc 0.90625\n",
      "2017-11-06T04:31:02.878205: step 418, loss 0.322249, acc 0.9375\n",
      "2017-11-06T04:31:06.913072: step 419, loss 0.280392, acc 0.90625\n",
      "2017-11-06T04:31:10.978962: step 420, loss 0.327215, acc 0.875\n",
      "2017-11-06T04:31:14.941777: step 421, loss 0.227281, acc 0.90625\n",
      "2017-11-06T04:31:18.880576: step 422, loss 0.36941, acc 0.90625\n",
      "2017-11-06T04:31:22.817373: step 423, loss 0.438557, acc 0.9375\n",
      "2017-11-06T04:31:26.779188: step 424, loss 1.40583, acc 0.71875\n",
      "2017-11-06T04:31:30.911695: step 425, loss 0.715752, acc 0.78125\n",
      "2017-11-06T04:31:35.147705: step 426, loss 0.561377, acc 0.8125\n",
      "2017-11-06T04:31:39.650904: step 427, loss 0.15724, acc 0.9375\n",
      "2017-11-06T04:31:43.649745: step 428, loss 0.32441, acc 0.96875\n",
      "2017-11-06T04:31:47.556521: step 429, loss 1.12637, acc 0.65625\n",
      "2017-11-06T04:31:51.569373: step 430, loss 0.616139, acc 0.8125\n",
      "2017-11-06T04:31:55.531188: step 431, loss 0.205873, acc 0.9375\n",
      "2017-11-06T04:31:58.034967: step 432, loss 0.558239, acc 0.85\n",
      "2017-11-06T04:32:02.021799: step 433, loss 0.184233, acc 0.9375\n",
      "2017-11-06T04:32:06.033652: step 434, loss 0.155685, acc 0.90625\n",
      "2017-11-06T04:32:10.040497: step 435, loss 0.0204215, acc 1\n",
      "2017-11-06T04:32:13.979296: step 436, loss 0.4632, acc 0.90625\n",
      "2017-11-06T04:32:17.989147: step 437, loss 0.513142, acc 0.9375\n",
      "2017-11-06T04:32:21.953963: step 438, loss 0.795111, acc 0.875\n",
      "2017-11-06T04:32:25.891761: step 439, loss 0.206515, acc 0.9375\n",
      "2017-11-06T04:32:29.916620: step 440, loss 0.0156934, acc 1\n",
      "2017-11-06T04:32:34.031544: step 441, loss 0.591762, acc 0.84375\n",
      "2017-11-06T04:32:38.070414: step 442, loss 0.754667, acc 0.90625\n",
      "2017-11-06T04:32:42.299420: step 443, loss 0.884569, acc 0.84375\n",
      "2017-11-06T04:32:46.450368: step 444, loss 0.436489, acc 0.90625\n",
      "2017-11-06T04:32:50.414185: step 445, loss 0.00436546, acc 1\n",
      "2017-11-06T04:32:54.449053: step 446, loss 0.847098, acc 0.8125\n",
      "2017-11-06T04:32:58.390853: step 447, loss 0.303815, acc 0.84375\n",
      "2017-11-06T04:33:02.311638: step 448, loss 0.235632, acc 0.90625\n",
      "2017-11-06T04:33:06.266448: step 449, loss 0.26826, acc 0.9375\n",
      "2017-11-06T04:33:10.254281: step 450, loss 0.576115, acc 0.875\n",
      "2017-11-06T04:33:14.260128: step 451, loss 0.459297, acc 0.96875\n",
      "2017-11-06T04:33:18.213938: step 452, loss 0.158112, acc 0.9375\n",
      "2017-11-06T04:33:22.282830: step 453, loss 0.725367, acc 0.8125\n",
      "2017-11-06T04:33:26.479595: step 454, loss 0.427472, acc 0.90625\n",
      "2017-11-06T04:33:30.441409: step 455, loss 0.726879, acc 0.84375\n",
      "2017-11-06T04:33:34.407264: step 456, loss 0.62066, acc 0.84375\n",
      "2017-11-06T04:33:38.342059: step 457, loss 0.701228, acc 0.8125\n",
      "2017-11-06T04:33:42.462987: step 458, loss 1.43141, acc 0.75\n",
      "2017-11-06T04:33:46.808075: step 459, loss 0.309873, acc 0.90625\n",
      "2017-11-06T04:33:51.285256: step 460, loss 0.468559, acc 0.875\n",
      "2017-11-06T04:33:55.394175: step 461, loss 0.461358, acc 0.90625\n",
      "2017-11-06T04:33:59.544124: step 462, loss 0.423545, acc 0.875\n",
      "2017-11-06T04:34:03.660049: step 463, loss 0.528717, acc 0.9375\n",
      "2017-11-06T04:34:07.896059: step 464, loss 0.375334, acc 0.90625\n",
      "2017-11-06T04:34:11.945936: step 465, loss 0.266112, acc 0.9375\n",
      "2017-11-06T04:34:16.102890: step 466, loss 1.32292, acc 0.78125\n",
      "2017-11-06T04:34:20.111738: step 467, loss 0.842481, acc 0.78125\n",
      "2017-11-06T04:34:22.840678: step 468, loss 0.0263699, acc 1\n",
      "2017-11-06T04:34:26.981620: step 469, loss 0.277151, acc 0.9375\n",
      "2017-11-06T04:34:31.068523: step 470, loss 0.449156, acc 0.875\n",
      "2017-11-06T04:34:35.364577: step 471, loss 0.350111, acc 0.90625\n",
      "2017-11-06T04:34:39.498514: step 472, loss 0.466759, acc 0.875\n",
      "2017-11-06T04:34:43.512366: step 473, loss 0.273479, acc 0.9375\n",
      "2017-11-06T04:34:47.730363: step 474, loss 0.399494, acc 0.875\n",
      "2017-11-06T04:34:51.893321: step 475, loss 0.797593, acc 0.78125\n",
      "2017-11-06T04:34:56.263426: step 476, loss 0.838051, acc 0.8125\n",
      "2017-11-06T04:35:00.228243: step 477, loss 0.0884448, acc 0.96875\n",
      "2017-11-06T04:35:04.167042: step 478, loss 0.656281, acc 0.84375\n",
      "2017-11-06T04:35:08.113847: step 479, loss 0.131439, acc 0.96875\n",
      "2017-11-06T04:35:12.107684: step 480, loss 0.23902, acc 0.9375\n",
      "2017-11-06T04:35:16.048484: step 481, loss 0.778979, acc 0.8125\n",
      "2017-11-06T04:35:19.952257: step 482, loss 0.993569, acc 0.78125\n",
      "2017-11-06T04:35:23.985123: step 483, loss 0.289029, acc 0.9375\n",
      "2017-11-06T04:35:27.955946: step 484, loss 0.292494, acc 0.875\n",
      "2017-11-06T04:35:31.927767: step 485, loss 0.681567, acc 0.8125\n",
      "2017-11-06T04:35:35.984245: step 486, loss 0.660187, acc 0.84375\n",
      "2017-11-06T04:35:39.972080: step 487, loss 0.96457, acc 0.78125\n",
      "2017-11-06T04:35:43.918882: step 488, loss 0.400504, acc 0.875\n",
      "2017-11-06T04:35:47.850677: step 489, loss 0.425703, acc 0.9375\n",
      "2017-11-06T04:35:51.860528: step 490, loss 0.0551626, acc 0.96875\n",
      "2017-11-06T04:35:55.860367: step 491, loss 0.614892, acc 0.84375\n",
      "2017-11-06T04:36:00.247485: step 492, loss 0.35102, acc 0.875\n",
      "2017-11-06T04:36:04.260336: step 493, loss 0.303345, acc 0.84375\n",
      "2017-11-06T04:36:08.200136: step 494, loss 0.588309, acc 0.84375\n",
      "2017-11-06T04:36:12.169957: step 495, loss 0.495828, acc 0.9375\n",
      "2017-11-06T04:36:16.135774: step 496, loss 0.772343, acc 0.875\n",
      "2017-11-06T04:36:20.083581: step 497, loss 0.160144, acc 0.96875\n",
      "2017-11-06T04:36:24.083215: step 498, loss 0.579393, acc 0.875\n",
      "2017-11-06T04:36:28.026018: step 499, loss 0.329192, acc 0.90625\n",
      "2017-11-06T04:36:32.008847: step 500, loss 0.0275582, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:36:34.820844: step 500, loss 1.88739, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-500\n",
      "\n",
      "2017-11-06T04:36:40.447233: step 501, loss 0.756978, acc 0.84375\n",
      "2017-11-06T04:36:44.406046: step 502, loss 0.20552, acc 0.90625\n",
      "2017-11-06T04:36:48.330834: step 503, loss 1.05284, acc 0.8125\n",
      "2017-11-06T04:36:50.870640: step 504, loss 0.600223, acc 0.8\n",
      "2017-11-06T04:36:54.829452: step 505, loss 0.405441, acc 0.875\n",
      "2017-11-06T04:36:58.753240: step 506, loss 0.303741, acc 0.875\n",
      "2017-11-06T04:37:02.826134: step 507, loss 0.255816, acc 0.90625\n",
      "2017-11-06T04:37:07.117183: step 508, loss 0.678805, acc 0.875\n",
      "2017-11-06T04:37:11.095010: step 509, loss 0.123231, acc 0.90625\n",
      "2017-11-06T04:37:15.045817: step 510, loss 1.34956, acc 0.78125\n",
      "2017-11-06T04:37:18.961599: step 511, loss 0.374016, acc 0.90625\n",
      "2017-11-06T04:37:22.925416: step 512, loss 0.261885, acc 0.9375\n",
      "2017-11-06T04:37:26.856210: step 513, loss 0.269329, acc 0.9375\n",
      "2017-11-06T04:37:30.853050: step 514, loss 0.630605, acc 0.84375\n",
      "2017-11-06T04:37:34.881911: step 515, loss 0.29586, acc 0.90625\n",
      "2017-11-06T04:37:38.936793: step 516, loss 0.235477, acc 0.84375\n",
      "2017-11-06T04:37:42.891604: step 517, loss 0.314557, acc 0.9375\n",
      "2017-11-06T04:37:46.817392: step 518, loss 0.397486, acc 0.84375\n",
      "2017-11-06T04:37:50.792216: step 519, loss 0.260972, acc 0.90625\n",
      "2017-11-06T04:37:54.780049: step 520, loss 0.291295, acc 0.875\n",
      "2017-11-06T04:37:58.755874: step 521, loss 0.0450015, acc 1\n",
      "2017-11-06T04:38:02.673659: step 522, loss 0.472743, acc 0.875\n",
      "2017-11-06T04:38:06.617461: step 523, loss 0.612975, acc 0.875\n",
      "2017-11-06T04:38:11.030597: step 524, loss 0.771407, acc 0.84375\n",
      "2017-11-06T04:38:15.129509: step 525, loss 0.41165, acc 0.875\n",
      "2017-11-06T04:38:19.097330: step 526, loss 0.309274, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T04:38:23.175226: step 527, loss 0.479586, acc 0.90625\n",
      "2017-11-06T04:38:27.377211: step 528, loss 0.233028, acc 0.875\n",
      "2017-11-06T04:38:31.379055: step 529, loss 0.568073, acc 0.875\n",
      "2017-11-06T04:38:35.556023: step 530, loss 0.428233, acc 0.84375\n",
      "2017-11-06T04:38:39.513835: step 531, loss 0.535787, acc 0.90625\n",
      "2017-11-06T04:38:43.487659: step 532, loss 0.682918, acc 0.84375\n",
      "2017-11-06T04:38:47.450474: step 533, loss 0.248553, acc 0.875\n",
      "2017-11-06T04:38:51.450318: step 534, loss 0.564157, acc 0.875\n",
      "2017-11-06T04:38:55.399122: step 535, loss 1.00777, acc 0.84375\n",
      "2017-11-06T04:38:59.393961: step 536, loss 0.327947, acc 0.9375\n",
      "2017-11-06T04:39:03.343767: step 537, loss 0.377243, acc 0.875\n",
      "2017-11-06T04:39:07.315589: step 538, loss 0.593348, acc 0.875\n",
      "2017-11-06T04:39:11.242379: step 539, loss 0.551903, acc 0.875\n",
      "2017-11-06T04:39:13.952305: step 540, loss 0.457319, acc 0.85\n",
      "2017-11-06T04:39:18.229344: step 541, loss 0.520257, acc 0.90625\n",
      "2017-11-06T04:39:22.207172: step 542, loss 0.426309, acc 0.84375\n",
      "2017-11-06T04:39:26.137750: step 543, loss 0.403804, acc 0.875\n",
      "2017-11-06T04:39:30.117579: step 544, loss 0.359455, acc 0.9375\n",
      "2017-11-06T04:39:34.107413: step 545, loss 0.176253, acc 0.90625\n",
      "2017-11-06T04:39:38.081237: step 546, loss 0.688903, acc 0.84375\n",
      "2017-11-06T04:39:42.096089: step 547, loss 0.210721, acc 0.9375\n",
      "2017-11-06T04:39:46.102937: step 548, loss 0.358142, acc 0.9375\n",
      "2017-11-06T04:39:50.139805: step 549, loss 0.381655, acc 0.90625\n",
      "2017-11-06T04:39:54.170669: step 550, loss 0.2576, acc 0.9375\n",
      "2017-11-06T04:39:58.110468: step 551, loss 0.153597, acc 0.90625\n",
      "2017-11-06T04:40:02.351481: step 552, loss 0.745078, acc 0.84375\n",
      "2017-11-06T04:40:06.355327: step 553, loss 0.832762, acc 0.75\n",
      "2017-11-06T04:40:10.291124: step 554, loss 0.562383, acc 0.90625\n",
      "2017-11-06T04:40:14.233924: step 555, loss 0.267535, acc 0.90625\n",
      "2017-11-06T04:40:18.207749: step 556, loss 0.376574, acc 0.875\n",
      "2017-11-06T04:40:22.515810: step 557, loss 0.425443, acc 0.90625\n",
      "2017-11-06T04:40:26.550677: step 558, loss 0.39337, acc 0.875\n",
      "2017-11-06T04:40:30.523499: step 559, loss 0.425968, acc 0.90625\n",
      "2017-11-06T04:40:34.681454: step 560, loss 0.224381, acc 0.9375\n",
      "2017-11-06T04:40:38.681295: step 561, loss 0.517902, acc 0.8125\n",
      "2017-11-06T04:40:42.638107: step 562, loss 0.776467, acc 0.875\n",
      "2017-11-06T04:40:46.585912: step 563, loss 0.050896, acc 0.96875\n",
      "2017-11-06T04:40:50.522710: step 564, loss 0.415908, acc 0.875\n",
      "2017-11-06T04:40:54.465511: step 565, loss 0.105571, acc 0.9375\n",
      "2017-11-06T04:40:58.452343: step 566, loss 0.417992, acc 0.78125\n",
      "2017-11-06T04:41:02.405153: step 567, loss 0.396443, acc 0.875\n",
      "2017-11-06T04:41:06.327940: step 568, loss 0.161337, acc 0.96875\n",
      "2017-11-06T04:41:10.473885: step 569, loss 0.182524, acc 0.90625\n",
      "2017-11-06T04:41:15.666575: step 570, loss 0.331636, acc 0.90625\n",
      "2017-11-06T04:41:19.866561: step 571, loss 0.608779, acc 0.875\n",
      "2017-11-06T04:41:24.013506: step 572, loss 0.448952, acc 0.90625\n",
      "2017-11-06T04:41:28.445656: step 573, loss 0.749669, acc 0.84375\n",
      "2017-11-06T04:41:32.600608: step 574, loss 1.17551, acc 0.78125\n",
      "2017-11-06T04:41:36.546411: step 575, loss 0.894882, acc 0.84375\n",
      "2017-11-06T04:41:39.136251: step 576, loss 0.717651, acc 0.85\n",
      "2017-11-06T04:41:43.151104: step 577, loss 0.487516, acc 0.875\n",
      "2017-11-06T04:41:47.158952: step 578, loss 0.073231, acc 0.96875\n",
      "2017-11-06T04:41:51.274876: step 579, loss 0.341743, acc 0.90625\n",
      "2017-11-06T04:41:55.255705: step 580, loss 0.492812, acc 0.875\n",
      "2017-11-06T04:41:59.176491: step 581, loss 0.372958, acc 0.875\n",
      "2017-11-06T04:42:03.173331: step 582, loss 0.096048, acc 0.9375\n",
      "2017-11-06T04:42:07.122138: step 583, loss 0.3331, acc 0.90625\n",
      "2017-11-06T04:42:11.082951: step 584, loss 0.477964, acc 0.875\n",
      "2017-11-06T04:42:15.018748: step 585, loss 0.217959, acc 0.96875\n",
      "2017-11-06T04:42:18.958548: step 586, loss 0.233034, acc 0.9375\n",
      "2017-11-06T04:42:22.945382: step 587, loss 0.198671, acc 0.96875\n",
      "2017-11-06T04:42:26.929999: step 588, loss 0.425593, acc 0.9375\n",
      "2017-11-06T04:42:31.063935: step 589, loss 0.594192, acc 0.90625\n",
      "2017-11-06T04:42:35.614170: step 590, loss 0.61628, acc 0.8125\n",
      "2017-11-06T04:42:39.546963: step 591, loss 0.46264, acc 0.90625\n",
      "2017-11-06T04:42:43.496770: step 592, loss 0.511384, acc 0.90625\n",
      "2017-11-06T04:42:47.449579: step 593, loss 0.388071, acc 0.90625\n",
      "2017-11-06T04:42:51.388377: step 594, loss 0.0262264, acc 1\n",
      "2017-11-06T04:42:55.399227: step 595, loss 0.493345, acc 0.875\n",
      "2017-11-06T04:42:59.358039: step 596, loss 0.0893147, acc 0.9375\n",
      "2017-11-06T04:43:03.334866: step 597, loss 0.152263, acc 0.96875\n",
      "2017-11-06T04:43:07.371733: step 598, loss 0.354728, acc 0.9375\n",
      "2017-11-06T04:43:11.346558: step 599, loss 0.853295, acc 0.75\n",
      "2017-11-06T04:43:15.302369: step 600, loss 0.489387, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:43:17.889207: step 600, loss 1.7628, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-600\n",
      "\n",
      "2017-11-06T04:43:23.276340: step 601, loss 0.754771, acc 0.84375\n",
      "2017-11-06T04:43:27.324217: step 602, loss 0.964705, acc 0.8125\n",
      "2017-11-06T04:43:31.307046: step 603, loss 0.821193, acc 0.90625\n",
      "2017-11-06T04:43:35.368933: step 604, loss 0.745331, acc 0.875\n",
      "2017-11-06T04:43:39.707014: step 605, loss 0.253038, acc 0.96875\n",
      "2017-11-06T04:43:43.794920: step 606, loss 0.534237, acc 0.84375\n",
      "2017-11-06T04:43:47.779751: step 607, loss 0.640827, acc 0.84375\n",
      "2017-11-06T04:43:51.738565: step 608, loss 0.500475, acc 0.78125\n",
      "2017-11-06T04:43:55.795447: step 609, loss 0.373954, acc 0.90625\n",
      "2017-11-06T04:43:59.797289: step 610, loss 0.301513, acc 0.875\n",
      "2017-11-06T04:44:03.910212: step 611, loss 0.319889, acc 0.90625\n",
      "2017-11-06T04:44:06.527072: step 612, loss 0.338385, acc 0.9\n",
      "2017-11-06T04:44:10.515905: step 613, loss 0.565059, acc 0.875\n",
      "2017-11-06T04:44:14.588800: step 614, loss 0.115053, acc 0.9375\n",
      "2017-11-06T04:44:18.583638: step 615, loss 0.0649276, acc 0.96875\n",
      "2017-11-06T04:44:22.746596: step 616, loss 0.686935, acc 0.8125\n",
      "2017-11-06T04:44:26.952585: step 617, loss 0.324637, acc 0.90625\n",
      "2017-11-06T04:44:30.935414: step 618, loss 0.425899, acc 0.84375\n",
      "2017-11-06T04:44:35.115385: step 619, loss 0.32457, acc 0.90625\n",
      "2017-11-06T04:44:39.103218: step 620, loss 0.432552, acc 0.90625\n",
      "2017-11-06T04:44:43.336226: step 621, loss 0.397874, acc 0.875\n",
      "2017-11-06T04:44:47.847432: step 622, loss 0.695861, acc 0.84375\n",
      "2017-11-06T04:44:51.842270: step 623, loss 0.578387, acc 0.84375\n",
      "2017-11-06T04:44:55.921168: step 624, loss 0.212599, acc 0.90625\n",
      "2017-11-06T04:44:59.941024: step 625, loss 0.376153, acc 0.875\n",
      "2017-11-06T04:45:03.927858: step 626, loss 0.109562, acc 0.96875\n",
      "2017-11-06T04:45:07.968729: step 627, loss 0.864418, acc 0.78125\n",
      "2017-11-06T04:45:11.999593: step 628, loss 0.368448, acc 0.9375\n",
      "2017-11-06T04:45:16.076490: step 629, loss 0.413683, acc 0.90625\n",
      "2017-11-06T04:45:20.057318: step 630, loss 0.426558, acc 0.875\n",
      "2017-11-06T04:45:24.116958: step 631, loss 0.0975288, acc 0.9375\n",
      "2017-11-06T04:45:28.055756: step 632, loss 0.897903, acc 0.84375\n",
      "2017-11-06T04:45:32.053597: step 633, loss 0.979889, acc 0.78125\n",
      "2017-11-06T04:45:36.046435: step 634, loss 0.00837281, acc 1\n",
      "2017-11-06T04:45:40.071295: step 635, loss 0.235456, acc 0.90625\n",
      "2017-11-06T04:45:44.133180: step 636, loss 0.291591, acc 0.90625\n",
      "2017-11-06T04:45:48.268120: step 637, loss 0.56073, acc 0.75\n",
      "2017-11-06T04:45:52.572177: step 638, loss 0.0768205, acc 0.96875\n",
      "2017-11-06T04:45:56.568016: step 639, loss 0.627363, acc 0.84375\n",
      "2017-11-06T04:46:00.610888: step 640, loss 0.671836, acc 0.8125\n",
      "2017-11-06T04:46:04.616735: step 641, loss 0.431563, acc 0.90625\n",
      "2017-11-06T04:46:08.617578: step 642, loss 0.857518, acc 0.90625\n",
      "2017-11-06T04:46:12.652445: step 643, loss 0.397996, acc 0.875\n",
      "2017-11-06T04:46:16.669298: step 644, loss 0.22116, acc 0.9375\n",
      "2017-11-06T04:46:20.626110: step 645, loss 0.233636, acc 0.90625\n",
      "2017-11-06T04:46:24.863121: step 646, loss 0.39692, acc 0.875\n",
      "2017-11-06T04:46:28.886980: step 647, loss 0.143767, acc 0.96875\n",
      "2017-11-06T04:46:31.517849: step 648, loss 0.307163, acc 0.95\n",
      "2017-11-06T04:46:35.855931: step 649, loss 0.00273264, acc 1\n",
      "2017-11-06T04:46:39.915816: step 650, loss 0.567235, acc 0.875\n",
      "2017-11-06T04:46:43.874629: step 651, loss 0.632054, acc 0.8125\n",
      "2017-11-06T04:46:47.995556: step 652, loss 0.667443, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T04:46:52.043434: step 653, loss 0.461006, acc 0.9375\n",
      "2017-11-06T04:46:56.513609: step 654, loss 0.546312, acc 0.875\n",
      "2017-11-06T04:47:00.658555: step 655, loss 0.0801172, acc 0.96875\n",
      "2017-11-06T04:47:04.617367: step 656, loss 0.440053, acc 0.90625\n",
      "2017-11-06T04:47:08.637224: step 657, loss 0.519613, acc 0.875\n",
      "2017-11-06T04:47:12.661083: step 658, loss 0.244689, acc 0.90625\n",
      "2017-11-06T04:47:16.642914: step 659, loss 0.57091, acc 0.90625\n",
      "2017-11-06T04:47:20.533678: step 660, loss 0.264879, acc 0.9375\n",
      "2017-11-06T04:47:24.494491: step 661, loss 0.159877, acc 0.9375\n",
      "2017-11-06T04:47:28.434290: step 662, loss 0.795529, acc 0.8125\n",
      "2017-11-06T04:47:32.364084: step 663, loss 0.348905, acc 0.9375\n",
      "2017-11-06T04:47:36.303882: step 664, loss 0.232838, acc 0.90625\n",
      "2017-11-06T04:47:40.287714: step 665, loss 0.277428, acc 0.875\n",
      "2017-11-06T04:47:44.339891: step 666, loss 0.376975, acc 0.875\n",
      "2017-11-06T04:47:48.316716: step 667, loss 0.642, acc 0.90625\n",
      "2017-11-06T04:47:52.305550: step 668, loss 0.63865, acc 0.84375\n",
      "2017-11-06T04:47:56.226337: step 669, loss 0.501533, acc 0.875\n",
      "2017-11-06T04:48:00.451338: step 670, loss 0.195918, acc 0.9375\n",
      "2017-11-06T04:48:04.653324: step 671, loss 0.138708, acc 0.9375\n",
      "2017-11-06T04:48:08.589120: step 672, loss 0.628281, acc 0.90625\n",
      "2017-11-06T04:48:12.544931: step 673, loss 0.356664, acc 0.9375\n",
      "2017-11-06T04:48:16.553781: step 674, loss 0.338713, acc 0.84375\n",
      "2017-11-06T04:48:20.489576: step 675, loss 0.295546, acc 0.84375\n",
      "2017-11-06T04:48:24.623261: step 676, loss 0.430426, acc 0.8125\n",
      "2017-11-06T04:48:28.583076: step 677, loss 0.354553, acc 0.84375\n",
      "2017-11-06T04:48:32.621945: step 678, loss 0.00968122, acc 1\n",
      "2017-11-06T04:48:36.692837: step 679, loss 0.679607, acc 0.84375\n",
      "2017-11-06T04:48:40.646651: step 680, loss 0.035975, acc 1\n",
      "2017-11-06T04:48:44.609465: step 681, loss 0.643397, acc 0.9375\n",
      "2017-11-06T04:48:48.574280: step 682, loss 0.359828, acc 0.90625\n",
      "2017-11-06T04:48:52.531092: step 683, loss 0.641263, acc 0.84375\n",
      "2017-11-06T04:48:55.050882: step 684, loss 0.831437, acc 0.85\n",
      "2017-11-06T04:48:59.028710: step 685, loss 0.607018, acc 0.90625\n",
      "2017-11-06T04:49:02.948494: step 686, loss 0.532338, acc 0.84375\n",
      "2017-11-06T04:49:07.269564: step 687, loss 0.306391, acc 0.90625\n",
      "2017-11-06T04:49:11.422517: step 688, loss 0.735717, acc 0.78125\n",
      "2017-11-06T04:49:15.454380: step 689, loss 0.74206, acc 0.84375\n",
      "2017-11-06T04:49:19.439211: step 690, loss 0.620964, acc 0.875\n",
      "2017-11-06T04:49:23.408033: step 691, loss 0.120378, acc 0.9375\n",
      "2017-11-06T04:49:27.386859: step 692, loss 0.0487948, acc 0.96875\n",
      "2017-11-06T04:49:31.335664: step 693, loss 0.297168, acc 0.84375\n",
      "2017-11-06T04:49:35.311490: step 694, loss 0.958842, acc 0.8125\n",
      "2017-11-06T04:49:39.262296: step 695, loss 0.542011, acc 0.84375\n",
      "2017-11-06T04:49:43.211104: step 696, loss 0.0710513, acc 0.96875\n",
      "2017-11-06T04:49:47.317853: step 697, loss 0.2284, acc 0.9375\n",
      "2017-11-06T04:49:51.287674: step 698, loss 0.235678, acc 0.90625\n",
      "2017-11-06T04:49:55.241481: step 699, loss 0.557809, acc 0.84375\n",
      "2017-11-06T04:49:59.175296: step 700, loss 0.401412, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:50:01.989278: step 700, loss 1.37195, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-700\n",
      "\n",
      "2017-11-06T04:50:07.467577: step 701, loss 0.0694446, acc 0.9375\n",
      "2017-11-06T04:50:11.657553: step 702, loss 0.417091, acc 0.9375\n",
      "2017-11-06T04:50:15.907573: step 703, loss 0.265519, acc 0.90625\n",
      "2017-11-06T04:50:19.850374: step 704, loss 0.0852956, acc 0.9375\n",
      "2017-11-06T04:50:23.854219: step 705, loss 0.396984, acc 0.875\n",
      "2017-11-06T04:50:27.811030: step 706, loss 0.682062, acc 0.8125\n",
      "2017-11-06T04:50:31.765841: step 707, loss 0.270569, acc 0.875\n",
      "2017-11-06T04:50:35.959821: step 708, loss 0.142942, acc 0.96875\n",
      "2017-11-06T04:50:39.930642: step 709, loss 0.000797884, acc 1\n",
      "2017-11-06T04:50:43.933486: step 710, loss 0.521902, acc 0.84375\n",
      "2017-11-06T04:50:47.910312: step 711, loss 0.02039, acc 1\n",
      "2017-11-06T04:50:51.817088: step 712, loss 0.319877, acc 0.9375\n",
      "2017-11-06T04:50:55.866965: step 713, loss 0.461462, acc 0.8125\n",
      "2017-11-06T04:50:59.844792: step 714, loss 0.415157, acc 0.875\n",
      "2017-11-06T04:51:03.814613: step 715, loss 0.850493, acc 0.84375\n",
      "2017-11-06T04:51:07.771424: step 716, loss 0.943135, acc 0.8125\n",
      "2017-11-06T04:51:11.786276: step 717, loss 0.15182, acc 0.96875\n",
      "2017-11-06T04:51:16.012280: step 718, loss 0.24055, acc 0.90625\n",
      "2017-11-06T04:51:20.381384: step 719, loss 0.564332, acc 0.875\n",
      "2017-11-06T04:51:22.882161: step 720, loss 0.211772, acc 0.95\n",
      "2017-11-06T04:51:26.932909: step 721, loss 0.313763, acc 0.84375\n",
      "2017-11-06T04:51:30.868706: step 722, loss 0.54091, acc 0.84375\n",
      "2017-11-06T04:51:34.912579: step 723, loss 0.256463, acc 0.90625\n",
      "2017-11-06T04:51:39.006488: step 724, loss 0.594556, acc 0.84375\n",
      "2017-11-06T04:51:43.009331: step 725, loss 0.192838, acc 0.90625\n",
      "2017-11-06T04:51:47.006172: step 726, loss 0.0857961, acc 0.9375\n",
      "2017-11-06T04:51:51.034033: step 727, loss 0.441122, acc 0.90625\n",
      "2017-11-06T04:51:54.966828: step 728, loss 0.132868, acc 0.9375\n",
      "2017-11-06T04:51:58.967670: step 729, loss 0.00328965, acc 1\n",
      "2017-11-06T04:52:02.920479: step 730, loss 0.594817, acc 0.8125\n",
      "2017-11-06T04:52:06.866283: step 731, loss 0.216991, acc 0.96875\n",
      "2017-11-06T04:52:10.887140: step 732, loss 0.273782, acc 0.875\n",
      "2017-11-06T04:52:14.872972: step 733, loss 0.320435, acc 0.90625\n",
      "2017-11-06T04:52:18.882821: step 734, loss 0.255398, acc 0.875\n",
      "2017-11-06T04:52:23.133842: step 735, loss 0.249594, acc 0.9375\n",
      "2017-11-06T04:52:27.282790: step 736, loss 0.368387, acc 0.875\n",
      "2017-11-06T04:52:31.256614: step 737, loss 0.257854, acc 0.90625\n",
      "2017-11-06T04:52:35.460600: step 738, loss 0.4318, acc 0.84375\n",
      "2017-11-06T04:52:39.423418: step 739, loss 0.384161, acc 0.84375\n",
      "2017-11-06T04:52:43.367220: step 740, loss 0.230959, acc 0.90625\n",
      "2017-11-06T04:52:47.338041: step 741, loss 0.355841, acc 0.84375\n",
      "2017-11-06T04:52:51.293851: step 742, loss 0.442187, acc 0.875\n",
      "2017-11-06T04:52:55.284686: step 743, loss 0.538889, acc 0.84375\n",
      "2017-11-06T04:52:59.290533: step 744, loss 0.685496, acc 0.875\n",
      "2017-11-06T04:53:03.267358: step 745, loss 0.287217, acc 0.9375\n",
      "2017-11-06T04:53:07.256194: step 746, loss 0.719453, acc 0.84375\n",
      "2017-11-06T04:53:11.245027: step 747, loss 0.399836, acc 0.90625\n",
      "2017-11-06T04:53:15.278894: step 748, loss 0.446417, acc 0.84375\n",
      "2017-11-06T04:53:19.398820: step 749, loss 0.815864, acc 0.8125\n",
      "2017-11-06T04:53:23.530756: step 750, loss 0.232477, acc 0.90625\n",
      "2017-11-06T04:53:27.840818: step 751, loss 0.385381, acc 0.875\n",
      "2017-11-06T04:53:32.087836: step 752, loss 0.587412, acc 0.875\n",
      "2017-11-06T04:53:36.089680: step 753, loss 0.100736, acc 0.96875\n",
      "2017-11-06T04:53:40.013468: step 754, loss 0.765499, acc 0.8125\n",
      "2017-11-06T04:53:43.952266: step 755, loss 0.537212, acc 0.875\n",
      "2017-11-06T04:53:46.464051: step 756, loss 0.915198, acc 0.75\n",
      "2017-11-06T04:53:50.437875: step 757, loss 0.0337443, acc 0.96875\n",
      "2017-11-06T04:53:54.514671: step 758, loss 0.0772693, acc 0.96875\n",
      "2017-11-06T04:53:58.466479: step 759, loss 0.660887, acc 0.8125\n",
      "2017-11-06T04:54:02.405279: step 760, loss 0.350731, acc 0.875\n",
      "2017-11-06T04:54:06.366092: step 761, loss 0.309203, acc 0.9375\n",
      "2017-11-06T04:54:10.292882: step 762, loss 0.557438, acc 0.84375\n",
      "2017-11-06T04:54:14.256698: step 763, loss 0.147289, acc 0.90625\n",
      "2017-11-06T04:54:18.220514: step 764, loss 0.0373002, acc 1\n",
      "2017-11-06T04:54:22.185332: step 765, loss 0.245512, acc 0.9375\n",
      "2017-11-06T04:54:26.161922: step 766, loss 0.391509, acc 0.875\n",
      "2017-11-06T04:54:30.118733: step 767, loss 0.0536872, acc 0.96875\n",
      "2017-11-06T04:54:34.589909: step 768, loss 0.56263, acc 0.84375\n",
      "2017-11-06T04:54:38.798900: step 769, loss 0.164768, acc 0.90625\n",
      "2017-11-06T04:54:42.741701: step 770, loss 0.506659, acc 0.875\n",
      "2017-11-06T04:54:46.724533: step 771, loss 0.49985, acc 0.90625\n",
      "2017-11-06T04:54:50.822443: step 772, loss 0.421804, acc 0.84375\n",
      "2017-11-06T04:54:54.761243: step 773, loss 0.268385, acc 0.96875\n",
      "2017-11-06T04:54:58.722057: step 774, loss 0.0185134, acc 1\n",
      "2017-11-06T04:55:02.659854: step 775, loss 0.445442, acc 0.875\n",
      "2017-11-06T04:55:06.579640: step 776, loss 0.225992, acc 0.90625\n",
      "2017-11-06T04:55:10.553463: step 777, loss 0.299891, acc 0.875\n",
      "2017-11-06T04:55:14.506272: step 778, loss 0.486624, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T04:55:18.489101: step 779, loss 0.376679, acc 0.9375\n",
      "2017-11-06T04:55:22.461925: step 780, loss 0.0936948, acc 0.9375\n",
      "2017-11-06T04:55:26.425741: step 781, loss 0.721278, acc 0.875\n",
      "2017-11-06T04:55:30.355534: step 782, loss 0.407562, acc 0.90625\n",
      "2017-11-06T04:55:34.257307: step 783, loss 0.181428, acc 0.9375\n",
      "2017-11-06T04:55:38.352215: step 784, loss 0.00551553, acc 1\n",
      "2017-11-06T04:55:42.625253: step 785, loss 0.205766, acc 0.9375\n",
      "2017-11-06T04:55:46.525024: step 786, loss 0.100229, acc 0.9375\n",
      "2017-11-06T04:55:50.512856: step 787, loss 0.638208, acc 0.84375\n",
      "2017-11-06T04:55:54.516769: step 788, loss 0.278524, acc 0.875\n",
      "2017-11-06T04:55:58.496597: step 789, loss 0.46862, acc 0.84375\n",
      "2017-11-06T04:56:02.461415: step 790, loss 0.13579, acc 0.9375\n",
      "2017-11-06T04:56:06.441243: step 791, loss 0.506434, acc 0.8125\n",
      "2017-11-06T04:56:08.975042: step 792, loss 0.0967092, acc 0.95\n",
      "2017-11-06T04:56:12.931853: step 793, loss 0.169937, acc 0.9375\n",
      "2017-11-06T04:56:16.939701: step 794, loss 0.108671, acc 0.9375\n",
      "2017-11-06T04:56:20.900515: step 795, loss 0.392324, acc 0.875\n",
      "2017-11-06T04:56:24.883346: step 796, loss 0.602723, acc 0.78125\n",
      "2017-11-06T04:56:28.804131: step 797, loss 0.677899, acc 0.8125\n",
      "2017-11-06T04:56:32.753938: step 798, loss 0.308409, acc 0.90625\n",
      "2017-11-06T04:56:36.871864: step 799, loss 0.565168, acc 0.875\n",
      "2017-11-06T04:56:40.808662: step 800, loss 0.233737, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T04:56:43.530596: step 800, loss 1.43258, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-800\n",
      "\n",
      "2017-11-06T04:56:49.654960: step 801, loss 0.415185, acc 0.90625\n",
      "2017-11-06T04:56:53.650799: step 802, loss 0.074321, acc 0.96875\n",
      "2017-11-06T04:56:57.678661: step 803, loss 0.275373, acc 0.9375\n",
      "2017-11-06T04:57:01.645481: step 804, loss 0.0544989, acc 0.96875\n",
      "2017-11-06T04:57:05.588281: step 805, loss 0.419234, acc 0.84375\n",
      "2017-11-06T04:57:09.611139: step 806, loss 0.227409, acc 0.9375\n",
      "2017-11-06T04:57:13.601975: step 807, loss 0.0446057, acc 0.96875\n",
      "2017-11-06T04:57:17.659858: step 808, loss 0.0710707, acc 0.96875\n",
      "2017-11-06T04:57:21.646692: step 809, loss 0.35258, acc 0.9375\n",
      "2017-11-06T04:57:25.644270: step 810, loss 0.236047, acc 0.875\n",
      "2017-11-06T04:57:29.585070: step 811, loss 0.387879, acc 0.90625\n",
      "2017-11-06T04:57:33.616934: step 812, loss 0.465359, acc 0.8125\n",
      "2017-11-06T04:57:37.688828: step 813, loss 0.0879053, acc 0.9375\n",
      "2017-11-06T04:57:41.632630: step 814, loss 0.428845, acc 0.96875\n",
      "2017-11-06T04:57:45.594446: step 815, loss 0.275519, acc 0.9375\n",
      "2017-11-06T04:57:49.788425: step 816, loss 0.0188134, acc 1\n",
      "2017-11-06T04:57:54.117501: step 817, loss 0.232817, acc 0.9375\n",
      "2017-11-06T04:57:58.187393: step 818, loss 0.186423, acc 0.90625\n",
      "2017-11-06T04:58:02.195241: step 819, loss 0.433925, acc 0.84375\n",
      "2017-11-06T04:58:06.177071: step 820, loss 0.275276, acc 0.90625\n",
      "2017-11-06T04:58:10.217942: step 821, loss 0.117996, acc 0.9375\n",
      "2017-11-06T04:58:14.227790: step 822, loss 0.767238, acc 0.78125\n",
      "2017-11-06T04:58:18.239640: step 823, loss 0.551995, acc 0.8125\n",
      "2017-11-06T04:58:22.359568: step 824, loss 0.363765, acc 0.875\n",
      "2017-11-06T04:58:26.597580: step 825, loss 0.390171, acc 0.875\n",
      "2017-11-06T04:58:30.862610: step 826, loss 0.867224, acc 0.8125\n",
      "2017-11-06T04:58:34.996549: step 827, loss 0.0452171, acc 0.96875\n",
      "2017-11-06T04:58:37.644429: step 828, loss 0.418399, acc 0.8\n",
      "2017-11-06T04:58:41.625257: step 829, loss 0.192969, acc 0.9375\n",
      "2017-11-06T04:58:45.607086: step 830, loss 0.119223, acc 0.9375\n",
      "2017-11-06T04:58:49.654963: step 831, loss 0.158665, acc 0.90625\n",
      "2017-11-06T04:58:53.680824: step 832, loss 0.202351, acc 0.9375\n",
      "2017-11-06T04:58:58.071943: step 833, loss 0.351399, acc 0.875\n",
      "2017-11-06T04:59:02.202878: step 834, loss 0.408031, acc 0.84375\n",
      "2017-11-06T04:59:06.176704: step 835, loss 0.133066, acc 0.96875\n",
      "2017-11-06T04:59:10.193557: step 836, loss 0.0348318, acc 0.96875\n",
      "2017-11-06T04:59:14.156372: step 837, loss 0.390277, acc 0.875\n",
      "2017-11-06T04:59:18.116186: step 838, loss 0.0428406, acc 0.96875\n",
      "2017-11-06T04:59:22.086009: step 839, loss 0.0982323, acc 0.90625\n",
      "2017-11-06T04:59:26.129880: step 840, loss 0.290224, acc 0.9375\n",
      "2017-11-06T04:59:30.152737: step 841, loss 0.474479, acc 0.84375\n",
      "2017-11-06T04:59:34.099543: step 842, loss 0.306456, acc 0.875\n",
      "2017-11-06T04:59:38.096382: step 843, loss 0.196622, acc 0.9375\n",
      "2017-11-06T04:59:42.031178: step 844, loss 0.545275, acc 0.84375\n",
      "2017-11-06T04:59:45.997997: step 845, loss 0.3739, acc 0.84375\n",
      "2017-11-06T04:59:50.036866: step 846, loss 0.566731, acc 0.8125\n",
      "2017-11-06T04:59:54.018696: step 847, loss 0.261748, acc 0.90625\n",
      "2017-11-06T04:59:58.021540: step 848, loss 0.359176, acc 0.90625\n",
      "2017-11-06T05:00:02.752674: step 849, loss 0.415685, acc 0.8125\n",
      "2017-11-06T05:00:06.919634: step 850, loss 0.455088, acc 0.875\n",
      "2017-11-06T05:00:10.892456: step 851, loss 0.388037, acc 0.8125\n",
      "2017-11-06T05:00:14.874286: step 852, loss 0.390041, acc 0.9375\n",
      "2017-11-06T05:00:18.831097: step 853, loss 0.293985, acc 0.90625\n",
      "2017-11-06T05:00:22.868967: step 854, loss 0.109684, acc 0.96875\n",
      "2017-11-06T05:00:26.882710: step 855, loss 0.320281, acc 0.90625\n",
      "2017-11-06T05:00:30.906570: step 856, loss 0.0759178, acc 0.96875\n",
      "2017-11-06T05:00:35.004482: step 857, loss 0.490625, acc 0.875\n",
      "2017-11-06T05:00:39.026339: step 858, loss 0.471516, acc 0.84375\n",
      "2017-11-06T05:00:43.130345: step 859, loss 0.808371, acc 0.78125\n",
      "2017-11-06T05:00:47.132187: step 860, loss 0.348711, acc 0.96875\n",
      "2017-11-06T05:00:51.067985: step 861, loss 0.66082, acc 0.875\n",
      "2017-11-06T05:00:55.048812: step 862, loss 0.0643501, acc 0.96875\n",
      "2017-11-06T05:00:59.032643: step 863, loss 0.220608, acc 0.9375\n",
      "2017-11-06T05:01:01.665514: step 864, loss 0.0796194, acc 0.95\n",
      "2017-11-06T05:01:05.742411: step 865, loss 0.316099, acc 0.9375\n",
      "2017-11-06T05:01:10.210587: step 866, loss 0.253128, acc 0.90625\n",
      "2017-11-06T05:01:14.187412: step 867, loss 0.0187523, acc 1\n",
      "2017-11-06T05:01:18.213272: step 868, loss 0.567921, acc 0.875\n",
      "2017-11-06T05:01:22.212115: step 869, loss 0.4882, acc 0.875\n",
      "2017-11-06T05:01:26.231970: step 870, loss 0.148761, acc 0.90625\n",
      "2017-11-06T05:01:30.294857: step 871, loss 0.207452, acc 0.90625\n",
      "2017-11-06T05:01:34.320717: step 872, loss 0.107126, acc 0.96875\n",
      "2017-11-06T05:01:38.506691: step 873, loss 0.364394, acc 0.875\n",
      "2017-11-06T05:01:42.617612: step 874, loss 0.380384, acc 0.90625\n",
      "2017-11-06T05:01:46.639470: step 875, loss 0.192613, acc 0.90625\n",
      "2017-11-06T05:01:50.832449: step 876, loss 0.459735, acc 0.8125\n",
      "2017-11-06T05:01:54.941368: step 877, loss 0.349968, acc 0.875\n",
      "2017-11-06T05:01:58.924200: step 878, loss 0.38172, acc 0.9375\n",
      "2017-11-06T05:02:03.166213: step 879, loss 0.230824, acc 0.875\n",
      "2017-11-06T05:02:07.185069: step 880, loss 0.494677, acc 0.8125\n",
      "2017-11-06T05:02:11.221936: step 881, loss 0.166883, acc 0.90625\n",
      "2017-11-06T05:02:15.628068: step 882, loss 0.323292, acc 0.875\n",
      "2017-11-06T05:02:19.632913: step 883, loss 0.211859, acc 0.90625\n",
      "2017-11-06T05:02:23.596729: step 884, loss 0.171002, acc 0.9375\n",
      "2017-11-06T05:02:27.581562: step 885, loss 0.383604, acc 0.875\n",
      "2017-11-06T05:02:31.627435: step 886, loss 0.259436, acc 0.9375\n",
      "2017-11-06T05:02:35.825421: step 887, loss 0.530656, acc 0.8125\n",
      "2017-11-06T05:02:39.797241: step 888, loss 0.291437, acc 0.90625\n",
      "2017-11-06T05:02:43.803087: step 889, loss 0.192764, acc 0.9375\n",
      "2017-11-06T05:02:47.725875: step 890, loss 0.110055, acc 0.9375\n",
      "2017-11-06T05:02:51.702700: step 891, loss 0.331133, acc 0.875\n",
      "2017-11-06T05:02:55.656509: step 892, loss 0.420472, acc 0.875\n",
      "2017-11-06T05:02:59.627331: step 893, loss 0.332717, acc 0.90625\n",
      "2017-11-06T05:03:03.569134: step 894, loss 0.492819, acc 0.75\n",
      "2017-11-06T05:03:07.528945: step 895, loss 0.390661, acc 0.90625\n",
      "2017-11-06T05:03:11.490761: step 896, loss 0.397552, acc 0.875\n",
      "2017-11-06T05:03:15.463583: step 897, loss 0.062773, acc 0.9375\n",
      "2017-11-06T05:03:19.695591: step 898, loss 0.255033, acc 0.875\n",
      "2017-11-06T05:03:24.032673: step 899, loss 0.387074, acc 0.90625\n",
      "2017-11-06T05:03:26.658279: step 900, loss 0.383824, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:03:29.270135: step 900, loss 1.05393, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-900\n",
      "\n",
      "2017-11-06T05:03:35.657856: step 901, loss 0.279627, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T05:03:39.711737: step 902, loss 0.228689, acc 0.9375\n",
      "2017-11-06T05:03:43.785631: step 903, loss 0.283717, acc 0.875\n",
      "2017-11-06T05:03:47.855523: step 904, loss 0.150278, acc 0.90625\n",
      "2017-11-06T05:03:51.951434: step 905, loss 0.328212, acc 0.875\n",
      "2017-11-06T05:03:56.006315: step 906, loss 0.216298, acc 0.9375\n",
      "2017-11-06T05:04:00.136249: step 907, loss 0.0604638, acc 0.96875\n",
      "2017-11-06T05:04:04.415290: step 908, loss 0.302726, acc 0.90625\n",
      "2017-11-06T05:04:08.471171: step 909, loss 0.741208, acc 0.84375\n",
      "2017-11-06T05:04:12.543065: step 910, loss 0.207508, acc 0.96875\n",
      "2017-11-06T05:04:16.625966: step 911, loss 0.235535, acc 0.875\n",
      "2017-11-06T05:04:20.747895: step 912, loss 0.446601, acc 0.84375\n",
      "2017-11-06T05:04:24.998915: step 913, loss 0.98859, acc 0.78125\n",
      "2017-11-06T05:04:29.277955: step 914, loss 0.203521, acc 0.90625\n",
      "2017-11-06T05:04:33.561999: step 915, loss 0.189583, acc 0.90625\n",
      "2017-11-06T05:04:37.780998: step 916, loss 0.036, acc 0.96875\n",
      "2017-11-06T05:04:41.911934: step 917, loss 0.206316, acc 0.9375\n",
      "2017-11-06T05:04:46.021852: step 918, loss 0.202859, acc 0.90625\n",
      "2017-11-06T05:04:50.301894: step 919, loss 0.163329, acc 0.90625\n",
      "2017-11-06T05:04:54.347768: step 920, loss 0.148129, acc 0.9375\n",
      "2017-11-06T05:04:58.478704: step 921, loss 0.281707, acc 0.90625\n",
      "2017-11-06T05:05:02.518575: step 922, loss 0.414241, acc 0.875\n",
      "2017-11-06T05:05:06.475387: step 923, loss 0.197807, acc 0.875\n",
      "2017-11-06T05:05:10.450210: step 924, loss 0.374199, acc 0.90625\n",
      "2017-11-06T05:05:14.392011: step 925, loss 0.474154, acc 0.875\n",
      "2017-11-06T05:05:18.346821: step 926, loss 0.14693, acc 0.9375\n",
      "2017-11-06T05:05:22.378686: step 927, loss 0.051687, acc 0.96875\n",
      "2017-11-06T05:05:26.342502: step 928, loss 0.237734, acc 0.875\n",
      "2017-11-06T05:05:30.579513: step 929, loss 0.523067, acc 0.875\n",
      "2017-11-06T05:05:34.809520: step 930, loss 0.699837, acc 0.84375\n",
      "2017-11-06T05:05:38.764329: step 931, loss 0.446948, acc 0.875\n",
      "2017-11-06T05:05:42.745157: step 932, loss 0.834142, acc 0.875\n",
      "2017-11-06T05:05:46.691961: step 933, loss 0.339925, acc 0.875\n",
      "2017-11-06T05:05:50.671789: step 934, loss 0.33579, acc 0.84375\n",
      "2017-11-06T05:05:54.646617: step 935, loss 0.287552, acc 0.90625\n",
      "2017-11-06T05:05:57.118370: step 936, loss 0.245577, acc 0.9\n",
      "2017-11-06T05:06:01.171249: step 937, loss 0.103367, acc 0.9375\n",
      "2017-11-06T05:06:05.203219: step 938, loss 0.123123, acc 0.875\n",
      "2017-11-06T05:06:09.165034: step 939, loss 0.233545, acc 0.90625\n",
      "2017-11-06T05:06:13.097831: step 940, loss 0.502073, acc 0.84375\n",
      "2017-11-06T05:06:17.080659: step 941, loss 0.365162, acc 0.875\n",
      "2017-11-06T05:06:21.108521: step 942, loss 0.2642, acc 0.84375\n",
      "2017-11-06T05:06:25.049321: step 943, loss 0.564607, acc 0.875\n",
      "2017-11-06T05:06:29.006922: step 944, loss 0.263566, acc 0.875\n",
      "2017-11-06T05:06:33.064807: step 945, loss 0.147265, acc 0.90625\n",
      "2017-11-06T05:06:37.588019: step 946, loss 0.315011, acc 0.875\n",
      "2017-11-06T05:06:41.625888: step 947, loss 0.475825, acc 0.90625\n",
      "2017-11-06T05:06:45.673764: step 948, loss 0.344376, acc 0.90625\n",
      "2017-11-06T05:06:49.644586: step 949, loss 0.377418, acc 0.90625\n",
      "2017-11-06T05:06:53.704471: step 950, loss 0.183173, acc 0.96875\n",
      "2017-11-06T05:06:57.697307: step 951, loss 0.576953, acc 0.875\n",
      "2017-11-06T05:07:01.730173: step 952, loss 0.0313122, acc 1\n",
      "2017-11-06T05:07:05.694991: step 953, loss 0.563509, acc 0.75\n",
      "2017-11-06T05:07:09.669816: step 954, loss 0.33714, acc 0.84375\n",
      "2017-11-06T05:07:13.698677: step 955, loss 0.445825, acc 0.90625\n",
      "2017-11-06T05:07:17.690513: step 956, loss 0.0417032, acc 1\n",
      "2017-11-06T05:07:21.691357: step 957, loss 0.588321, acc 0.875\n",
      "2017-11-06T05:07:25.677189: step 958, loss 0.178897, acc 0.90625\n",
      "2017-11-06T05:07:29.598975: step 959, loss 0.180193, acc 0.9375\n",
      "2017-11-06T05:07:33.561791: step 960, loss 0.326921, acc 0.9375\n",
      "2017-11-06T05:07:37.615671: step 961, loss 0.199381, acc 0.9375\n",
      "2017-11-06T05:07:41.922732: step 962, loss 0.3438, acc 0.8125\n",
      "2017-11-06T05:07:46.193767: step 963, loss 0.312986, acc 0.90625\n",
      "2017-11-06T05:07:50.256653: step 964, loss 0.627977, acc 0.75\n",
      "2017-11-06T05:07:54.285516: step 965, loss 0.402858, acc 0.90625\n",
      "2017-11-06T05:07:58.232320: step 966, loss 0.242753, acc 0.90625\n",
      "2017-11-06T05:08:02.211148: step 967, loss 0.248915, acc 0.875\n",
      "2017-11-06T05:08:06.239010: step 968, loss 0.142012, acc 0.90625\n",
      "2017-11-06T05:08:10.258865: step 969, loss 0.367431, acc 0.84375\n",
      "2017-11-06T05:08:14.274719: step 970, loss 0.229576, acc 0.90625\n",
      "2017-11-06T05:08:18.289572: step 971, loss 0.33185, acc 0.90625\n",
      "2017-11-06T05:08:20.842385: step 972, loss 0.351373, acc 0.85\n",
      "2017-11-06T05:08:25.059382: step 973, loss 0.169454, acc 0.9375\n",
      "2017-11-06T05:08:29.314405: step 974, loss 0.353065, acc 0.90625\n",
      "2017-11-06T05:08:33.366285: step 975, loss 0.427246, acc 0.8125\n",
      "2017-11-06T05:08:37.522237: step 976, loss 0.0530009, acc 0.96875\n",
      "2017-11-06T05:08:41.505067: step 977, loss 0.395644, acc 0.875\n",
      "2017-11-06T05:08:45.614988: step 978, loss 0.184705, acc 0.90625\n",
      "2017-11-06T05:08:50.004106: step 979, loss 0.0501918, acc 0.96875\n",
      "2017-11-06T05:08:54.030967: step 980, loss 0.153383, acc 0.9375\n",
      "2017-11-06T05:08:58.000789: step 981, loss 0.314694, acc 0.9375\n",
      "2017-11-06T05:09:02.047664: step 982, loss 0.140442, acc 0.96875\n",
      "2017-11-06T05:09:06.016484: step 983, loss 0.167979, acc 0.90625\n",
      "2017-11-06T05:09:10.066362: step 984, loss 0.227638, acc 0.90625\n",
      "2017-11-06T05:09:14.045188: step 985, loss 0.123599, acc 0.9375\n",
      "2017-11-06T05:09:18.052037: step 986, loss 0.0623328, acc 0.96875\n",
      "2017-11-06T05:09:22.045873: step 987, loss 0.221552, acc 0.875\n",
      "2017-11-06T05:09:26.037710: step 988, loss 0.127953, acc 0.90625\n",
      "2017-11-06T05:09:30.069341: step 989, loss 0.10072, acc 0.96875\n",
      "2017-11-06T05:09:34.038163: step 990, loss 0.418805, acc 0.84375\n",
      "2017-11-06T05:09:38.019991: step 991, loss 0.325713, acc 0.875\n",
      "2017-11-06T05:09:42.043852: step 992, loss 0.140815, acc 0.9375\n",
      "2017-11-06T05:09:46.004665: step 993, loss 0.410231, acc 0.90625\n",
      "2017-11-06T05:09:50.008509: step 994, loss 0.776673, acc 0.8125\n",
      "2017-11-06T05:09:54.382617: step 995, loss 0.161604, acc 0.96875\n",
      "2017-11-06T05:09:58.431494: step 996, loss 0.202657, acc 0.90625\n",
      "2017-11-06T05:10:02.705530: step 997, loss 0.271375, acc 0.90625\n",
      "2017-11-06T05:10:06.730941: step 998, loss 0.23668, acc 0.90625\n",
      "2017-11-06T05:10:10.697760: step 999, loss 0.471479, acc 0.8125\n",
      "2017-11-06T05:10:14.641562: step 1000, loss 0.384465, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:10:17.219394: step 1000, loss 1.06759, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-06T05:10:22.838158: step 1001, loss 0.160196, acc 0.9375\n",
      "2017-11-06T05:10:26.900043: step 1002, loss 0.217097, acc 0.84375\n",
      "2017-11-06T05:10:30.970936: step 1003, loss 0.326378, acc 0.875\n",
      "2017-11-06T05:10:35.157911: step 1004, loss 0.270251, acc 0.90625\n",
      "2017-11-06T05:10:39.174765: step 1005, loss 0.294939, acc 0.875\n",
      "2017-11-06T05:10:43.257667: step 1006, loss 0.402087, acc 0.875\n",
      "2017-11-06T05:10:47.308546: step 1007, loss 0.298361, acc 0.90625\n",
      "2017-11-06T05:10:49.855354: step 1008, loss 0.268914, acc 0.85\n",
      "2017-11-06T05:10:53.888219: step 1009, loss 0.32237, acc 0.8125\n",
      "2017-11-06T05:10:58.005145: step 1010, loss 0.182616, acc 0.96875\n",
      "2017-11-06T05:11:02.467315: step 1011, loss 0.495469, acc 0.78125\n",
      "2017-11-06T05:11:06.568231: step 1012, loss 0.494035, acc 0.875\n",
      "2017-11-06T05:11:10.653131: step 1013, loss 0.100335, acc 0.96875\n",
      "2017-11-06T05:11:14.717019: step 1014, loss 0.121561, acc 0.96875\n",
      "2017-11-06T05:11:18.760893: step 1015, loss 0.387322, acc 0.8125\n",
      "2017-11-06T05:11:22.873815: step 1016, loss 0.266364, acc 0.90625\n",
      "2017-11-06T05:11:26.878661: step 1017, loss 0.210459, acc 0.90625\n",
      "2017-11-06T05:11:30.923534: step 1018, loss 0.147904, acc 0.9375\n",
      "2017-11-06T05:11:34.962404: step 1019, loss 0.34007, acc 0.90625\n",
      "2017-11-06T05:11:38.942232: step 1020, loss 0.0495488, acc 0.96875\n",
      "2017-11-06T05:11:42.972096: step 1021, loss 0.0169243, acc 1\n",
      "2017-11-06T05:11:47.025977: step 1022, loss 0.0868293, acc 0.96875\n",
      "2017-11-06T05:11:51.228963: step 1023, loss 0.332182, acc 0.90625\n",
      "2017-11-06T05:11:55.313865: step 1024, loss 0.212838, acc 0.9375\n",
      "2017-11-06T05:11:59.430790: step 1025, loss 0.51331, acc 0.875\n",
      "2017-11-06T05:12:03.618766: step 1026, loss 0.111931, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T05:12:08.107956: step 1027, loss 0.420268, acc 0.84375\n",
      "2017-11-06T05:12:12.314944: step 1028, loss 0.179116, acc 0.96875\n",
      "2017-11-06T05:12:16.347811: step 1029, loss 0.43493, acc 0.84375\n",
      "2017-11-06T05:12:20.431712: step 1030, loss 0.495053, acc 0.75\n",
      "2017-11-06T05:12:24.431557: step 1031, loss 0.329778, acc 0.875\n",
      "2017-11-06T05:12:28.495246: step 1032, loss 0.408431, acc 0.875\n",
      "2017-11-06T05:12:32.662205: step 1033, loss 0.151409, acc 0.90625\n",
      "2017-11-06T05:12:36.843176: step 1034, loss 0.0517879, acc 1\n",
      "2017-11-06T05:12:40.899057: step 1035, loss 0.171548, acc 0.9375\n",
      "2017-11-06T05:12:44.945934: step 1036, loss 0.173021, acc 0.96875\n",
      "2017-11-06T05:12:48.965789: step 1037, loss 0.265132, acc 0.9375\n",
      "2017-11-06T05:12:53.009664: step 1038, loss 0.602367, acc 0.8125\n",
      "2017-11-06T05:12:57.032521: step 1039, loss 0.637542, acc 0.875\n",
      "2017-11-06T05:13:01.117423: step 1040, loss 0.132455, acc 0.9375\n",
      "2017-11-06T05:13:05.152292: step 1041, loss 0.391906, acc 0.84375\n",
      "2017-11-06T05:13:09.183154: step 1042, loss 0.465828, acc 0.84375\n",
      "2017-11-06T05:13:13.697362: step 1043, loss 0.236074, acc 0.90625\n",
      "2017-11-06T05:13:16.348246: step 1044, loss 0.186743, acc 0.95\n",
      "2017-11-06T05:13:20.434150: step 1045, loss 0.395781, acc 0.90625\n",
      "2017-11-06T05:13:24.713190: step 1046, loss 0.363049, acc 0.875\n",
      "2017-11-06T05:13:28.973216: step 1047, loss 0.212325, acc 0.9375\n",
      "2017-11-06T05:13:33.274273: step 1048, loss 0.322213, acc 0.90625\n",
      "2017-11-06T05:13:37.350170: step 1049, loss 0.153892, acc 0.875\n",
      "2017-11-06T05:13:41.358017: step 1050, loss 0.275647, acc 0.90625\n",
      "2017-11-06T05:13:45.458930: step 1051, loss 0.245751, acc 0.90625\n",
      "2017-11-06T05:13:49.477785: step 1052, loss 0.408015, acc 0.8125\n",
      "2017-11-06T05:13:53.476628: step 1053, loss 0.460692, acc 0.84375\n",
      "2017-11-06T05:13:57.466462: step 1054, loss 0.321493, acc 0.9375\n",
      "2017-11-06T05:14:01.500329: step 1055, loss 0.216539, acc 0.90625\n",
      "2017-11-06T05:14:05.549205: step 1056, loss 0.247222, acc 0.875\n",
      "2017-11-06T05:14:09.521028: step 1057, loss 0.175543, acc 0.875\n",
      "2017-11-06T05:14:13.585916: step 1058, loss 0.216712, acc 0.9375\n",
      "2017-11-06T05:14:17.851948: step 1059, loss 0.195169, acc 0.9375\n",
      "2017-11-06T05:14:22.018909: step 1060, loss 0.129623, acc 0.9375\n",
      "2017-11-06T05:14:26.081795: step 1061, loss 0.0248924, acc 1\n",
      "2017-11-06T05:14:30.096647: step 1062, loss 0.184229, acc 0.90625\n",
      "2017-11-06T05:14:34.245595: step 1063, loss 0.0734577, acc 0.96875\n",
      "2017-11-06T05:14:38.364522: step 1064, loss 0.456092, acc 0.875\n",
      "2017-11-06T05:14:42.349353: step 1065, loss 0.494058, acc 0.90625\n",
      "2017-11-06T05:14:46.419245: step 1066, loss 0.374835, acc 0.84375\n",
      "2017-11-06T05:14:50.443104: step 1067, loss 0.300144, acc 0.90625\n",
      "2017-11-06T05:14:54.517999: step 1068, loss 0.441318, acc 0.875\n",
      "2017-11-06T05:14:58.562874: step 1069, loss 0.172393, acc 0.90625\n",
      "2017-11-06T05:15:02.619758: step 1070, loss 0.466161, acc 0.84375\n",
      "2017-11-06T05:15:06.692650: step 1071, loss 0.124345, acc 0.9375\n",
      "2017-11-06T05:15:10.694494: step 1072, loss 0.354244, acc 0.875\n",
      "2017-11-06T05:15:14.739368: step 1073, loss 0.48796, acc 0.875\n",
      "2017-11-06T05:15:18.721198: step 1074, loss 0.0645002, acc 0.96875\n",
      "2017-11-06T05:15:23.039266: step 1075, loss 0.482166, acc 0.84375\n",
      "2017-11-06T05:15:27.347326: step 1076, loss 0.0850612, acc 0.96875\n",
      "2017-11-06T05:15:31.353928: step 1077, loss 0.478964, acc 0.84375\n",
      "2017-11-06T05:15:35.339760: step 1078, loss 0.369654, acc 0.8125\n",
      "2017-11-06T05:15:39.334599: step 1079, loss 0.176789, acc 0.90625\n",
      "2017-11-06T05:15:41.959464: step 1080, loss 0.24473, acc 0.85\n",
      "2017-11-06T05:15:46.047369: step 1081, loss 0.100846, acc 0.96875\n",
      "2017-11-06T05:15:50.115259: step 1082, loss 0.164429, acc 0.875\n",
      "2017-11-06T05:15:54.132113: step 1083, loss 0.22746, acc 0.90625\n",
      "2017-11-06T05:15:58.178989: step 1084, loss 0.115627, acc 0.90625\n",
      "2017-11-06T05:16:02.203848: step 1085, loss 0.757317, acc 0.8125\n",
      "2017-11-06T05:16:06.321774: step 1086, loss 0.229471, acc 0.90625\n",
      "2017-11-06T05:16:10.413682: step 1087, loss 0.324661, acc 0.875\n",
      "2017-11-06T05:16:14.440564: step 1088, loss 0.772998, acc 0.78125\n",
      "2017-11-06T05:16:18.503430: step 1089, loss 0.283795, acc 0.9375\n",
      "2017-11-06T05:16:22.494266: step 1090, loss 0.178732, acc 0.9375\n",
      "2017-11-06T05:16:26.579167: step 1091, loss 0.066301, acc 0.96875\n",
      "2017-11-06T05:16:31.023326: step 1092, loss 0.179885, acc 0.90625\n",
      "2017-11-06T05:16:35.300365: step 1093, loss 0.417855, acc 0.875\n",
      "2017-11-06T05:16:39.356247: step 1094, loss 0.272675, acc 0.875\n",
      "2017-11-06T05:16:43.369097: step 1095, loss 0.252736, acc 0.90625\n",
      "2017-11-06T05:16:47.408968: step 1096, loss 0.319659, acc 0.84375\n",
      "2017-11-06T05:16:51.453843: step 1097, loss 0.0514543, acc 0.96875\n",
      "2017-11-06T05:16:55.452684: step 1098, loss 0.290657, acc 0.84375\n",
      "2017-11-06T05:16:59.446522: step 1099, loss 0.236024, acc 0.84375\n",
      "2017-11-06T05:17:03.448365: step 1100, loss 0.325563, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:17:05.987169: step 1100, loss 1.08688, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-06T05:17:11.377356: step 1101, loss 0.0933755, acc 0.90625\n",
      "2017-11-06T05:17:15.420229: step 1102, loss 0.346734, acc 0.84375\n",
      "2017-11-06T05:17:19.447090: step 1103, loss 0.355908, acc 0.78125\n",
      "2017-11-06T05:17:23.479956: step 1104, loss 0.154341, acc 0.9375\n",
      "2017-11-06T05:17:27.484801: step 1105, loss 0.14558, acc 0.96875\n",
      "2017-11-06T05:17:31.519669: step 1106, loss 0.387156, acc 0.90625\n",
      "2017-11-06T05:17:35.895778: step 1107, loss 0.349822, acc 0.90625\n",
      "2017-11-06T05:17:40.053731: step 1108, loss 0.0699832, acc 1\n",
      "2017-11-06T05:17:44.142637: step 1109, loss 0.48038, acc 0.78125\n",
      "2017-11-06T05:17:48.198519: step 1110, loss 0.252588, acc 0.9375\n",
      "2017-11-06T05:17:52.200362: step 1111, loss 0.200032, acc 0.90625\n",
      "2017-11-06T05:17:56.238232: step 1112, loss 0.187761, acc 0.96875\n",
      "2017-11-06T05:18:00.270097: step 1113, loss 0.284339, acc 0.90625\n",
      "2017-11-06T05:18:04.243920: step 1114, loss 0.280853, acc 0.90625\n",
      "2017-11-06T05:18:08.292798: step 1115, loss 0.4924, acc 0.78125\n",
      "2017-11-06T05:18:10.821594: step 1116, loss 0.30499, acc 0.9\n",
      "2017-11-06T05:18:14.883479: step 1117, loss 0.200612, acc 0.875\n",
      "2017-11-06T05:18:18.902336: step 1118, loss 0.153726, acc 0.9375\n",
      "2017-11-06T05:18:23.076301: step 1119, loss 0.554413, acc 0.84375\n",
      "2017-11-06T05:18:27.236257: step 1120, loss 0.120195, acc 0.90625\n",
      "2017-11-06T05:18:31.343939: step 1121, loss 0.199255, acc 0.90625\n",
      "2017-11-06T05:18:35.626982: step 1122, loss 0.318998, acc 0.875\n",
      "2017-11-06T05:18:39.927037: step 1123, loss 0.243141, acc 0.90625\n",
      "2017-11-06T05:18:44.237099: step 1124, loss 0.167362, acc 0.875\n",
      "2017-11-06T05:18:48.255955: step 1125, loss 0.120599, acc 0.9375\n",
      "2017-11-06T05:18:52.302830: step 1126, loss 0.282883, acc 0.84375\n",
      "2017-11-06T05:18:56.368720: step 1127, loss 0.315107, acc 0.875\n",
      "2017-11-06T05:19:00.348548: step 1128, loss 0.347614, acc 0.875\n",
      "2017-11-06T05:19:04.302358: step 1129, loss 0.339411, acc 0.875\n",
      "2017-11-06T05:19:08.357238: step 1130, loss 0.274032, acc 0.9375\n",
      "2017-11-06T05:19:12.308045: step 1131, loss 0.220579, acc 0.875\n",
      "2017-11-06T05:19:16.279867: step 1132, loss 0.259898, acc 0.90625\n",
      "2017-11-06T05:19:20.221668: step 1133, loss 0.0961752, acc 1\n",
      "2017-11-06T05:19:24.196493: step 1134, loss 0.397589, acc 0.875\n",
      "2017-11-06T05:19:28.184326: step 1135, loss 0.404042, acc 0.8125\n",
      "2017-11-06T05:19:32.120122: step 1136, loss 0.379716, acc 0.84375\n",
      "2017-11-06T05:19:36.112960: step 1137, loss 0.158527, acc 0.9375\n",
      "2017-11-06T05:19:40.046756: step 1138, loss 0.0732241, acc 0.9375\n",
      "2017-11-06T05:19:44.131657: step 1139, loss 0.303109, acc 0.875\n",
      "2017-11-06T05:19:48.506766: step 1140, loss 0.529191, acc 0.8125\n",
      "2017-11-06T05:19:52.533627: step 1141, loss 0.115933, acc 0.96875\n",
      "2017-11-06T05:19:56.550481: step 1142, loss 0.292338, acc 0.875\n",
      "2017-11-06T05:20:00.602360: step 1143, loss 0.129756, acc 0.96875\n",
      "2017-11-06T05:20:04.692266: step 1144, loss 0.077213, acc 0.96875\n",
      "2017-11-06T05:20:08.647076: step 1145, loss 0.145911, acc 0.90625\n",
      "2017-11-06T05:20:12.650958: step 1146, loss 0.309521, acc 0.875\n",
      "2017-11-06T05:20:16.656803: step 1147, loss 0.313156, acc 0.84375\n",
      "2017-11-06T05:20:20.610611: step 1148, loss 0.217016, acc 0.90625\n",
      "2017-11-06T05:20:24.567424: step 1149, loss 0.0996625, acc 0.9375\n",
      "2017-11-06T05:20:28.569266: step 1150, loss 0.213069, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T05:20:32.572111: step 1151, loss 0.307132, acc 0.90625\n",
      "2017-11-06T05:20:35.267025: step 1152, loss 0.459432, acc 0.85\n",
      "2017-11-06T05:20:39.221836: step 1153, loss 0.0755247, acc 0.96875\n",
      "2017-11-06T05:20:43.177646: step 1154, loss 0.263736, acc 0.84375\n",
      "2017-11-06T05:20:47.187497: step 1155, loss 0.0997172, acc 0.9375\n",
      "2017-11-06T05:20:51.408494: step 1156, loss 0.386191, acc 0.8125\n",
      "2017-11-06T05:20:55.655512: step 1157, loss 0.137513, acc 0.9375\n",
      "2017-11-06T05:20:59.566291: step 1158, loss 0.258918, acc 0.90625\n",
      "2017-11-06T05:21:03.516097: step 1159, loss 0.410738, acc 0.875\n",
      "2017-11-06T05:21:07.492925: step 1160, loss 0.119688, acc 0.9375\n",
      "2017-11-06T05:21:11.452736: step 1161, loss 0.278795, acc 0.9375\n",
      "2017-11-06T05:21:15.449577: step 1162, loss 0.532383, acc 0.875\n",
      "2017-11-06T05:21:19.378368: step 1163, loss 0.238636, acc 0.84375\n",
      "2017-11-06T05:21:23.322171: step 1164, loss 0.166112, acc 0.9375\n",
      "2017-11-06T05:21:27.283985: step 1165, loss 0.234117, acc 0.90625\n",
      "2017-11-06T05:21:31.241560: step 1166, loss 0.175654, acc 0.875\n",
      "2017-11-06T05:21:35.175357: step 1167, loss 0.210334, acc 0.9375\n",
      "2017-11-06T05:21:39.066119: step 1168, loss 0.179778, acc 0.90625\n",
      "2017-11-06T05:21:43.003917: step 1169, loss 0.127214, acc 0.9375\n",
      "2017-11-06T05:21:46.963731: step 1170, loss 0.126952, acc 0.9375\n",
      "2017-11-06T05:21:50.927548: step 1171, loss 0.230824, acc 0.875\n",
      "2017-11-06T05:21:55.001442: step 1172, loss 0.109798, acc 0.9375\n",
      "2017-11-06T05:21:59.354535: step 1173, loss 0.213808, acc 0.875\n",
      "2017-11-06T05:22:03.348373: step 1174, loss 0.0992975, acc 0.9375\n",
      "2017-11-06T05:22:07.323199: step 1175, loss 0.0408106, acc 1\n",
      "2017-11-06T05:22:11.245985: step 1176, loss 0.324459, acc 0.8125\n",
      "2017-11-06T05:22:15.384961: step 1177, loss 0.42086, acc 0.84375\n",
      "2017-11-06T05:22:19.350779: step 1178, loss 0.0347342, acc 0.96875\n",
      "2017-11-06T05:22:23.282573: step 1179, loss 0.444697, acc 0.875\n",
      "2017-11-06T05:22:27.342457: step 1180, loss 0.157371, acc 0.90625\n",
      "2017-11-06T05:22:31.374323: step 1181, loss 0.424412, acc 0.90625\n",
      "2017-11-06T05:22:35.569304: step 1182, loss 0.152331, acc 0.9375\n",
      "2017-11-06T05:22:39.556136: step 1183, loss 0.299665, acc 0.875\n",
      "2017-11-06T05:22:43.537966: step 1184, loss 0.0874677, acc 0.96875\n",
      "2017-11-06T05:22:47.540811: step 1185, loss 0.335192, acc 0.84375\n",
      "2017-11-06T05:22:51.505628: step 1186, loss 0.325459, acc 0.8125\n",
      "2017-11-06T05:22:55.492459: step 1187, loss 0.140708, acc 0.96875\n",
      "2017-11-06T05:22:58.122328: step 1188, loss 0.127964, acc 0.95\n",
      "2017-11-06T05:23:02.328316: step 1189, loss 0.112437, acc 0.9375\n",
      "2017-11-06T05:23:06.546313: step 1190, loss 0.307677, acc 0.84375\n",
      "2017-11-06T05:23:10.569172: step 1191, loss 0.143504, acc 0.96875\n",
      "2017-11-06T05:23:14.504968: step 1192, loss 0.177673, acc 0.875\n",
      "2017-11-06T05:23:18.466785: step 1193, loss 0.223045, acc 0.90625\n",
      "2017-11-06T05:23:22.422594: step 1194, loss 0.16673, acc 0.9375\n",
      "2017-11-06T05:23:26.361393: step 1195, loss 0.28038, acc 0.90625\n",
      "2017-11-06T05:23:30.363236: step 1196, loss 0.200401, acc 0.9375\n",
      "2017-11-06T05:23:34.275016: step 1197, loss 0.0796831, acc 0.96875\n",
      "2017-11-06T05:23:38.268856: step 1198, loss 0.0156787, acc 1\n",
      "2017-11-06T05:23:42.195644: step 1199, loss 0.137183, acc 0.9375\n",
      "2017-11-06T05:23:46.151454: step 1200, loss 0.2726, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:23:48.778322: step 1200, loss 1.00116, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-06T05:23:53.989318: step 1201, loss 0.271497, acc 0.84375\n",
      "2017-11-06T05:23:58.008174: step 1202, loss 0.463063, acc 0.78125\n",
      "2017-11-06T05:24:01.932961: step 1203, loss 0.204376, acc 0.90625\n",
      "2017-11-06T05:24:05.925799: step 1204, loss 0.675905, acc 0.6875\n",
      "2017-11-06T05:24:10.367955: step 1205, loss 0.118481, acc 0.96875\n",
      "2017-11-06T05:24:14.315760: step 1206, loss 0.0810223, acc 0.96875\n",
      "2017-11-06T05:24:18.414672: step 1207, loss 0.49889, acc 0.875\n",
      "2017-11-06T05:24:22.506580: step 1208, loss 0.351219, acc 0.90625\n",
      "2017-11-06T05:24:26.650526: step 1209, loss 0.224345, acc 0.90625\n",
      "2017-11-06T05:24:30.681214: step 1210, loss 0.151699, acc 0.96875\n",
      "2017-11-06T05:24:34.829161: step 1211, loss 0.345584, acc 0.90625\n",
      "2017-11-06T05:24:38.859025: step 1212, loss 0.304705, acc 0.875\n",
      "2017-11-06T05:24:42.754793: step 1213, loss 0.154724, acc 0.96875\n",
      "2017-11-06T05:24:46.788660: step 1214, loss 0.195628, acc 0.90625\n",
      "2017-11-06T05:24:50.794506: step 1215, loss 0.540553, acc 0.8125\n",
      "2017-11-06T05:24:54.902424: step 1216, loss 0.250193, acc 0.875\n",
      "2017-11-06T05:24:58.864240: step 1217, loss 0.244187, acc 0.875\n",
      "2017-11-06T05:25:02.857078: step 1218, loss 0.110781, acc 0.9375\n",
      "2017-11-06T05:25:06.794874: step 1219, loss 0.394619, acc 0.84375\n",
      "2017-11-06T05:25:10.785710: step 1220, loss 0.266915, acc 0.9375\n",
      "2017-11-06T05:25:15.261890: step 1221, loss 0.403637, acc 0.875\n",
      "2017-11-06T05:25:19.340788: step 1222, loss 0.081649, acc 0.96875\n",
      "2017-11-06T05:25:23.286592: step 1223, loss 0.214274, acc 0.875\n",
      "2017-11-06T05:25:25.838405: step 1224, loss 0.0497427, acc 0.95\n",
      "2017-11-06T05:25:29.917304: step 1225, loss 0.07707, acc 0.96875\n",
      "2017-11-06T05:25:33.875116: step 1226, loss 0.553397, acc 0.8125\n",
      "2017-11-06T05:25:37.859947: step 1227, loss 0.072845, acc 0.96875\n",
      "2017-11-06T05:25:41.810757: step 1228, loss 0.351116, acc 0.84375\n",
      "2017-11-06T05:25:45.776572: step 1229, loss 0.3797, acc 0.78125\n",
      "2017-11-06T05:25:49.791429: step 1230, loss 0.175904, acc 0.90625\n",
      "2017-11-06T05:25:53.779258: step 1231, loss 0.295113, acc 0.96875\n",
      "2017-11-06T05:25:57.710053: step 1232, loss 0.340602, acc 0.8125\n",
      "2017-11-06T05:26:01.787950: step 1233, loss 0.150715, acc 0.96875\n",
      "2017-11-06T05:26:05.865846: step 1234, loss 0.434932, acc 0.875\n",
      "2017-11-06T05:26:09.818655: step 1235, loss 0.22474, acc 0.90625\n",
      "2017-11-06T05:26:13.821500: step 1236, loss 0.25515, acc 0.875\n",
      "2017-11-06T05:26:17.903400: step 1237, loss 0.0869459, acc 0.96875\n",
      "2017-11-06T05:26:22.341554: step 1238, loss 0.16076, acc 0.90625\n",
      "2017-11-06T05:26:26.302367: step 1239, loss 0.267346, acc 0.9375\n",
      "2017-11-06T05:26:30.368258: step 1240, loss 0.220917, acc 0.90625\n",
      "2017-11-06T05:26:34.544224: step 1241, loss 0.219584, acc 0.875\n",
      "2017-11-06T05:26:38.561078: step 1242, loss 0.115932, acc 0.9375\n",
      "2017-11-06T05:26:42.532902: step 1243, loss 0.146074, acc 0.90625\n",
      "2017-11-06T05:26:46.457689: step 1244, loss 0.299872, acc 0.875\n",
      "2017-11-06T05:26:50.442521: step 1245, loss 0.289251, acc 0.875\n",
      "2017-11-06T05:26:54.446365: step 1246, loss 0.494315, acc 0.84375\n",
      "2017-11-06T05:26:58.392170: step 1247, loss 0.266842, acc 0.875\n",
      "2017-11-06T05:27:02.364991: step 1248, loss 0.435887, acc 0.875\n",
      "2017-11-06T05:27:06.342818: step 1249, loss 0.13846, acc 0.9375\n",
      "2017-11-06T05:27:10.327650: step 1250, loss 0.25616, acc 0.84375\n",
      "2017-11-06T05:27:14.364518: step 1251, loss 0.239074, acc 0.875\n",
      "2017-11-06T05:27:18.347347: step 1252, loss 0.262392, acc 0.9375\n",
      "2017-11-06T05:27:22.292152: step 1253, loss 0.156854, acc 0.9375\n",
      "2017-11-06T05:27:26.726301: step 1254, loss 0.34172, acc 0.8125\n",
      "2017-11-06T05:27:30.809000: step 1255, loss 0.21613, acc 0.90625\n",
      "2017-11-06T05:27:34.799835: step 1256, loss 0.0707214, acc 0.96875\n",
      "2017-11-06T05:27:38.766654: step 1257, loss 0.50285, acc 0.84375\n",
      "2017-11-06T05:27:42.735475: step 1258, loss 0.239985, acc 0.84375\n",
      "2017-11-06T05:27:46.679276: step 1259, loss 0.256855, acc 0.90625\n",
      "2017-11-06T05:27:49.278122: step 1260, loss 0.392346, acc 0.8\n",
      "2017-11-06T05:27:53.300981: step 1261, loss 0.395008, acc 0.875\n",
      "2017-11-06T05:27:57.262796: step 1262, loss 0.15623, acc 0.9375\n",
      "2017-11-06T05:28:01.227613: step 1263, loss 0.451619, acc 0.84375\n",
      "2017-11-06T05:28:05.250472: step 1264, loss 0.111603, acc 1\n",
      "2017-11-06T05:28:09.226297: step 1265, loss 0.305272, acc 0.90625\n",
      "2017-11-06T05:28:13.199120: step 1266, loss 0.340703, acc 0.78125\n",
      "2017-11-06T05:28:17.236988: step 1267, loss 0.457206, acc 0.8125\n",
      "2017-11-06T05:28:21.205809: step 1268, loss 0.217851, acc 0.90625\n",
      "2017-11-06T05:28:25.365765: step 1269, loss 0.491958, acc 0.875\n",
      "2017-11-06T05:28:29.532725: step 1270, loss 0.259786, acc 0.875\n",
      "2017-11-06T05:28:34.034924: step 1271, loss 0.30328, acc 0.8125\n",
      "2017-11-06T05:28:38.072793: step 1272, loss 0.521582, acc 0.8125\n",
      "2017-11-06T05:28:41.967561: step 1273, loss 0.213882, acc 0.875\n",
      "2017-11-06T05:28:46.028446: step 1274, loss 0.237164, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T05:28:49.993263: step 1275, loss 0.0896161, acc 0.96875\n",
      "2017-11-06T05:28:53.944071: step 1276, loss 0.0927399, acc 0.9375\n",
      "2017-11-06T05:28:57.944913: step 1277, loss 0.149874, acc 0.9375\n",
      "2017-11-06T05:29:01.948758: step 1278, loss 0.166274, acc 0.96875\n",
      "2017-11-06T05:29:05.968614: step 1279, loss 0.31022, acc 0.90625\n",
      "2017-11-06T05:29:09.916420: step 1280, loss 0.356476, acc 0.84375\n",
      "2017-11-06T05:29:13.929271: step 1281, loss 0.0498731, acc 1\n",
      "2017-11-06T05:29:17.897091: step 1282, loss 0.206917, acc 0.90625\n",
      "2017-11-06T05:29:21.862908: step 1283, loss 0.542634, acc 0.78125\n",
      "2017-11-06T05:29:25.947812: step 1284, loss 0.158738, acc 0.90625\n",
      "2017-11-06T05:29:29.934643: step 1285, loss 0.106738, acc 0.9375\n",
      "2017-11-06T05:29:33.844423: step 1286, loss 0.318556, acc 0.875\n",
      "2017-11-06T05:29:38.291581: step 1287, loss 0.254482, acc 0.90625\n",
      "2017-11-06T05:29:42.321445: step 1288, loss 0.151892, acc 0.9375\n",
      "2017-11-06T05:29:46.253239: step 1289, loss 0.305619, acc 0.8125\n",
      "2017-11-06T05:29:50.281100: step 1290, loss 0.182948, acc 0.90625\n",
      "2017-11-06T05:29:54.249921: step 1291, loss 0.287662, acc 0.875\n",
      "2017-11-06T05:29:58.181714: step 1292, loss 0.217029, acc 0.875\n",
      "2017-11-06T05:30:02.566829: step 1293, loss 0.0911031, acc 0.96875\n",
      "2017-11-06T05:30:06.604698: step 1294, loss 0.172471, acc 0.90625\n",
      "2017-11-06T05:30:10.566514: step 1295, loss 0.56906, acc 0.78125\n",
      "2017-11-06T05:30:13.145347: step 1296, loss 0.0405898, acc 1\n",
      "2017-11-06T05:30:17.190220: step 1297, loss 0.352672, acc 0.8125\n",
      "2017-11-06T05:30:21.155037: step 1298, loss 0.468045, acc 0.75\n",
      "2017-11-06T05:30:25.146873: step 1299, loss 0.0832132, acc 0.96875\n",
      "2017-11-06T05:30:29.227774: step 1300, loss 0.334126, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:30:31.783405: step 1300, loss 0.93183, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-06T05:30:37.760123: step 1301, loss 0.0662737, acc 1\n",
      "2017-11-06T05:30:41.936090: step 1302, loss 0.223851, acc 0.90625\n",
      "2017-11-06T05:30:46.155087: step 1303, loss 0.206538, acc 0.9375\n",
      "2017-11-06T05:30:50.069869: step 1304, loss 0.255148, acc 0.8125\n",
      "2017-11-06T05:30:54.082720: step 1305, loss 0.311864, acc 0.90625\n",
      "2017-11-06T05:30:58.017517: step 1306, loss 0.0655965, acc 0.9375\n",
      "2017-11-06T05:31:02.014357: step 1307, loss 0.115646, acc 0.96875\n",
      "2017-11-06T05:31:06.079245: step 1308, loss 0.149976, acc 0.96875\n",
      "2017-11-06T05:31:10.110110: step 1309, loss 0.411557, acc 0.875\n",
      "2017-11-06T05:31:14.082932: step 1310, loss 0.227724, acc 0.90625\n",
      "2017-11-06T05:31:18.038742: step 1311, loss 0.345422, acc 0.84375\n",
      "2017-11-06T05:31:21.961530: step 1312, loss 0.132316, acc 0.90625\n",
      "2017-11-06T05:31:25.992394: step 1313, loss 0.212325, acc 0.9375\n",
      "2017-11-06T05:31:29.922187: step 1314, loss 0.0697506, acc 1\n",
      "2017-11-06T05:31:33.936039: step 1315, loss 0.462296, acc 0.84375\n",
      "2017-11-06T05:31:38.202069: step 1316, loss 0.3942, acc 0.84375\n",
      "2017-11-06T05:31:42.286972: step 1317, loss 0.0558989, acc 1\n",
      "2017-11-06T05:31:46.363869: step 1318, loss 0.284321, acc 0.84375\n",
      "2017-11-06T05:31:50.738977: step 1319, loss 0.207764, acc 0.875\n",
      "2017-11-06T05:31:54.675775: step 1320, loss 0.128329, acc 0.9375\n",
      "2017-11-06T05:31:58.641592: step 1321, loss 0.266548, acc 0.8125\n",
      "2017-11-06T05:32:02.611413: step 1322, loss 0.302814, acc 0.875\n",
      "2017-11-06T05:32:06.669296: step 1323, loss 0.369864, acc 0.84375\n",
      "2017-11-06T05:32:10.681148: step 1324, loss 0.283382, acc 0.9375\n",
      "2017-11-06T05:32:14.631955: step 1325, loss 0.219121, acc 0.90625\n",
      "2017-11-06T05:32:18.583762: step 1326, loss 0.149687, acc 0.9375\n",
      "2017-11-06T05:32:22.568593: step 1327, loss 0.57554, acc 0.75\n",
      "2017-11-06T05:32:26.533412: step 1328, loss 0.0820986, acc 0.96875\n",
      "2017-11-06T05:32:30.642541: step 1329, loss 0.14741, acc 0.9375\n",
      "2017-11-06T05:32:34.798496: step 1330, loss 0.403044, acc 0.84375\n",
      "2017-11-06T05:32:38.815350: step 1331, loss 0.404697, acc 0.875\n",
      "2017-11-06T05:32:41.387176: step 1332, loss 0.277576, acc 0.9\n",
      "2017-11-06T05:32:45.359999: step 1333, loss 0.202602, acc 0.875\n",
      "2017-11-06T05:32:49.351836: step 1334, loss 0.315276, acc 0.875\n",
      "2017-11-06T05:32:53.627874: step 1335, loss 0.148785, acc 0.9375\n",
      "2017-11-06T05:32:57.894907: step 1336, loss 0.421834, acc 0.84375\n",
      "2017-11-06T05:33:01.992819: step 1337, loss 0.178741, acc 0.90625\n",
      "2017-11-06T05:33:05.993660: step 1338, loss 0.300423, acc 0.875\n",
      "2017-11-06T05:33:09.987498: step 1339, loss 0.163099, acc 0.9375\n",
      "2017-11-06T05:33:13.959321: step 1340, loss 0.140332, acc 0.96875\n",
      "2017-11-06T05:33:17.957160: step 1341, loss 0.27228, acc 0.84375\n",
      "2017-11-06T05:33:21.992027: step 1342, loss 0.130948, acc 0.90625\n",
      "2017-11-06T05:33:26.242048: step 1343, loss 0.0972488, acc 0.9375\n",
      "2017-11-06T05:33:30.152578: step 1344, loss 0.0209439, acc 1\n",
      "2017-11-06T05:33:34.106386: step 1345, loss 0.260279, acc 0.84375\n",
      "2017-11-06T05:33:38.068202: step 1346, loss 0.161969, acc 0.9375\n",
      "2017-11-06T05:33:42.205140: step 1347, loss 0.315716, acc 0.8125\n",
      "2017-11-06T05:33:46.263024: step 1348, loss 0.120637, acc 0.96875\n",
      "2017-11-06T05:33:50.407969: step 1349, loss 0.457907, acc 0.8125\n",
      "2017-11-06T05:33:54.441835: step 1350, loss 0.338991, acc 0.875\n",
      "2017-11-06T05:33:58.836959: step 1351, loss 0.0880573, acc 0.9375\n",
      "2017-11-06T05:34:03.199058: step 1352, loss 0.440724, acc 0.84375\n",
      "2017-11-06T05:34:07.474095: step 1353, loss 0.0664437, acc 1\n",
      "2017-11-06T05:34:11.623044: step 1354, loss 0.154095, acc 0.9375\n",
      "2017-11-06T05:34:15.767988: step 1355, loss 0.166844, acc 0.90625\n",
      "2017-11-06T05:34:19.800854: step 1356, loss 0.252429, acc 0.90625\n",
      "2017-11-06T05:34:23.924784: step 1357, loss 0.415311, acc 0.84375\n",
      "2017-11-06T05:34:28.074733: step 1358, loss 0.264893, acc 0.875\n",
      "2017-11-06T05:34:32.291730: step 1359, loss 0.201717, acc 0.90625\n",
      "2017-11-06T05:34:36.649826: step 1360, loss 0.270347, acc 0.875\n",
      "2017-11-06T05:34:40.704707: step 1361, loss 0.318614, acc 0.875\n",
      "2017-11-06T05:34:44.873669: step 1362, loss 0.173488, acc 0.875\n",
      "2017-11-06T05:34:49.067649: step 1363, loss 0.171629, acc 0.875\n",
      "2017-11-06T05:34:53.079500: step 1364, loss 0.229985, acc 0.96875\n",
      "2017-11-06T05:34:57.134381: step 1365, loss 0.130699, acc 0.90625\n",
      "2017-11-06T05:35:01.069177: step 1366, loss 0.335085, acc 0.8125\n",
      "2017-11-06T05:35:05.362228: step 1367, loss 0.148778, acc 0.90625\n",
      "2017-11-06T05:35:08.069151: step 1368, loss 0.362871, acc 0.8\n",
      "2017-11-06T05:35:12.052983: step 1369, loss 0.30866, acc 0.84375\n",
      "2017-11-06T05:35:16.025804: step 1370, loss 0.385001, acc 0.8125\n",
      "2017-11-06T05:35:19.938585: step 1371, loss 0.194512, acc 0.90625\n",
      "2017-11-06T05:35:23.921415: step 1372, loss 0.169985, acc 0.9375\n",
      "2017-11-06T05:35:27.858212: step 1373, loss 0.225063, acc 0.84375\n",
      "2017-11-06T05:35:31.783002: step 1374, loss 0.252202, acc 0.875\n",
      "2017-11-06T05:35:35.742814: step 1375, loss 0.167674, acc 0.90625\n",
      "2017-11-06T05:35:39.732649: step 1376, loss 0.333165, acc 0.90625\n",
      "2017-11-06T05:35:43.678453: step 1377, loss 0.330846, acc 0.84375\n",
      "2017-11-06T05:35:47.634263: step 1378, loss 0.280224, acc 0.875\n",
      "2017-11-06T05:35:51.614091: step 1379, loss 0.265374, acc 0.8125\n",
      "2017-11-06T05:35:55.627943: step 1380, loss 0.214347, acc 0.90625\n",
      "2017-11-06T05:35:59.602768: step 1381, loss 0.12217, acc 0.9375\n",
      "2017-11-06T05:36:03.651645: step 1382, loss 0.387533, acc 0.875\n",
      "2017-11-06T05:36:07.695517: step 1383, loss 0.216809, acc 0.9375\n",
      "2017-11-06T05:36:12.117660: step 1384, loss 0.0829894, acc 0.9375\n",
      "2017-11-06T05:36:16.141519: step 1385, loss 0.456241, acc 0.84375\n",
      "2017-11-06T05:36:20.090324: step 1386, loss 0.371454, acc 0.8125\n",
      "2017-11-06T05:36:24.090166: step 1387, loss 0.17597, acc 0.90625\n",
      "2017-11-06T05:36:28.041977: step 1388, loss 0.243543, acc 0.875\n",
      "2017-11-06T05:36:32.034574: step 1389, loss 0.327204, acc 0.875\n",
      "2017-11-06T05:36:36.389868: step 1390, loss 0.153918, acc 0.9375\n",
      "2017-11-06T05:36:40.365693: step 1391, loss 0.151909, acc 0.9375\n",
      "2017-11-06T05:36:44.279473: step 1392, loss 0.307176, acc 0.875\n",
      "2017-11-06T05:36:48.231283: step 1393, loss 0.0805655, acc 0.96875\n",
      "2017-11-06T05:36:52.142061: step 1394, loss 0.329987, acc 0.84375\n",
      "2017-11-06T05:36:56.101874: step 1395, loss 0.355571, acc 0.875\n",
      "2017-11-06T05:37:00.061688: step 1396, loss 0.0908865, acc 0.9375\n",
      "2017-11-06T05:37:04.004488: step 1397, loss 0.113549, acc 0.90625\n",
      "2017-11-06T05:37:07.936282: step 1398, loss 0.1321, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T05:37:11.902101: step 1399, loss 0.269027, acc 0.90625\n",
      "2017-11-06T05:37:16.302227: step 1400, loss 0.170594, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:37:19.083203: step 1400, loss 1.07467, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-06T05:37:24.834385: step 1401, loss 0.705563, acc 0.75\n",
      "2017-11-06T05:37:28.868251: step 1402, loss 0.0350014, acc 1\n",
      "2017-11-06T05:37:32.993182: step 1403, loss 0.221949, acc 0.90625\n",
      "2017-11-06T05:37:35.586025: step 1404, loss 0.340927, acc 0.8\n",
      "2017-11-06T05:37:39.643908: step 1405, loss 0.219856, acc 0.90625\n",
      "2017-11-06T05:37:43.665766: step 1406, loss 0.0753825, acc 0.9375\n",
      "2017-11-06T05:37:47.691626: step 1407, loss 0.136169, acc 0.96875\n",
      "2017-11-06T05:37:51.720489: step 1408, loss 0.151483, acc 0.90625\n",
      "2017-11-06T05:37:55.716329: step 1409, loss 0.303049, acc 0.875\n",
      "2017-11-06T05:37:59.678143: step 1410, loss 0.514496, acc 0.8125\n",
      "2017-11-06T05:38:03.739029: step 1411, loss 0.304115, acc 0.875\n",
      "2017-11-06T05:38:07.752880: step 1412, loss 0.238385, acc 0.90625\n",
      "2017-11-06T05:38:11.771737: step 1413, loss 0.325692, acc 0.84375\n",
      "2017-11-06T05:38:15.854637: step 1414, loss 0.0793678, acc 0.96875\n",
      "2017-11-06T05:38:20.047616: step 1415, loss 0.156299, acc 0.9375\n",
      "2017-11-06T05:38:24.568830: step 1416, loss 0.409729, acc 0.8125\n",
      "2017-11-06T05:38:28.735790: step 1417, loss 0.568063, acc 0.75\n",
      "2017-11-06T05:38:32.815688: step 1418, loss 0.31007, acc 0.875\n",
      "2017-11-06T05:38:37.103745: step 1419, loss 0.0217744, acc 1\n",
      "2017-11-06T05:38:41.212665: step 1420, loss 0.10131, acc 0.9375\n",
      "2017-11-06T05:38:45.268546: step 1421, loss 0.206801, acc 0.96875\n",
      "2017-11-06T05:38:49.431504: step 1422, loss 0.188799, acc 0.90625\n",
      "2017-11-06T05:38:53.534419: step 1423, loss 0.130707, acc 0.90625\n",
      "2017-11-06T05:38:57.482225: step 1424, loss 0.303199, acc 0.875\n",
      "2017-11-06T05:39:01.485069: step 1425, loss 0.170193, acc 0.90625\n",
      "2017-11-06T05:39:05.486912: step 1426, loss 0.183397, acc 0.9375\n",
      "2017-11-06T05:39:09.440722: step 1427, loss 0.291304, acc 0.8125\n",
      "2017-11-06T05:39:13.434559: step 1428, loss 0.160721, acc 0.96875\n",
      "2017-11-06T05:39:17.462421: step 1429, loss 0.185272, acc 0.90625\n",
      "2017-11-06T05:39:21.606366: step 1430, loss 0.0780342, acc 0.96875\n",
      "2017-11-06T05:39:25.879402: step 1431, loss 0.145198, acc 0.96875\n",
      "2017-11-06T05:39:30.270521: step 1432, loss 0.224704, acc 0.875\n",
      "2017-11-06T05:39:34.319155: step 1433, loss 0.377727, acc 0.84375\n",
      "2017-11-06T05:39:38.265959: step 1434, loss 0.333414, acc 0.8125\n",
      "2017-11-06T05:39:42.305830: step 1435, loss 0.449674, acc 0.84375\n",
      "2017-11-06T05:39:46.295665: step 1436, loss 0.466581, acc 0.8125\n",
      "2017-11-06T05:39:50.321527: step 1437, loss 0.338725, acc 0.875\n",
      "2017-11-06T05:39:54.350389: step 1438, loss 0.277616, acc 0.84375\n",
      "2017-11-06T05:39:58.303199: step 1439, loss 0.107021, acc 0.9375\n",
      "2017-11-06T05:40:01.157226: step 1440, loss 0.629136, acc 0.65\n",
      "2017-11-06T05:40:05.265144: step 1441, loss 0.202523, acc 0.90625\n",
      "2017-11-06T05:40:09.270990: step 1442, loss 0.11617, acc 0.9375\n",
      "2017-11-06T05:40:13.273834: step 1443, loss 0.117984, acc 0.9375\n",
      "2017-11-06T05:40:17.388758: step 1444, loss 0.283299, acc 0.875\n",
      "2017-11-06T05:40:21.394604: step 1445, loss 0.160471, acc 0.90625\n",
      "2017-11-06T05:40:25.413461: step 1446, loss 0.465147, acc 0.8125\n",
      "2017-11-06T05:40:29.368271: step 1447, loss 0.0797836, acc 0.96875\n",
      "2017-11-06T05:40:33.907495: step 1448, loss 0.188252, acc 0.90625\n",
      "2017-11-06T05:40:38.156514: step 1449, loss 0.256353, acc 0.90625\n",
      "2017-11-06T05:40:42.103319: step 1450, loss 0.209901, acc 0.90625\n",
      "2017-11-06T05:40:46.115169: step 1451, loss 0.379677, acc 0.84375\n",
      "2017-11-06T05:40:50.134026: step 1452, loss 0.187158, acc 0.90625\n",
      "2017-11-06T05:40:54.174896: step 1453, loss 0.291035, acc 0.84375\n",
      "2017-11-06T05:40:58.104688: step 1454, loss 0.291253, acc 0.90625\n",
      "2017-11-06T05:41:02.105531: step 1455, loss 0.351141, acc 0.84375\n",
      "2017-11-06T05:41:06.095366: step 1456, loss 0.247874, acc 0.875\n",
      "2017-11-06T05:41:10.410433: step 1457, loss 0.314784, acc 0.90625\n",
      "2017-11-06T05:41:15.509055: step 1458, loss 0.251442, acc 0.90625\n",
      "2017-11-06T05:41:19.575945: step 1459, loss 0.150668, acc 0.90625\n",
      "2017-11-06T05:41:23.690869: step 1460, loss 0.158283, acc 0.9375\n",
      "2017-11-06T05:41:27.674701: step 1461, loss 0.190121, acc 0.875\n",
      "2017-11-06T05:41:31.713569: step 1462, loss 0.274944, acc 0.875\n",
      "2017-11-06T05:41:35.800473: step 1463, loss 0.100659, acc 0.96875\n",
      "2017-11-06T05:41:40.223616: step 1464, loss 0.202985, acc 0.9375\n",
      "2017-11-06T05:41:44.224460: step 1465, loss 0.179616, acc 0.90625\n",
      "2017-11-06T05:41:48.220298: step 1466, loss 0.0797067, acc 0.96875\n",
      "2017-11-06T05:41:52.204128: step 1467, loss 0.164437, acc 0.9375\n",
      "2017-11-06T05:41:56.363085: step 1468, loss 0.172932, acc 0.90625\n",
      "2017-11-06T05:42:00.354920: step 1469, loss 0.342512, acc 0.875\n",
      "2017-11-06T05:42:04.333747: step 1470, loss 0.239248, acc 0.9375\n",
      "2017-11-06T05:42:08.336591: step 1471, loss 0.369884, acc 0.90625\n",
      "2017-11-06T05:42:12.259379: step 1472, loss 0.308853, acc 0.90625\n",
      "2017-11-06T05:42:16.325267: step 1473, loss 0.333664, acc 0.875\n",
      "2017-11-06T05:42:20.317104: step 1474, loss 0.364029, acc 0.90625\n",
      "2017-11-06T05:42:24.364980: step 1475, loss 0.222904, acc 0.90625\n",
      "2017-11-06T05:42:26.868759: step 1476, loss 0.0279006, acc 1\n",
      "2017-11-06T05:42:30.903400: step 1477, loss 0.258143, acc 0.875\n",
      "2017-11-06T05:42:35.131403: step 1478, loss 0.175875, acc 0.9375\n",
      "2017-11-06T05:42:39.350607: step 1479, loss 0.129881, acc 0.96875\n",
      "2017-11-06T05:42:43.505560: step 1480, loss 0.248071, acc 0.84375\n",
      "2017-11-06T05:42:47.755579: step 1481, loss 0.0293011, acc 1\n",
      "2017-11-06T05:42:51.771433: step 1482, loss 0.135223, acc 0.9375\n",
      "2017-11-06T05:42:55.798295: step 1483, loss 0.129482, acc 0.96875\n",
      "2017-11-06T05:42:59.802140: step 1484, loss 0.227394, acc 0.90625\n",
      "2017-11-06T05:43:03.846012: step 1485, loss 0.255066, acc 0.875\n",
      "2017-11-06T05:43:07.916905: step 1486, loss 0.295197, acc 0.8125\n",
      "2017-11-06T05:43:11.946768: step 1487, loss 0.169849, acc 0.875\n",
      "2017-11-06T05:43:16.033672: step 1488, loss 0.212377, acc 0.90625\n",
      "2017-11-06T05:43:20.076545: step 1489, loss 0.160665, acc 0.90625\n",
      "2017-11-06T05:43:24.199474: step 1490, loss 0.247793, acc 0.875\n",
      "2017-11-06T05:43:28.322404: step 1491, loss 0.126191, acc 0.96875\n",
      "2017-11-06T05:43:32.326249: step 1492, loss 0.0907239, acc 0.96875\n",
      "2017-11-06T05:43:36.354110: step 1493, loss 0.354151, acc 0.9375\n",
      "2017-11-06T05:43:40.306919: step 1494, loss 0.277171, acc 0.84375\n",
      "2017-11-06T05:43:44.376813: step 1495, loss 0.199597, acc 0.90625\n",
      "2017-11-06T05:43:48.566789: step 1496, loss 0.160773, acc 0.9375\n",
      "2017-11-06T05:43:52.940896: step 1497, loss 0.260947, acc 0.90625\n",
      "2017-11-06T05:43:56.946743: step 1498, loss 0.204979, acc 0.90625\n",
      "2017-11-06T05:44:00.893547: step 1499, loss 0.249179, acc 0.875\n",
      "2017-11-06T05:44:04.905398: step 1500, loss 0.425263, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:44:07.472221: step 1500, loss 0.894135, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-06T05:44:12.916511: step 1501, loss 0.247939, acc 0.84375\n",
      "2017-11-06T05:44:16.924359: step 1502, loss 0.109638, acc 0.90625\n",
      "2017-11-06T05:44:20.893179: step 1503, loss 0.20317, acc 0.875\n",
      "2017-11-06T05:44:24.980083: step 1504, loss 0.391877, acc 0.8125\n",
      "2017-11-06T05:44:28.949903: step 1505, loss 0.29218, acc 0.875\n",
      "2017-11-06T05:44:33.028803: step 1506, loss 0.384137, acc 0.875\n",
      "2017-11-06T05:44:37.116706: step 1507, loss 0.152975, acc 0.9375\n",
      "2017-11-06T05:44:41.198105: step 1508, loss 0.136119, acc 0.96875\n",
      "2017-11-06T05:44:45.202950: step 1509, loss 0.273794, acc 0.8125\n",
      "2017-11-06T05:44:49.181777: step 1510, loss 0.228961, acc 0.875\n",
      "2017-11-06T05:44:53.221647: step 1511, loss 0.187531, acc 0.90625\n",
      "2017-11-06T05:44:56.140721: step 1512, loss 0.3929, acc 0.85\n",
      "2017-11-06T05:45:00.305681: step 1513, loss 0.0720162, acc 0.9375\n",
      "2017-11-06T05:45:04.254487: step 1514, loss 0.302043, acc 0.90625\n",
      "2017-11-06T05:45:08.235315: step 1515, loss 0.314674, acc 0.875\n",
      "2017-11-06T05:45:12.261177: step 1516, loss 0.25322, acc 0.9375\n",
      "2017-11-06T05:45:16.290038: step 1517, loss 0.107703, acc 0.9375\n",
      "2017-11-06T05:45:20.294884: step 1518, loss 0.403207, acc 0.84375\n",
      "2017-11-06T05:45:24.282718: step 1519, loss 0.192891, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T05:45:28.267549: step 1520, loss 0.183687, acc 0.90625\n",
      "2017-11-06T05:45:32.232218: step 1521, loss 0.348932, acc 0.8125\n",
      "2017-11-06T05:45:36.260080: step 1522, loss 0.153876, acc 0.9375\n",
      "2017-11-06T05:45:40.280937: step 1523, loss 0.164434, acc 0.90625\n",
      "2017-11-06T05:45:44.226740: step 1524, loss 0.532463, acc 0.78125\n",
      "2017-11-06T05:45:48.221578: step 1525, loss 0.205028, acc 0.90625\n",
      "2017-11-06T05:45:52.262450: step 1526, loss 0.36773, acc 0.875\n",
      "2017-11-06T05:45:56.275301: step 1527, loss 0.322015, acc 0.84375\n",
      "2017-11-06T05:46:00.477287: step 1528, loss 0.247411, acc 0.875\n",
      "2017-11-06T05:46:04.652254: step 1529, loss 0.172659, acc 0.90625\n",
      "2017-11-06T05:46:08.658099: step 1530, loss 0.160688, acc 0.96875\n",
      "2017-11-06T05:46:12.702974: step 1531, loss 0.0939923, acc 0.96875\n",
      "2017-11-06T05:46:16.657783: step 1532, loss 0.169183, acc 0.90625\n",
      "2017-11-06T05:46:20.638613: step 1533, loss 0.177041, acc 0.90625\n",
      "2017-11-06T05:46:24.631449: step 1534, loss 0.100371, acc 0.96875\n",
      "2017-11-06T05:46:28.621284: step 1535, loss 0.207995, acc 0.90625\n",
      "2017-11-06T05:46:32.629132: step 1536, loss 0.461943, acc 0.8125\n",
      "2017-11-06T05:46:36.861139: step 1537, loss 0.192726, acc 0.9375\n",
      "2017-11-06T05:46:40.813948: step 1538, loss 0.157904, acc 0.90625\n",
      "2017-11-06T05:46:44.847814: step 1539, loss 0.420149, acc 0.875\n",
      "2017-11-06T05:46:48.838650: step 1540, loss 0.191649, acc 0.875\n",
      "2017-11-06T05:46:52.826483: step 1541, loss 0.0800377, acc 0.96875\n",
      "2017-11-06T05:46:56.825324: step 1542, loss 0.126368, acc 0.9375\n",
      "2017-11-06T05:47:00.767126: step 1543, loss 0.202286, acc 0.90625\n",
      "2017-11-06T05:47:04.877046: step 1544, loss 0.180465, acc 0.90625\n",
      "2017-11-06T05:47:09.310195: step 1545, loss 0.0940478, acc 0.96875\n",
      "2017-11-06T05:47:13.294026: step 1546, loss 0.264398, acc 0.9375\n",
      "2017-11-06T05:47:17.286865: step 1547, loss 0.224687, acc 0.875\n",
      "2017-11-06T05:47:19.795646: step 1548, loss 0.17236, acc 0.95\n",
      "2017-11-06T05:47:23.806495: step 1549, loss 0.351476, acc 0.875\n",
      "2017-11-06T05:47:27.747296: step 1550, loss 0.237534, acc 0.84375\n",
      "2017-11-06T05:47:31.755144: step 1551, loss 0.176589, acc 0.90625\n",
      "2017-11-06T05:47:35.678933: step 1552, loss 0.213433, acc 0.9375\n",
      "2017-11-06T05:47:39.677774: step 1553, loss 0.43807, acc 0.875\n",
      "2017-11-06T05:47:43.686621: step 1554, loss 0.233874, acc 0.9375\n",
      "2017-11-06T05:47:47.675455: step 1555, loss 0.0810311, acc 0.9375\n",
      "2017-11-06T05:47:51.695312: step 1556, loss 0.220191, acc 0.90625\n",
      "2017-11-06T05:47:55.685147: step 1557, loss 0.142065, acc 0.9375\n",
      "2017-11-06T05:47:59.684990: step 1558, loss 0.140509, acc 0.9375\n",
      "2017-11-06T05:48:03.666818: step 1559, loss 0.283471, acc 0.78125\n",
      "2017-11-06T05:48:07.665660: step 1560, loss 0.211109, acc 0.875\n",
      "2017-11-06T05:48:11.863643: step 1561, loss 0.144795, acc 0.9375\n",
      "2017-11-06T05:48:16.210733: step 1562, loss 0.329202, acc 0.84375\n",
      "2017-11-06T05:48:20.333660: step 1563, loss 0.137338, acc 0.96875\n",
      "2017-11-06T05:48:24.575676: step 1564, loss 0.115476, acc 0.90625\n",
      "2017-11-06T05:48:28.896745: step 1565, loss 0.111403, acc 0.9375\n",
      "2017-11-06T05:48:32.951414: step 1566, loss 0.145271, acc 0.96875\n",
      "2017-11-06T05:48:37.139389: step 1567, loss 0.484616, acc 0.8125\n",
      "2017-11-06T05:48:41.125222: step 1568, loss 0.260437, acc 0.9375\n",
      "2017-11-06T05:48:45.156269: step 1569, loss 0.297275, acc 0.84375\n",
      "2017-11-06T05:48:49.156112: step 1570, loss 0.0244018, acc 1\n",
      "2017-11-06T05:48:53.127933: step 1571, loss 0.0993236, acc 0.9375\n",
      "2017-11-06T05:48:57.114766: step 1572, loss 0.129584, acc 0.9375\n",
      "2017-11-06T05:49:01.018540: step 1573, loss 0.312141, acc 0.875\n",
      "2017-11-06T05:49:04.933321: step 1574, loss 0.137174, acc 0.9375\n",
      "2017-11-06T05:49:08.951176: step 1575, loss 0.187421, acc 0.90625\n",
      "2017-11-06T05:49:12.897980: step 1576, loss 0.300649, acc 0.875\n",
      "2017-11-06T05:49:17.136992: step 1577, loss 0.0820236, acc 0.9375\n",
      "2017-11-06T05:49:21.341980: step 1578, loss 0.167464, acc 0.90625\n",
      "2017-11-06T05:49:25.329814: step 1579, loss 0.35977, acc 0.8125\n",
      "2017-11-06T05:49:29.288627: step 1580, loss 0.118431, acc 0.9375\n",
      "2017-11-06T05:49:33.320492: step 1581, loss 0.219076, acc 0.90625\n",
      "2017-11-06T05:49:37.295316: step 1582, loss 0.137806, acc 0.9375\n",
      "2017-11-06T05:49:41.263135: step 1583, loss 0.258642, acc 0.875\n",
      "2017-11-06T05:49:43.785928: step 1584, loss 0.117132, acc 0.95\n",
      "2017-11-06T05:49:47.717721: step 1585, loss 0.233358, acc 0.90625\n",
      "2017-11-06T05:49:51.677535: step 1586, loss 0.301006, acc 0.875\n",
      "2017-11-06T05:49:55.641352: step 1587, loss 0.265587, acc 0.8125\n",
      "2017-11-06T05:49:59.622182: step 1588, loss 0.151929, acc 0.90625\n",
      "2017-11-06T05:50:03.898219: step 1589, loss 0.147656, acc 0.9375\n",
      "2017-11-06T05:50:07.824007: step 1590, loss 0.282016, acc 0.875\n",
      "2017-11-06T05:50:11.755803: step 1591, loss 0.227525, acc 0.90625\n",
      "2017-11-06T05:50:15.696602: step 1592, loss 0.272222, acc 0.875\n",
      "2017-11-06T05:50:19.642405: step 1593, loss 0.194713, acc 0.90625\n",
      "2017-11-06T05:50:23.993496: step 1594, loss 0.124714, acc 0.9375\n",
      "2017-11-06T05:50:28.135440: step 1595, loss 0.228874, acc 0.90625\n",
      "2017-11-06T05:50:32.121272: step 1596, loss 0.286211, acc 0.90625\n",
      "2017-11-06T05:50:36.329263: step 1597, loss 0.312494, acc 0.90625\n",
      "2017-11-06T05:50:40.284073: step 1598, loss 0.186405, acc 0.90625\n",
      "2017-11-06T05:50:44.239884: step 1599, loss 0.25683, acc 0.875\n",
      "2017-11-06T05:50:48.294118: step 1600, loss 0.185723, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:50:50.840927: step 1600, loss 0.986861, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-06T05:50:56.332162: step 1601, loss 0.0928662, acc 0.9375\n",
      "2017-11-06T05:51:00.357022: step 1602, loss 0.235895, acc 0.90625\n",
      "2017-11-06T05:51:04.362868: step 1603, loss 0.00899094, acc 1\n",
      "2017-11-06T05:51:08.382724: step 1604, loss 0.418119, acc 0.84375\n",
      "2017-11-06T05:51:12.311516: step 1605, loss 0.328054, acc 0.875\n",
      "2017-11-06T05:51:16.332374: step 1606, loss 0.237229, acc 0.84375\n",
      "2017-11-06T05:51:20.335218: step 1607, loss 0.293701, acc 0.875\n",
      "2017-11-06T05:51:24.349069: step 1608, loss 0.0631688, acc 0.96875\n",
      "2017-11-06T05:51:28.601090: step 1609, loss 0.155432, acc 0.90625\n",
      "2017-11-06T05:51:32.757804: step 1610, loss 0.266315, acc 0.875\n",
      "2017-11-06T05:51:36.779661: step 1611, loss 0.218372, acc 0.875\n",
      "2017-11-06T05:51:40.766494: step 1612, loss 0.222389, acc 0.90625\n",
      "2017-11-06T05:51:44.777345: step 1613, loss 0.0762315, acc 0.9375\n",
      "2017-11-06T05:51:48.749166: step 1614, loss 0.0807819, acc 0.96875\n",
      "2017-11-06T05:51:52.718987: step 1615, loss 0.344226, acc 0.8125\n",
      "2017-11-06T05:51:56.676799: step 1616, loss 0.0634883, acc 1\n",
      "2017-11-06T05:52:00.653625: step 1617, loss 0.134884, acc 0.9375\n",
      "2017-11-06T05:52:04.650465: step 1618, loss 0.121667, acc 0.9375\n",
      "2017-11-06T05:52:08.654310: step 1619, loss 0.117665, acc 0.90625\n",
      "2017-11-06T05:52:11.206123: step 1620, loss 0.135079, acc 0.95\n",
      "2017-11-06T05:52:15.253999: step 1621, loss 0.274147, acc 0.90625\n",
      "2017-11-06T05:52:19.254842: step 1622, loss 0.240529, acc 0.90625\n",
      "2017-11-06T05:52:23.271696: step 1623, loss 0.589292, acc 0.6875\n",
      "2017-11-06T05:52:27.220502: step 1624, loss 0.13893, acc 0.90625\n",
      "2017-11-06T05:52:31.315411: step 1625, loss 0.133694, acc 0.9375\n",
      "2017-11-06T05:52:35.865645: step 1626, loss 0.0913358, acc 0.9375\n",
      "2017-11-06T05:52:39.998581: step 1627, loss 0.23889, acc 0.875\n",
      "2017-11-06T05:52:43.966400: step 1628, loss 0.123038, acc 0.9375\n",
      "2017-11-06T05:52:47.952234: step 1629, loss 0.196525, acc 0.875\n",
      "2017-11-06T05:52:52.030891: step 1630, loss 0.243517, acc 0.90625\n",
      "2017-11-06T05:52:55.957683: step 1631, loss 0.327428, acc 0.84375\n",
      "2017-11-06T05:52:59.975536: step 1632, loss 0.127397, acc 0.9375\n",
      "2017-11-06T05:53:04.047430: step 1633, loss 0.109189, acc 0.9375\n",
      "2017-11-06T05:53:08.106313: step 1634, loss 0.303845, acc 0.875\n",
      "2017-11-06T05:53:12.089144: step 1635, loss 0.140804, acc 0.90625\n",
      "2017-11-06T05:53:16.084983: step 1636, loss 0.122569, acc 0.9375\n",
      "2017-11-06T05:53:20.208913: step 1637, loss 0.194975, acc 0.9375\n",
      "2017-11-06T05:53:24.391885: step 1638, loss 0.111506, acc 0.96875\n",
      "2017-11-06T05:53:28.562848: step 1639, loss 0.0121641, acc 1\n",
      "2017-11-06T05:53:32.602719: step 1640, loss 0.00402445, acc 1\n",
      "2017-11-06T05:53:36.692625: step 1641, loss 0.407429, acc 0.875\n",
      "2017-11-06T05:53:41.157799: step 1642, loss 0.21194, acc 0.9375\n",
      "2017-11-06T05:53:45.364787: step 1643, loss 0.229788, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T05:53:49.404657: step 1644, loss 0.205145, acc 0.875\n",
      "2017-11-06T05:53:53.392491: step 1645, loss 0.279504, acc 0.90625\n",
      "2017-11-06T05:53:57.350303: step 1646, loss 0.266913, acc 0.84375\n",
      "2017-11-06T05:54:01.355148: step 1647, loss 0.0878913, acc 0.9375\n",
      "2017-11-06T05:54:05.302954: step 1648, loss 0.234492, acc 0.9375\n",
      "2017-11-06T05:54:09.303797: step 1649, loss 0.207628, acc 0.9375\n",
      "2017-11-06T05:54:13.277620: step 1650, loss 0.17783, acc 0.875\n",
      "2017-11-06T05:54:17.393545: step 1651, loss 0.10285, acc 0.9375\n",
      "2017-11-06T05:54:21.308327: step 1652, loss 0.10748, acc 0.90625\n",
      "2017-11-06T05:54:25.289155: step 1653, loss 0.335702, acc 0.84375\n",
      "2017-11-06T05:54:29.368053: step 1654, loss 0.127831, acc 0.96875\n",
      "2017-11-06T05:54:33.497755: step 1655, loss 0.463288, acc 0.875\n",
      "2017-11-06T05:54:36.211682: step 1656, loss 0.290743, acc 0.85\n",
      "2017-11-06T05:54:40.213526: step 1657, loss 0.227771, acc 0.9375\n",
      "2017-11-06T05:54:44.312439: step 1658, loss 0.264364, acc 0.78125\n",
      "2017-11-06T05:54:48.723573: step 1659, loss 0.305246, acc 0.875\n",
      "2017-11-06T05:54:52.729523: step 1660, loss 0.0698627, acc 0.96875\n",
      "2017-11-06T05:54:56.839444: step 1661, loss 0.151376, acc 0.90625\n",
      "2017-11-06T05:55:00.798258: step 1662, loss 0.129706, acc 0.9375\n",
      "2017-11-06T05:55:04.717041: step 1663, loss 0.165149, acc 0.90625\n",
      "2017-11-06T05:55:08.693868: step 1664, loss 0.157551, acc 0.9375\n",
      "2017-11-06T05:55:12.634667: step 1665, loss 0.203836, acc 0.9375\n",
      "2017-11-06T05:55:16.607490: step 1666, loss 0.113622, acc 0.90625\n",
      "2017-11-06T05:55:20.552293: step 1667, loss 0.155611, acc 0.9375\n",
      "2017-11-06T05:55:24.559140: step 1668, loss 0.207954, acc 0.90625\n",
      "2017-11-06T05:55:28.566987: step 1669, loss 0.341604, acc 0.84375\n",
      "2017-11-06T05:55:32.568831: step 1670, loss 0.200399, acc 0.9375\n",
      "2017-11-06T05:55:36.570676: step 1671, loss 0.350847, acc 0.875\n",
      "2017-11-06T05:55:40.575520: step 1672, loss 0.187842, acc 0.9375\n",
      "2017-11-06T05:55:44.583370: step 1673, loss 0.200909, acc 0.875\n",
      "2017-11-06T05:55:48.581208: step 1674, loss 0.496289, acc 0.78125\n",
      "2017-11-06T05:55:53.017360: step 1675, loss 0.201823, acc 0.875\n",
      "2017-11-06T05:55:57.076244: step 1676, loss 0.200739, acc 0.9375\n",
      "2017-11-06T05:56:01.072084: step 1677, loss 0.229313, acc 0.875\n",
      "2017-11-06T05:56:05.058916: step 1678, loss 0.194863, acc 0.875\n",
      "2017-11-06T05:56:09.024734: step 1679, loss 0.243536, acc 0.90625\n",
      "2017-11-06T05:56:13.012568: step 1680, loss 0.380359, acc 0.84375\n",
      "2017-11-06T05:56:16.976386: step 1681, loss 0.124794, acc 0.9375\n",
      "2017-11-06T05:56:20.987234: step 1682, loss 0.328988, acc 0.78125\n",
      "2017-11-06T05:56:25.009093: step 1683, loss 0.111076, acc 0.9375\n",
      "2017-11-06T05:56:28.955896: step 1684, loss 0.078858, acc 0.96875\n",
      "2017-11-06T05:56:33.059812: step 1685, loss 0.403356, acc 0.8125\n",
      "2017-11-06T05:56:37.115693: step 1686, loss 0.276759, acc 0.875\n",
      "2017-11-06T05:56:41.144557: step 1687, loss 0.186931, acc 0.96875\n",
      "2017-11-06T05:56:45.115378: step 1688, loss 0.295795, acc 0.90625\n",
      "2017-11-06T05:56:49.121224: step 1689, loss 0.265828, acc 0.875\n",
      "2017-11-06T05:56:53.197189: step 1690, loss 0.130361, acc 0.9375\n",
      "2017-11-06T05:56:57.449210: step 1691, loss 0.176404, acc 0.9375\n",
      "2017-11-06T05:57:00.179149: step 1692, loss 0.187364, acc 0.9\n",
      "2017-11-06T05:57:04.263051: step 1693, loss 0.0999472, acc 0.96875\n",
      "2017-11-06T05:57:08.246881: step 1694, loss 0.220379, acc 0.90625\n",
      "2017-11-06T05:57:12.160663: step 1695, loss 0.272422, acc 0.84375\n",
      "2017-11-06T05:57:16.137490: step 1696, loss 0.269279, acc 0.90625\n",
      "2017-11-06T05:57:20.117317: step 1697, loss 0.173174, acc 0.90625\n",
      "2017-11-06T05:57:24.144196: step 1698, loss 0.0392654, acc 1\n",
      "2017-11-06T05:57:28.064963: step 1699, loss 0.294377, acc 0.90625\n",
      "2017-11-06T05:57:31.998759: step 1700, loss 0.50659, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T05:57:34.547329: step 1700, loss 0.919129, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-06T05:57:39.806523: step 1701, loss 0.228088, acc 0.875\n",
      "2017-11-06T05:57:43.811369: step 1702, loss 0.39155, acc 0.78125\n",
      "2017-11-06T05:57:47.734157: step 1703, loss 0.13622, acc 0.90625\n",
      "2017-11-06T05:57:51.709981: step 1704, loss 0.352618, acc 0.8125\n",
      "2017-11-06T05:57:55.625765: step 1705, loss 0.170511, acc 0.90625\n",
      "2017-11-06T05:57:59.547550: step 1706, loss 0.249235, acc 0.875\n",
      "2017-11-06T05:58:03.902645: step 1707, loss 0.268567, acc 0.90625\n",
      "2017-11-06T05:58:07.954524: step 1708, loss 0.424292, acc 0.78125\n",
      "2017-11-06T05:58:11.866305: step 1709, loss 0.129417, acc 0.96875\n",
      "2017-11-06T05:58:15.868147: step 1710, loss 0.22516, acc 0.875\n",
      "2017-11-06T05:58:19.782929: step 1711, loss 0.20201, acc 0.875\n",
      "2017-11-06T05:58:23.914864: step 1712, loss 0.0430636, acc 1\n",
      "2017-11-06T05:58:28.007773: step 1713, loss 0.20204, acc 0.875\n",
      "2017-11-06T05:58:31.954577: step 1714, loss 0.163516, acc 0.96875\n",
      "2017-11-06T05:58:36.145555: step 1715, loss 0.434067, acc 0.875\n",
      "2017-11-06T05:58:40.178421: step 1716, loss 0.175721, acc 0.9375\n",
      "2017-11-06T05:58:44.187269: step 1717, loss 0.22913, acc 0.90625\n",
      "2017-11-06T05:58:48.139078: step 1718, loss 0.0838739, acc 0.96875\n",
      "2017-11-06T05:58:52.165938: step 1719, loss 0.254241, acc 0.84375\n",
      "2017-11-06T05:58:56.202864: step 1720, loss 0.108237, acc 0.96875\n",
      "2017-11-06T05:59:00.191699: step 1721, loss 0.153667, acc 0.90625\n",
      "2017-11-06T05:59:04.184536: step 1722, loss 0.21381, acc 0.875\n",
      "2017-11-06T05:59:08.411540: step 1723, loss 0.377332, acc 0.875\n",
      "2017-11-06T05:59:12.602518: step 1724, loss 0.178909, acc 0.90625\n",
      "2017-11-06T05:59:16.559328: step 1725, loss 0.232286, acc 0.90625\n",
      "2017-11-06T05:59:20.522145: step 1726, loss 0.138676, acc 0.90625\n",
      "2017-11-06T05:59:24.529994: step 1727, loss 0.293969, acc 0.875\n",
      "2017-11-06T05:59:27.051784: step 1728, loss 0.243303, acc 0.85\n",
      "2017-11-06T05:59:31.034614: step 1729, loss 0.28723, acc 0.84375\n",
      "2017-11-06T05:59:34.966410: step 1730, loss 0.244019, acc 0.875\n",
      "2017-11-06T05:59:38.903205: step 1731, loss 0.126383, acc 0.9375\n",
      "2017-11-06T05:59:42.848008: step 1732, loss 0.169212, acc 0.9375\n",
      "2017-11-06T05:59:46.847850: step 1733, loss 0.134811, acc 0.90625\n",
      "2017-11-06T05:59:50.839686: step 1734, loss 0.0452598, acc 1\n",
      "2017-11-06T05:59:54.798499: step 1735, loss 0.284329, acc 0.875\n",
      "2017-11-06T05:59:58.746304: step 1736, loss 0.0273511, acc 1\n",
      "2017-11-06T06:00:03.018340: step 1737, loss 0.334517, acc 0.84375\n",
      "2017-11-06T06:00:06.989161: step 1738, loss 0.462154, acc 0.8125\n",
      "2017-11-06T06:00:10.914950: step 1739, loss 0.0407105, acc 1\n",
      "2017-11-06T06:00:15.340095: step 1740, loss 0.211099, acc 0.9375\n",
      "2017-11-06T06:00:19.368958: step 1741, loss 0.193017, acc 0.90625\n",
      "2017-11-06T06:00:23.367798: step 1742, loss 0.245432, acc 0.84375\n",
      "2017-11-06T06:00:27.290586: step 1743, loss 0.350889, acc 0.84375\n",
      "2017-11-06T06:00:31.252403: step 1744, loss 0.389569, acc 0.875\n",
      "2017-11-06T06:00:35.383253: step 1745, loss 0.354697, acc 0.84375\n",
      "2017-11-06T06:00:39.345065: step 1746, loss 0.218502, acc 0.90625\n",
      "2017-11-06T06:00:43.316887: step 1747, loss 0.105527, acc 0.96875\n",
      "2017-11-06T06:00:47.293714: step 1748, loss 0.0957882, acc 0.96875\n",
      "2017-11-06T06:00:51.268537: step 1749, loss 0.103574, acc 0.9375\n",
      "2017-11-06T06:00:55.211339: step 1750, loss 0.20268, acc 0.90625\n",
      "2017-11-06T06:00:59.289701: step 1751, loss 0.368492, acc 0.84375\n",
      "2017-11-06T06:01:03.279537: step 1752, loss 0.333424, acc 0.84375\n",
      "2017-11-06T06:01:07.237350: step 1753, loss 0.353331, acc 0.875\n",
      "2017-11-06T06:01:11.198163: step 1754, loss 0.415463, acc 0.8125\n",
      "2017-11-06T06:01:15.134960: step 1755, loss 0.436254, acc 0.78125\n",
      "2017-11-06T06:01:19.381978: step 1756, loss 0.173292, acc 0.96875\n",
      "2017-11-06T06:01:23.650013: step 1757, loss 0.151496, acc 0.9375\n",
      "2017-11-06T06:01:27.630839: step 1758, loss 0.319624, acc 0.875\n",
      "2017-11-06T06:01:31.596657: step 1759, loss 0.108261, acc 0.9375\n",
      "2017-11-06T06:01:35.696571: step 1760, loss 0.221451, acc 0.875\n",
      "2017-11-06T06:01:39.859528: step 1761, loss 0.15973, acc 0.96875\n",
      "2017-11-06T06:01:43.898399: step 1762, loss 0.206263, acc 0.84375\n",
      "2017-11-06T06:01:47.911250: step 1763, loss 0.090861, acc 0.96875\n",
      "2017-11-06T06:01:50.508095: step 1764, loss 0.104679, acc 0.95\n",
      "2017-11-06T06:01:54.500931: step 1765, loss 0.0250737, acc 1\n",
      "2017-11-06T06:01:58.493769: step 1766, loss 0.444028, acc 0.8125\n",
      "2017-11-06T06:02:02.489608: step 1767, loss 0.151921, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T06:02:06.503460: step 1768, loss 0.0704569, acc 0.96875\n",
      "2017-11-06T06:02:10.526319: step 1769, loss 0.14754, acc 0.9375\n",
      "2017-11-06T06:02:14.566189: step 1770, loss 0.259711, acc 0.90625\n",
      "2017-11-06T06:02:18.545016: step 1771, loss 0.24567, acc 0.9375\n",
      "2017-11-06T06:02:22.557867: step 1772, loss 0.30033, acc 0.875\n",
      "2017-11-06T06:02:26.906958: step 1773, loss 0.190787, acc 0.9375\n",
      "2017-11-06T06:02:30.900795: step 1774, loss 0.180787, acc 0.90625\n",
      "2017-11-06T06:02:35.138807: step 1775, loss 0.173627, acc 0.90625\n",
      "2017-11-06T06:02:39.162666: step 1776, loss 0.151773, acc 0.9375\n",
      "2017-11-06T06:02:43.122479: step 1777, loss 0.149578, acc 0.9375\n",
      "2017-11-06T06:02:47.095303: step 1778, loss 0.101236, acc 0.96875\n",
      "2017-11-06T06:02:51.121162: step 1779, loss 0.261855, acc 0.875\n",
      "2017-11-06T06:02:55.086980: step 1780, loss 0.21319, acc 0.875\n",
      "2017-11-06T06:02:58.999760: step 1781, loss 0.237875, acc 0.875\n",
      "2017-11-06T06:03:03.050790: step 1782, loss 0.244201, acc 0.8125\n",
      "2017-11-06T06:03:06.996592: step 1783, loss 0.233252, acc 0.78125\n",
      "2017-11-06T06:03:10.965412: step 1784, loss 0.139092, acc 0.90625\n",
      "2017-11-06T06:03:14.947242: step 1785, loss 0.338847, acc 0.875\n",
      "2017-11-06T06:03:18.919063: step 1786, loss 0.177103, acc 0.875\n",
      "2017-11-06T06:03:22.857862: step 1787, loss 0.294303, acc 0.8125\n",
      "2017-11-06T06:03:26.778648: step 1788, loss 0.18132, acc 0.875\n",
      "2017-11-06T06:03:31.055687: step 1789, loss 0.1567, acc 0.90625\n",
      "2017-11-06T06:03:35.283465: step 1790, loss 0.315846, acc 0.84375\n",
      "2017-11-06T06:03:39.354357: step 1791, loss 0.107229, acc 1\n",
      "2017-11-06T06:03:43.637400: step 1792, loss 0.258477, acc 0.875\n",
      "2017-11-06T06:03:47.701287: step 1793, loss 0.189763, acc 0.9375\n",
      "2017-11-06T06:03:51.896268: step 1794, loss 0.217625, acc 0.875\n",
      "2017-11-06T06:03:55.997184: step 1795, loss 0.478639, acc 0.875\n",
      "2017-11-06T06:04:00.112106: step 1796, loss 0.0808675, acc 0.96875\n",
      "2017-11-06T06:04:04.262055: step 1797, loss 0.14994, acc 0.90625\n",
      "2017-11-06T06:04:08.365970: step 1798, loss 0.304672, acc 0.84375\n",
      "2017-11-06T06:04:12.441868: step 1799, loss 0.20629, acc 0.9375\n",
      "2017-11-06T06:04:15.022701: step 1800, loss 0.208651, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:04:17.669582: step 1800, loss 0.973833, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-06T06:04:23.260554: step 1801, loss 0.238947, acc 0.84375\n",
      "2017-11-06T06:04:27.690701: step 1802, loss 0.291953, acc 0.875\n",
      "2017-11-06T06:04:31.797620: step 1803, loss 0.0869895, acc 0.9375\n",
      "2017-11-06T06:04:36.362863: step 1804, loss 0.119463, acc 0.96875\n",
      "2017-11-06T06:04:40.793011: step 1805, loss 0.291754, acc 0.875\n",
      "2017-11-06T06:04:44.995998: step 1806, loss 0.316355, acc 0.875\n",
      "2017-11-06T06:04:49.042873: step 1807, loss 0.137412, acc 0.90625\n",
      "2017-11-06T06:04:53.070736: step 1808, loss 0.14592, acc 0.90625\n",
      "2017-11-06T06:04:57.054566: step 1809, loss 0.284705, acc 0.875\n",
      "2017-11-06T06:05:01.048404: step 1810, loss 0.319538, acc 0.90625\n",
      "2017-11-06T06:05:05.058287: step 1811, loss 0.128616, acc 0.9375\n",
      "2017-11-06T06:05:09.079143: step 1812, loss 0.269396, acc 0.875\n",
      "2017-11-06T06:05:13.028950: step 1813, loss 0.121897, acc 0.90625\n",
      "2017-11-06T06:05:17.038799: step 1814, loss 0.284752, acc 0.90625\n",
      "2017-11-06T06:05:20.983604: step 1815, loss 0.130019, acc 0.90625\n",
      "2017-11-06T06:05:25.010464: step 1816, loss 0.115624, acc 0.96875\n",
      "2017-11-06T06:05:28.953265: step 1817, loss 0.157589, acc 0.90625\n",
      "2017-11-06T06:05:32.989133: step 1818, loss 0.115944, acc 0.9375\n",
      "2017-11-06T06:05:36.966959: step 1819, loss 0.0766973, acc 0.96875\n",
      "2017-11-06T06:05:41.144928: step 1820, loss 0.304017, acc 0.84375\n",
      "2017-11-06T06:05:45.424968: step 1821, loss 0.278059, acc 0.875\n",
      "2017-11-06T06:05:49.384783: step 1822, loss 0.277631, acc 0.90625\n",
      "2017-11-06T06:05:53.392630: step 1823, loss 0.244386, acc 0.875\n",
      "2017-11-06T06:05:57.345438: step 1824, loss 0.129938, acc 0.9375\n",
      "2017-11-06T06:06:01.340278: step 1825, loss 0.486439, acc 0.6875\n",
      "2017-11-06T06:06:05.321106: step 1826, loss 0.184657, acc 0.90625\n",
      "2017-11-06T06:06:09.250898: step 1827, loss 0.223245, acc 0.875\n",
      "2017-11-06T06:06:13.225722: step 1828, loss 0.194338, acc 0.90625\n",
      "2017-11-06T06:06:17.173527: step 1829, loss 0.290393, acc 0.875\n",
      "2017-11-06T06:06:21.126337: step 1830, loss 0.32709, acc 0.8125\n",
      "2017-11-06T06:06:25.087150: step 1831, loss 0.154617, acc 0.9375\n",
      "2017-11-06T06:06:29.040959: step 1832, loss 0.307139, acc 0.875\n",
      "2017-11-06T06:06:33.116856: step 1833, loss 0.214714, acc 0.875\n",
      "2017-11-06T06:06:37.183601: step 1834, loss 0.268299, acc 0.875\n",
      "2017-11-06T06:06:41.125384: step 1835, loss 0.12906, acc 0.90625\n",
      "2017-11-06T06:06:43.667188: step 1836, loss 0.0363599, acc 1\n",
      "2017-11-06T06:06:47.932219: step 1837, loss 0.153314, acc 0.96875\n",
      "2017-11-06T06:06:52.109187: step 1838, loss 0.167966, acc 0.90625\n",
      "2017-11-06T06:06:56.123041: step 1839, loss 0.12855, acc 0.90625\n",
      "2017-11-06T06:07:00.042824: step 1840, loss 0.297623, acc 0.875\n",
      "2017-11-06T06:07:03.975618: step 1841, loss 0.0841696, acc 0.96875\n",
      "2017-11-06T06:07:08.015549: step 1842, loss 0.0864123, acc 0.96875\n",
      "2017-11-06T06:07:11.956349: step 1843, loss 0.207411, acc 0.875\n",
      "2017-11-06T06:07:15.930174: step 1844, loss 0.166664, acc 0.9375\n",
      "2017-11-06T06:07:19.889986: step 1845, loss 0.132155, acc 0.9375\n",
      "2017-11-06T06:07:23.915848: step 1846, loss 0.237661, acc 0.90625\n",
      "2017-11-06T06:07:27.897676: step 1847, loss 0.110241, acc 0.90625\n",
      "2017-11-06T06:07:31.867498: step 1848, loss 0.274248, acc 0.875\n",
      "2017-11-06T06:07:35.785281: step 1849, loss 0.219217, acc 0.90625\n",
      "2017-11-06T06:07:39.747095: step 1850, loss 0.314187, acc 0.90625\n",
      "2017-11-06T06:07:43.701905: step 1851, loss 0.237032, acc 0.84375\n",
      "2017-11-06T06:07:47.676730: step 1852, loss 0.193368, acc 0.90625\n",
      "2017-11-06T06:07:51.809666: step 1853, loss 0.231787, acc 0.875\n",
      "2017-11-06T06:07:56.189779: step 1854, loss 0.231899, acc 0.875\n",
      "2017-11-06T06:08:00.151594: step 1855, loss 0.0616035, acc 0.96875\n",
      "2017-11-06T06:08:04.139427: step 1856, loss 0.202614, acc 0.90625\n",
      "2017-11-06T06:08:08.136268: step 1857, loss 0.19879, acc 0.9375\n",
      "2017-11-06T06:08:12.062056: step 1858, loss 0.304158, acc 0.90625\n",
      "2017-11-06T06:08:16.010862: step 1859, loss 0.201474, acc 0.875\n",
      "2017-11-06T06:08:19.960670: step 1860, loss 0.113305, acc 0.9375\n",
      "2017-11-06T06:08:23.923485: step 1861, loss 0.104622, acc 0.96875\n",
      "2017-11-06T06:08:27.895307: step 1862, loss 0.256077, acc 0.875\n",
      "2017-11-06T06:08:31.855121: step 1863, loss 0.249348, acc 0.8125\n",
      "2017-11-06T06:08:36.033090: step 1864, loss 0.271084, acc 0.9375\n",
      "2017-11-06T06:08:39.993903: step 1865, loss 0.207195, acc 0.84375\n",
      "2017-11-06T06:08:43.903681: step 1866, loss 0.206791, acc 0.875\n",
      "2017-11-06T06:08:47.904525: step 1867, loss 0.203845, acc 0.84375\n",
      "2017-11-06T06:08:51.851329: step 1868, loss 0.0453737, acc 0.96875\n",
      "2017-11-06T06:08:55.867182: step 1869, loss 0.123994, acc 0.96875\n",
      "2017-11-06T06:09:00.264306: step 1870, loss 0.617661, acc 0.8125\n",
      "2017-11-06T06:09:04.311182: step 1871, loss 0.3374, acc 0.875\n",
      "2017-11-06T06:09:06.971380: step 1872, loss 0.139683, acc 0.95\n",
      "2017-11-06T06:09:10.964218: step 1873, loss 0.111067, acc 0.9375\n",
      "2017-11-06T06:09:14.948051: step 1874, loss 0.245468, acc 0.875\n",
      "2017-11-06T06:09:18.867833: step 1875, loss 0.268686, acc 0.90625\n",
      "2017-11-06T06:09:23.008775: step 1876, loss 0.161669, acc 0.9375\n",
      "2017-11-06T06:09:27.076665: step 1877, loss 0.331501, acc 0.84375\n",
      "2017-11-06T06:09:31.034477: step 1878, loss 0.310046, acc 0.8125\n",
      "2017-11-06T06:09:35.003151: step 1879, loss 0.353044, acc 0.8125\n",
      "2017-11-06T06:09:38.980976: step 1880, loss 0.132079, acc 0.96875\n",
      "2017-11-06T06:09:42.958802: step 1881, loss 0.0403397, acc 0.96875\n",
      "2017-11-06T06:09:46.935628: step 1882, loss 0.247152, acc 0.875\n",
      "2017-11-06T06:09:50.931468: step 1883, loss 0.0601438, acc 0.96875\n",
      "2017-11-06T06:09:54.869266: step 1884, loss 0.231317, acc 0.90625\n",
      "2017-11-06T06:09:58.883117: step 1885, loss 0.19333, acc 0.90625\n",
      "2017-11-06T06:10:03.318269: step 1886, loss 0.137243, acc 0.90625\n",
      "2017-11-06T06:10:07.685372: step 1887, loss 0.0178676, acc 1\n",
      "2017-11-06T06:10:11.651190: step 1888, loss 0.199576, acc 0.9375\n",
      "2017-11-06T06:10:15.599995: step 1889, loss 0.205953, acc 0.9375\n",
      "2017-11-06T06:10:19.502769: step 1890, loss 0.298735, acc 0.875\n",
      "2017-11-06T06:10:23.476592: step 1891, loss 0.196719, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T06:10:27.501454: step 1892, loss 0.160205, acc 0.9375\n",
      "2017-11-06T06:10:31.411232: step 1893, loss 0.261674, acc 0.90625\n",
      "2017-11-06T06:10:35.613217: step 1894, loss 0.104514, acc 0.96875\n",
      "2017-11-06T06:10:39.533001: step 1895, loss 0.22928, acc 0.9375\n",
      "2017-11-06T06:10:43.458790: step 1896, loss 0.152984, acc 0.96875\n",
      "2017-11-06T06:10:47.446624: step 1897, loss 0.320377, acc 0.84375\n",
      "2017-11-06T06:10:51.393428: step 1898, loss 0.241094, acc 0.90625\n",
      "2017-11-06T06:10:55.365252: step 1899, loss 0.358099, acc 0.84375\n",
      "2017-11-06T06:10:59.298045: step 1900, loss 0.274576, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:11:01.859865: step 1900, loss 0.836268, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-06T06:11:07.558265: step 1901, loss 0.0923633, acc 0.96875\n",
      "2017-11-06T06:11:12.042897: step 1902, loss 0.178276, acc 0.90625\n",
      "2017-11-06T06:11:16.034734: step 1903, loss 0.269338, acc 0.875\n",
      "2017-11-06T06:11:19.986540: step 1904, loss 0.266381, acc 0.90625\n",
      "2017-11-06T06:11:23.998391: step 1905, loss 0.181093, acc 0.875\n",
      "2017-11-06T06:11:27.886154: step 1906, loss 0.22516, acc 0.9375\n",
      "2017-11-06T06:11:31.896003: step 1907, loss 0.0886206, acc 0.9375\n",
      "2017-11-06T06:11:34.551890: step 1908, loss 0.109297, acc 0.95\n",
      "2017-11-06T06:11:38.513707: step 1909, loss 0.267745, acc 0.875\n",
      "2017-11-06T06:11:42.427486: step 1910, loss 0.277346, acc 0.90625\n",
      "2017-11-06T06:11:46.420323: step 1911, loss 0.212132, acc 0.875\n",
      "2017-11-06T06:11:50.381138: step 1912, loss 0.213851, acc 0.875\n",
      "2017-11-06T06:11:54.322938: step 1913, loss 0.293132, acc 0.875\n",
      "2017-11-06T06:11:58.273745: step 1914, loss 0.0418084, acc 1\n",
      "2017-11-06T06:12:02.334631: step 1915, loss 0.31032, acc 0.90625\n",
      "2017-11-06T06:12:06.285438: step 1916, loss 0.234967, acc 0.84375\n",
      "2017-11-06T06:12:10.279276: step 1917, loss 0.195569, acc 0.90625\n",
      "2017-11-06T06:12:14.458245: step 1918, loss 0.0939092, acc 0.96875\n",
      "2017-11-06T06:12:18.767307: step 1919, loss 0.216622, acc 0.875\n",
      "2017-11-06T06:12:22.776155: step 1920, loss 0.16519, acc 0.875\n",
      "2017-11-06T06:12:26.804017: step 1921, loss 0.280266, acc 0.875\n",
      "2017-11-06T06:12:30.790850: step 1922, loss 0.23836, acc 0.875\n",
      "2017-11-06T06:12:35.019613: step 1923, loss 0.165097, acc 0.96875\n",
      "2017-11-06T06:12:39.000443: step 1924, loss 0.256907, acc 0.90625\n",
      "2017-11-06T06:12:42.920227: step 1925, loss 0.149147, acc 0.90625\n",
      "2017-11-06T06:12:46.920069: step 1926, loss 0.275258, acc 0.875\n",
      "2017-11-06T06:12:50.885887: step 1927, loss 0.377883, acc 0.875\n",
      "2017-11-06T06:12:54.874721: step 1928, loss 0.0878603, acc 0.96875\n",
      "2017-11-06T06:12:58.912591: step 1929, loss 0.335723, acc 0.84375\n",
      "2017-11-06T06:13:02.878408: step 1930, loss 0.13452, acc 0.96875\n",
      "2017-11-06T06:13:06.890259: step 1931, loss 0.198367, acc 0.84375\n",
      "2017-11-06T06:13:10.847070: step 1932, loss 0.201645, acc 0.875\n",
      "2017-11-06T06:13:14.925115: step 1933, loss 0.21063, acc 0.875\n",
      "2017-11-06T06:13:18.979977: step 1934, loss 0.19774, acc 0.9375\n",
      "2017-11-06T06:13:23.390111: step 1935, loss 0.158454, acc 0.9375\n",
      "2017-11-06T06:13:27.619116: step 1936, loss 0.162202, acc 0.90625\n",
      "2017-11-06T06:13:31.597943: step 1937, loss 0.118859, acc 0.96875\n",
      "2017-11-06T06:13:35.572767: step 1938, loss 0.373771, acc 0.78125\n",
      "2017-11-06T06:13:39.533582: step 1939, loss 0.274934, acc 0.875\n",
      "2017-11-06T06:13:43.496397: step 1940, loss 0.156515, acc 0.9375\n",
      "2017-11-06T06:13:47.528262: step 1941, loss 0.103703, acc 0.96875\n",
      "2017-11-06T06:13:51.649190: step 1942, loss 0.237465, acc 0.90625\n",
      "2017-11-06T06:13:55.638024: step 1943, loss 0.147572, acc 0.9375\n",
      "2017-11-06T06:13:58.253883: step 1944, loss 0.178594, acc 0.95\n",
      "2017-11-06T06:14:02.314769: step 1945, loss 0.156555, acc 0.9375\n",
      "2017-11-06T06:14:06.363645: step 1946, loss 0.0768989, acc 0.96875\n",
      "2017-11-06T06:14:10.404516: step 1947, loss 0.122508, acc 0.9375\n",
      "2017-11-06T06:14:14.435380: step 1948, loss 0.13436, acc 0.9375\n",
      "2017-11-06T06:14:18.411205: step 1949, loss 0.125615, acc 0.90625\n",
      "2017-11-06T06:14:22.405044: step 1950, loss 0.320211, acc 0.90625\n",
      "2017-11-06T06:14:26.690088: step 1951, loss 0.110928, acc 0.96875\n",
      "2017-11-06T06:14:30.993147: step 1952, loss 0.144251, acc 0.9375\n",
      "2017-11-06T06:14:35.153102: step 1953, loss 0.241376, acc 0.875\n",
      "2017-11-06T06:14:39.136932: step 1954, loss 0.222876, acc 0.90625\n",
      "2017-11-06T06:14:43.115759: step 1955, loss 0.124744, acc 0.9375\n",
      "2017-11-06T06:14:47.065566: step 1956, loss 0.243099, acc 0.84375\n",
      "2017-11-06T06:14:51.029382: step 1957, loss 0.137645, acc 0.9375\n",
      "2017-11-06T06:14:54.991198: step 1958, loss 0.228157, acc 0.875\n",
      "2017-11-06T06:14:59.072096: step 1959, loss 0.333126, acc 0.84375\n",
      "2017-11-06T06:15:03.042918: step 1960, loss 0.0627605, acc 0.96875\n",
      "2017-11-06T06:15:07.028750: step 1961, loss 0.161406, acc 0.9375\n",
      "2017-11-06T06:15:11.022589: step 1962, loss 0.330115, acc 0.8125\n",
      "2017-11-06T06:15:15.017426: step 1963, loss 0.167045, acc 0.9375\n",
      "2017-11-06T06:15:18.986246: step 1964, loss 0.182679, acc 0.90625\n",
      "2017-11-06T06:15:22.946060: step 1965, loss 0.235851, acc 0.90625\n",
      "2017-11-06T06:15:26.926889: step 1966, loss 0.318495, acc 0.84375\n",
      "2017-11-06T06:15:31.062827: step 1967, loss 0.109537, acc 0.9375\n",
      "2017-11-06T06:15:35.356657: step 1968, loss 0.186973, acc 0.9375\n",
      "2017-11-06T06:15:39.308465: step 1969, loss 0.236738, acc 0.84375\n",
      "2017-11-06T06:15:43.333325: step 1970, loss 0.110944, acc 0.96875\n",
      "2017-11-06T06:15:47.294140: step 1971, loss 0.302702, acc 0.8125\n",
      "2017-11-06T06:15:51.313996: step 1972, loss 0.213302, acc 0.875\n",
      "2017-11-06T06:15:55.282816: step 1973, loss 0.296965, acc 0.875\n",
      "2017-11-06T06:15:59.252636: step 1974, loss 0.325047, acc 0.84375\n",
      "2017-11-06T06:16:03.214451: step 1975, loss 0.144549, acc 0.9375\n",
      "2017-11-06T06:16:07.248318: step 1976, loss 0.3063, acc 0.875\n",
      "2017-11-06T06:16:11.335223: step 1977, loss 0.188192, acc 0.875\n",
      "2017-11-06T06:16:15.301041: step 1978, loss 0.113607, acc 0.96875\n",
      "2017-11-06T06:16:19.242841: step 1979, loss 0.142615, acc 0.90625\n",
      "2017-11-06T06:16:21.770637: step 1980, loss 0.0359842, acc 1\n",
      "2017-11-06T06:16:25.753466: step 1981, loss 0.309033, acc 0.875\n",
      "2017-11-06T06:16:29.754309: step 1982, loss 0.0674071, acc 1\n",
      "2017-11-06T06:16:33.931277: step 1983, loss 0.137883, acc 0.9375\n",
      "2017-11-06T06:16:38.250346: step 1984, loss 0.253428, acc 0.875\n",
      "2017-11-06T06:16:42.387285: step 1985, loss 0.157156, acc 0.90625\n",
      "2017-11-06T06:16:46.330087: step 1986, loss 0.173561, acc 0.875\n",
      "2017-11-06T06:16:50.366956: step 1987, loss 0.192895, acc 0.9375\n",
      "2017-11-06T06:16:54.338777: step 1988, loss 0.483569, acc 0.75\n",
      "2017-11-06T06:16:58.295589: step 1989, loss 0.31198, acc 0.84375\n",
      "2017-11-06T06:17:02.319448: step 1990, loss 0.207949, acc 0.875\n",
      "2017-11-06T06:17:06.274257: step 1991, loss 0.255836, acc 0.875\n",
      "2017-11-06T06:17:10.284107: step 1992, loss 0.13054, acc 0.9375\n",
      "2017-11-06T06:17:14.254929: step 1993, loss 0.264746, acc 0.875\n",
      "2017-11-06T06:17:18.303840: step 1994, loss 0.267036, acc 0.875\n",
      "2017-11-06T06:17:22.220624: step 1995, loss 0.133044, acc 0.9375\n",
      "2017-11-06T06:17:26.200470: step 1996, loss 0.149262, acc 0.9375\n",
      "2017-11-06T06:17:30.209300: step 1997, loss 0.397273, acc 0.8125\n",
      "2017-11-06T06:17:34.180121: step 1998, loss 0.101294, acc 0.9375\n",
      "2017-11-06T06:17:38.153946: step 1999, loss 0.195851, acc 0.875\n",
      "2017-11-06T06:17:42.245852: step 2000, loss 0.297925, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:17:45.109887: step 2000, loss 1.03704, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-06T06:17:50.555402: step 2001, loss 0.361155, acc 0.90625\n",
      "2017-11-06T06:17:54.490198: step 2002, loss 0.146277, acc 0.9375\n",
      "2017-11-06T06:17:58.451013: step 2003, loss 0.145018, acc 0.9375\n",
      "2017-11-06T06:18:02.444850: step 2004, loss 0.070183, acc 0.96875\n",
      "2017-11-06T06:18:06.410668: step 2005, loss 0.334448, acc 0.875\n",
      "2017-11-06T06:18:10.472554: step 2006, loss 0.115969, acc 0.9375\n",
      "2017-11-06T06:18:14.478400: step 2007, loss 0.158865, acc 0.96875\n",
      "2017-11-06T06:18:18.454225: step 2008, loss 0.178367, acc 0.90625\n",
      "2017-11-06T06:18:22.583159: step 2009, loss 0.274395, acc 0.84375\n",
      "2017-11-06T06:18:26.885216: step 2010, loss 0.151499, acc 0.90625\n",
      "2017-11-06T06:18:30.931091: step 2011, loss 0.257862, acc 0.875\n",
      "2017-11-06T06:18:35.069031: step 2012, loss 0.3274, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T06:18:39.036606: step 2013, loss 0.15395, acc 0.90625\n",
      "2017-11-06T06:18:42.987414: step 2014, loss 0.219168, acc 0.875\n",
      "2017-11-06T06:18:46.986255: step 2015, loss 0.137599, acc 0.90625\n",
      "2017-11-06T06:18:49.871305: step 2016, loss 0.172092, acc 0.95\n",
      "2017-11-06T06:18:54.022254: step 2017, loss 0.0696748, acc 0.96875\n",
      "2017-11-06T06:18:57.995078: step 2018, loss 0.179847, acc 0.90625\n",
      "2017-11-06T06:19:02.031946: step 2019, loss 0.0639833, acc 0.96875\n",
      "2017-11-06T06:19:05.978750: step 2020, loss 0.122527, acc 0.9375\n",
      "2017-11-06T06:19:10.056647: step 2021, loss 0.112494, acc 0.9375\n",
      "2017-11-06T06:19:14.067498: step 2022, loss 0.28045, acc 0.875\n",
      "2017-11-06T06:19:17.994287: step 2023, loss 0.0611839, acc 0.9375\n",
      "2017-11-06T06:19:22.029294: step 2024, loss 0.262361, acc 0.875\n",
      "2017-11-06T06:19:26.004119: step 2025, loss 0.164258, acc 0.875\n",
      "2017-11-06T06:19:29.950922: step 2026, loss 0.20702, acc 0.90625\n",
      "2017-11-06T06:19:33.957770: step 2027, loss 0.275458, acc 0.84375\n",
      "2017-11-06T06:19:37.887563: step 2028, loss 0.242141, acc 0.84375\n",
      "2017-11-06T06:19:41.796339: step 2029, loss 0.224409, acc 0.8125\n",
      "2017-11-06T06:19:45.769163: step 2030, loss 0.114037, acc 0.9375\n",
      "2017-11-06T06:19:49.705959: step 2031, loss 0.157808, acc 0.90625\n",
      "2017-11-06T06:19:53.845901: step 2032, loss 0.264051, acc 0.90625\n",
      "2017-11-06T06:19:58.160966: step 2033, loss 0.0889089, acc 0.9375\n",
      "2017-11-06T06:20:02.415992: step 2034, loss 0.205915, acc 0.875\n",
      "2017-11-06T06:20:06.358791: step 2035, loss 0.153265, acc 0.90625\n",
      "2017-11-06T06:20:10.395659: step 2036, loss 0.260483, acc 0.90625\n",
      "2017-11-06T06:20:14.324451: step 2037, loss 0.218608, acc 0.90625\n",
      "2017-11-06T06:20:18.282265: step 2038, loss 0.222444, acc 0.90625\n",
      "2017-11-06T06:20:22.204050: step 2039, loss 0.3334, acc 0.84375\n",
      "2017-11-06T06:20:26.189882: step 2040, loss 0.148989, acc 0.9375\n",
      "2017-11-06T06:20:30.116672: step 2041, loss 0.0614742, acc 1\n",
      "2017-11-06T06:20:34.276628: step 2042, loss 0.104053, acc 0.96875\n",
      "2017-11-06T06:20:38.252454: step 2043, loss 0.206823, acc 0.90625\n",
      "2017-11-06T06:20:42.197256: step 2044, loss 0.283419, acc 0.875\n",
      "2017-11-06T06:20:46.113039: step 2045, loss 0.267689, acc 0.875\n",
      "2017-11-06T06:20:50.101872: step 2046, loss 0.305013, acc 0.875\n",
      "2017-11-06T06:20:54.054682: step 2047, loss 0.372251, acc 0.84375\n",
      "2017-11-06T06:20:58.013494: step 2048, loss 0.24809, acc 0.875\n",
      "2017-11-06T06:21:02.395607: step 2049, loss 0.275689, acc 0.90625\n",
      "2017-11-06T06:21:06.424470: step 2050, loss 0.124384, acc 0.90625\n",
      "2017-11-06T06:21:10.390288: step 2051, loss 0.22753, acc 0.84375\n",
      "2017-11-06T06:21:12.954110: step 2052, loss 0.232174, acc 0.9\n",
      "2017-11-06T06:21:16.892910: step 2053, loss 0.169726, acc 0.9375\n",
      "2017-11-06T06:21:20.867733: step 2054, loss 0.109539, acc 0.9375\n",
      "2017-11-06T06:21:24.891017: step 2055, loss 0.306646, acc 0.875\n",
      "2017-11-06T06:21:28.843826: step 2056, loss 0.25784, acc 0.875\n",
      "2017-11-06T06:21:32.804639: step 2057, loss 0.241837, acc 0.875\n",
      "2017-11-06T06:21:36.749202: step 2058, loss 0.123979, acc 0.96875\n",
      "2017-11-06T06:21:40.693004: step 2059, loss 0.322773, acc 0.84375\n",
      "2017-11-06T06:21:44.651819: step 2060, loss 0.187341, acc 0.9375\n",
      "2017-11-06T06:21:48.632648: step 2061, loss 0.209454, acc 0.9375\n",
      "2017-11-06T06:21:52.596462: step 2062, loss 0.234409, acc 0.90625\n",
      "2017-11-06T06:21:56.552273: step 2063, loss 0.330186, acc 0.78125\n",
      "2017-11-06T06:22:00.491072: step 2064, loss 0.41275, acc 0.78125\n",
      "2017-11-06T06:22:04.545953: step 2065, loss 0.171542, acc 0.875\n",
      "2017-11-06T06:22:08.873027: step 2066, loss 0.18964, acc 0.875\n",
      "2017-11-06T06:22:12.853860: step 2067, loss 0.28793, acc 0.875\n",
      "2017-11-06T06:22:16.854699: step 2068, loss 0.310493, acc 0.875\n",
      "2017-11-06T06:22:20.782490: step 2069, loss 0.13022, acc 0.9375\n",
      "2017-11-06T06:22:24.720289: step 2070, loss 0.34019, acc 0.8125\n",
      "2017-11-06T06:22:28.642074: step 2071, loss 0.260065, acc 0.875\n",
      "2017-11-06T06:22:32.743989: step 2072, loss 0.214736, acc 0.90625\n",
      "2017-11-06T06:22:36.844902: step 2073, loss 0.255881, acc 0.875\n",
      "2017-11-06T06:22:40.779699: step 2074, loss 0.322336, acc 0.875\n",
      "2017-11-06T06:22:44.709491: step 2075, loss 0.158958, acc 0.9375\n",
      "2017-11-06T06:22:48.625273: step 2076, loss 0.142447, acc 0.9375\n",
      "2017-11-06T06:22:52.615108: step 2077, loss 0.125886, acc 0.96875\n",
      "2017-11-06T06:22:56.554908: step 2078, loss 0.134852, acc 0.90625\n",
      "2017-11-06T06:23:00.546744: step 2079, loss 0.12685, acc 0.96875\n",
      "2017-11-06T06:23:04.587616: step 2080, loss 0.331497, acc 0.8125\n",
      "2017-11-06T06:23:08.603468: step 2081, loss 0.127607, acc 0.9375\n",
      "2017-11-06T06:23:13.006597: step 2082, loss 0.157055, acc 0.9375\n",
      "2017-11-06T06:23:17.209584: step 2083, loss 0.443455, acc 0.8125\n",
      "2017-11-06T06:23:21.171399: step 2084, loss 0.217226, acc 0.875\n",
      "2017-11-06T06:23:25.405408: step 2085, loss 0.0962268, acc 0.9375\n",
      "2017-11-06T06:23:29.595384: step 2086, loss 0.211573, acc 0.9375\n",
      "2017-11-06T06:23:33.596227: step 2087, loss 0.175792, acc 0.90625\n",
      "2017-11-06T06:23:36.160049: step 2088, loss 0.340093, acc 0.85\n",
      "2017-11-06T06:23:40.144879: step 2089, loss 0.333796, acc 0.875\n",
      "2017-11-06T06:23:44.123707: step 2090, loss 0.177521, acc 0.90625\n",
      "2017-11-06T06:23:48.124550: step 2091, loss 0.211189, acc 0.9375\n",
      "2017-11-06T06:23:52.124392: step 2092, loss 0.315155, acc 0.78125\n",
      "2017-11-06T06:23:56.123234: step 2093, loss 0.12699, acc 0.9375\n",
      "2017-11-06T06:24:00.101060: step 2094, loss 0.0588619, acc 1\n",
      "2017-11-06T06:24:04.025849: step 2095, loss 0.367238, acc 0.875\n",
      "2017-11-06T06:24:08.005676: step 2096, loss 0.22646, acc 0.90625\n",
      "2017-11-06T06:24:12.021530: step 2097, loss 0.0938087, acc 0.9375\n",
      "2017-11-06T06:24:16.176482: step 2098, loss 0.269352, acc 0.8125\n",
      "2017-11-06T06:24:20.516566: step 2099, loss 0.0874807, acc 0.96875\n",
      "2017-11-06T06:24:24.574450: step 2100, loss 0.234939, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:24:27.181301: step 2100, loss 0.937661, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-06T06:24:32.777886: step 2101, loss 0.0748958, acc 0.96875\n",
      "2017-11-06T06:24:36.866573: step 2102, loss 0.703241, acc 0.75\n",
      "2017-11-06T06:24:40.830390: step 2103, loss 0.433373, acc 0.875\n",
      "2017-11-06T06:24:44.795205: step 2104, loss 0.146257, acc 0.90625\n",
      "2017-11-06T06:24:48.720994: step 2105, loss 0.170637, acc 0.90625\n",
      "2017-11-06T06:24:52.695818: step 2106, loss 0.199443, acc 0.90625\n",
      "2017-11-06T06:24:56.717676: step 2107, loss 0.0654293, acc 0.96875\n",
      "2017-11-06T06:25:00.844609: step 2108, loss 0.368419, acc 0.8125\n",
      "2017-11-06T06:25:04.758390: step 2109, loss 0.231435, acc 0.875\n",
      "2017-11-06T06:25:08.786252: step 2110, loss 0.37228, acc 0.875\n",
      "2017-11-06T06:25:12.802106: step 2111, loss 0.188608, acc 0.90625\n",
      "2017-11-06T06:25:16.774928: step 2112, loss 0.09419, acc 0.96875\n",
      "2017-11-06T06:25:20.858830: step 2113, loss 0.249808, acc 0.875\n",
      "2017-11-06T06:25:25.222933: step 2114, loss 0.203815, acc 0.875\n",
      "2017-11-06T06:25:29.248791: step 2115, loss 0.176325, acc 0.9375\n",
      "2017-11-06T06:25:33.209606: step 2116, loss 0.145485, acc 0.9375\n",
      "2017-11-06T06:25:37.166417: step 2117, loss 0.193709, acc 0.90625\n",
      "2017-11-06T06:25:41.149248: step 2118, loss 0.241381, acc 0.90625\n",
      "2017-11-06T06:25:45.155093: step 2119, loss 0.113227, acc 0.96875\n",
      "2017-11-06T06:25:49.146930: step 2120, loss 0.263036, acc 0.875\n",
      "2017-11-06T06:25:53.138766: step 2121, loss 0.235681, acc 0.875\n",
      "2017-11-06T06:25:57.115592: step 2122, loss 0.255591, acc 0.875\n",
      "2017-11-06T06:26:01.102425: step 2123, loss 0.313351, acc 0.84375\n",
      "2017-11-06T06:26:03.738298: step 2124, loss 0.25426, acc 0.9\n",
      "2017-11-06T06:26:07.707117: step 2125, loss 0.0927984, acc 0.96875\n",
      "2017-11-06T06:26:11.788017: step 2126, loss 0.222502, acc 0.875\n",
      "2017-11-06T06:26:15.777852: step 2127, loss 0.111959, acc 0.90625\n",
      "2017-11-06T06:26:19.787703: step 2128, loss 0.188928, acc 0.875\n",
      "2017-11-06T06:26:23.793547: step 2129, loss 0.214617, acc 0.875\n",
      "2017-11-06T06:26:27.981524: step 2130, loss 0.305474, acc 0.8125\n",
      "2017-11-06T06:26:32.353630: step 2131, loss 0.208586, acc 0.90625\n",
      "2017-11-06T06:26:36.579632: step 2132, loss 0.178161, acc 0.875\n",
      "2017-11-06T06:26:40.521434: step 2133, loss 0.411346, acc 0.78125\n",
      "2017-11-06T06:26:44.495257: step 2134, loss 0.22699, acc 0.875\n",
      "2017-11-06T06:26:48.521118: step 2135, loss 0.287535, acc 0.84375\n",
      "2017-11-06T06:26:52.481932: step 2136, loss 0.165322, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T06:26:56.408722: step 2137, loss 0.18957, acc 0.9375\n",
      "2017-11-06T06:27:00.391552: step 2138, loss 0.345917, acc 0.90625\n",
      "2017-11-06T06:27:04.397399: step 2139, loss 0.143232, acc 0.90625\n",
      "2017-11-06T06:27:08.372222: step 2140, loss 0.0626122, acc 0.96875\n",
      "2017-11-06T06:27:12.343044: step 2141, loss 0.388229, acc 0.84375\n",
      "2017-11-06T06:27:16.345888: step 2142, loss 0.329484, acc 0.84375\n",
      "2017-11-06T06:27:20.308705: step 2143, loss 0.384089, acc 0.84375\n",
      "2017-11-06T06:27:24.380597: step 2144, loss 0.200034, acc 0.90625\n",
      "2017-11-06T06:27:28.363427: step 2145, loss 0.179569, acc 0.90625\n",
      "2017-11-06T06:27:32.517506: step 2146, loss 0.20865, acc 0.90625\n",
      "2017-11-06T06:27:36.757284: step 2147, loss 0.193395, acc 0.9375\n",
      "2017-11-06T06:27:40.776139: step 2148, loss 0.316125, acc 0.84375\n",
      "2017-11-06T06:27:44.695924: step 2149, loss 0.225643, acc 0.90625\n",
      "2017-11-06T06:27:48.690762: step 2150, loss 0.0982382, acc 1\n",
      "2017-11-06T06:27:52.693607: step 2151, loss 0.262905, acc 0.90625\n",
      "2017-11-06T06:27:56.614393: step 2152, loss 0.233304, acc 0.875\n",
      "2017-11-06T06:28:00.609231: step 2153, loss 0.204973, acc 0.875\n",
      "2017-11-06T06:28:04.592062: step 2154, loss 0.173467, acc 0.875\n",
      "2017-11-06T06:28:08.584898: step 2155, loss 0.237824, acc 0.90625\n",
      "2017-11-06T06:28:12.584740: step 2156, loss 0.152587, acc 0.9375\n",
      "2017-11-06T06:28:16.535547: step 2157, loss 0.352715, acc 0.90625\n",
      "2017-11-06T06:28:20.566412: step 2158, loss 0.153069, acc 0.90625\n",
      "2017-11-06T06:28:24.782408: step 2159, loss 0.177034, acc 0.9375\n",
      "2017-11-06T06:28:27.359238: step 2160, loss 0.661186, acc 0.75\n",
      "2017-11-06T06:28:31.367087: step 2161, loss 0.117334, acc 0.96875\n",
      "2017-11-06T06:28:35.580079: step 2162, loss 0.280233, acc 0.8125\n",
      "2017-11-06T06:28:39.793073: step 2163, loss 0.245544, acc 0.90625\n",
      "2017-11-06T06:28:44.094129: step 2164, loss 0.212136, acc 0.90625\n",
      "2017-11-06T06:28:48.019919: step 2165, loss 0.219983, acc 0.90625\n",
      "2017-11-06T06:28:51.975731: step 2166, loss 0.245716, acc 0.875\n",
      "2017-11-06T06:28:55.984577: step 2167, loss 0.327518, acc 0.875\n",
      "2017-11-06T06:28:59.923376: step 2168, loss 0.281451, acc 0.90625\n",
      "2017-11-06T06:29:03.829151: step 2169, loss 0.226766, acc 0.875\n",
      "2017-11-06T06:29:07.848007: step 2170, loss 0.269969, acc 0.84375\n",
      "2017-11-06T06:29:11.830836: step 2171, loss 0.283143, acc 0.8125\n",
      "2017-11-06T06:29:15.838685: step 2172, loss 0.112361, acc 0.96875\n",
      "2017-11-06T06:29:19.814511: step 2173, loss 0.547114, acc 0.8125\n",
      "2017-11-06T06:29:23.842372: step 2174, loss 0.209437, acc 0.90625\n",
      "2017-11-06T06:29:27.785174: step 2175, loss 0.137343, acc 0.96875\n",
      "2017-11-06T06:29:31.742985: step 2176, loss 0.265857, acc 0.875\n",
      "2017-11-06T06:29:35.778873: step 2177, loss 0.224135, acc 0.90625\n",
      "2017-11-06T06:29:39.727678: step 2178, loss 0.338792, acc 0.8125\n",
      "2017-11-06T06:29:43.805576: step 2179, loss 0.266109, acc 0.9375\n",
      "2017-11-06T06:29:48.244730: step 2180, loss 0.0990286, acc 0.9375\n",
      "2017-11-06T06:29:52.219555: step 2181, loss 0.0572706, acc 1\n",
      "2017-11-06T06:29:56.181372: step 2182, loss 0.165049, acc 0.90625\n",
      "2017-11-06T06:30:00.214236: step 2183, loss 0.209201, acc 0.875\n",
      "2017-11-06T06:30:04.371188: step 2184, loss 0.475984, acc 0.75\n",
      "2017-11-06T06:30:08.320996: step 2185, loss 0.297463, acc 0.84375\n",
      "2017-11-06T06:30:12.244783: step 2186, loss 0.183908, acc 0.90625\n",
      "2017-11-06T06:30:16.246628: step 2187, loss 0.122478, acc 0.90625\n",
      "2017-11-06T06:30:20.237464: step 2188, loss 0.226274, acc 0.90625\n",
      "2017-11-06T06:30:24.202280: step 2189, loss 0.0764584, acc 1\n",
      "2017-11-06T06:30:28.197118: step 2190, loss 0.0889351, acc 0.96875\n",
      "2017-11-06T06:30:32.166939: step 2191, loss 0.189364, acc 0.90625\n",
      "2017-11-06T06:30:36.328896: step 2192, loss 0.0258265, acc 1\n",
      "2017-11-06T06:30:40.302522: step 2193, loss 0.351813, acc 0.84375\n",
      "2017-11-06T06:30:44.227312: step 2194, loss 0.408985, acc 0.875\n",
      "2017-11-06T06:30:48.187125: step 2195, loss 0.360132, acc 0.84375\n",
      "2017-11-06T06:30:50.960096: step 2196, loss 0.36842, acc 0.75\n",
      "2017-11-06T06:30:55.268156: step 2197, loss 0.173368, acc 0.90625\n",
      "2017-11-06T06:30:59.232974: step 2198, loss 0.1193, acc 0.90625\n",
      "2017-11-06T06:31:03.210801: step 2199, loss 0.306965, acc 0.90625\n",
      "2017-11-06T06:31:07.212644: step 2200, loss 0.225871, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:31:09.800483: step 2200, loss 0.747613, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-06T06:31:15.286449: step 2201, loss 0.227181, acc 0.9375\n",
      "2017-11-06T06:31:19.234254: step 2202, loss 0.225255, acc 0.90625\n",
      "2017-11-06T06:31:23.239100: step 2203, loss 0.329245, acc 0.84375\n",
      "2017-11-06T06:31:27.227934: step 2204, loss 0.172452, acc 0.90625\n",
      "2017-11-06T06:31:31.193752: step 2205, loss 0.186014, acc 0.875\n",
      "2017-11-06T06:31:35.358712: step 2206, loss 0.145021, acc 0.90625\n",
      "2017-11-06T06:31:39.538681: step 2207, loss 0.220255, acc 0.90625\n",
      "2017-11-06T06:31:43.501497: step 2208, loss 0.267633, acc 0.90625\n",
      "2017-11-06T06:31:47.499338: step 2209, loss 0.271306, acc 0.90625\n",
      "2017-11-06T06:31:51.515191: step 2210, loss 0.148243, acc 0.9375\n",
      "2017-11-06T06:31:55.701165: step 2211, loss 0.0142798, acc 1\n",
      "2017-11-06T06:32:00.036246: step 2212, loss 0.250258, acc 0.9375\n",
      "2017-11-06T06:32:04.102135: step 2213, loss 0.247826, acc 0.875\n",
      "2017-11-06T06:32:08.063950: step 2214, loss 0.222329, acc 0.9375\n",
      "2017-11-06T06:32:12.063792: step 2215, loss 0.370452, acc 0.8125\n",
      "2017-11-06T06:32:16.132683: step 2216, loss 0.25368, acc 0.90625\n",
      "2017-11-06T06:32:20.119516: step 2217, loss 0.180074, acc 0.90625\n",
      "2017-11-06T06:32:24.100345: step 2218, loss 0.211141, acc 0.875\n",
      "2017-11-06T06:32:28.080173: step 2219, loss 0.270524, acc 0.78125\n",
      "2017-11-06T06:32:32.088022: step 2220, loss 0.304878, acc 0.84375\n",
      "2017-11-06T06:32:36.383071: step 2221, loss 0.161395, acc 0.90625\n",
      "2017-11-06T06:32:40.396923: step 2222, loss 0.185157, acc 0.90625\n",
      "2017-11-06T06:32:44.345729: step 2223, loss 0.298157, acc 0.84375\n",
      "2017-11-06T06:32:48.321555: step 2224, loss 0.160518, acc 0.875\n",
      "2017-11-06T06:32:52.287373: step 2225, loss 0.240254, acc 0.90625\n",
      "2017-11-06T06:32:56.257193: step 2226, loss 0.0527066, acc 0.96875\n",
      "2017-11-06T06:33:00.248028: step 2227, loss 0.131757, acc 0.90625\n",
      "2017-11-06T06:33:04.705195: step 2228, loss 0.595531, acc 0.8125\n",
      "2017-11-06T06:33:08.796102: step 2229, loss 0.278939, acc 0.875\n",
      "2017-11-06T06:33:12.740905: step 2230, loss 0.17099, acc 0.90625\n",
      "2017-11-06T06:33:16.848824: step 2231, loss 0.226155, acc 0.875\n",
      "2017-11-06T06:33:19.352604: step 2232, loss 0.221094, acc 0.85\n",
      "2017-11-06T06:33:23.374461: step 2233, loss 0.176299, acc 0.90625\n",
      "2017-11-06T06:33:27.337277: step 2234, loss 0.198586, acc 0.96875\n",
      "2017-11-06T06:33:31.388155: step 2235, loss 0.0329116, acc 1\n",
      "2017-11-06T06:33:35.386999: step 2236, loss 0.0655354, acc 1\n",
      "2017-11-06T06:33:39.744265: step 2237, loss 0.0881612, acc 0.9375\n",
      "2017-11-06T06:33:43.842178: step 2238, loss 0.109277, acc 0.9375\n",
      "2017-11-06T06:33:47.963105: step 2239, loss 0.17527, acc 0.90625\n",
      "2017-11-06T06:33:52.024991: step 2240, loss 0.236639, acc 0.90625\n",
      "2017-11-06T06:33:56.089879: step 2241, loss 0.0510863, acc 0.96875\n",
      "2017-11-06T06:34:00.231823: step 2242, loss 0.29833, acc 0.84375\n",
      "2017-11-06T06:34:04.394781: step 2243, loss 0.440786, acc 0.84375\n",
      "2017-11-06T06:34:08.857952: step 2244, loss 0.499474, acc 0.8125\n",
      "2017-11-06T06:34:13.259079: step 2245, loss 0.258303, acc 0.875\n",
      "2017-11-06T06:34:17.338977: step 2246, loss 0.232072, acc 0.875\n",
      "2017-11-06T06:34:21.441894: step 2247, loss 0.244509, acc 0.875\n",
      "2017-11-06T06:34:25.916073: step 2248, loss 0.165046, acc 0.875\n",
      "2017-11-06T06:34:29.956944: step 2249, loss 0.177026, acc 0.90625\n",
      "2017-11-06T06:34:34.259000: step 2250, loss 0.163493, acc 0.90625\n",
      "2017-11-06T06:34:38.528034: step 2251, loss 0.29496, acc 0.84375\n",
      "2017-11-06T06:34:42.960183: step 2252, loss 0.263808, acc 0.875\n",
      "2017-11-06T06:34:47.149159: step 2253, loss 0.14524, acc 0.90625\n",
      "2017-11-06T06:34:51.343139: step 2254, loss 0.265466, acc 0.875\n",
      "2017-11-06T06:34:55.455061: step 2255, loss 0.0881992, acc 0.96875\n",
      "2017-11-06T06:34:59.578991: step 2256, loss 0.187163, acc 0.90625\n",
      "2017-11-06T06:35:03.651885: step 2257, loss 0.160236, acc 0.90625\n",
      "2017-11-06T06:35:07.587682: step 2258, loss 0.134581, acc 0.9375\n",
      "2017-11-06T06:35:11.620547: step 2259, loss 0.350998, acc 0.875\n",
      "2017-11-06T06:35:16.017673: step 2260, loss 0.370736, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T06:35:19.945462: step 2261, loss 0.194286, acc 0.90625\n",
      "2017-11-06T06:35:23.942302: step 2262, loss 0.151349, acc 0.90625\n",
      "2017-11-06T06:35:27.944148: step 2263, loss 0.363144, acc 0.8125\n",
      "2017-11-06T06:35:31.905963: step 2264, loss 0.196628, acc 0.9375\n",
      "2017-11-06T06:35:35.877784: step 2265, loss 0.0882263, acc 0.96875\n",
      "2017-11-06T06:35:40.172439: step 2266, loss 0.116398, acc 0.9375\n",
      "2017-11-06T06:35:44.130251: step 2267, loss 0.346348, acc 0.78125\n",
      "2017-11-06T06:35:46.615016: step 2268, loss 0.125816, acc 0.9\n",
      "2017-11-06T06:35:50.605852: step 2269, loss 0.167992, acc 0.90625\n",
      "2017-11-06T06:35:54.586681: step 2270, loss 0.191974, acc 0.90625\n",
      "2017-11-06T06:35:58.612542: step 2271, loss 0.143873, acc 0.9375\n",
      "2017-11-06T06:36:02.581361: step 2272, loss 0.386221, acc 0.84375\n",
      "2017-11-06T06:36:06.480131: step 2273, loss 0.235799, acc 0.90625\n",
      "2017-11-06T06:36:10.429938: step 2274, loss 0.117565, acc 0.90625\n",
      "2017-11-06T06:36:14.365734: step 2275, loss 0.337745, acc 0.875\n",
      "2017-11-06T06:36:18.482659: step 2276, loss 0.245326, acc 0.875\n",
      "2017-11-06T06:36:22.787719: step 2277, loss 0.194058, acc 0.90625\n",
      "2017-11-06T06:36:26.752535: step 2278, loss 0.285374, acc 0.84375\n",
      "2017-11-06T06:36:30.666317: step 2279, loss 0.154766, acc 0.875\n",
      "2017-11-06T06:36:34.865300: step 2280, loss 0.184929, acc 0.875\n",
      "2017-11-06T06:36:38.842905: step 2281, loss 0.0918016, acc 0.96875\n",
      "2017-11-06T06:36:42.782705: step 2282, loss 0.259138, acc 0.90625\n",
      "2017-11-06T06:36:46.683476: step 2283, loss 0.171812, acc 0.90625\n",
      "2017-11-06T06:36:50.614269: step 2284, loss 0.277863, acc 0.90625\n",
      "2017-11-06T06:36:54.548066: step 2285, loss 0.0944443, acc 0.9375\n",
      "2017-11-06T06:36:58.469851: step 2286, loss 0.177555, acc 0.90625\n",
      "2017-11-06T06:37:02.421660: step 2287, loss 0.236365, acc 0.84375\n",
      "2017-11-06T06:37:06.351451: step 2288, loss 0.242739, acc 0.875\n",
      "2017-11-06T06:37:10.297255: step 2289, loss 0.36149, acc 0.875\n",
      "2017-11-06T06:37:14.300100: step 2290, loss 0.0637644, acc 0.96875\n",
      "2017-11-06T06:37:18.312950: step 2291, loss 0.381461, acc 0.875\n",
      "2017-11-06T06:37:22.226731: step 2292, loss 0.311868, acc 0.875\n",
      "2017-11-06T06:37:26.641869: step 2293, loss 0.177554, acc 0.875\n",
      "2017-11-06T06:37:30.615692: step 2294, loss 0.39282, acc 0.84375\n",
      "2017-11-06T06:37:34.603525: step 2295, loss 0.471213, acc 0.78125\n",
      "2017-11-06T06:37:38.619379: step 2296, loss 0.320469, acc 0.875\n",
      "2017-11-06T06:37:42.904290: step 2297, loss 0.249882, acc 0.84375\n",
      "2017-11-06T06:37:46.794054: step 2298, loss 0.0840626, acc 0.9375\n",
      "2017-11-06T06:37:50.790895: step 2299, loss 0.145508, acc 0.96875\n",
      "2017-11-06T06:37:54.730695: step 2300, loss 0.240039, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:37:57.328540: step 2300, loss 0.841664, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-06T06:38:03.307739: step 2301, loss 0.0624598, acc 0.96875\n",
      "2017-11-06T06:38:07.239534: step 2302, loss 0.187342, acc 0.90625\n",
      "2017-11-06T06:38:11.212356: step 2303, loss 0.288586, acc 0.90625\n",
      "2017-11-06T06:38:13.743154: step 2304, loss 0.43515, acc 0.8\n",
      "2017-11-06T06:38:17.727987: step 2305, loss 0.098042, acc 0.9375\n",
      "2017-11-06T06:38:21.740838: step 2306, loss 0.29906, acc 0.90625\n",
      "2017-11-06T06:38:25.800722: step 2307, loss 0.102826, acc 0.96875\n",
      "2017-11-06T06:38:29.876618: step 2308, loss 0.190108, acc 0.90625\n",
      "2017-11-06T06:38:34.349796: step 2309, loss 0.313261, acc 0.84375\n",
      "2017-11-06T06:38:38.380660: step 2310, loss 0.18182, acc 0.90625\n",
      "2017-11-06T06:38:42.361489: step 2311, loss 0.228619, acc 0.875\n",
      "2017-11-06T06:38:46.329309: step 2312, loss 0.190036, acc 0.84375\n",
      "2017-11-06T06:38:50.298128: step 2313, loss 0.143797, acc 0.96875\n",
      "2017-11-06T06:38:54.271952: step 2314, loss 0.0987505, acc 0.9375\n",
      "2017-11-06T06:38:58.213753: step 2315, loss 0.108295, acc 0.9375\n",
      "2017-11-06T06:39:02.165561: step 2316, loss 0.0998882, acc 0.96875\n",
      "2017-11-06T06:39:06.147391: step 2317, loss 0.134485, acc 0.90625\n",
      "2017-11-06T06:39:10.118214: step 2318, loss 0.136467, acc 0.9375\n",
      "2017-11-06T06:39:14.007976: step 2319, loss 0.18276, acc 0.9375\n",
      "2017-11-06T06:39:17.960783: step 2320, loss 0.131108, acc 0.9375\n",
      "2017-11-06T06:39:21.961627: step 2321, loss 0.231997, acc 0.84375\n",
      "2017-11-06T06:39:25.912434: step 2322, loss 0.33784, acc 0.8125\n",
      "2017-11-06T06:39:29.845229: step 2323, loss 0.156033, acc 0.9375\n",
      "2017-11-06T06:39:33.867086: step 2324, loss 0.326004, acc 0.8125\n",
      "2017-11-06T06:39:38.391071: step 2325, loss 0.222767, acc 0.90625\n",
      "2017-11-06T06:39:42.410927: step 2326, loss 0.119753, acc 0.9375\n",
      "2017-11-06T06:39:46.412771: step 2327, loss 0.331883, acc 0.875\n",
      "2017-11-06T06:39:50.351571: step 2328, loss 0.102319, acc 0.96875\n",
      "2017-11-06T06:39:54.328395: step 2329, loss 0.29409, acc 0.875\n",
      "2017-11-06T06:39:58.311225: step 2330, loss 0.243407, acc 0.84375\n",
      "2017-11-06T06:40:02.535226: step 2331, loss 0.195542, acc 0.90625\n",
      "2017-11-06T06:40:06.529064: step 2332, loss 0.115615, acc 0.96875\n",
      "2017-11-06T06:40:10.510893: step 2333, loss 0.20101, acc 0.9375\n",
      "2017-11-06T06:40:14.484719: step 2334, loss 0.173246, acc 0.90625\n",
      "2017-11-06T06:40:18.453537: step 2335, loss 0.251892, acc 0.875\n",
      "2017-11-06T06:40:22.429362: step 2336, loss 0.294528, acc 0.84375\n",
      "2017-11-06T06:40:26.491248: step 2337, loss 0.377825, acc 0.78125\n",
      "2017-11-06T06:40:30.483084: step 2338, loss 0.0660109, acc 0.96875\n",
      "2017-11-06T06:40:34.606015: step 2339, loss 0.139629, acc 0.90625\n",
      "2017-11-06T06:40:37.229878: step 2340, loss 0.192645, acc 0.9\n",
      "2017-11-06T06:40:41.456881: step 2341, loss 0.246575, acc 0.84375\n",
      "2017-11-06T06:40:45.720913: step 2342, loss 0.144019, acc 0.90625\n",
      "2017-11-06T06:40:49.680725: step 2343, loss 0.238966, acc 0.84375\n",
      "2017-11-06T06:40:53.628530: step 2344, loss 0.288307, acc 0.8125\n",
      "2017-11-06T06:40:57.621367: step 2345, loss 0.318387, acc 0.84375\n",
      "2017-11-06T06:41:01.525143: step 2346, loss 0.172793, acc 0.90625\n",
      "2017-11-06T06:41:05.600037: step 2347, loss 0.235631, acc 0.875\n",
      "2017-11-06T06:41:09.773004: step 2348, loss 0.228883, acc 0.84375\n",
      "2017-11-06T06:41:15.039744: step 2349, loss 0.0840806, acc 0.96875\n",
      "2017-11-06T06:41:20.097337: step 2350, loss 0.238529, acc 0.84375\n",
      "2017-11-06T06:41:24.225270: step 2351, loss 0.193298, acc 0.90625\n",
      "2017-11-06T06:41:28.215105: step 2352, loss 0.0811488, acc 0.96875\n",
      "2017-11-06T06:41:32.290000: step 2353, loss 0.125537, acc 0.9375\n",
      "2017-11-06T06:41:36.278837: step 2354, loss 0.119088, acc 0.9375\n",
      "2017-11-06T06:41:40.315704: step 2355, loss 0.204648, acc 0.90625\n",
      "2017-11-06T06:41:44.278519: step 2356, loss 0.2065, acc 0.90625\n",
      "2017-11-06T06:41:49.054488: step 2357, loss 0.214757, acc 0.90625\n",
      "2017-11-06T06:41:53.129384: step 2358, loss 0.236846, acc 0.84375\n",
      "2017-11-06T06:41:57.097202: step 2359, loss 0.199744, acc 0.90625\n",
      "2017-11-06T06:42:01.074028: step 2360, loss 0.26216, acc 0.90625\n",
      "2017-11-06T06:42:05.050854: step 2361, loss 0.451092, acc 0.6875\n",
      "2017-11-06T06:42:09.025678: step 2362, loss 0.255362, acc 0.90625\n",
      "2017-11-06T06:42:13.017515: step 2363, loss 0.202707, acc 0.90625\n",
      "2017-11-06T06:42:16.990338: step 2364, loss 0.300694, acc 0.875\n",
      "2017-11-06T06:42:20.996186: step 2365, loss 0.407832, acc 0.8125\n",
      "2017-11-06T06:42:24.972008: step 2366, loss 0.204239, acc 0.9375\n",
      "2017-11-06T06:42:28.999873: step 2367, loss 0.142987, acc 0.9375\n",
      "2017-11-06T06:42:33.139813: step 2368, loss 0.0736926, acc 0.96875\n",
      "2017-11-06T06:42:37.191692: step 2369, loss 0.175828, acc 0.84375\n",
      "2017-11-06T06:42:41.163314: step 2370, loss 0.246926, acc 0.90625\n",
      "2017-11-06T06:42:45.181170: step 2371, loss 0.0388584, acc 1\n",
      "2017-11-06T06:42:49.216035: step 2372, loss 0.0633017, acc 1\n",
      "2017-11-06T06:42:53.432032: step 2373, loss 0.132067, acc 0.9375\n",
      "2017-11-06T06:42:57.634017: step 2374, loss 0.223546, acc 0.90625\n",
      "2017-11-06T06:43:01.711914: step 2375, loss 0.204115, acc 0.90625\n",
      "2017-11-06T06:43:04.285744: step 2376, loss 0.41814, acc 0.8\n",
      "2017-11-06T06:43:08.261569: step 2377, loss 0.197552, acc 0.875\n",
      "2017-11-06T06:43:12.241398: step 2378, loss 0.193268, acc 0.84375\n",
      "2017-11-06T06:43:16.201212: step 2379, loss 0.231053, acc 0.875\n",
      "2017-11-06T06:43:20.178036: step 2380, loss 0.272678, acc 0.90625\n",
      "2017-11-06T06:43:24.347999: step 2381, loss 0.243297, acc 0.84375\n",
      "2017-11-06T06:43:28.554988: step 2382, loss 0.175216, acc 0.9375\n",
      "2017-11-06T06:43:32.464766: step 2383, loss 0.125641, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T06:43:36.412571: step 2384, loss 0.254792, acc 0.84375\n",
      "2017-11-06T06:43:40.397402: step 2385, loss 0.423322, acc 0.78125\n",
      "2017-11-06T06:43:44.427266: step 2386, loss 0.147899, acc 0.90625\n",
      "2017-11-06T06:43:48.611118: step 2387, loss 0.128833, acc 0.9375\n",
      "2017-11-06T06:43:52.608958: step 2388, loss 0.15241, acc 0.96875\n",
      "2017-11-06T06:43:56.638821: step 2389, loss 0.200821, acc 0.90625\n",
      "2017-11-06T06:44:01.066967: step 2390, loss 0.226179, acc 0.90625\n",
      "2017-11-06T06:44:05.022778: step 2391, loss 0.128257, acc 0.9375\n",
      "2017-11-06T06:44:09.048638: step 2392, loss 0.198546, acc 0.90625\n",
      "2017-11-06T06:44:13.011455: step 2393, loss 0.219909, acc 0.9375\n",
      "2017-11-06T06:44:17.019302: step 2394, loss 0.0593539, acc 0.96875\n",
      "2017-11-06T06:44:21.003133: step 2395, loss 0.181629, acc 0.84375\n",
      "2017-11-06T06:44:25.016985: step 2396, loss 0.137019, acc 0.96875\n",
      "2017-11-06T06:44:29.002818: step 2397, loss 0.168854, acc 0.9375\n",
      "2017-11-06T06:44:33.104733: step 2398, loss 0.509403, acc 0.8125\n",
      "2017-11-06T06:44:37.158612: step 2399, loss 0.20154, acc 0.875\n",
      "2017-11-06T06:44:41.151449: step 2400, loss 0.242989, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:44:43.830355: step 2400, loss 0.819731, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-06T06:44:49.256512: step 2401, loss 0.20603, acc 0.90625\n",
      "2017-11-06T06:44:53.312393: step 2402, loss 0.205001, acc 0.90625\n",
      "2017-11-06T06:44:57.391292: step 2403, loss 0.414914, acc 0.8125\n",
      "2017-11-06T06:45:01.477195: step 2404, loss 0.187753, acc 0.90625\n",
      "2017-11-06T06:45:06.044440: step 2405, loss 0.556709, acc 0.75\n",
      "2017-11-06T06:45:10.205398: step 2406, loss 0.0661283, acc 0.96875\n",
      "2017-11-06T06:45:14.318319: step 2407, loss 0.0917011, acc 0.96875\n",
      "2017-11-06T06:45:18.504293: step 2408, loss 0.268658, acc 0.84375\n",
      "2017-11-06T06:45:22.556172: step 2409, loss 0.346873, acc 0.875\n",
      "2017-11-06T06:45:26.581032: step 2410, loss 0.183148, acc 0.90625\n",
      "2017-11-06T06:45:30.688951: step 2411, loss 0.137815, acc 0.90625\n",
      "2017-11-06T06:45:33.259778: step 2412, loss 0.404303, acc 0.85\n",
      "2017-11-06T06:45:37.301650: step 2413, loss 0.209375, acc 0.90625\n",
      "2017-11-06T06:45:41.363319: step 2414, loss 0.0699119, acc 1\n",
      "2017-11-06T06:45:45.395184: step 2415, loss 0.149218, acc 0.96875\n",
      "2017-11-06T06:45:49.468078: step 2416, loss 0.0620893, acc 0.96875\n",
      "2017-11-06T06:45:53.444904: step 2417, loss 0.159046, acc 0.875\n",
      "2017-11-06T06:45:57.411722: step 2418, loss 0.324744, acc 0.90625\n",
      "2017-11-06T06:46:01.455595: step 2419, loss 0.378419, acc 0.8125\n",
      "2017-11-06T06:46:05.526488: step 2420, loss 0.0788918, acc 0.96875\n",
      "2017-11-06T06:46:09.829545: step 2421, loss 0.180836, acc 0.9375\n",
      "2017-11-06T06:46:14.149615: step 2422, loss 0.219556, acc 0.84375\n",
      "2017-11-06T06:46:18.110430: step 2423, loss 0.26276, acc 0.875\n",
      "2017-11-06T06:46:22.227355: step 2424, loss 0.0982306, acc 0.96875\n",
      "2017-11-06T06:46:26.306253: step 2425, loss 0.458771, acc 0.75\n",
      "2017-11-06T06:46:30.327110: step 2426, loss 0.229014, acc 0.90625\n",
      "2017-11-06T06:46:34.532098: step 2427, loss 0.291974, acc 0.8125\n",
      "2017-11-06T06:46:38.601990: step 2428, loss 0.259376, acc 0.90625\n",
      "2017-11-06T06:46:42.671883: step 2429, loss 0.265339, acc 0.90625\n",
      "2017-11-06T06:46:46.747777: step 2430, loss 0.104604, acc 0.96875\n",
      "2017-11-06T06:46:50.783645: step 2431, loss 0.269959, acc 0.90625\n",
      "2017-11-06T06:46:54.835524: step 2432, loss 0.141547, acc 0.90625\n",
      "2017-11-06T06:46:58.977467: step 2433, loss 0.19887, acc 0.90625\n",
      "2017-11-06T06:47:03.062370: step 2434, loss 0.0718079, acc 0.96875\n",
      "2017-11-06T06:47:07.085228: step 2435, loss 0.264024, acc 0.875\n",
      "2017-11-06T06:47:11.067059: step 2436, loss 0.314259, acc 0.90625\n",
      "2017-11-06T06:47:15.281051: step 2437, loss 0.4695, acc 0.78125\n",
      "2017-11-06T06:47:19.625138: step 2438, loss 0.0671814, acc 0.9375\n",
      "2017-11-06T06:47:23.783094: step 2439, loss 0.128661, acc 0.96875\n",
      "2017-11-06T06:47:27.789939: step 2440, loss 0.201382, acc 0.875\n",
      "2017-11-06T06:47:31.800789: step 2441, loss 0.180394, acc 0.875\n",
      "2017-11-06T06:47:35.837658: step 2442, loss 0.279451, acc 0.90625\n",
      "2017-11-06T06:47:39.805477: step 2443, loss 0.213619, acc 0.90625\n",
      "2017-11-06T06:47:43.783303: step 2444, loss 0.32357, acc 0.8125\n",
      "2017-11-06T06:47:47.800158: step 2445, loss 0.442897, acc 0.8125\n",
      "2017-11-06T06:47:51.832077: step 2446, loss 0.17051, acc 0.875\n",
      "2017-11-06T06:47:55.730828: step 2447, loss 0.212099, acc 0.875\n",
      "2017-11-06T06:47:58.248618: step 2448, loss 0.135162, acc 0.9\n",
      "2017-11-06T06:48:02.227445: step 2449, loss 0.122809, acc 0.9375\n",
      "2017-11-06T06:48:06.110203: step 2450, loss 0.0218782, acc 1\n",
      "2017-11-06T06:48:10.051003: step 2451, loss 0.145878, acc 0.9375\n",
      "2017-11-06T06:48:13.983798: step 2452, loss 0.11118, acc 0.9375\n",
      "2017-11-06T06:48:17.910588: step 2453, loss 0.155806, acc 0.90625\n",
      "2017-11-06T06:48:22.225654: step 2454, loss 0.189174, acc 0.9375\n",
      "2017-11-06T06:48:26.495688: step 2455, loss 0.170262, acc 0.90625\n",
      "2017-11-06T06:48:30.678660: step 2456, loss 0.104089, acc 0.9375\n",
      "2017-11-06T06:48:34.812598: step 2457, loss 0.0572659, acc 0.96875\n",
      "2017-11-06T06:48:38.742389: step 2458, loss 0.37866, acc 0.8125\n",
      "2017-11-06T06:48:42.719079: step 2459, loss 0.356533, acc 0.78125\n",
      "2017-11-06T06:48:46.680895: step 2460, loss 0.0720831, acc 0.9375\n",
      "2017-11-06T06:48:50.656720: step 2461, loss 0.145114, acc 0.9375\n",
      "2017-11-06T06:48:54.669571: step 2462, loss 0.300785, acc 0.84375\n",
      "2017-11-06T06:48:58.611372: step 2463, loss 0.106658, acc 0.9375\n",
      "2017-11-06T06:49:02.568183: step 2464, loss 0.346437, acc 0.84375\n",
      "2017-11-06T06:49:06.477961: step 2465, loss 0.292563, acc 0.90625\n",
      "2017-11-06T06:49:10.429769: step 2466, loss 0.161575, acc 0.9375\n",
      "2017-11-06T06:49:14.412599: step 2467, loss 0.144808, acc 0.9375\n",
      "2017-11-06T06:49:18.330384: step 2468, loss 0.489727, acc 0.8125\n",
      "2017-11-06T06:49:22.316215: step 2469, loss 0.235123, acc 0.90625\n",
      "2017-11-06T06:49:26.446150: step 2470, loss 0.229835, acc 0.875\n",
      "2017-11-06T06:49:30.672154: step 2471, loss 0.514763, acc 0.8125\n",
      "2017-11-06T06:49:34.628964: step 2472, loss 0.163673, acc 0.9375\n",
      "2017-11-06T06:49:38.614796: step 2473, loss 0.0982661, acc 0.96875\n",
      "2017-11-06T06:49:42.605632: step 2474, loss 0.210647, acc 0.84375\n",
      "2017-11-06T06:49:46.535424: step 2475, loss 0.094691, acc 0.9375\n",
      "2017-11-06T06:49:50.528261: step 2476, loss 0.121868, acc 0.9375\n",
      "2017-11-06T06:49:54.591327: step 2477, loss 0.012993, acc 1\n",
      "2017-11-06T06:49:58.540129: step 2478, loss 0.332582, acc 0.875\n",
      "2017-11-06T06:50:02.796154: step 2479, loss 0.249935, acc 0.875\n",
      "2017-11-06T06:50:06.773979: step 2480, loss 0.13876, acc 0.9375\n",
      "2017-11-06T06:50:10.804843: step 2481, loss 0.291456, acc 0.90625\n",
      "2017-11-06T06:50:14.785671: step 2482, loss 0.377487, acc 0.8125\n",
      "2017-11-06T06:50:18.771504: step 2483, loss 0.287859, acc 0.84375\n",
      "2017-11-06T06:50:21.339328: step 2484, loss 0.169319, acc 0.9\n",
      "2017-11-06T06:50:25.743320: step 2485, loss 0.146891, acc 0.90625\n",
      "2017-11-06T06:50:29.683119: step 2486, loss 0.21416, acc 0.875\n",
      "2017-11-06T06:50:34.194325: step 2487, loss 0.0772962, acc 0.96875\n",
      "2017-11-06T06:50:38.293237: step 2488, loss 0.205868, acc 0.90625\n",
      "2017-11-06T06:50:42.234037: step 2489, loss 0.249263, acc 0.90625\n",
      "2017-11-06T06:50:46.187847: step 2490, loss 0.261448, acc 0.84375\n",
      "2017-11-06T06:50:50.152664: step 2491, loss 0.304614, acc 0.84375\n",
      "2017-11-06T06:50:54.139497: step 2492, loss 0.151337, acc 0.9375\n",
      "2017-11-06T06:50:58.132335: step 2493, loss 0.174023, acc 0.90625\n",
      "2017-11-06T06:51:02.098151: step 2494, loss 0.193942, acc 0.90625\n",
      "2017-11-06T06:51:06.047959: step 2495, loss 0.291725, acc 0.78125\n",
      "2017-11-06T06:51:10.011776: step 2496, loss 0.0830889, acc 0.96875\n",
      "2017-11-06T06:51:13.939569: step 2497, loss 0.185279, acc 0.90625\n",
      "2017-11-06T06:51:17.941410: step 2498, loss 0.0990148, acc 0.96875\n",
      "2017-11-06T06:51:21.858192: step 2499, loss 0.251679, acc 0.8125\n",
      "2017-11-06T06:51:25.781980: step 2500, loss 0.197829, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:51:28.327789: step 2500, loss 0.802966, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-06T06:51:33.875299: step 2501, loss 0.0939539, acc 0.96875\n",
      "2017-11-06T06:51:38.051266: step 2502, loss 0.196442, acc 0.90625\n",
      "2017-11-06T06:51:42.348133: step 2503, loss 0.183138, acc 0.90625\n",
      "2017-11-06T06:51:46.323958: step 2504, loss 0.0919973, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T06:51:50.348817: step 2505, loss 0.124262, acc 0.90625\n",
      "2017-11-06T06:51:54.390689: step 2506, loss 0.34218, acc 0.875\n",
      "2017-11-06T06:51:58.383526: step 2507, loss 0.117141, acc 0.90625\n",
      "2017-11-06T06:52:02.404383: step 2508, loss 0.26508, acc 0.875\n",
      "2017-11-06T06:52:06.358195: step 2509, loss 0.334343, acc 0.84375\n",
      "2017-11-06T06:52:10.352031: step 2510, loss 0.0812826, acc 0.9375\n",
      "2017-11-06T06:52:14.343868: step 2511, loss 0.158072, acc 0.96875\n",
      "2017-11-06T06:52:18.340707: step 2512, loss 0.297305, acc 0.84375\n",
      "2017-11-06T06:52:22.357561: step 2513, loss 0.124534, acc 0.9375\n",
      "2017-11-06T06:52:26.309368: step 2514, loss 0.432971, acc 0.8125\n",
      "2017-11-06T06:52:30.311212: step 2515, loss 0.186885, acc 0.90625\n",
      "2017-11-06T06:52:34.544220: step 2516, loss 0.202487, acc 0.90625\n",
      "2017-11-06T06:52:38.541060: step 2517, loss 0.246081, acc 0.875\n",
      "2017-11-06T06:52:42.687005: step 2518, loss 0.068798, acc 0.9375\n",
      "2017-11-06T06:52:47.038097: step 2519, loss 0.154684, acc 0.90625\n",
      "2017-11-06T06:52:49.555888: step 2520, loss 0.239856, acc 0.85\n",
      "2017-11-06T06:52:53.534714: step 2521, loss 0.0898105, acc 0.9375\n",
      "2017-11-06T06:52:57.537558: step 2522, loss 0.273649, acc 0.875\n",
      "2017-11-06T06:53:01.566422: step 2523, loss 0.117782, acc 0.9375\n",
      "2017-11-06T06:53:05.582273: step 2524, loss 0.211089, acc 0.90625\n",
      "2017-11-06T06:53:09.591123: step 2525, loss 0.239152, acc 0.875\n",
      "2017-11-06T06:53:13.543933: step 2526, loss 0.114322, acc 0.96875\n",
      "2017-11-06T06:53:17.527762: step 2527, loss 0.174558, acc 0.9375\n",
      "2017-11-06T06:53:21.538613: step 2528, loss 0.171342, acc 0.90625\n",
      "2017-11-06T06:53:25.670548: step 2529, loss 0.117547, acc 0.90625\n",
      "2017-11-06T06:53:29.701412: step 2530, loss 0.282094, acc 0.90625\n",
      "2017-11-06T06:53:33.706258: step 2531, loss 0.246488, acc 0.84375\n",
      "2017-11-06T06:53:37.757135: step 2532, loss 0.113336, acc 0.96875\n",
      "2017-11-06T06:53:41.709946: step 2533, loss 0.0681014, acc 0.9375\n",
      "2017-11-06T06:53:45.764825: step 2534, loss 0.139086, acc 0.875\n",
      "2017-11-06T06:53:50.069884: step 2535, loss 0.242703, acc 0.90625\n",
      "2017-11-06T06:53:54.247853: step 2536, loss 0.26649, acc 0.84375\n",
      "2017-11-06T06:53:58.278716: step 2537, loss 0.294213, acc 0.8125\n",
      "2017-11-06T06:54:02.440773: step 2538, loss 0.200563, acc 0.9375\n",
      "2017-11-06T06:54:06.426604: step 2539, loss 0.214553, acc 0.84375\n",
      "2017-11-06T06:54:10.547532: step 2540, loss 0.170497, acc 0.9375\n",
      "2017-11-06T06:54:14.510349: step 2541, loss 0.306439, acc 0.8125\n",
      "2017-11-06T06:54:18.578238: step 2542, loss 0.23153, acc 0.875\n",
      "2017-11-06T06:54:22.787229: step 2543, loss 0.180058, acc 0.90625\n",
      "2017-11-06T06:54:26.946184: step 2544, loss 0.0455441, acc 1\n",
      "2017-11-06T06:54:30.911001: step 2545, loss 0.279062, acc 0.875\n",
      "2017-11-06T06:54:35.135003: step 2546, loss 0.101854, acc 0.9375\n",
      "2017-11-06T06:54:39.141850: step 2547, loss 0.212425, acc 0.90625\n",
      "2017-11-06T06:54:43.160465: step 2548, loss 0.197587, acc 0.90625\n",
      "2017-11-06T06:54:47.158308: step 2549, loss 0.176384, acc 0.90625\n",
      "2017-11-06T06:54:51.174160: step 2550, loss 0.102081, acc 0.9375\n",
      "2017-11-06T06:54:55.401162: step 2551, loss 0.181613, acc 0.84375\n",
      "2017-11-06T06:54:59.693212: step 2552, loss 0.18472, acc 0.9375\n",
      "2017-11-06T06:55:03.790123: step 2553, loss 0.119503, acc 0.9375\n",
      "2017-11-06T06:55:07.969092: step 2554, loss 0.21758, acc 0.90625\n",
      "2017-11-06T06:55:11.976940: step 2555, loss 0.344627, acc 0.8125\n",
      "2017-11-06T06:55:14.581791: step 2556, loss 0.399986, acc 0.8\n",
      "2017-11-06T06:55:18.736743: step 2557, loss 0.283083, acc 0.8125\n",
      "2017-11-06T06:55:22.790624: step 2558, loss 0.122927, acc 0.9375\n",
      "2017-11-06T06:55:26.891537: step 2559, loss 0.121972, acc 0.96875\n",
      "2017-11-06T06:55:30.909392: step 2560, loss 0.173952, acc 0.84375\n",
      "2017-11-06T06:55:34.932252: step 2561, loss 0.153544, acc 0.9375\n",
      "2017-11-06T06:55:38.948105: step 2562, loss 0.066879, acc 0.96875\n",
      "2017-11-06T06:55:42.890906: step 2563, loss 0.277157, acc 0.78125\n",
      "2017-11-06T06:55:46.913764: step 2564, loss 0.261334, acc 0.875\n",
      "2017-11-06T06:55:50.932620: step 2565, loss 0.326562, acc 0.8125\n",
      "2017-11-06T06:55:54.832390: step 2566, loss 0.420561, acc 0.78125\n",
      "2017-11-06T06:55:58.981339: step 2567, loss 0.136598, acc 0.9375\n",
      "2017-11-06T06:56:03.524566: step 2568, loss 0.457351, acc 0.78125\n",
      "2017-11-06T06:56:07.498390: step 2569, loss 0.303352, acc 0.875\n",
      "2017-11-06T06:56:11.478218: step 2570, loss 0.241111, acc 0.875\n",
      "2017-11-06T06:56:15.508082: step 2571, loss 0.175971, acc 0.875\n",
      "2017-11-06T06:56:19.451884: step 2572, loss 0.300103, acc 0.84375\n",
      "2017-11-06T06:56:23.483749: step 2573, loss 0.298637, acc 0.8125\n",
      "2017-11-06T06:56:27.523619: step 2574, loss 0.168743, acc 0.875\n",
      "2017-11-06T06:56:31.500445: step 2575, loss 0.180849, acc 0.9375\n",
      "2017-11-06T06:56:35.708434: step 2576, loss 0.341453, acc 0.8125\n",
      "2017-11-06T06:56:39.697268: step 2577, loss 0.186253, acc 0.9375\n",
      "2017-11-06T06:56:43.758154: step 2578, loss 0.112228, acc 0.96875\n",
      "2017-11-06T06:56:47.729976: step 2579, loss 0.180439, acc 0.9375\n",
      "2017-11-06T06:56:51.751834: step 2580, loss 0.232608, acc 0.84375\n",
      "2017-11-06T06:56:55.750676: step 2581, loss 0.0577555, acc 1\n",
      "2017-11-06T06:56:59.780539: step 2582, loss 0.0915998, acc 0.9375\n",
      "2017-11-06T06:57:03.741353: step 2583, loss 0.110404, acc 0.96875\n",
      "2017-11-06T06:57:08.229544: step 2584, loss 0.356375, acc 0.90625\n",
      "2017-11-06T06:57:12.328455: step 2585, loss 0.17459, acc 0.9375\n",
      "2017-11-06T06:57:16.301278: step 2586, loss 0.505454, acc 0.875\n",
      "2017-11-06T06:57:20.266095: step 2587, loss 0.303479, acc 0.90625\n",
      "2017-11-06T06:57:24.277947: step 2588, loss 0.193609, acc 0.9375\n",
      "2017-11-06T06:57:28.227752: step 2589, loss 0.199919, acc 0.96875\n",
      "2017-11-06T06:57:32.211583: step 2590, loss 0.0881965, acc 0.96875\n",
      "2017-11-06T06:57:36.178404: step 2591, loss 0.0216836, acc 1\n",
      "2017-11-06T06:57:38.754233: step 2592, loss 0.396211, acc 0.75\n",
      "2017-11-06T06:57:42.736831: step 2593, loss 0.159345, acc 0.9375\n",
      "2017-11-06T06:57:46.688639: step 2594, loss 0.152865, acc 0.875\n",
      "2017-11-06T06:57:50.725509: step 2595, loss 0.1984, acc 0.9375\n",
      "2017-11-06T06:57:54.711339: step 2596, loss 0.230524, acc 0.875\n",
      "2017-11-06T06:57:58.655142: step 2597, loss 0.196874, acc 0.875\n",
      "2017-11-06T06:58:02.674999: step 2598, loss 0.35343, acc 0.875\n",
      "2017-11-06T06:58:06.754897: step 2599, loss 0.0629637, acc 0.96875\n",
      "2017-11-06T06:58:10.885833: step 2600, loss 0.223266, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T06:58:13.805908: step 2600, loss 0.908145, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-06T06:58:19.286040: step 2601, loss 0.427265, acc 0.8125\n",
      "2017-11-06T06:58:23.492028: step 2602, loss 0.215912, acc 0.875\n",
      "2017-11-06T06:58:27.595946: step 2603, loss 0.113694, acc 0.9375\n",
      "2017-11-06T06:58:31.640818: step 2604, loss 0.150475, acc 0.9375\n",
      "2017-11-06T06:58:35.906850: step 2605, loss 0.135242, acc 0.9375\n",
      "2017-11-06T06:58:39.841645: step 2606, loss 0.118356, acc 0.9375\n",
      "2017-11-06T06:58:43.794454: step 2607, loss 0.0739531, acc 1\n",
      "2017-11-06T06:58:47.843331: step 2608, loss 0.162654, acc 0.90625\n",
      "2017-11-06T06:58:51.876196: step 2609, loss 0.103363, acc 0.96875\n",
      "2017-11-06T06:58:55.897054: step 2610, loss 0.23626, acc 0.90625\n",
      "2017-11-06T06:58:59.882886: step 2611, loss 0.136903, acc 0.90625\n",
      "2017-11-06T06:59:03.939768: step 2612, loss 0.122323, acc 0.9375\n",
      "2017-11-06T06:59:07.940611: step 2613, loss 0.144688, acc 0.90625\n",
      "2017-11-06T06:59:12.021511: step 2614, loss 0.234139, acc 0.90625\n",
      "2017-11-06T06:59:16.030359: step 2615, loss 0.131063, acc 0.90625\n",
      "2017-11-06T06:59:20.422480: step 2616, loss 0.270794, acc 0.90625\n",
      "2017-11-06T06:59:24.787582: step 2617, loss 0.246599, acc 0.84375\n",
      "2017-11-06T06:59:28.987566: step 2618, loss 0.207753, acc 0.875\n",
      "2017-11-06T06:59:33.161532: step 2619, loss 0.520985, acc 0.84375\n",
      "2017-11-06T06:59:37.433568: step 2620, loss 0.268736, acc 0.875\n",
      "2017-11-06T06:59:41.714609: step 2621, loss 0.193079, acc 0.875\n",
      "2017-11-06T06:59:45.825529: step 2622, loss 0.404656, acc 0.84375\n",
      "2017-11-06T06:59:49.824371: step 2623, loss 0.135724, acc 0.96875\n",
      "2017-11-06T06:59:53.909274: step 2624, loss 0.0972597, acc 0.9375\n",
      "2017-11-06T06:59:57.881096: step 2625, loss 0.259279, acc 0.9375\n",
      "2017-11-06T07:00:02.211173: step 2626, loss 0.356189, acc 0.8125\n",
      "2017-11-06T07:00:06.331100: step 2627, loss 0.0842401, acc 0.9375\n",
      "2017-11-06T07:00:09.235590: step 2628, loss 0.191157, acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:00:13.249442: step 2629, loss 0.0480559, acc 0.96875\n",
      "2017-11-06T07:00:17.232273: step 2630, loss 0.478128, acc 0.75\n",
      "2017-11-06T07:00:21.238119: step 2631, loss 0.157215, acc 0.96875\n",
      "2017-11-06T07:00:25.734314: step 2632, loss 0.181554, acc 0.9375\n",
      "2017-11-06T07:00:29.752168: step 2633, loss 0.0639877, acc 0.96875\n",
      "2017-11-06T07:00:33.883104: step 2634, loss 0.101875, acc 0.96875\n",
      "2017-11-06T07:00:37.929981: step 2635, loss 0.116551, acc 0.9375\n",
      "2017-11-06T07:00:41.940611: step 2636, loss 0.292505, acc 0.84375\n",
      "2017-11-06T07:00:46.439808: step 2637, loss 0.210717, acc 0.875\n",
      "2017-11-06T07:00:50.526731: step 2638, loss 0.107129, acc 0.96875\n",
      "2017-11-06T07:00:54.507542: step 2639, loss 0.0789769, acc 0.96875\n",
      "2017-11-06T07:00:58.498376: step 2640, loss 0.287758, acc 0.84375\n",
      "2017-11-06T07:01:02.630312: step 2641, loss 0.306545, acc 0.84375\n",
      "2017-11-06T07:01:07.180545: step 2642, loss 0.104364, acc 0.96875\n",
      "2017-11-06T07:01:11.395541: step 2643, loss 0.293987, acc 0.90625\n",
      "2017-11-06T07:01:15.368363: step 2644, loss 0.219213, acc 0.875\n",
      "2017-11-06T07:01:19.364202: step 2645, loss 0.341134, acc 0.875\n",
      "2017-11-06T07:01:23.338026: step 2646, loss 0.111386, acc 0.9375\n",
      "2017-11-06T07:01:27.449947: step 2647, loss 0.188682, acc 0.90625\n",
      "2017-11-06T07:01:31.855077: step 2648, loss 0.203765, acc 0.9375\n",
      "2017-11-06T07:01:36.053061: step 2649, loss 0.101526, acc 0.96875\n",
      "2017-11-06T07:01:40.116948: step 2650, loss 0.270192, acc 0.9375\n",
      "2017-11-06T07:01:44.113788: step 2651, loss 0.222429, acc 0.90625\n",
      "2017-11-06T07:01:48.155660: step 2652, loss 0.119925, acc 0.9375\n",
      "2017-11-06T07:01:52.174515: step 2653, loss 0.0989291, acc 0.96875\n",
      "2017-11-06T07:01:56.117317: step 2654, loss 0.222643, acc 0.9375\n",
      "2017-11-06T07:02:00.123164: step 2655, loss 0.251594, acc 0.90625\n",
      "2017-11-06T07:02:04.249095: step 2656, loss 0.213542, acc 0.90625\n",
      "2017-11-06T07:02:08.349008: step 2657, loss 0.291026, acc 0.875\n",
      "2017-11-06T07:02:12.435225: step 2658, loss 0.193731, acc 0.875\n",
      "2017-11-06T07:02:16.367018: step 2659, loss 0.322437, acc 0.90625\n",
      "2017-11-06T07:02:20.394881: step 2660, loss 0.251381, acc 0.84375\n",
      "2017-11-06T07:02:24.348689: step 2661, loss 0.0970098, acc 0.96875\n",
      "2017-11-06T07:02:28.398567: step 2662, loss 0.228978, acc 0.875\n",
      "2017-11-06T07:02:32.525500: step 2663, loss 0.231776, acc 0.90625\n",
      "2017-11-06T07:02:35.620700: step 2664, loss 0.199206, acc 0.9\n",
      "2017-11-06T07:02:39.841698: step 2665, loss 0.0660048, acc 0.96875\n",
      "2017-11-06T07:02:43.849546: step 2666, loss 0.188931, acc 0.875\n",
      "2017-11-06T07:02:47.829374: step 2667, loss 0.149128, acc 0.9375\n",
      "2017-11-06T07:02:51.843227: step 2668, loss 0.11835, acc 0.96875\n",
      "2017-11-06T07:02:55.840066: step 2669, loss 0.085279, acc 0.9375\n",
      "2017-11-06T07:02:59.755848: step 2670, loss 0.144659, acc 0.90625\n",
      "2017-11-06T07:03:03.744683: step 2671, loss 0.142413, acc 0.90625\n",
      "2017-11-06T07:03:07.725512: step 2672, loss 0.239289, acc 0.875\n",
      "2017-11-06T07:03:11.753373: step 2673, loss 0.0865666, acc 0.96875\n",
      "2017-11-06T07:03:15.714188: step 2674, loss 0.352016, acc 0.8125\n",
      "2017-11-06T07:03:19.666996: step 2675, loss 0.17656, acc 0.90625\n",
      "2017-11-06T07:03:23.871984: step 2676, loss 0.173405, acc 0.90625\n",
      "2017-11-06T07:03:28.249094: step 2677, loss 0.208779, acc 0.78125\n",
      "2017-11-06T07:03:32.216913: step 2678, loss 0.112686, acc 0.96875\n",
      "2017-11-06T07:03:36.159715: step 2679, loss 0.196637, acc 0.875\n",
      "2017-11-06T07:03:40.550835: step 2680, loss 0.304423, acc 0.84375\n",
      "2017-11-06T07:03:44.853673: step 2681, loss 0.168458, acc 0.90625\n",
      "2017-11-06T07:03:49.033643: step 2682, loss 0.278243, acc 0.875\n",
      "2017-11-06T07:03:53.113541: step 2683, loss 0.284124, acc 0.9375\n",
      "2017-11-06T07:03:57.134398: step 2684, loss 0.200383, acc 0.875\n",
      "2017-11-06T07:04:01.299358: step 2685, loss 0.145911, acc 0.9375\n",
      "2017-11-06T07:04:05.412280: step 2686, loss 0.288983, acc 0.84375\n",
      "2017-11-06T07:04:09.534209: step 2687, loss 0.082661, acc 0.96875\n",
      "2017-11-06T07:04:13.789232: step 2688, loss 0.249503, acc 0.875\n",
      "2017-11-06T07:04:17.897151: step 2689, loss 0.117306, acc 0.90625\n",
      "2017-11-06T07:04:21.964041: step 2690, loss 0.231758, acc 0.875\n",
      "2017-11-06T07:04:26.053947: step 2691, loss 0.277142, acc 0.8125\n",
      "2017-11-06T07:04:30.233917: step 2692, loss 0.455896, acc 0.75\n",
      "2017-11-06T07:04:34.581006: step 2693, loss 0.0940242, acc 0.96875\n",
      "2017-11-06T07:04:38.727952: step 2694, loss 0.322445, acc 0.84375\n",
      "2017-11-06T07:04:42.860889: step 2695, loss 0.106209, acc 0.96875\n",
      "2017-11-06T07:04:47.374096: step 2696, loss 0.15535, acc 0.875\n",
      "2017-11-06T07:04:51.601100: step 2697, loss 0.193073, acc 0.90625\n",
      "2017-11-06T07:04:55.620956: step 2698, loss 0.123247, acc 0.90625\n",
      "2017-11-06T07:04:59.719867: step 2699, loss 0.389272, acc 0.84375\n",
      "2017-11-06T07:05:02.258672: step 2700, loss 0.0314034, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:05:04.814489: step 2700, loss 0.828136, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-06T07:05:11.615191: step 2701, loss 0.355468, acc 0.875\n",
      "2017-11-06T07:05:15.634047: step 2702, loss 0.0718668, acc 1\n",
      "2017-11-06T07:05:19.596863: step 2703, loss 0.0321183, acc 1\n",
      "2017-11-06T07:05:23.591701: step 2704, loss 0.166454, acc 0.875\n",
      "2017-11-06T07:05:27.586540: step 2705, loss 0.288732, acc 0.90625\n",
      "2017-11-06T07:05:31.576375: step 2706, loss 0.0997662, acc 0.96875\n",
      "2017-11-06T07:05:35.537189: step 2707, loss 0.220525, acc 0.90625\n",
      "2017-11-06T07:05:39.538031: step 2708, loss 0.337319, acc 0.875\n",
      "2017-11-06T07:05:43.551884: step 2709, loss 0.15307, acc 0.9375\n",
      "2017-11-06T07:05:47.535716: step 2710, loss 0.185717, acc 0.90625\n",
      "2017-11-06T07:05:51.775727: step 2711, loss 0.193844, acc 0.9375\n",
      "2017-11-06T07:05:55.927677: step 2712, loss 0.310192, acc 0.875\n",
      "2017-11-06T07:05:59.824446: step 2713, loss 0.562785, acc 0.71875\n",
      "2017-11-06T07:06:03.821287: step 2714, loss 0.0812136, acc 0.96875\n",
      "2017-11-06T07:06:07.879169: step 2715, loss 0.106678, acc 0.90625\n",
      "2017-11-06T07:06:11.868004: step 2716, loss 0.135267, acc 0.9375\n",
      "2017-11-06T07:06:15.955187: step 2717, loss 0.176551, acc 0.90625\n",
      "2017-11-06T07:06:19.899990: step 2718, loss 0.23671, acc 0.875\n",
      "2017-11-06T07:06:23.906838: step 2719, loss 0.279633, acc 0.78125\n",
      "2017-11-06T07:06:27.889668: step 2720, loss 0.255303, acc 0.875\n",
      "2017-11-06T07:06:31.843477: step 2721, loss 0.119801, acc 0.90625\n",
      "2017-11-06T07:06:35.991425: step 2722, loss 0.204489, acc 0.90625\n",
      "2017-11-06T07:06:39.920216: step 2723, loss 0.168703, acc 0.9375\n",
      "2017-11-06T07:06:43.875801: step 2724, loss 0.103487, acc 0.96875\n",
      "2017-11-06T07:06:47.875644: step 2725, loss 0.233671, acc 0.9375\n",
      "2017-11-06T07:06:51.815442: step 2726, loss 0.126695, acc 0.9375\n",
      "2017-11-06T07:06:55.997415: step 2727, loss 0.337475, acc 0.8125\n",
      "2017-11-06T07:07:00.284460: step 2728, loss 0.20023, acc 0.90625\n",
      "2017-11-06T07:07:04.268291: step 2729, loss 0.409767, acc 0.8125\n",
      "2017-11-06T07:07:08.245116: step 2730, loss 0.182975, acc 0.90625\n",
      "2017-11-06T07:07:12.199927: step 2731, loss 0.16628, acc 0.90625\n",
      "2017-11-06T07:07:16.216781: step 2732, loss 0.264902, acc 0.90625\n",
      "2017-11-06T07:07:20.181599: step 2733, loss 0.171958, acc 0.9375\n",
      "2017-11-06T07:07:24.117394: step 2734, loss 0.305561, acc 0.9375\n",
      "2017-11-06T07:07:28.101225: step 2735, loss 0.0678431, acc 1\n",
      "2017-11-06T07:07:30.623017: step 2736, loss 0.397526, acc 0.75\n",
      "2017-11-06T07:07:34.620858: step 2737, loss 0.126046, acc 0.9375\n",
      "2017-11-06T07:07:38.606690: step 2738, loss 0.18957, acc 0.90625\n",
      "2017-11-06T07:07:42.611537: step 2739, loss 0.111608, acc 0.9375\n",
      "2017-11-06T07:07:46.574352: step 2740, loss 0.277273, acc 0.875\n",
      "2017-11-06T07:07:50.537168: step 2741, loss 0.211235, acc 0.875\n",
      "2017-11-06T07:07:54.513993: step 2742, loss 0.376128, acc 0.8125\n",
      "2017-11-06T07:07:58.471805: step 2743, loss 0.126849, acc 0.9375\n",
      "2017-11-06T07:08:02.736835: step 2744, loss 0.316808, acc 0.875\n",
      "2017-11-06T07:08:06.934818: step 2745, loss 0.221347, acc 0.875\n",
      "2017-11-06T07:08:11.019721: step 2746, loss 0.160552, acc 0.90625\n",
      "2017-11-06T07:08:15.028569: step 2747, loss 0.241593, acc 0.8125\n",
      "2017-11-06T07:08:19.087489: step 2748, loss 0.225369, acc 0.90625\n",
      "2017-11-06T07:08:23.201412: step 2749, loss 0.192923, acc 0.90625\n",
      "2017-11-06T07:08:27.237280: step 2750, loss 0.205061, acc 0.9375\n",
      "2017-11-06T07:08:31.231117: step 2751, loss 0.131726, acc 0.96875\n",
      "2017-11-06T07:08:35.453118: step 2752, loss 0.18494, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:08:39.464967: step 2753, loss 0.0928582, acc 1\n",
      "2017-11-06T07:08:43.378748: step 2754, loss 0.341387, acc 0.8125\n",
      "2017-11-06T07:08:47.322552: step 2755, loss 0.138944, acc 0.9375\n",
      "2017-11-06T07:08:51.334402: step 2756, loss 0.198212, acc 0.875\n",
      "2017-11-06T07:08:55.326238: step 2757, loss 0.256522, acc 0.84375\n",
      "2017-11-06T07:08:59.285052: step 2758, loss 0.186918, acc 0.90625\n",
      "2017-11-06T07:09:03.273886: step 2759, loss 0.187134, acc 0.90625\n",
      "2017-11-06T07:09:07.365793: step 2760, loss 0.143858, acc 0.9375\n",
      "2017-11-06T07:09:11.733897: step 2761, loss 0.147349, acc 0.9375\n",
      "2017-11-06T07:09:15.759757: step 2762, loss 0.324593, acc 0.90625\n",
      "2017-11-06T07:09:19.742587: step 2763, loss 0.181175, acc 0.875\n",
      "2017-11-06T07:09:23.677384: step 2764, loss 0.381036, acc 0.8125\n",
      "2017-11-06T07:09:27.592165: step 2765, loss 0.18588, acc 0.9375\n",
      "2017-11-06T07:09:31.538969: step 2766, loss 0.149779, acc 0.875\n",
      "2017-11-06T07:09:35.491778: step 2767, loss 0.291493, acc 0.8125\n",
      "2017-11-06T07:09:39.482613: step 2768, loss 0.148675, acc 0.9375\n",
      "2017-11-06T07:09:43.432192: step 2769, loss 0.183406, acc 0.9375\n",
      "2017-11-06T07:09:47.431031: step 2770, loss 0.21293, acc 0.90625\n",
      "2017-11-06T07:09:51.399852: step 2771, loss 0.248459, acc 0.84375\n",
      "2017-11-06T07:09:53.920643: step 2772, loss 0.289045, acc 0.9\n",
      "2017-11-06T07:09:57.855439: step 2773, loss 0.21681, acc 0.875\n",
      "2017-11-06T07:10:02.204529: step 2774, loss 0.194384, acc 0.90625\n",
      "2017-11-06T07:10:06.174351: step 2775, loss 0.134763, acc 0.9375\n",
      "2017-11-06T07:10:10.183198: step 2776, loss 0.275852, acc 0.84375\n",
      "2017-11-06T07:10:14.481252: step 2777, loss 0.307903, acc 0.875\n",
      "2017-11-06T07:10:18.719265: step 2778, loss 0.119281, acc 0.90625\n",
      "2017-11-06T07:10:22.913140: step 2779, loss 0.115208, acc 0.9375\n",
      "2017-11-06T07:10:26.923992: step 2780, loss 0.0610708, acc 0.96875\n",
      "2017-11-06T07:10:30.934840: step 2781, loss 0.112921, acc 0.96875\n",
      "2017-11-06T07:10:35.052767: step 2782, loss 0.220926, acc 0.875\n",
      "2017-11-06T07:10:39.011579: step 2783, loss 0.137534, acc 0.90625\n",
      "2017-11-06T07:10:42.976398: step 2784, loss 0.0897912, acc 0.9375\n",
      "2017-11-06T07:10:46.959228: step 2785, loss 0.170359, acc 0.9375\n",
      "2017-11-06T07:10:50.909052: step 2786, loss 0.07921, acc 0.96875\n",
      "2017-11-06T07:10:54.845830: step 2787, loss 0.246704, acc 0.875\n",
      "2017-11-06T07:10:58.767620: step 2788, loss 0.0960779, acc 0.9375\n",
      "2017-11-06T07:11:02.693408: step 2789, loss 0.0825092, acc 0.9375\n",
      "2017-11-06T07:11:06.654220: step 2790, loss 0.20828, acc 0.9375\n",
      "2017-11-06T07:11:10.684084: step 2791, loss 0.298406, acc 0.875\n",
      "2017-11-06T07:11:14.668915: step 2792, loss 0.209032, acc 0.90625\n",
      "2017-11-06T07:11:18.777835: step 2793, loss 0.126219, acc 0.9375\n",
      "2017-11-06T07:11:23.209984: step 2794, loss 0.341457, acc 0.875\n",
      "2017-11-06T07:11:27.150784: step 2795, loss 0.10453, acc 0.9375\n",
      "2017-11-06T07:11:31.140619: step 2796, loss 0.170187, acc 0.90625\n",
      "2017-11-06T07:11:35.066408: step 2797, loss 0.339723, acc 0.90625\n",
      "2017-11-06T07:11:39.003206: step 2798, loss 0.306006, acc 0.84375\n",
      "2017-11-06T07:11:42.956016: step 2799, loss 0.171219, acc 0.90625\n",
      "2017-11-06T07:11:46.906821: step 2800, loss 0.258891, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:11:49.597734: step 2800, loss 0.806893, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-06T07:11:55.660585: step 2801, loss 0.294439, acc 0.8125\n",
      "2017-11-06T07:11:59.687447: step 2802, loss 0.106488, acc 0.96875\n",
      "2017-11-06T07:12:03.660270: step 2803, loss 0.273725, acc 0.78125\n",
      "2017-11-06T07:12:07.681127: step 2804, loss 0.331964, acc 0.8125\n",
      "2017-11-06T07:12:11.769032: step 2805, loss 0.24193, acc 0.90625\n",
      "2017-11-06T07:12:15.794892: step 2806, loss 0.21305, acc 0.90625\n",
      "2017-11-06T07:12:19.862783: step 2807, loss 0.261215, acc 0.8125\n",
      "2017-11-06T07:12:22.468366: step 2808, loss 0.438853, acc 0.75\n",
      "2017-11-06T07:12:26.921531: step 2809, loss 0.241963, acc 0.90625\n",
      "2017-11-06T07:12:30.994424: step 2810, loss 0.160436, acc 0.90625\n",
      "2017-11-06T07:12:35.309490: step 2811, loss 0.130862, acc 0.90625\n",
      "2017-11-06T07:12:39.347360: step 2812, loss 0.114896, acc 0.96875\n",
      "2017-11-06T07:12:43.362972: step 2813, loss 0.11051, acc 0.90625\n",
      "2017-11-06T07:12:47.379845: step 2814, loss 0.0572035, acc 1\n",
      "2017-11-06T07:12:51.381669: step 2815, loss 0.168806, acc 0.875\n",
      "2017-11-06T07:12:55.389517: step 2816, loss 0.258612, acc 0.90625\n",
      "2017-11-06T07:12:59.417378: step 2817, loss 0.277006, acc 0.875\n",
      "2017-11-06T07:13:03.433232: step 2818, loss 0.129264, acc 0.9375\n",
      "2017-11-06T07:13:07.458092: step 2819, loss 0.280956, acc 0.84375\n",
      "2017-11-06T07:13:11.541994: step 2820, loss 0.155892, acc 0.9375\n",
      "2017-11-06T07:13:15.520822: step 2821, loss 0.301202, acc 0.78125\n",
      "2017-11-06T07:13:19.503651: step 2822, loss 0.182169, acc 0.875\n",
      "2017-11-06T07:13:23.794700: step 2823, loss 0.165242, acc 0.9375\n",
      "2017-11-06T07:13:27.965664: step 2824, loss 0.245024, acc 0.875\n",
      "2017-11-06T07:13:32.417827: step 2825, loss 0.154052, acc 0.9375\n",
      "2017-11-06T07:13:36.587791: step 2826, loss 0.169685, acc 0.90625\n",
      "2017-11-06T07:13:40.546602: step 2827, loss 0.233967, acc 0.90625\n",
      "2017-11-06T07:13:44.585473: step 2828, loss 0.510561, acc 0.75\n",
      "2017-11-06T07:13:48.585315: step 2829, loss 0.250279, acc 0.875\n",
      "2017-11-06T07:13:52.620184: step 2830, loss 0.128284, acc 0.90625\n",
      "2017-11-06T07:13:56.633033: step 2831, loss 0.133597, acc 0.90625\n",
      "2017-11-06T07:14:00.631874: step 2832, loss 0.199889, acc 0.90625\n",
      "2017-11-06T07:14:04.650730: step 2833, loss 0.195937, acc 0.9375\n",
      "2017-11-06T07:14:08.654577: step 2834, loss 0.124969, acc 0.90625\n",
      "2017-11-06T07:14:12.683438: step 2835, loss 0.343952, acc 0.8125\n",
      "2017-11-06T07:14:16.647254: step 2836, loss 0.0579956, acc 0.96875\n",
      "2017-11-06T07:14:20.675116: step 2837, loss 0.304201, acc 0.84375\n",
      "2017-11-06T07:14:24.756050: step 2838, loss 0.0582291, acc 0.96875\n",
      "2017-11-06T07:14:28.703855: step 2839, loss 0.12007, acc 0.96875\n",
      "2017-11-06T07:14:32.798765: step 2840, loss 0.271032, acc 0.84375\n",
      "2017-11-06T07:14:37.262937: step 2841, loss 0.448576, acc 0.78125\n",
      "2017-11-06T07:14:41.426895: step 2842, loss 0.140242, acc 0.9375\n",
      "2017-11-06T07:14:45.457759: step 2843, loss 0.212158, acc 0.875\n",
      "2017-11-06T07:14:47.950532: step 2844, loss 0.0900166, acc 0.95\n",
      "2017-11-06T07:14:51.993403: step 2845, loss 0.23922, acc 0.84375\n",
      "2017-11-06T07:14:56.017262: step 2846, loss 0.345136, acc 0.84375\n",
      "2017-11-06T07:15:00.025110: step 2847, loss 0.0497611, acc 0.96875\n",
      "2017-11-06T07:15:03.995931: step 2848, loss 0.30614, acc 0.90625\n",
      "2017-11-06T07:15:08.107853: step 2849, loss 0.196895, acc 0.9375\n",
      "2017-11-06T07:15:12.255800: step 2850, loss 0.256332, acc 0.875\n",
      "2017-11-06T07:15:16.276657: step 2851, loss 0.160998, acc 0.875\n",
      "2017-11-06T07:15:20.262489: step 2852, loss 0.0217082, acc 1\n",
      "2017-11-06T07:15:24.432452: step 2853, loss 0.140969, acc 0.9375\n",
      "2017-11-06T07:15:28.488337: step 2854, loss 0.222207, acc 0.90625\n",
      "2017-11-06T07:15:32.616267: step 2855, loss 0.123286, acc 0.9375\n",
      "2017-11-06T07:15:36.614108: step 2856, loss 0.494899, acc 0.78125\n",
      "2017-11-06T07:15:40.735037: step 2857, loss 0.28003, acc 0.875\n",
      "2017-11-06T07:15:45.163936: step 2858, loss 0.387714, acc 0.84375\n",
      "2017-11-06T07:15:49.203807: step 2859, loss 0.220421, acc 0.90625\n",
      "2017-11-06T07:15:53.258688: step 2860, loss 0.448121, acc 0.84375\n",
      "2017-11-06T07:15:57.353598: step 2861, loss 0.288648, acc 0.84375\n",
      "2017-11-06T07:16:01.345436: step 2862, loss 0.44381, acc 0.75\n",
      "2017-11-06T07:16:05.391309: step 2863, loss 0.177941, acc 0.9375\n",
      "2017-11-06T07:16:09.415168: step 2864, loss 0.220353, acc 0.875\n",
      "2017-11-06T07:16:13.441030: step 2865, loss 0.183803, acc 0.875\n",
      "2017-11-06T07:16:17.429863: step 2866, loss 0.110423, acc 0.90625\n",
      "2017-11-06T07:16:21.506760: step 2867, loss 0.180567, acc 0.9375\n",
      "2017-11-06T07:16:25.567645: step 2868, loss 0.233429, acc 0.90625\n",
      "2017-11-06T07:16:29.582497: step 2869, loss 0.0938911, acc 0.9375\n",
      "2017-11-06T07:16:33.761467: step 2870, loss 0.134375, acc 0.9375\n",
      "2017-11-06T07:16:37.848371: step 2871, loss 0.35929, acc 0.84375\n",
      "2017-11-06T07:16:41.840208: step 2872, loss 0.102892, acc 0.90625\n",
      "2017-11-06T07:16:45.935117: step 2873, loss 0.217627, acc 0.875\n",
      "2017-11-06T07:16:50.303223: step 2874, loss 0.231718, acc 0.90625\n",
      "2017-11-06T07:16:54.339088: step 2875, loss 0.264808, acc 0.90625\n",
      "2017-11-06T07:16:58.358945: step 2876, loss 0.109741, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:17:02.286736: step 2877, loss 0.0790147, acc 0.9375\n",
      "2017-11-06T07:17:06.353625: step 2878, loss 0.0738768, acc 0.96875\n",
      "2017-11-06T07:17:10.361473: step 2879, loss 0.166764, acc 0.9375\n",
      "2017-11-06T07:17:12.888268: step 2880, loss 0.168143, acc 0.9\n",
      "2017-11-06T07:17:16.905123: step 2881, loss 0.0782122, acc 0.96875\n",
      "2017-11-06T07:17:20.878946: step 2882, loss 0.322239, acc 0.75\n",
      "2017-11-06T07:17:24.922820: step 2883, loss 0.28209, acc 0.8125\n",
      "2017-11-06T07:17:28.943677: step 2884, loss 0.238785, acc 0.90625\n",
      "2017-11-06T07:17:32.946522: step 2885, loss 0.220685, acc 0.90625\n",
      "2017-11-06T07:17:36.934355: step 2886, loss 0.204344, acc 0.90625\n",
      "2017-11-06T07:17:40.885161: step 2887, loss 0.121997, acc 0.9375\n",
      "2017-11-06T07:17:44.927033: step 2888, loss 0.11338, acc 0.90625\n",
      "2017-11-06T07:17:48.948891: step 2889, loss 0.205297, acc 0.90625\n",
      "2017-11-06T07:17:53.187903: step 2890, loss 0.104967, acc 0.90625\n",
      "2017-11-06T07:17:57.402898: step 2891, loss 0.25956, acc 0.84375\n",
      "2017-11-06T07:18:01.296664: step 2892, loss 0.290619, acc 0.875\n",
      "2017-11-06T07:18:05.230460: step 2893, loss 0.259689, acc 0.875\n",
      "2017-11-06T07:18:09.198279: step 2894, loss 0.27406, acc 0.875\n",
      "2017-11-06T07:18:13.180108: step 2895, loss 0.106596, acc 0.9375\n",
      "2017-11-06T07:18:17.125912: step 2896, loss 0.0960984, acc 0.9375\n",
      "2017-11-06T07:18:21.083725: step 2897, loss 0.114665, acc 0.9375\n",
      "2017-11-06T07:18:25.191643: step 2898, loss 0.135528, acc 0.90625\n",
      "2017-11-06T07:18:29.303565: step 2899, loss 0.243894, acc 0.875\n",
      "2017-11-06T07:18:33.333428: step 2900, loss 0.209859, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:18:36.080380: step 2900, loss 0.921882, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-06T07:18:41.822547: step 2901, loss 0.287754, acc 0.9375\n",
      "2017-11-06T07:18:45.754133: step 2902, loss 0.417181, acc 0.8125\n",
      "2017-11-06T07:18:49.688929: step 2903, loss 0.29054, acc 0.84375\n",
      "2017-11-06T07:18:53.684768: step 2904, loss 0.331252, acc 0.84375\n",
      "2017-11-06T07:18:57.746654: step 2905, loss 0.0560245, acc 1\n",
      "2017-11-06T07:19:02.074748: step 2906, loss 0.130468, acc 0.90625\n",
      "2017-11-06T07:19:06.050554: step 2907, loss 0.189366, acc 0.875\n",
      "2017-11-06T07:19:10.006366: step 2908, loss 0.123743, acc 0.96875\n",
      "2017-11-06T07:19:14.071253: step 2909, loss 0.232487, acc 0.84375\n",
      "2017-11-06T07:19:18.071095: step 2910, loss 0.21712, acc 0.875\n",
      "2017-11-06T07:19:22.110968: step 2911, loss 0.377112, acc 0.8125\n",
      "2017-11-06T07:19:26.070779: step 2912, loss 0.251533, acc 0.90625\n",
      "2017-11-06T07:19:30.123659: step 2913, loss 0.149726, acc 0.9375\n",
      "2017-11-06T07:19:34.120499: step 2914, loss 0.485918, acc 0.8125\n",
      "2017-11-06T07:19:38.118340: step 2915, loss 0.21445, acc 0.875\n",
      "2017-11-06T07:19:40.632126: step 2916, loss 0.121784, acc 0.95\n",
      "2017-11-06T07:19:44.582933: step 2917, loss 0.236782, acc 0.90625\n",
      "2017-11-06T07:19:48.563762: step 2918, loss 0.0731451, acc 0.96875\n",
      "2017-11-06T07:19:52.570609: step 2919, loss 0.199803, acc 0.90625\n",
      "2017-11-06T07:19:56.613481: step 2920, loss 0.182095, acc 0.9375\n",
      "2017-11-06T07:20:00.727405: step 2921, loss 0.106784, acc 0.9375\n",
      "2017-11-06T07:20:05.138539: step 2922, loss 0.208435, acc 0.9375\n",
      "2017-11-06T07:20:09.289488: step 2923, loss 0.107661, acc 0.9375\n",
      "2017-11-06T07:20:13.261310: step 2924, loss 0.221619, acc 0.9375\n",
      "2017-11-06T07:20:17.261152: step 2925, loss 0.196958, acc 0.90625\n",
      "2017-11-06T07:20:21.285011: step 2926, loss 0.0966719, acc 0.96875\n",
      "2017-11-06T07:20:25.288856: step 2927, loss 0.240507, acc 0.84375\n",
      "2017-11-06T07:20:29.376006: step 2928, loss 0.220619, acc 0.875\n",
      "2017-11-06T07:20:33.582995: step 2929, loss 0.250607, acc 0.90625\n",
      "2017-11-06T07:20:37.661894: step 2930, loss 0.183928, acc 0.90625\n",
      "2017-11-06T07:20:41.630714: step 2931, loss 0.303602, acc 0.875\n",
      "2017-11-06T07:20:45.595531: step 2932, loss 0.155701, acc 0.9375\n",
      "2017-11-06T07:20:49.584365: step 2933, loss 0.34989, acc 0.875\n",
      "2017-11-06T07:20:53.597216: step 2934, loss 0.0905797, acc 0.96875\n",
      "2017-11-06T07:20:57.571040: step 2935, loss 0.171801, acc 0.9375\n",
      "2017-11-06T07:21:01.516843: step 2936, loss 0.22579, acc 0.90625\n",
      "2017-11-06T07:21:05.438630: step 2937, loss 0.12016, acc 0.9375\n",
      "2017-11-06T07:21:09.651624: step 2938, loss 0.178606, acc 0.90625\n",
      "2017-11-06T07:21:13.983702: step 2939, loss 0.36443, acc 0.78125\n",
      "2017-11-06T07:21:18.037584: step 2940, loss 0.144314, acc 0.9375\n",
      "2017-11-06T07:21:22.187531: step 2941, loss 0.148052, acc 0.90625\n",
      "2017-11-06T07:21:26.457565: step 2942, loss 0.298465, acc 0.84375\n",
      "2017-11-06T07:21:30.728600: step 2943, loss 0.445338, acc 0.84375\n",
      "2017-11-06T07:21:35.047668: step 2944, loss 0.159687, acc 0.9375\n",
      "2017-11-06T07:21:39.326709: step 2945, loss 0.108052, acc 0.96875\n",
      "2017-11-06T07:21:43.565572: step 2946, loss 0.152868, acc 0.90625\n",
      "2017-11-06T07:21:47.869630: step 2947, loss 0.360844, acc 0.875\n",
      "2017-11-06T07:21:52.261750: step 2948, loss 0.12932, acc 0.9375\n",
      "2017-11-06T07:21:56.734929: step 2949, loss 0.159698, acc 0.9375\n",
      "2017-11-06T07:22:01.237129: step 2950, loss 0.271053, acc 0.84375\n",
      "2017-11-06T07:22:05.624245: step 2951, loss 0.173467, acc 0.9375\n",
      "2017-11-06T07:22:08.475271: step 2952, loss 0.264403, acc 0.9\n",
      "2017-11-06T07:22:12.799343: step 2953, loss 0.320931, acc 0.8125\n",
      "2017-11-06T07:22:17.633778: step 2954, loss 0.240635, acc 0.9375\n",
      "2017-11-06T07:22:22.134978: step 2955, loss 0.0463922, acc 0.96875\n",
      "2017-11-06T07:22:26.533101: step 2956, loss 0.309552, acc 0.84375\n",
      "2017-11-06T07:22:31.006280: step 2957, loss 0.169905, acc 0.90625\n",
      "2017-11-06T07:22:35.487464: step 2958, loss 0.331855, acc 0.84375\n",
      "2017-11-06T07:22:39.883588: step 2959, loss 0.350218, acc 0.84375\n",
      "2017-11-06T07:22:44.146617: step 2960, loss 0.136644, acc 0.96875\n",
      "2017-11-06T07:22:48.858967: step 2961, loss 0.162654, acc 0.875\n",
      "2017-11-06T07:22:52.915848: step 2962, loss 0.380153, acc 0.84375\n",
      "2017-11-06T07:22:56.942709: step 2963, loss 0.142241, acc 0.9375\n",
      "2017-11-06T07:23:00.894517: step 2964, loss 0.164861, acc 0.9375\n",
      "2017-11-06T07:23:04.899363: step 2965, loss 0.276156, acc 0.90625\n",
      "2017-11-06T07:23:08.920220: step 2966, loss 0.17597, acc 0.90625\n",
      "2017-11-06T07:23:12.925065: step 2967, loss 0.0546892, acc 0.96875\n",
      "2017-11-06T07:23:17.828550: step 2968, loss 0.227516, acc 0.90625\n",
      "2017-11-06T07:23:23.022240: step 2969, loss 0.258172, acc 0.875\n",
      "2017-11-06T07:23:27.388341: step 2970, loss 0.147457, acc 0.9375\n",
      "2017-11-06T07:23:31.447226: step 2971, loss 0.244932, acc 0.90625\n",
      "2017-11-06T07:23:35.442064: step 2972, loss 0.244528, acc 0.90625\n",
      "2017-11-06T07:23:39.448911: step 2973, loss 0.163211, acc 0.875\n",
      "2017-11-06T07:23:43.461762: step 2974, loss 0.253434, acc 0.875\n",
      "2017-11-06T07:23:47.422578: step 2975, loss 0.181445, acc 0.875\n",
      "2017-11-06T07:23:51.420417: step 2976, loss 0.316948, acc 0.84375\n",
      "2017-11-06T07:23:55.368223: step 2977, loss 0.20689, acc 0.84375\n",
      "2017-11-06T07:23:59.404090: step 2978, loss 0.19377, acc 0.90625\n",
      "2017-11-06T07:24:03.416942: step 2979, loss 0.142148, acc 0.90625\n",
      "2017-11-06T07:24:07.411780: step 2980, loss 0.152583, acc 0.90625\n",
      "2017-11-06T07:24:11.396611: step 2981, loss 0.191118, acc 0.90625\n",
      "2017-11-06T07:24:15.401457: step 2982, loss 0.302475, acc 0.84375\n",
      "2017-11-06T07:24:19.360270: step 2983, loss 0.285201, acc 0.875\n",
      "2017-11-06T07:24:23.378124: step 2984, loss 0.252548, acc 0.875\n",
      "2017-11-06T07:24:27.676179: step 2985, loss 0.205035, acc 0.90625\n",
      "2017-11-06T07:24:31.927201: step 2986, loss 0.280019, acc 0.875\n",
      "2017-11-06T07:24:36.318319: step 2987, loss 0.122587, acc 0.9375\n",
      "2017-11-06T07:24:39.015236: step 2988, loss 0.0815967, acc 0.95\n",
      "2017-11-06T07:24:43.149173: step 2989, loss 0.0685061, acc 0.96875\n",
      "2017-11-06T07:24:47.103806: step 2990, loss 0.217146, acc 0.9375\n",
      "2017-11-06T07:24:51.119660: step 2991, loss 0.254616, acc 0.84375\n",
      "2017-11-06T07:24:55.092483: step 2992, loss 0.0595004, acc 1\n",
      "2017-11-06T07:24:59.094326: step 2993, loss 0.0902545, acc 0.96875\n",
      "2017-11-06T07:25:03.041130: step 2994, loss 0.133924, acc 0.90625\n",
      "2017-11-06T07:25:06.991938: step 2995, loss 0.0793288, acc 0.9375\n",
      "2017-11-06T07:25:11.098856: step 2996, loss 0.373408, acc 0.78125\n",
      "2017-11-06T07:25:15.184759: step 2997, loss 0.126547, acc 0.90625\n",
      "2017-11-06T07:25:19.188604: step 2998, loss 0.0782842, acc 0.96875\n",
      "2017-11-06T07:25:23.228475: step 2999, loss 0.329831, acc 0.84375\n",
      "2017-11-06T07:25:27.263342: step 3000, loss 0.2574, acc 0.875\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:25:29.777128: step 3000, loss 0.950409, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-06T07:25:35.639001: step 3001, loss 0.0476914, acc 1\n",
      "2017-11-06T07:25:39.717899: step 3002, loss 0.313929, acc 0.90625\n",
      "2017-11-06T07:25:43.711739: step 3003, loss 0.178794, acc 0.90625\n",
      "2017-11-06T07:25:47.658541: step 3004, loss 0.295946, acc 0.84375\n",
      "2017-11-06T07:25:51.665389: step 3005, loss 0.125068, acc 0.9375\n",
      "2017-11-06T07:25:55.631207: step 3006, loss 0.255104, acc 0.875\n",
      "2017-11-06T07:25:59.634050: step 3007, loss 0.316701, acc 0.8125\n",
      "2017-11-06T07:26:03.725958: step 3008, loss 0.136159, acc 0.9375\n",
      "2017-11-06T07:26:07.781840: step 3009, loss 0.191171, acc 0.875\n",
      "2017-11-06T07:26:11.873749: step 3010, loss 0.29901, acc 0.875\n",
      "2017-11-06T07:26:15.975662: step 3011, loss 0.362013, acc 0.875\n",
      "2017-11-06T07:26:20.035546: step 3012, loss 0.313655, acc 0.8125\n",
      "2017-11-06T07:26:24.064409: step 3013, loss 0.0988014, acc 0.96875\n",
      "2017-11-06T07:26:28.184337: step 3014, loss 0.239524, acc 0.84375\n",
      "2017-11-06T07:26:32.234215: step 3015, loss 0.329507, acc 0.84375\n",
      "2017-11-06T07:26:36.614326: step 3016, loss 0.190715, acc 0.875\n",
      "2017-11-06T07:26:41.111522: step 3017, loss 0.245172, acc 0.875\n",
      "2017-11-06T07:26:45.093351: step 3018, loss 0.140297, acc 0.90625\n",
      "2017-11-06T07:26:49.072178: step 3019, loss 0.240669, acc 0.84375\n",
      "2017-11-06T07:26:53.111048: step 3020, loss 0.302206, acc 0.9375\n",
      "2017-11-06T07:26:57.130904: step 3021, loss 0.218558, acc 0.90625\n",
      "2017-11-06T07:27:01.138752: step 3022, loss 0.082933, acc 0.9375\n",
      "2017-11-06T07:27:05.111575: step 3023, loss 0.143544, acc 0.9375\n",
      "2017-11-06T07:27:07.671395: step 3024, loss 0.182903, acc 0.9\n",
      "2017-11-06T07:27:11.742287: step 3025, loss 0.25238, acc 0.9375\n",
      "2017-11-06T07:27:15.763143: step 3026, loss 0.160381, acc 0.9375\n",
      "2017-11-06T07:27:19.767989: step 3027, loss 0.203945, acc 0.9375\n",
      "2017-11-06T07:27:23.785844: step 3028, loss 0.118471, acc 0.96875\n",
      "2017-11-06T07:27:27.762671: step 3029, loss 0.424805, acc 0.75\n",
      "2017-11-06T07:27:31.754506: step 3030, loss 0.164249, acc 0.96875\n",
      "2017-11-06T07:27:35.755349: step 3031, loss 0.305466, acc 0.875\n",
      "2017-11-06T07:27:39.776205: step 3032, loss 0.147727, acc 0.9375\n",
      "2017-11-06T07:27:44.071013: step 3033, loss 0.182406, acc 0.84375\n",
      "2017-11-06T07:27:48.304021: step 3034, loss 0.115266, acc 0.96875\n",
      "2017-11-06T07:27:52.358901: step 3035, loss 0.0364186, acc 1\n",
      "2017-11-06T07:27:56.337729: step 3036, loss 0.223261, acc 0.875\n",
      "2017-11-06T07:28:00.360587: step 3037, loss 0.152986, acc 0.9375\n",
      "2017-11-06T07:28:04.344418: step 3038, loss 0.371534, acc 0.8125\n",
      "2017-11-06T07:28:08.321243: step 3039, loss 0.313623, acc 0.875\n",
      "2017-11-06T07:28:12.310078: step 3040, loss 0.244876, acc 0.875\n",
      "2017-11-06T07:28:16.319927: step 3041, loss 0.192868, acc 0.9375\n",
      "2017-11-06T07:28:20.305760: step 3042, loss 0.268045, acc 0.8125\n",
      "2017-11-06T07:28:24.618824: step 3043, loss 0.093322, acc 0.96875\n",
      "2017-11-06T07:28:28.619667: step 3044, loss 0.119191, acc 0.9375\n",
      "2017-11-06T07:28:32.791985: step 3045, loss 0.093601, acc 0.96875\n",
      "2017-11-06T07:28:36.925923: step 3046, loss 0.242727, acc 0.90625\n",
      "2017-11-06T07:28:40.934771: step 3047, loss 0.458232, acc 0.875\n",
      "2017-11-06T07:28:44.967637: step 3048, loss 0.132716, acc 0.9375\n",
      "2017-11-06T07:28:49.137600: step 3049, loss 0.148775, acc 0.9375\n",
      "2017-11-06T07:28:53.389621: step 3050, loss 0.0950751, acc 0.96875\n",
      "2017-11-06T07:28:57.430493: step 3051, loss 0.354276, acc 0.84375\n",
      "2017-11-06T07:29:01.420328: step 3052, loss 0.192376, acc 0.90625\n",
      "2017-11-06T07:29:05.363129: step 3053, loss 0.0948618, acc 0.96875\n",
      "2017-11-06T07:29:09.357967: step 3054, loss 0.127705, acc 0.9375\n",
      "2017-11-06T07:29:13.343801: step 3055, loss 0.18681, acc 0.9375\n",
      "2017-11-06T07:29:17.366658: step 3056, loss 0.324807, acc 0.875\n",
      "2017-11-06T07:29:21.319467: step 3057, loss 0.323137, acc 0.84375\n",
      "2017-11-06T07:29:25.303297: step 3058, loss 0.346028, acc 0.84375\n",
      "2017-11-06T07:29:29.222081: step 3059, loss 0.253663, acc 0.84375\n",
      "2017-11-06T07:29:31.771893: step 3060, loss 0.15609, acc 0.9\n",
      "2017-11-06T07:29:35.846788: step 3061, loss 0.233841, acc 0.875\n",
      "2017-11-06T07:29:39.799598: step 3062, loss 0.338547, acc 0.90625\n",
      "2017-11-06T07:29:43.822455: step 3063, loss 0.253164, acc 0.90625\n",
      "2017-11-06T07:29:47.768260: step 3064, loss 0.0777189, acc 0.96875\n",
      "2017-11-06T07:29:51.756093: step 3065, loss 0.410947, acc 0.875\n",
      "2017-11-06T07:29:56.073160: step 3066, loss 0.102036, acc 0.9375\n",
      "2017-11-06T07:30:00.311171: step 3067, loss 0.33694, acc 0.78125\n",
      "2017-11-06T07:30:04.478133: step 3068, loss 0.0535868, acc 1\n",
      "2017-11-06T07:30:08.527010: step 3069, loss 0.301435, acc 0.84375\n",
      "2017-11-06T07:30:12.482820: step 3070, loss 0.177102, acc 0.9375\n",
      "2017-11-06T07:30:16.495672: step 3071, loss 0.156337, acc 0.90625\n",
      "2017-11-06T07:30:20.427465: step 3072, loss 0.22989, acc 0.875\n",
      "2017-11-06T07:30:24.452325: step 3073, loss 0.182967, acc 0.90625\n",
      "2017-11-06T07:30:28.480187: step 3074, loss 0.249038, acc 0.84375\n",
      "2017-11-06T07:30:32.460015: step 3075, loss 0.0940404, acc 0.9375\n",
      "2017-11-06T07:30:36.668156: step 3076, loss 0.113467, acc 0.90625\n",
      "2017-11-06T07:30:40.615962: step 3077, loss 0.051998, acc 1\n",
      "2017-11-06T07:30:44.602544: step 3078, loss 0.137544, acc 0.9375\n",
      "2017-11-06T07:30:48.550349: step 3079, loss 0.166495, acc 0.9375\n",
      "2017-11-06T07:30:52.499153: step 3080, loss 0.155172, acc 0.90625\n",
      "2017-11-06T07:30:56.418939: step 3081, loss 0.459834, acc 0.6875\n",
      "2017-11-06T07:31:00.586900: step 3082, loss 0.403513, acc 0.8125\n",
      "2017-11-06T07:31:04.777878: step 3083, loss 0.134473, acc 0.9375\n",
      "2017-11-06T07:31:08.760708: step 3084, loss 0.0398302, acc 1\n",
      "2017-11-06T07:31:12.711515: step 3085, loss 0.21323, acc 0.9375\n",
      "2017-11-06T07:31:16.742379: step 3086, loss 0.14554, acc 0.90625\n",
      "2017-11-06T07:31:20.713201: step 3087, loss 0.543765, acc 0.84375\n",
      "2017-11-06T07:31:24.644996: step 3088, loss 0.322173, acc 0.9375\n",
      "2017-11-06T07:31:28.553773: step 3089, loss 0.472961, acc 0.8125\n",
      "2017-11-06T07:31:32.500578: step 3090, loss 0.245947, acc 0.90625\n",
      "2017-11-06T07:31:36.711568: step 3091, loss 0.347834, acc 0.90625\n",
      "2017-11-06T07:31:40.811481: step 3092, loss 0.250379, acc 0.875\n",
      "2017-11-06T07:31:44.781303: step 3093, loss 0.325916, acc 0.90625\n",
      "2017-11-06T07:31:48.692081: step 3094, loss 0.317792, acc 0.875\n",
      "2017-11-06T07:31:52.630879: step 3095, loss 0.442269, acc 0.8125\n",
      "2017-11-06T07:31:55.156675: step 3096, loss 0.169162, acc 0.95\n",
      "2017-11-06T07:31:59.109483: step 3097, loss 0.184618, acc 0.90625\n",
      "2017-11-06T07:32:03.057290: step 3098, loss 0.191314, acc 0.9375\n",
      "2017-11-06T07:32:07.433397: step 3099, loss 0.0583618, acc 0.96875\n",
      "2017-11-06T07:32:11.523303: step 3100, loss 0.0443633, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:32:14.140163: step 3100, loss 1.0069, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-06T07:32:19.703320: step 3101, loss 0.283298, acc 0.90625\n",
      "2017-11-06T07:32:23.713169: step 3102, loss 0.218065, acc 0.875\n",
      "2017-11-06T07:32:27.729022: step 3103, loss 0.220207, acc 0.875\n",
      "2017-11-06T07:32:31.692839: step 3104, loss 0.165215, acc 0.9375\n",
      "2017-11-06T07:32:35.910836: step 3105, loss 0.0557018, acc 0.96875\n",
      "2017-11-06T07:32:40.146704: step 3106, loss 0.239021, acc 0.9375\n",
      "2017-11-06T07:32:44.113523: step 3107, loss 0.151353, acc 0.9375\n",
      "2017-11-06T07:32:48.146388: step 3108, loss 0.0969614, acc 0.9375\n",
      "2017-11-06T07:32:52.186259: step 3109, loss 0.203931, acc 0.90625\n",
      "2017-11-06T07:32:56.124057: step 3110, loss 0.290681, acc 0.84375\n",
      "2017-11-06T07:33:00.157923: step 3111, loss 0.293974, acc 0.875\n",
      "2017-11-06T07:33:04.188786: step 3112, loss 0.0690774, acc 1\n",
      "2017-11-06T07:33:08.211645: step 3113, loss 0.157024, acc 0.90625\n",
      "2017-11-06T07:33:12.381608: step 3114, loss 0.520361, acc 0.78125\n",
      "2017-11-06T07:33:16.538563: step 3115, loss 0.126126, acc 0.9375\n",
      "2017-11-06T07:33:20.583435: step 3116, loss 0.143203, acc 0.9375\n",
      "2017-11-06T07:33:24.614301: step 3117, loss 0.136425, acc 0.9375\n",
      "2017-11-06T07:33:28.601132: step 3118, loss 0.302761, acc 0.84375\n",
      "2017-11-06T07:33:32.553942: step 3119, loss 0.208917, acc 0.9375\n",
      "2017-11-06T07:33:36.558787: step 3120, loss 0.420199, acc 0.8125\n",
      "2017-11-06T07:33:40.941901: step 3121, loss 0.134646, acc 0.9375\n",
      "2017-11-06T07:33:45.027565: step 3122, loss 0.172046, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:33:49.218542: step 3123, loss 0.149314, acc 0.90625\n",
      "2017-11-06T07:33:53.326461: step 3124, loss 0.20555, acc 0.875\n",
      "2017-11-06T07:33:57.488418: step 3125, loss 0.103562, acc 0.9375\n",
      "2017-11-06T07:34:01.559311: step 3126, loss 0.249997, acc 0.84375\n",
      "2017-11-06T07:34:05.613192: step 3127, loss 0.20254, acc 0.90625\n",
      "2017-11-06T07:34:09.836192: step 3128, loss 0.205104, acc 0.875\n",
      "2017-11-06T07:34:13.847042: step 3129, loss 0.305019, acc 0.84375\n",
      "2017-11-06T07:34:18.424294: step 3130, loss 0.141976, acc 0.90625\n",
      "2017-11-06T07:34:22.816415: step 3131, loss 0.264422, acc 0.84375\n",
      "2017-11-06T07:34:25.856575: step 3132, loss 0.34323, acc 0.8\n",
      "2017-11-06T07:34:30.060563: step 3133, loss 0.186205, acc 0.84375\n",
      "2017-11-06T07:34:34.418659: step 3134, loss 0.145659, acc 0.9375\n",
      "2017-11-06T07:34:38.556599: step 3135, loss 0.164865, acc 0.9375\n",
      "2017-11-06T07:34:42.820629: step 3136, loss 0.291362, acc 0.90625\n",
      "2017-11-06T07:34:47.013609: step 3137, loss 0.0923137, acc 0.96875\n",
      "2017-11-06T07:34:51.032464: step 3138, loss 0.0724354, acc 0.9375\n",
      "2017-11-06T07:34:55.051319: step 3139, loss 0.224313, acc 0.90625\n",
      "2017-11-06T07:34:59.051161: step 3140, loss 0.203689, acc 0.875\n",
      "2017-11-06T07:35:03.040996: step 3141, loss 0.223648, acc 0.875\n",
      "2017-11-06T07:35:06.985799: step 3142, loss 0.179178, acc 0.875\n",
      "2017-11-06T07:35:10.936607: step 3143, loss 0.214794, acc 0.875\n",
      "2017-11-06T07:35:15.160607: step 3144, loss 0.182053, acc 0.875\n",
      "2017-11-06T07:35:19.118421: step 3145, loss 0.207309, acc 0.90625\n",
      "2017-11-06T07:35:23.399462: step 3146, loss 0.136315, acc 0.90625\n",
      "2017-11-06T07:35:27.608452: step 3147, loss 0.188335, acc 0.84375\n",
      "2017-11-06T07:35:31.574271: step 3148, loss 0.0927572, acc 0.9375\n",
      "2017-11-06T07:35:35.591125: step 3149, loss 0.138403, acc 0.9375\n",
      "2017-11-06T07:35:39.578958: step 3150, loss 0.16712, acc 0.90625\n",
      "2017-11-06T07:35:43.607820: step 3151, loss 0.118676, acc 0.9375\n",
      "2017-11-06T07:35:47.542617: step 3152, loss 0.199814, acc 0.90625\n",
      "2017-11-06T07:35:51.567477: step 3153, loss 0.0246797, acc 1\n",
      "2017-11-06T07:35:55.616354: step 3154, loss 0.23339, acc 0.875\n",
      "2017-11-06T07:35:59.587175: step 3155, loss 0.354229, acc 0.84375\n",
      "2017-11-06T07:36:03.606030: step 3156, loss 0.31137, acc 0.8125\n",
      "2017-11-06T07:36:07.591863: step 3157, loss 0.1157, acc 0.9375\n",
      "2017-11-06T07:36:11.616722: step 3158, loss 0.221432, acc 0.90625\n",
      "2017-11-06T07:36:15.628573: step 3159, loss 0.339115, acc 0.84375\n",
      "2017-11-06T07:36:19.708472: step 3160, loss 0.333639, acc 0.875\n",
      "2017-11-06T07:36:23.688300: step 3161, loss 0.192264, acc 0.90625\n",
      "2017-11-06T07:36:27.870272: step 3162, loss 0.220753, acc 0.90625\n",
      "2017-11-06T07:36:32.255387: step 3163, loss 0.199749, acc 0.9375\n",
      "2017-11-06T07:36:36.524420: step 3164, loss 0.219194, acc 0.875\n",
      "2017-11-06T07:36:40.618329: step 3165, loss 0.273821, acc 0.875\n",
      "2017-11-06T07:36:44.697989: step 3166, loss 0.154484, acc 0.875\n",
      "2017-11-06T07:36:48.696830: step 3167, loss 0.127253, acc 0.96875\n",
      "2017-11-06T07:36:51.338707: step 3168, loss 0.0193358, acc 1\n",
      "2017-11-06T07:36:55.457633: step 3169, loss 0.269251, acc 0.875\n",
      "2017-11-06T07:36:59.509512: step 3170, loss 0.115708, acc 0.9375\n",
      "2017-11-06T07:37:03.487339: step 3171, loss 0.119071, acc 0.90625\n",
      "2017-11-06T07:37:07.456159: step 3172, loss 0.148653, acc 0.9375\n",
      "2017-11-06T07:37:11.392956: step 3173, loss 0.39832, acc 0.84375\n",
      "2017-11-06T07:37:16.018244: step 3174, loss 0.175762, acc 0.90625\n",
      "2017-11-06T07:37:20.526446: step 3175, loss 0.314282, acc 0.875\n",
      "2017-11-06T07:37:24.706419: step 3176, loss 0.227299, acc 0.875\n",
      "2017-11-06T07:37:28.757295: step 3177, loss 0.23173, acc 0.84375\n",
      "2017-11-06T07:37:32.801168: step 3178, loss 0.307328, acc 0.875\n",
      "2017-11-06T07:37:37.197292: step 3179, loss 0.0983802, acc 0.96875\n",
      "2017-11-06T07:37:41.131087: step 3180, loss 0.28866, acc 0.90625\n",
      "2017-11-06T07:37:45.100907: step 3181, loss 0.161287, acc 0.9375\n",
      "2017-11-06T07:37:49.092744: step 3182, loss 0.359686, acc 0.78125\n",
      "2017-11-06T07:37:53.121606: step 3183, loss 0.290482, acc 0.875\n",
      "2017-11-06T07:37:57.075415: step 3184, loss 0.0703679, acc 0.96875\n",
      "2017-11-06T07:38:01.162319: step 3185, loss 0.213591, acc 0.90625\n",
      "2017-11-06T07:38:05.129138: step 3186, loss 0.151832, acc 0.9375\n",
      "2017-11-06T07:38:09.147994: step 3187, loss 0.211053, acc 0.90625\n",
      "2017-11-06T07:38:13.126821: step 3188, loss 0.0375418, acc 0.96875\n",
      "2017-11-06T07:38:17.191709: step 3189, loss 0.0956239, acc 0.96875\n",
      "2017-11-06T07:38:21.298627: step 3190, loss 0.0397188, acc 0.96875\n",
      "2017-11-06T07:38:25.243430: step 3191, loss 0.297894, acc 0.90625\n",
      "2017-11-06T07:38:29.251278: step 3192, loss 0.359311, acc 0.90625\n",
      "2017-11-06T07:38:33.446259: step 3193, loss 0.279942, acc 0.9375\n",
      "2017-11-06T07:38:37.513148: step 3194, loss 0.34584, acc 0.90625\n",
      "2017-11-06T07:38:41.998335: step 3195, loss 0.323066, acc 0.875\n",
      "2017-11-06T07:38:46.108256: step 3196, loss 0.306761, acc 0.875\n",
      "2017-11-06T07:38:50.064066: step 3197, loss 0.198403, acc 0.875\n",
      "2017-11-06T07:38:54.102936: step 3198, loss 0.0961747, acc 0.9375\n",
      "2017-11-06T07:38:58.172827: step 3199, loss 0.348211, acc 0.84375\n",
      "2017-11-06T07:39:02.223706: step 3200, loss 0.176319, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:39:04.834561: step 3200, loss 0.791796, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-06T07:39:10.412656: step 3201, loss 0.298989, acc 0.84375\n",
      "2017-11-06T07:39:14.390482: step 3202, loss 0.113868, acc 0.96875\n",
      "2017-11-06T07:39:18.364308: step 3203, loss 0.183248, acc 0.875\n",
      "2017-11-06T07:39:20.956148: step 3204, loss 0.241132, acc 0.85\n",
      "2017-11-06T07:39:25.103095: step 3205, loss 0.0545022, acc 0.96875\n",
      "2017-11-06T07:39:29.372127: step 3206, loss 0.19658, acc 0.875\n",
      "2017-11-06T07:39:33.365966: step 3207, loss 0.146198, acc 0.9375\n",
      "2017-11-06T07:39:37.329782: step 3208, loss 0.0458506, acc 1\n",
      "2017-11-06T07:39:41.310611: step 3209, loss 0.10252, acc 0.96875\n",
      "2017-11-06T07:39:45.532372: step 3210, loss 0.214393, acc 0.875\n",
      "2017-11-06T07:39:49.808411: step 3211, loss 0.331809, acc 0.84375\n",
      "2017-11-06T07:39:53.819261: step 3212, loss 0.242355, acc 0.90625\n",
      "2017-11-06T07:39:57.787080: step 3213, loss 0.320257, acc 0.875\n",
      "2017-11-06T07:40:02.110152: step 3214, loss 0.394878, acc 0.78125\n",
      "2017-11-06T07:40:06.182045: step 3215, loss 0.201551, acc 0.875\n",
      "2017-11-06T07:40:10.141858: step 3216, loss 0.0998737, acc 0.96875\n",
      "2017-11-06T07:40:14.131695: step 3217, loss 0.238187, acc 0.90625\n",
      "2017-11-06T07:40:18.102515: step 3218, loss 0.241735, acc 0.84375\n",
      "2017-11-06T07:40:22.150391: step 3219, loss 0.115837, acc 0.96875\n",
      "2017-11-06T07:40:26.140226: step 3220, loss 0.242237, acc 0.90625\n",
      "2017-11-06T07:40:30.136065: step 3221, loss 0.13462, acc 0.90625\n",
      "2017-11-06T07:40:34.319037: step 3222, loss 0.215906, acc 0.9375\n",
      "2017-11-06T07:40:38.336892: step 3223, loss 0.210309, acc 0.875\n",
      "2017-11-06T07:40:42.339736: step 3224, loss 0.226622, acc 0.875\n",
      "2017-11-06T07:40:46.302552: step 3225, loss 0.22498, acc 0.90625\n",
      "2017-11-06T07:40:50.443494: step 3226, loss 0.343746, acc 0.84375\n",
      "2017-11-06T07:40:54.758560: step 3227, loss 0.167318, acc 0.90625\n",
      "2017-11-06T07:40:58.937530: step 3228, loss 0.248859, acc 0.875\n",
      "2017-11-06T07:41:02.944377: step 3229, loss 0.519735, acc 0.8125\n",
      "2017-11-06T07:41:06.926206: step 3230, loss 0.230024, acc 0.90625\n",
      "2017-11-06T07:41:11.304317: step 3231, loss 0.347451, acc 0.8125\n",
      "2017-11-06T07:41:16.266844: step 3232, loss 0.225943, acc 0.90625\n",
      "2017-11-06T07:41:20.320724: step 3233, loss 0.255633, acc 0.875\n",
      "2017-11-06T07:41:24.455661: step 3234, loss 0.14981, acc 0.9375\n",
      "2017-11-06T07:41:28.403467: step 3235, loss 0.0882852, acc 0.96875\n",
      "2017-11-06T07:41:32.405311: step 3236, loss 0.30458, acc 0.84375\n",
      "2017-11-06T07:41:36.367125: step 3237, loss 0.0992767, acc 0.96875\n",
      "2017-11-06T07:41:40.350956: step 3238, loss 0.313233, acc 0.875\n",
      "2017-11-06T07:41:44.349797: step 3239, loss 0.249313, acc 0.875\n",
      "2017-11-06T07:41:46.921626: step 3240, loss 0.138288, acc 0.9\n",
      "2017-11-06T07:41:50.999523: step 3241, loss 0.294162, acc 0.8125\n",
      "2017-11-06T07:41:54.994361: step 3242, loss 0.184752, acc 0.90625\n",
      "2017-11-06T07:41:59.488554: step 3243, loss 0.355028, acc 0.78125\n",
      "2017-11-06T07:42:03.570454: step 3244, loss 0.170297, acc 0.9375\n",
      "2017-11-06T07:42:07.587309: step 3245, loss 0.155697, acc 0.90625\n",
      "2017-11-06T07:42:11.549123: step 3246, loss 0.0891384, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:42:15.604005: step 3247, loss 0.0779583, acc 0.96875\n",
      "2017-11-06T07:42:19.564819: step 3248, loss 0.0804175, acc 0.96875\n",
      "2017-11-06T07:42:23.620701: step 3249, loss 0.125801, acc 0.96875\n",
      "2017-11-06T07:42:27.633552: step 3250, loss 0.117902, acc 0.9375\n",
      "2017-11-06T07:42:31.616382: step 3251, loss 0.34221, acc 0.8125\n",
      "2017-11-06T07:42:35.792350: step 3252, loss 0.212347, acc 0.875\n",
      "2017-11-06T07:42:39.769175: step 3253, loss 0.0575713, acc 0.96875\n",
      "2017-11-06T07:42:43.736994: step 3254, loss 0.138762, acc 0.9375\n",
      "2017-11-06T07:42:47.853029: step 3255, loss 0.0988945, acc 0.9375\n",
      "2017-11-06T07:42:51.887896: step 3256, loss 0.208735, acc 0.90625\n",
      "2017-11-06T07:42:55.860719: step 3257, loss 0.0523212, acc 0.96875\n",
      "2017-11-06T07:42:59.846551: step 3258, loss 0.161852, acc 0.9375\n",
      "2017-11-06T07:43:04.223661: step 3259, loss 0.263908, acc 0.78125\n",
      "2017-11-06T07:43:08.298556: step 3260, loss 0.152848, acc 0.90625\n",
      "2017-11-06T07:43:12.232352: step 3261, loss 0.309453, acc 0.8125\n",
      "2017-11-06T07:43:16.193166: step 3262, loss 0.277904, acc 0.875\n",
      "2017-11-06T07:43:20.173995: step 3263, loss 0.266167, acc 0.875\n",
      "2017-11-06T07:43:24.354965: step 3264, loss 0.206642, acc 0.875\n",
      "2017-11-06T07:43:28.363813: step 3265, loss 0.194274, acc 0.90625\n",
      "2017-11-06T07:43:32.350646: step 3266, loss 0.23214, acc 0.84375\n",
      "2017-11-06T07:43:36.303455: step 3267, loss 0.1621, acc 0.9375\n",
      "2017-11-06T07:43:40.295292: step 3268, loss 0.244089, acc 0.84375\n",
      "2017-11-06T07:43:44.241095: step 3269, loss 0.248284, acc 0.84375\n",
      "2017-11-06T07:43:48.173891: step 3270, loss 0.348074, acc 0.8125\n",
      "2017-11-06T07:43:52.158721: step 3271, loss 0.22526, acc 0.8125\n",
      "2017-11-06T07:43:56.119535: step 3272, loss 0.176406, acc 0.90625\n",
      "2017-11-06T07:44:00.138390: step 3273, loss 0.212228, acc 0.90625\n",
      "2017-11-06T07:44:04.149240: step 3274, loss 0.0700969, acc 0.9375\n",
      "2017-11-06T07:44:08.315200: step 3275, loss 0.108284, acc 0.96875\n",
      "2017-11-06T07:44:11.125198: step 3276, loss 0.423639, acc 0.8\n",
      "2017-11-06T07:44:15.107027: step 3277, loss 0.0559252, acc 0.96875\n",
      "2017-11-06T07:44:19.056835: step 3278, loss 0.16004, acc 0.90625\n",
      "2017-11-06T07:44:23.109713: step 3279, loss 0.0979785, acc 0.9375\n",
      "2017-11-06T07:44:27.046511: step 3280, loss 0.364021, acc 0.84375\n",
      "2017-11-06T07:44:31.023336: step 3281, loss 0.125492, acc 0.9375\n",
      "2017-11-06T07:44:35.150270: step 3282, loss 0.357824, acc 0.875\n",
      "2017-11-06T07:44:39.134099: step 3283, loss 0.234442, acc 0.875\n",
      "2017-11-06T07:44:43.111925: step 3284, loss 0.234875, acc 0.875\n",
      "2017-11-06T07:44:47.061732: step 3285, loss 0.090194, acc 0.96875\n",
      "2017-11-06T07:44:51.043562: step 3286, loss 0.21415, acc 0.84375\n",
      "2017-11-06T07:44:55.034397: step 3287, loss 0.298409, acc 0.84375\n",
      "2017-11-06T07:44:58.961187: step 3288, loss 0.0377872, acc 1\n",
      "2017-11-06T07:45:03.114138: step 3289, loss 0.190333, acc 0.90625\n",
      "2017-11-06T07:45:07.213050: step 3290, loss 0.122113, acc 0.90625\n",
      "2017-11-06T07:45:11.149848: step 3291, loss 0.226616, acc 0.90625\n",
      "2017-11-06T07:45:15.677064: step 3292, loss 0.0494554, acc 1\n",
      "2017-11-06T07:45:19.771975: step 3293, loss 0.303181, acc 0.875\n",
      "2017-11-06T07:45:23.754803: step 3294, loss 0.654184, acc 0.71875\n",
      "2017-11-06T07:45:27.750643: step 3295, loss 0.207971, acc 0.90625\n",
      "2017-11-06T07:45:31.672430: step 3296, loss 0.143393, acc 0.96875\n",
      "2017-11-06T07:45:35.664266: step 3297, loss 0.269142, acc 0.875\n",
      "2017-11-06T07:45:39.619076: step 3298, loss 0.174426, acc 0.9375\n",
      "2017-11-06T07:45:43.627924: step 3299, loss 0.364571, acc 0.84375\n",
      "2017-11-06T07:45:47.574576: step 3300, loss 0.223276, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:45:50.129389: step 3300, loss 0.796325, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-06T07:45:55.796464: step 3301, loss 0.375271, acc 0.8125\n",
      "2017-11-06T07:45:59.709245: step 3302, loss 0.130695, acc 0.9375\n",
      "2017-11-06T07:46:03.844182: step 3303, loss 0.0614039, acc 0.96875\n",
      "2017-11-06T07:46:07.921079: step 3304, loss 0.207367, acc 0.90625\n",
      "2017-11-06T07:46:11.897905: step 3305, loss 0.270404, acc 0.90625\n",
      "2017-11-06T07:46:15.946783: step 3306, loss 0.145145, acc 0.9375\n",
      "2017-11-06T07:46:20.266852: step 3307, loss 0.378134, acc 0.875\n",
      "2017-11-06T07:46:24.507866: step 3308, loss 0.181744, acc 0.90625\n",
      "2017-11-06T07:46:28.517714: step 3309, loss 0.316282, acc 0.875\n",
      "2017-11-06T07:46:32.531566: step 3310, loss 0.585638, acc 0.8125\n",
      "2017-11-06T07:46:36.854637: step 3311, loss 0.0489911, acc 0.96875\n",
      "2017-11-06T07:46:39.476500: step 3312, loss 0.139123, acc 0.95\n",
      "2017-11-06T07:46:43.536386: step 3313, loss 0.239817, acc 0.875\n",
      "2017-11-06T07:46:47.632295: step 3314, loss 0.249303, acc 0.90625\n",
      "2017-11-06T07:46:51.671166: step 3315, loss 0.182439, acc 0.90625\n",
      "2017-11-06T07:46:55.737055: step 3316, loss 0.202984, acc 0.90625\n",
      "2017-11-06T07:46:59.759912: step 3317, loss 0.119512, acc 0.90625\n",
      "2017-11-06T07:47:03.866831: step 3318, loss 0.073715, acc 1\n",
      "2017-11-06T07:47:07.955736: step 3319, loss 0.118658, acc 0.9375\n",
      "2017-11-06T07:47:12.044641: step 3320, loss 0.220253, acc 0.875\n",
      "2017-11-06T07:47:16.124540: step 3321, loss 0.306166, acc 0.84375\n",
      "2017-11-06T07:47:20.120380: step 3322, loss 0.212406, acc 0.875\n",
      "2017-11-06T07:47:24.414431: step 3323, loss 0.204272, acc 0.875\n",
      "2017-11-06T07:47:28.808555: step 3324, loss 0.391937, acc 0.8125\n",
      "2017-11-06T07:47:32.824407: step 3325, loss 0.229534, acc 0.90625\n",
      "2017-11-06T07:47:36.865278: step 3326, loss 0.13411, acc 0.9375\n",
      "2017-11-06T07:47:40.850109: step 3327, loss 0.0726904, acc 1\n",
      "2017-11-06T07:47:44.926005: step 3328, loss 0.45639, acc 0.8125\n",
      "2017-11-06T07:47:48.911837: step 3329, loss 0.13879, acc 0.9375\n",
      "2017-11-06T07:47:52.907677: step 3330, loss 0.294143, acc 0.90625\n",
      "2017-11-06T07:47:56.911522: step 3331, loss 0.120019, acc 0.9375\n",
      "2017-11-06T07:48:00.899355: step 3332, loss 0.105334, acc 0.9375\n",
      "2017-11-06T07:48:04.884186: step 3333, loss 0.209158, acc 0.875\n",
      "2017-11-06T07:48:09.033134: step 3334, loss 0.162431, acc 0.9375\n",
      "2017-11-06T07:48:13.049989: step 3335, loss 0.274417, acc 0.875\n",
      "2017-11-06T07:48:17.064841: step 3336, loss 0.101298, acc 0.9375\n",
      "2017-11-06T07:48:21.064683: step 3337, loss 0.0903281, acc 0.96875\n",
      "2017-11-06T07:48:25.223640: step 3338, loss 0.277974, acc 0.8125\n",
      "2017-11-06T07:48:29.700819: step 3339, loss 0.119959, acc 0.96875\n",
      "2017-11-06T07:48:34.634326: step 3340, loss 0.198319, acc 0.90625\n",
      "2017-11-06T07:48:39.010435: step 3341, loss 0.421959, acc 0.8125\n",
      "2017-11-06T07:48:43.201412: step 3342, loss 0.255269, acc 0.84375\n",
      "2017-11-06T07:48:47.273124: step 3343, loss 0.288207, acc 0.8125\n",
      "2017-11-06T07:48:51.282973: step 3344, loss 0.199365, acc 0.9375\n",
      "2017-11-06T07:48:55.715703: step 3345, loss 0.396004, acc 0.75\n",
      "2017-11-06T07:48:59.658505: step 3346, loss 0.252783, acc 0.875\n",
      "2017-11-06T07:49:03.652343: step 3347, loss 0.00677555, acc 1\n",
      "2017-11-06T07:49:06.201154: step 3348, loss 0.148275, acc 0.95\n",
      "2017-11-06T07:49:10.183984: step 3349, loss 0.401034, acc 0.90625\n",
      "2017-11-06T07:49:14.171817: step 3350, loss 0.073209, acc 0.96875\n",
      "2017-11-06T07:49:18.207685: step 3351, loss 0.265928, acc 0.875\n",
      "2017-11-06T07:49:22.148486: step 3352, loss 0.155409, acc 0.875\n",
      "2017-11-06T07:49:26.164338: step 3353, loss 0.348776, acc 0.875\n",
      "2017-11-06T07:49:30.112145: step 3354, loss 0.131669, acc 0.9375\n",
      "2017-11-06T07:49:34.059948: step 3355, loss 0.185637, acc 0.90625\n",
      "2017-11-06T07:49:38.571154: step 3356, loss 0.211881, acc 0.84375\n",
      "2017-11-06T07:49:42.607021: step 3357, loss 0.30581, acc 0.90625\n",
      "2017-11-06T07:49:46.641888: step 3358, loss 0.0907016, acc 0.9375\n",
      "2017-11-06T07:49:50.697770: step 3359, loss 0.0653345, acc 1\n",
      "2017-11-06T07:49:54.670593: step 3360, loss 0.0739491, acc 0.96875\n",
      "2017-11-06T07:49:58.619401: step 3361, loss 0.129884, acc 0.90625\n",
      "2017-11-06T07:50:02.866418: step 3362, loss 0.490254, acc 0.65625\n",
      "2017-11-06T07:50:06.884273: step 3363, loss 0.182957, acc 0.90625\n",
      "2017-11-06T07:50:10.899124: step 3364, loss 0.0225702, acc 1\n",
      "2017-11-06T07:50:14.956007: step 3365, loss 0.0819294, acc 0.9375\n",
      "2017-11-06T07:50:18.887802: step 3366, loss 0.236921, acc 0.875\n",
      "2017-11-06T07:50:22.912661: step 3367, loss 0.25994, acc 0.90625\n",
      "2017-11-06T07:50:26.962538: step 3368, loss 0.150881, acc 0.90625\n",
      "2017-11-06T07:50:30.891330: step 3369, loss 0.154417, acc 0.875\n",
      "2017-11-06T07:50:35.169370: step 3370, loss 0.168747, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:50:39.198232: step 3371, loss 0.097333, acc 0.9375\n",
      "2017-11-06T07:50:43.649395: step 3372, loss 0.309445, acc 0.875\n",
      "2017-11-06T07:50:47.798343: step 3373, loss 0.0421177, acc 1\n",
      "2017-11-06T07:50:51.843216: step 3374, loss 0.199257, acc 0.875\n",
      "2017-11-06T07:50:56.026614: step 3375, loss 0.252862, acc 0.84375\n",
      "2017-11-06T07:50:59.988430: step 3376, loss 0.0477126, acc 1\n",
      "2017-11-06T07:51:03.940238: step 3377, loss 0.310624, acc 0.90625\n",
      "2017-11-06T07:51:07.998120: step 3378, loss 0.169414, acc 0.9375\n",
      "2017-11-06T07:51:12.031986: step 3379, loss 0.306586, acc 0.90625\n",
      "2017-11-06T07:51:16.036832: step 3380, loss 0.0724679, acc 0.96875\n",
      "2017-11-06T07:51:19.952614: step 3381, loss 0.273415, acc 0.84375\n",
      "2017-11-06T07:51:24.034514: step 3382, loss 0.1397, acc 0.9375\n",
      "2017-11-06T07:51:28.045364: step 3383, loss 0.349704, acc 0.8125\n",
      "2017-11-06T07:51:30.568157: step 3384, loss 0.327992, acc 0.8\n",
      "2017-11-06T07:51:34.557991: step 3385, loss 0.419732, acc 0.78125\n",
      "2017-11-06T07:51:38.571844: step 3386, loss 0.0990253, acc 0.96875\n",
      "2017-11-06T07:51:42.521650: step 3387, loss 0.106926, acc 0.96875\n",
      "2017-11-06T07:51:46.653344: step 3388, loss 0.301967, acc 0.84375\n",
      "2017-11-06T07:51:50.969411: step 3389, loss 0.233865, acc 0.875\n",
      "2017-11-06T07:51:55.001275: step 3390, loss 0.0977625, acc 0.96875\n",
      "2017-11-06T07:51:58.976100: step 3391, loss 0.395669, acc 0.8125\n",
      "2017-11-06T07:52:02.961932: step 3392, loss 0.164622, acc 0.90625\n",
      "2017-11-06T07:52:06.993797: step 3393, loss 0.157642, acc 0.875\n",
      "2017-11-06T07:52:10.983632: step 3394, loss 0.178014, acc 0.84375\n",
      "2017-11-06T07:52:15.063530: step 3395, loss 0.135017, acc 0.9375\n",
      "2017-11-06T07:52:19.040356: step 3396, loss 0.0227157, acc 1\n",
      "2017-11-06T07:52:23.004173: step 3397, loss 0.295811, acc 0.8125\n",
      "2017-11-06T07:52:27.141112: step 3398, loss 0.184212, acc 0.9375\n",
      "2017-11-06T07:52:31.159968: step 3399, loss 0.104882, acc 0.9375\n",
      "2017-11-06T07:52:35.344942: step 3400, loss 0.205797, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:52:37.923776: step 3400, loss 0.88313, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-06T07:52:43.528700: step 3401, loss 0.335965, acc 0.8125\n",
      "2017-11-06T07:52:47.502524: step 3402, loss 0.207508, acc 0.90625\n",
      "2017-11-06T07:52:51.653474: step 3403, loss 0.142538, acc 0.9375\n",
      "2017-11-06T07:52:56.031584: step 3404, loss 0.0862245, acc 0.96875\n",
      "2017-11-06T07:53:00.056444: step 3405, loss 0.303516, acc 0.84375\n",
      "2017-11-06T07:53:04.002248: step 3406, loss 0.307009, acc 0.8125\n",
      "2017-11-06T07:53:08.042119: step 3407, loss 0.146421, acc 0.9375\n",
      "2017-11-06T07:53:12.052968: step 3408, loss 0.253016, acc 0.90625\n",
      "2017-11-06T07:53:16.111853: step 3409, loss 0.171439, acc 0.875\n",
      "2017-11-06T07:53:20.088679: step 3410, loss 0.121858, acc 0.9375\n",
      "2017-11-06T07:53:24.199599: step 3411, loss 0.12088, acc 0.96875\n",
      "2017-11-06T07:53:28.330534: step 3412, loss 0.27061, acc 0.84375\n",
      "2017-11-06T07:53:32.295352: step 3413, loss 0.0663727, acc 0.96875\n",
      "2017-11-06T07:53:36.309204: step 3414, loss 0.354289, acc 0.78125\n",
      "2017-11-06T07:53:40.287030: step 3415, loss 0.318592, acc 0.875\n",
      "2017-11-06T07:53:44.251847: step 3416, loss 0.362153, acc 0.84375\n",
      "2017-11-06T07:53:48.276707: step 3417, loss 0.209983, acc 0.875\n",
      "2017-11-06T07:53:52.268543: step 3418, loss 0.0877771, acc 0.96875\n",
      "2017-11-06T07:53:56.309414: step 3419, loss 0.217903, acc 0.96875\n",
      "2017-11-06T07:53:59.091391: step 3420, loss 0.352951, acc 0.75\n",
      "2017-11-06T07:54:03.305385: step 3421, loss 0.0805359, acc 0.96875\n",
      "2017-11-06T07:54:07.358265: step 3422, loss 0.151235, acc 0.90625\n",
      "2017-11-06T07:54:11.448171: step 3423, loss 0.127069, acc 0.96875\n",
      "2017-11-06T07:54:15.575104: step 3424, loss 0.249247, acc 0.90625\n",
      "2017-11-06T07:54:19.589956: step 3425, loss 0.295917, acc 0.84375\n",
      "2017-11-06T07:54:23.621821: step 3426, loss 0.177865, acc 0.9375\n",
      "2017-11-06T07:54:27.727739: step 3427, loss 0.152129, acc 0.9375\n",
      "2017-11-06T07:54:31.754600: step 3428, loss 0.116124, acc 0.9375\n",
      "2017-11-06T07:54:35.990610: step 3429, loss 0.392587, acc 0.78125\n",
      "2017-11-06T07:54:40.069508: step 3430, loss 0.188337, acc 0.875\n",
      "2017-11-06T07:54:44.110379: step 3431, loss 0.0624635, acc 0.96875\n",
      "2017-11-06T07:54:48.211141: step 3432, loss 0.128336, acc 0.90625\n",
      "2017-11-06T07:54:52.265021: step 3433, loss 0.0514775, acc 0.96875\n",
      "2017-11-06T07:54:56.357929: step 3434, loss 0.060978, acc 0.96875\n",
      "2017-11-06T07:55:00.394798: step 3435, loss 0.0894699, acc 0.96875\n",
      "2017-11-06T07:55:04.641815: step 3436, loss 0.299445, acc 0.875\n",
      "2017-11-06T07:55:08.965887: step 3437, loss 0.1232, acc 0.96875\n",
      "2017-11-06T07:55:13.003756: step 3438, loss 0.281081, acc 0.90625\n",
      "2017-11-06T07:55:17.135692: step 3439, loss 0.384104, acc 0.84375\n",
      "2017-11-06T07:55:21.124528: step 3440, loss 0.182386, acc 0.9375\n",
      "2017-11-06T07:55:25.210430: step 3441, loss 0.196459, acc 0.90625\n",
      "2017-11-06T07:55:29.216276: step 3442, loss 0.169202, acc 0.9375\n",
      "2017-11-06T07:55:33.216118: step 3443, loss 0.4323, acc 0.78125\n",
      "2017-11-06T07:55:37.236976: step 3444, loss 0.182673, acc 0.9375\n",
      "2017-11-06T07:55:41.262836: step 3445, loss 0.0869523, acc 0.9375\n",
      "2017-11-06T07:55:45.253671: step 3446, loss 0.174379, acc 0.875\n",
      "2017-11-06T07:55:49.325565: step 3447, loss 0.1744, acc 0.96875\n",
      "2017-11-06T07:55:53.340417: step 3448, loss 0.244887, acc 0.875\n",
      "2017-11-06T07:55:57.333254: step 3449, loss 0.337099, acc 0.875\n",
      "2017-11-06T07:56:01.269051: step 3450, loss 0.36155, acc 0.78125\n",
      "2017-11-06T07:56:05.294911: step 3451, loss 0.25566, acc 0.84375\n",
      "2017-11-06T07:56:09.579957: step 3452, loss 0.291355, acc 0.78125\n",
      "2017-11-06T07:56:13.824972: step 3453, loss 0.0585143, acc 1\n",
      "2017-11-06T07:56:17.915879: step 3454, loss 0.329832, acc 0.8125\n",
      "2017-11-06T07:56:22.009788: step 3455, loss 0.305835, acc 0.78125\n",
      "2017-11-06T07:56:24.556598: step 3456, loss 0.189205, acc 0.9\n",
      "2017-11-06T07:56:28.608477: step 3457, loss 0.101463, acc 0.96875\n",
      "2017-11-06T07:56:32.660356: step 3458, loss 0.202303, acc 0.9375\n",
      "2017-11-06T07:56:36.797296: step 3459, loss 0.173985, acc 0.90625\n",
      "2017-11-06T07:56:40.827159: step 3460, loss 0.259478, acc 0.84375\n",
      "2017-11-06T07:56:44.900052: step 3461, loss 0.124941, acc 0.9375\n",
      "2017-11-06T07:56:48.852861: step 3462, loss 0.203575, acc 0.9375\n",
      "2017-11-06T07:56:52.905741: step 3463, loss 0.144662, acc 0.90625\n",
      "2017-11-06T07:56:56.874561: step 3464, loss 0.0949991, acc 0.9375\n",
      "2017-11-06T07:57:00.981890: step 3465, loss 0.301982, acc 0.84375\n",
      "2017-11-06T07:57:05.008751: step 3466, loss 0.14281, acc 0.9375\n",
      "2017-11-06T07:57:09.101660: step 3467, loss 0.221025, acc 0.875\n",
      "2017-11-06T07:57:13.147534: step 3468, loss 0.117616, acc 0.9375\n",
      "2017-11-06T07:57:17.608704: step 3469, loss 0.208279, acc 0.875\n",
      "2017-11-06T07:57:21.794678: step 3470, loss 0.440081, acc 0.84375\n",
      "2017-11-06T07:57:25.859567: step 3471, loss 0.245074, acc 0.8125\n",
      "2017-11-06T07:57:29.949473: step 3472, loss 0.361484, acc 0.8125\n",
      "2017-11-06T07:57:33.968328: step 3473, loss 0.189233, acc 0.84375\n",
      "2017-11-06T07:57:37.971172: step 3474, loss 0.125413, acc 0.90625\n",
      "2017-11-06T07:57:42.029056: step 3475, loss 0.166963, acc 0.90625\n",
      "2017-11-06T07:57:46.057918: step 3476, loss 0.0654431, acc 0.96875\n",
      "2017-11-06T07:57:50.133596: step 3477, loss 0.0682595, acc 0.96875\n",
      "2017-11-06T07:57:54.198484: step 3478, loss 0.398357, acc 0.84375\n",
      "2017-11-06T07:57:58.240355: step 3479, loss 0.181037, acc 0.90625\n",
      "2017-11-06T07:58:02.262214: step 3480, loss 0.218167, acc 0.90625\n",
      "2017-11-06T07:58:06.266058: step 3481, loss 0.283198, acc 0.875\n",
      "2017-11-06T07:58:10.325943: step 3482, loss 0.115815, acc 0.9375\n",
      "2017-11-06T07:58:14.307772: step 3483, loss 0.234003, acc 0.875\n",
      "2017-11-06T07:58:18.338636: step 3484, loss 0.374067, acc 0.875\n",
      "2017-11-06T07:58:22.841836: step 3485, loss 0.176619, acc 0.9375\n",
      "2017-11-06T07:58:26.995789: step 3486, loss 0.3018, acc 0.84375\n",
      "2017-11-06T07:58:30.977616: step 3487, loss 0.324803, acc 0.84375\n",
      "2017-11-06T07:58:35.151583: step 3488, loss 0.231074, acc 0.90625\n",
      "2017-11-06T07:58:39.250495: step 3489, loss 0.116802, acc 0.96875\n",
      "2017-11-06T07:58:43.335397: step 3490, loss 0.147848, acc 0.9375\n",
      "2017-11-06T07:58:47.363260: step 3491, loss 0.100455, acc 0.90625\n",
      "2017-11-06T07:58:49.989125: step 3492, loss 0.288704, acc 0.8\n",
      "2017-11-06T07:58:54.118059: step 3493, loss 0.208979, acc 0.9375\n",
      "2017-11-06T07:58:58.177945: step 3494, loss 0.254907, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T07:59:02.336067: step 3495, loss 0.217895, acc 0.90625\n",
      "2017-11-06T07:59:06.350922: step 3496, loss 0.312505, acc 0.90625\n",
      "2017-11-06T07:59:10.382785: step 3497, loss 0.0899674, acc 0.96875\n",
      "2017-11-06T07:59:14.499712: step 3498, loss 0.148479, acc 0.9375\n",
      "2017-11-06T07:59:18.439510: step 3499, loss 0.113339, acc 0.9375\n",
      "2017-11-06T07:59:22.578451: step 3500, loss 0.273234, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T07:59:25.516538: step 3500, loss 0.810419, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-06T07:59:31.332055: step 3501, loss 0.166104, acc 0.90625\n",
      "2017-11-06T07:59:35.385934: step 3502, loss 0.226245, acc 0.84375\n",
      "2017-11-06T07:59:39.433811: step 3503, loss 0.301684, acc 0.8125\n",
      "2017-11-06T07:59:43.448663: step 3504, loss 0.121482, acc 0.9375\n",
      "2017-11-06T07:59:47.483530: step 3505, loss 0.23514, acc 0.84375\n",
      "2017-11-06T07:59:51.574436: step 3506, loss 0.168644, acc 0.96875\n",
      "2017-11-06T07:59:55.597295: step 3507, loss 0.196259, acc 0.9375\n",
      "2017-11-06T07:59:59.610147: step 3508, loss 0.290733, acc 0.9375\n",
      "2017-11-06T08:00:03.866170: step 3509, loss 0.421127, acc 0.84375\n",
      "2017-11-06T08:00:07.863012: step 3510, loss 0.365121, acc 0.875\n",
      "2017-11-06T08:00:11.884868: step 3511, loss 0.229874, acc 0.90625\n",
      "2017-11-06T08:00:15.925739: step 3512, loss 0.185289, acc 0.9375\n",
      "2017-11-06T08:00:19.897562: step 3513, loss 0.184232, acc 0.90625\n",
      "2017-11-06T08:00:23.982464: step 3514, loss 0.042446, acc 0.96875\n",
      "2017-11-06T08:00:28.017331: step 3515, loss 0.280814, acc 0.8125\n",
      "2017-11-06T08:00:32.381433: step 3516, loss 0.212657, acc 0.9375\n",
      "2017-11-06T08:00:36.783560: step 3517, loss 0.324317, acc 0.875\n",
      "2017-11-06T08:00:40.781400: step 3518, loss 0.375825, acc 0.875\n",
      "2017-11-06T08:00:44.718197: step 3519, loss 0.360125, acc 0.78125\n",
      "2017-11-06T08:00:48.794862: step 3520, loss 0.167066, acc 0.9375\n",
      "2017-11-06T08:00:52.745671: step 3521, loss 0.148065, acc 0.90625\n",
      "2017-11-06T08:00:56.707486: step 3522, loss 0.218117, acc 0.875\n",
      "2017-11-06T08:01:00.750358: step 3523, loss 0.269115, acc 0.875\n",
      "2017-11-06T08:01:04.675146: step 3524, loss 0.216257, acc 0.90625\n",
      "2017-11-06T08:01:08.728025: step 3525, loss 0.369768, acc 0.84375\n",
      "2017-11-06T08:01:12.729869: step 3526, loss 0.245365, acc 0.9375\n",
      "2017-11-06T08:01:16.681677: step 3527, loss 0.154971, acc 0.9375\n",
      "2017-11-06T08:01:19.304543: step 3528, loss 0.300089, acc 0.85\n",
      "2017-11-06T08:01:23.391446: step 3529, loss 0.172448, acc 0.9375\n",
      "2017-11-06T08:01:27.425311: step 3530, loss 0.0374166, acc 0.96875\n",
      "2017-11-06T08:01:31.423151: step 3531, loss 0.200954, acc 0.90625\n",
      "2017-11-06T08:01:35.628140: step 3532, loss 0.0716084, acc 0.96875\n",
      "2017-11-06T08:01:40.210395: step 3533, loss 0.0694391, acc 0.96875\n",
      "2017-11-06T08:01:44.302302: step 3534, loss 0.209266, acc 0.875\n",
      "2017-11-06T08:01:48.226091: step 3535, loss 0.17092, acc 0.90625\n",
      "2017-11-06T08:01:52.315996: step 3536, loss 0.312594, acc 0.84375\n",
      "2017-11-06T08:01:56.332851: step 3537, loss 0.284935, acc 0.84375\n",
      "2017-11-06T08:02:00.327690: step 3538, loss 0.0971579, acc 0.9375\n",
      "2017-11-06T08:02:04.307517: step 3539, loss 0.240137, acc 0.875\n",
      "2017-11-06T08:02:08.270333: step 3540, loss 0.176347, acc 0.9375\n",
      "2017-11-06T08:02:12.238152: step 3541, loss 0.317061, acc 0.8125\n",
      "2017-11-06T08:02:16.241997: step 3542, loss 0.440618, acc 0.875\n",
      "2017-11-06T08:02:20.297879: step 3543, loss 0.140912, acc 0.90625\n",
      "2017-11-06T08:02:24.297722: step 3544, loss 0.151665, acc 0.90625\n",
      "2017-11-06T08:02:28.369614: step 3545, loss 0.09057, acc 0.9375\n",
      "2017-11-06T08:02:32.470528: step 3546, loss 0.219089, acc 0.90625\n",
      "2017-11-06T08:02:36.694529: step 3547, loss 0.1342, acc 0.9375\n",
      "2017-11-06T08:02:40.662349: step 3548, loss 0.476973, acc 0.8125\n",
      "2017-11-06T08:02:45.078486: step 3549, loss 0.126171, acc 0.96875\n",
      "2017-11-06T08:02:49.210422: step 3550, loss 0.194788, acc 0.90625\n",
      "2017-11-06T08:02:53.254296: step 3551, loss 0.325794, acc 0.875\n",
      "2017-11-06T08:02:57.242130: step 3552, loss 0.134269, acc 0.90625\n",
      "2017-11-06T08:03:01.290005: step 3553, loss 0.208621, acc 0.84375\n",
      "2017-11-06T08:03:05.248610: step 3554, loss 0.142964, acc 0.9375\n",
      "2017-11-06T08:03:09.257459: step 3555, loss 0.107783, acc 0.9375\n",
      "2017-11-06T08:03:13.252296: step 3556, loss 0.279157, acc 0.875\n",
      "2017-11-06T08:03:17.212111: step 3557, loss 0.519665, acc 0.75\n",
      "2017-11-06T08:03:21.216956: step 3558, loss 0.159043, acc 0.9375\n",
      "2017-11-06T08:03:25.294854: step 3559, loss 0.0950105, acc 0.9375\n",
      "2017-11-06T08:03:29.389764: step 3560, loss 0.178613, acc 0.90625\n",
      "2017-11-06T08:03:33.414623: step 3561, loss 0.280833, acc 0.875\n",
      "2017-11-06T08:03:37.410462: step 3562, loss 0.159708, acc 0.875\n",
      "2017-11-06T08:03:41.655478: step 3563, loss 0.426453, acc 0.78125\n",
      "2017-11-06T08:03:44.303360: step 3564, loss 0.115167, acc 0.95\n",
      "2017-11-06T08:03:48.595178: step 3565, loss 0.24048, acc 0.8125\n",
      "2017-11-06T08:03:53.045340: step 3566, loss 0.197772, acc 0.875\n",
      "2017-11-06T08:03:57.178276: step 3567, loss 0.231642, acc 0.875\n",
      "2017-11-06T08:04:01.300206: step 3568, loss 0.18901, acc 0.875\n",
      "2017-11-06T08:04:05.424135: step 3569, loss 0.191244, acc 0.90625\n",
      "2017-11-06T08:04:09.523048: step 3570, loss 0.185717, acc 0.9375\n",
      "2017-11-06T08:04:13.638972: step 3571, loss 0.224606, acc 0.84375\n",
      "2017-11-06T08:04:17.746892: step 3572, loss 0.062091, acc 0.96875\n",
      "2017-11-06T08:04:21.796769: step 3573, loss 0.335568, acc 0.8125\n",
      "2017-11-06T08:04:25.928705: step 3574, loss 0.219231, acc 0.875\n",
      "2017-11-06T08:04:30.074651: step 3575, loss 0.097476, acc 0.9375\n",
      "2017-11-06T08:04:34.336678: step 3576, loss 0.375605, acc 0.8125\n",
      "2017-11-06T08:04:38.472618: step 3577, loss 0.19996, acc 0.875\n",
      "2017-11-06T08:04:42.584539: step 3578, loss 0.25939, acc 0.90625\n",
      "2017-11-06T08:04:46.811542: step 3579, loss 0.196812, acc 0.90625\n",
      "2017-11-06T08:04:50.937474: step 3580, loss 0.349841, acc 0.84375\n",
      "2017-11-06T08:04:55.393640: step 3581, loss 0.283026, acc 0.8125\n",
      "2017-11-06T08:04:59.691694: step 3582, loss 0.351653, acc 0.84375\n",
      "2017-11-06T08:05:03.914695: step 3583, loss 0.128666, acc 0.9375\n",
      "2017-11-06T08:05:07.878536: step 3584, loss 0.16744, acc 0.9375\n",
      "2017-11-06T08:05:11.884383: step 3585, loss 0.263936, acc 0.875\n",
      "2017-11-06T08:05:16.005310: step 3586, loss 0.147543, acc 0.90625\n",
      "2017-11-06T08:05:19.973129: step 3587, loss 0.0573781, acc 0.96875\n",
      "2017-11-06T08:05:23.941950: step 3588, loss 0.213862, acc 0.875\n",
      "2017-11-06T08:05:27.936788: step 3589, loss 0.30457, acc 0.8125\n",
      "2017-11-06T08:05:31.991670: step 3590, loss 0.197397, acc 0.90625\n",
      "2017-11-06T08:05:35.995516: step 3591, loss 0.179241, acc 0.96875\n",
      "2017-11-06T08:05:40.017372: step 3592, loss 0.389805, acc 0.84375\n",
      "2017-11-06T08:05:44.054240: step 3593, loss 0.256596, acc 0.90625\n",
      "2017-11-06T08:05:48.012053: step 3594, loss 0.200848, acc 0.9375\n",
      "2017-11-06T08:05:52.059929: step 3595, loss 0.141877, acc 0.9375\n",
      "2017-11-06T08:05:56.054768: step 3596, loss 0.314496, acc 0.875\n",
      "2017-11-06T08:06:00.287775: step 3597, loss 0.178012, acc 0.9375\n",
      "2017-11-06T08:06:04.595836: step 3598, loss 0.217972, acc 0.84375\n",
      "2017-11-06T08:06:08.621696: step 3599, loss 0.209679, acc 0.90625\n",
      "2017-11-06T08:06:11.222545: step 3600, loss 0.198403, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:06:13.907453: step 3600, loss 0.867712, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\5\\1509915773\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C3730CF8>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\n",
      "\n",
      "2017-11-06T08:06:24.287689: step 1, loss 1.37569, acc 0.6875\n",
      "2017-11-06T08:06:28.307546: step 2, loss 0.738376, acc 0.875\n",
      "2017-11-06T08:06:32.410461: step 3, loss 1.16111, acc 0.8125\n",
      "2017-11-06T08:06:36.628458: step 4, loss 1.02457, acc 0.84375\n",
      "2017-11-06T08:06:40.620294: step 5, loss 0.0935092, acc 0.96875\n",
      "2017-11-06T08:06:44.721208: step 6, loss 0.276914, acc 0.90625\n",
      "2017-11-06T08:06:48.710810: step 7, loss 0.912373, acc 0.9375\n",
      "2017-11-06T08:06:52.730666: step 8, loss 2.20556, acc 0.875\n",
      "2017-11-06T08:06:56.754525: step 9, loss 2.39948, acc 0.90625\n",
      "2017-11-06T08:07:00.821414: step 10, loss 3.24225, acc 0.8125\n",
      "2017-11-06T08:07:04.965359: step 11, loss 0.616088, acc 0.96875\n",
      "2017-11-06T08:07:09.305443: step 12, loss 2.75289, acc 0.8125\n",
      "2017-11-06T08:07:13.329302: step 13, loss 0.881727, acc 0.90625\n",
      "2017-11-06T08:07:17.390188: step 14, loss 3.36774, acc 0.71875\n",
      "2017-11-06T08:07:21.354004: step 15, loss 0.8084, acc 0.90625\n",
      "2017-11-06T08:07:25.387871: step 16, loss 2.41301, acc 0.84375\n",
      "2017-11-06T08:07:29.471772: step 17, loss 0.981516, acc 0.84375\n",
      "2017-11-06T08:07:33.542664: step 18, loss 1.76618, acc 0.71875\n",
      "2017-11-06T08:07:37.604550: step 19, loss 0.888012, acc 0.78125\n",
      "2017-11-06T08:07:41.675443: step 20, loss 1.57789, acc 0.65625\n",
      "2017-11-06T08:07:45.711311: step 21, loss 2.20319, acc 0.59375\n",
      "2017-11-06T08:07:49.693141: step 22, loss 1.38222, acc 0.78125\n",
      "2017-11-06T08:07:53.794054: step 23, loss 1.41061, acc 0.71875\n",
      "2017-11-06T08:07:57.800901: step 24, loss 1.08607, acc 0.78125\n",
      "2017-11-06T08:08:01.786733: step 25, loss 1.02402, acc 0.65625\n",
      "2017-11-06T08:08:05.835610: step 26, loss 1.25514, acc 0.75\n",
      "2017-11-06T08:08:09.932521: step 27, loss 1.42173, acc 0.78125\n",
      "2017-11-06T08:08:14.408702: step 28, loss 0.563656, acc 0.875\n",
      "2017-11-06T08:08:18.451574: step 29, loss 0.807147, acc 0.9375\n",
      "2017-11-06T08:08:22.474434: step 30, loss 1.19935, acc 0.78125\n",
      "2017-11-06T08:08:26.679420: step 31, loss 2.4161, acc 0.8125\n",
      "2017-11-06T08:08:30.750312: step 32, loss 0.672995, acc 0.9375\n",
      "2017-11-06T08:08:34.931284: step 33, loss 0.258248, acc 0.9375\n",
      "2017-11-06T08:08:39.019188: step 34, loss 1.18019, acc 0.90625\n",
      "2017-11-06T08:08:43.028036: step 35, loss 1.64129, acc 0.8125\n",
      "2017-11-06T08:08:45.559836: step 36, loss 2.30308, acc 0.85\n",
      "2017-11-06T08:08:49.545667: step 37, loss 2.18446, acc 0.84375\n",
      "2017-11-06T08:08:53.533501: step 38, loss 0.424734, acc 0.90625\n",
      "2017-11-06T08:08:57.568368: step 39, loss 1.06911, acc 0.84375\n",
      "2017-11-06T08:09:01.514173: step 40, loss 0.347919, acc 0.90625\n",
      "2017-11-06T08:09:05.521018: step 41, loss 1.22517, acc 0.75\n",
      "2017-11-06T08:09:09.487837: step 42, loss 1.67359, acc 0.78125\n",
      "2017-11-06T08:09:13.548723: step 43, loss 1.11421, acc 0.6875\n",
      "2017-11-06T08:09:17.858786: step 44, loss 0.390445, acc 0.84375\n",
      "2017-11-06T08:09:22.039756: step 45, loss 0.806737, acc 0.875\n",
      "2017-11-06T08:09:26.093636: step 46, loss 0.425563, acc 0.875\n",
      "2017-11-06T08:09:30.130505: step 47, loss 0.548756, acc 0.875\n",
      "2017-11-06T08:09:34.140354: step 48, loss 1.22596, acc 0.84375\n",
      "2017-11-06T08:09:38.235264: step 49, loss 0.966155, acc 0.8125\n",
      "2017-11-06T08:09:42.248115: step 50, loss 1.20967, acc 0.90625\n",
      "2017-11-06T08:09:46.248958: step 51, loss 1.93403, acc 0.75\n",
      "2017-11-06T08:09:50.282592: step 52, loss 0.895859, acc 0.84375\n",
      "2017-11-06T08:09:54.278431: step 53, loss 0.886406, acc 0.8125\n",
      "2017-11-06T08:09:58.279273: step 54, loss 0.923946, acc 0.84375\n",
      "2017-11-06T08:10:02.626362: step 55, loss 1.16186, acc 0.8125\n",
      "2017-11-06T08:10:06.636211: step 56, loss 0.750661, acc 0.875\n",
      "2017-11-06T08:10:10.641057: step 57, loss 0.479049, acc 0.875\n",
      "2017-11-06T08:10:14.675924: step 58, loss 0.284542, acc 0.9375\n",
      "2017-11-06T08:10:18.637740: step 59, loss 0.272617, acc 0.9375\n",
      "2017-11-06T08:10:22.884757: step 60, loss 1.17085, acc 0.90625\n",
      "2017-11-06T08:10:27.115763: step 61, loss 1.67616, acc 0.75\n",
      "2017-11-06T08:10:31.170644: step 62, loss 1.68502, acc 0.8125\n",
      "2017-11-06T08:10:35.342609: step 63, loss 0.677993, acc 0.875\n",
      "2017-11-06T08:10:39.425511: step 64, loss 1.74958, acc 0.75\n",
      "2017-11-06T08:10:43.413343: step 65, loss 0.839125, acc 0.875\n",
      "2017-11-06T08:10:47.403178: step 66, loss 0.927293, acc 0.8125\n",
      "2017-11-06T08:10:51.440046: step 67, loss 1.07136, acc 0.8125\n",
      "2017-11-06T08:10:55.676056: step 68, loss 1.11282, acc 0.78125\n",
      "2017-11-06T08:10:59.719930: step 69, loss 1.51701, acc 0.65625\n",
      "2017-11-06T08:11:03.745790: step 70, loss 1.21764, acc 0.78125\n",
      "2017-11-06T08:11:07.762644: step 71, loss 0.834234, acc 0.8125\n",
      "2017-11-06T08:11:10.241405: step 72, loss 0.560636, acc 0.8\n",
      "2017-11-06T08:11:14.295287: step 73, loss 0.297408, acc 0.90625\n",
      "2017-11-06T08:11:18.241090: step 74, loss 0.740994, acc 0.84375\n",
      "2017-11-06T08:11:22.163879: step 75, loss 1.54731, acc 0.84375\n",
      "2017-11-06T08:11:26.220760: step 76, loss 0.698203, acc 0.90625\n",
      "2017-11-06T08:11:30.606876: step 77, loss 0.864111, acc 0.90625\n",
      "2017-11-06T08:11:34.665761: step 78, loss 1.80765, acc 0.8125\n",
      "2017-11-06T08:11:38.692621: step 79, loss 0.560452, acc 0.9375\n",
      "2017-11-06T08:11:42.672449: step 80, loss 0.0233211, acc 1\n",
      "2017-11-06T08:11:46.628262: step 81, loss 0.623354, acc 0.84375\n",
      "2017-11-06T08:11:50.669131: step 82, loss 0.292935, acc 0.90625\n",
      "2017-11-06T08:11:54.664971: step 83, loss 0.636992, acc 0.90625\n",
      "2017-11-06T08:11:58.669816: step 84, loss 1.13845, acc 0.8125\n",
      "2017-11-06T08:12:02.665655: step 85, loss 1.50012, acc 0.8125\n",
      "2017-11-06T08:12:06.627470: step 86, loss 0.80981, acc 0.90625\n",
      "2017-11-06T08:12:10.626312: step 87, loss 0.976901, acc 0.8125\n",
      "2017-11-06T08:12:14.650171: step 88, loss 0.499776, acc 0.875\n",
      "2017-11-06T08:12:18.725067: step 89, loss 0.716517, acc 0.875\n",
      "2017-11-06T08:12:22.715050: step 90, loss 0.666649, acc 0.875\n",
      "2017-11-06T08:12:26.781939: step 91, loss 0.370033, acc 0.90625\n",
      "2017-11-06T08:12:30.779781: step 92, loss 0.730459, acc 0.84375\n",
      "2017-11-06T08:12:35.313001: step 93, loss 0.903567, acc 0.875\n",
      "2017-11-06T08:12:39.458947: step 94, loss 0.221246, acc 0.90625\n",
      "2017-11-06T08:12:43.455788: step 95, loss 0.542299, acc 0.8125\n",
      "2017-11-06T08:12:47.476644: step 96, loss 0.600587, acc 0.96875\n",
      "2017-11-06T08:12:51.532472: step 97, loss 0.268373, acc 0.9375\n",
      "2017-11-06T08:12:55.547324: step 98, loss 1.02728, acc 0.78125\n",
      "2017-11-06T08:12:59.557174: step 99, loss 0.910867, acc 0.84375\n",
      "2017-11-06T08:13:03.541004: step 100, loss 1.00887, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:13:06.142853: step 100, loss 1.38388, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-100\n",
      "\n",
      "2017-11-06T08:13:11.723848: step 101, loss 0.160126, acc 0.9375\n",
      "2017-11-06T08:13:15.755712: step 102, loss 0.689984, acc 0.78125\n",
      "2017-11-06T08:13:19.652481: step 103, loss 1.01248, acc 0.8125\n",
      "2017-11-06T08:13:23.580272: step 104, loss 0.828927, acc 0.84375\n",
      "2017-11-06T08:13:27.545089: step 105, loss 0.744512, acc 0.875\n",
      "2017-11-06T08:13:31.448864: step 106, loss 0.266292, acc 0.9375\n",
      "2017-11-06T08:13:35.385662: step 107, loss 1.00237, acc 0.875\n",
      "2017-11-06T08:13:37.933471: step 108, loss 1.78496, acc 0.85\n",
      "2017-11-06T08:13:42.289567: step 109, loss 0.182593, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T08:13:46.190339: step 110, loss 1.68139, acc 0.75\n",
      "2017-11-06T08:13:50.149151: step 111, loss 1.01387, acc 0.84375\n",
      "2017-11-06T08:13:54.117970: step 112, loss 0.480322, acc 0.875\n",
      "2017-11-06T08:13:58.044780: step 113, loss 0.506968, acc 0.875\n",
      "2017-11-06T08:14:01.971551: step 114, loss 0.457838, acc 0.84375\n",
      "2017-11-06T08:14:05.895339: step 115, loss 0.646398, acc 0.875\n",
      "2017-11-06T08:14:09.869163: step 116, loss 1.18823, acc 0.75\n",
      "2017-11-06T08:14:13.780942: step 117, loss 0.385645, acc 0.90625\n",
      "2017-11-06T08:14:17.719741: step 118, loss 0.741159, acc 0.71875\n",
      "2017-11-06T08:14:21.674550: step 119, loss 0.720045, acc 0.8125\n",
      "2017-11-06T08:14:25.965602: step 120, loss 0.379148, acc 0.90625\n",
      "2017-11-06T08:14:30.082525: step 121, loss 0.663398, acc 0.84375\n",
      "2017-11-06T08:14:34.361565: step 122, loss 0.268111, acc 0.9375\n",
      "2017-11-06T08:14:38.481493: step 123, loss 0.368744, acc 0.875\n",
      "2017-11-06T08:14:42.458318: step 124, loss 0.597094, acc 0.90625\n",
      "2017-11-06T08:14:46.840432: step 125, loss 1.08455, acc 0.84375\n",
      "2017-11-06T08:14:50.968367: step 126, loss 0.899273, acc 0.84375\n",
      "2017-11-06T08:14:54.974211: step 127, loss 0.37755, acc 0.9375\n",
      "2017-11-06T08:14:58.923017: step 128, loss 1.08967, acc 0.84375\n",
      "2017-11-06T08:15:02.869822: step 129, loss 0.525329, acc 0.90625\n",
      "2017-11-06T08:15:06.839643: step 130, loss 0.452708, acc 0.9375\n",
      "2017-11-06T08:15:10.787447: step 131, loss 0.0818531, acc 0.96875\n",
      "2017-11-06T08:15:14.708233: step 132, loss 0.358097, acc 0.875\n",
      "2017-11-06T08:15:18.753108: step 133, loss 0.881525, acc 0.78125\n",
      "2017-11-06T08:15:22.700913: step 134, loss 1.31771, acc 0.84375\n",
      "2017-11-06T08:15:26.624700: step 135, loss 0.209516, acc 0.9375\n",
      "2017-11-06T08:15:30.537481: step 136, loss 1.25858, acc 0.8125\n",
      "2017-11-06T08:15:34.521312: step 137, loss 0.494493, acc 0.9375\n",
      "2017-11-06T08:15:38.478122: step 138, loss 0.507903, acc 0.90625\n",
      "2017-11-06T08:15:42.391903: step 139, loss 0.867442, acc 0.90625\n",
      "2017-11-06T08:15:46.351718: step 140, loss 0.539175, acc 0.84375\n",
      "2017-11-06T08:15:50.505431: step 141, loss 0.787938, acc 0.875\n",
      "2017-11-06T08:15:54.751447: step 142, loss 1.26219, acc 0.78125\n",
      "2017-11-06T08:15:58.717264: step 143, loss 1.23967, acc 0.8125\n",
      "2017-11-06T08:16:01.260071: step 144, loss 1.25009, acc 0.85\n",
      "2017-11-06T08:16:05.221887: step 145, loss 0.835397, acc 0.84375\n",
      "2017-11-06T08:16:09.255753: step 146, loss 1.32547, acc 0.78125\n",
      "2017-11-06T08:16:13.219570: step 147, loss 0.286236, acc 0.9375\n",
      "2017-11-06T08:16:17.317481: step 148, loss 1.48001, acc 0.6875\n",
      "2017-11-06T08:16:21.257280: step 149, loss 0.531286, acc 0.90625\n",
      "2017-11-06T08:16:25.340182: step 150, loss 0.914055, acc 0.78125\n",
      "2017-11-06T08:16:29.669100: step 151, loss 0.623917, acc 0.84375\n",
      "2017-11-06T08:16:33.814047: step 152, loss 1.48498, acc 0.625\n",
      "2017-11-06T08:16:37.872949: step 153, loss 0.562793, acc 0.84375\n",
      "2017-11-06T08:16:41.797718: step 154, loss 0.639646, acc 0.8125\n",
      "2017-11-06T08:16:45.710498: step 155, loss 0.255975, acc 0.84375\n",
      "2017-11-06T08:16:49.655302: step 156, loss 0.197594, acc 0.875\n",
      "2017-11-06T08:16:53.601105: step 157, loss 0.316081, acc 0.9375\n",
      "2017-11-06T08:16:57.900160: step 158, loss 1.31379, acc 0.84375\n",
      "2017-11-06T08:17:02.026092: step 159, loss 0.365593, acc 0.9375\n",
      "2017-11-06T08:17:05.973898: step 160, loss 0.453571, acc 0.875\n",
      "2017-11-06T08:17:09.865662: step 161, loss 1.82244, acc 0.78125\n",
      "2017-11-06T08:17:13.774439: step 162, loss 0.437498, acc 0.9375\n",
      "2017-11-06T08:17:17.762273: step 163, loss 0.502349, acc 0.96875\n",
      "2017-11-06T08:17:21.685060: step 164, loss 0.678288, acc 0.84375\n",
      "2017-11-06T08:17:25.668892: step 165, loss 0.113805, acc 0.9375\n",
      "2017-11-06T08:17:29.660727: step 166, loss 1.29118, acc 0.90625\n",
      "2017-11-06T08:17:33.604529: step 167, loss 1.2877, acc 0.84375\n",
      "2017-11-06T08:17:37.575351: step 168, loss 1.02466, acc 0.78125\n",
      "2017-11-06T08:17:41.604214: step 169, loss 0.371977, acc 0.875\n",
      "2017-11-06T08:17:45.514992: step 170, loss 0.812468, acc 0.8125\n",
      "2017-11-06T08:17:49.553862: step 171, loss 0.458919, acc 0.875\n",
      "2017-11-06T08:17:53.522682: step 172, loss 1.09129, acc 0.8125\n",
      "2017-11-06T08:17:57.595576: step 173, loss 0.552598, acc 0.875\n",
      "2017-11-06T08:18:01.804566: step 174, loss 0.462087, acc 0.875\n",
      "2017-11-06T08:18:06.058590: step 175, loss 0.762892, acc 0.84375\n",
      "2017-11-06T08:18:10.041420: step 176, loss 0.152227, acc 0.9375\n",
      "2017-11-06T08:18:14.029253: step 177, loss 0.840497, acc 0.8125\n",
      "2017-11-06T08:18:18.032097: step 178, loss 0.613138, acc 0.84375\n",
      "2017-11-06T08:18:22.066965: step 179, loss 0.613711, acc 0.90625\n",
      "2017-11-06T08:18:24.690828: step 180, loss 0.78443, acc 0.85\n",
      "2017-11-06T08:18:28.660649: step 181, loss 0.688217, acc 0.875\n",
      "2017-11-06T08:18:32.751722: step 182, loss 1.17191, acc 0.84375\n",
      "2017-11-06T08:18:36.822614: step 183, loss 0.281405, acc 0.90625\n",
      "2017-11-06T08:18:40.748403: step 184, loss 0.304548, acc 0.9375\n",
      "2017-11-06T08:18:44.714222: step 185, loss 0.754317, acc 0.90625\n",
      "2017-11-06T08:18:48.692048: step 186, loss 0.560215, acc 0.9375\n",
      "2017-11-06T08:18:52.653656: step 187, loss 1.94114, acc 0.78125\n",
      "2017-11-06T08:18:56.656499: step 188, loss 1.28291, acc 0.90625\n",
      "2017-11-06T08:19:00.633326: step 189, loss 0.437362, acc 0.90625\n",
      "2017-11-06T08:19:04.649180: step 190, loss 0.356094, acc 0.9375\n",
      "2017-11-06T08:19:08.937226: step 191, loss 0.0374526, acc 1\n",
      "2017-11-06T08:19:13.074164: step 192, loss 0.291475, acc 0.9375\n",
      "2017-11-06T08:19:17.075007: step 193, loss 0.451775, acc 0.90625\n",
      "2017-11-06T08:19:21.022813: step 194, loss 0.283514, acc 0.90625\n",
      "2017-11-06T08:19:25.020653: step 195, loss 0.285634, acc 0.875\n",
      "2017-11-06T08:19:29.012491: step 196, loss 0.415516, acc 0.875\n",
      "2017-11-06T08:19:32.947285: step 197, loss 0.623024, acc 0.78125\n",
      "2017-11-06T08:19:36.863067: step 198, loss 0.523376, acc 0.875\n",
      "2017-11-06T08:19:40.848901: step 199, loss 0.0187852, acc 1\n",
      "2017-11-06T08:19:44.787699: step 200, loss 0.707811, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:19:47.315494: step 200, loss 1.42554, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-200\n",
      "\n",
      "2017-11-06T08:19:52.894736: step 201, loss 0.455966, acc 0.84375\n",
      "2017-11-06T08:19:56.867559: step 202, loss 0.340331, acc 0.875\n",
      "2017-11-06T08:20:00.951461: step 203, loss 0.834117, acc 0.78125\n",
      "2017-11-06T08:20:04.989331: step 204, loss 0.90179, acc 0.875\n",
      "2017-11-06T08:20:08.886099: step 205, loss 0.399182, acc 0.9375\n",
      "2017-11-06T08:20:13.093088: step 206, loss 0.976297, acc 0.78125\n",
      "2017-11-06T08:20:17.288069: step 207, loss 1.28315, acc 0.75\n",
      "2017-11-06T08:20:21.307925: step 208, loss 0.404968, acc 0.84375\n",
      "2017-11-06T08:20:25.237717: step 209, loss 0.336167, acc 0.90625\n",
      "2017-11-06T08:20:29.171513: step 210, loss 0.471408, acc 0.90625\n",
      "2017-11-06T08:20:33.274428: step 211, loss 1.00761, acc 0.875\n",
      "2017-11-06T08:20:37.394356: step 212, loss 0.526186, acc 0.90625\n",
      "2017-11-06T08:20:41.334155: step 213, loss 0.239896, acc 0.9375\n",
      "2017-11-06T08:20:45.309979: step 214, loss 0.0589197, acc 0.96875\n",
      "2017-11-06T08:20:49.341864: step 215, loss 0.381144, acc 0.90625\n",
      "2017-11-06T08:20:51.850627: step 216, loss 0.738474, acc 0.75\n",
      "2017-11-06T08:20:55.809440: step 217, loss 0.741093, acc 0.8125\n",
      "2017-11-06T08:20:59.743237: step 218, loss 0.555497, acc 0.84375\n",
      "2017-11-06T08:21:03.730068: step 219, loss 0.0945077, acc 0.96875\n",
      "2017-11-06T08:21:07.683877: step 220, loss 0.98837, acc 0.84375\n",
      "2017-11-06T08:21:11.556629: step 221, loss 0.746352, acc 0.90625\n",
      "2017-11-06T08:21:15.556472: step 222, loss 0.0239521, acc 1\n",
      "2017-11-06T08:21:19.924575: step 223, loss 0.254754, acc 0.90625\n",
      "2017-11-06T08:21:24.101543: step 224, loss 0.938104, acc 0.875\n",
      "2017-11-06T08:21:27.998311: step 225, loss 0.442897, acc 0.84375\n",
      "2017-11-06T08:21:31.963129: step 226, loss 0.489736, acc 0.90625\n",
      "2017-11-06T08:21:35.896924: step 227, loss 0.535013, acc 0.84375\n",
      "2017-11-06T08:21:39.908775: step 228, loss 0.322075, acc 0.90625\n",
      "2017-11-06T08:21:43.824557: step 229, loss 1.38853, acc 0.8125\n",
      "2017-11-06T08:21:47.814393: step 230, loss 0.146199, acc 0.90625\n",
      "2017-11-06T08:21:51.779962: step 231, loss 0.514337, acc 0.8125\n",
      "2017-11-06T08:21:55.761789: step 232, loss 1.38101, acc 0.8125\n",
      "2017-11-06T08:21:59.732611: step 233, loss 0.860485, acc 0.78125\n",
      "2017-11-06T08:22:03.731452: step 234, loss 0.765315, acc 0.875\n",
      "2017-11-06T08:22:07.699272: step 235, loss 0.637754, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T08:22:11.667090: step 236, loss 0.501172, acc 0.875\n",
      "2017-11-06T08:22:15.578870: step 237, loss 0.350341, acc 0.9375\n",
      "2017-11-06T08:22:19.519670: step 238, loss 0.56275, acc 0.8125\n",
      "2017-11-06T08:22:23.616581: step 239, loss 0.0887929, acc 1\n",
      "2017-11-06T08:22:27.893622: step 240, loss 0.0696512, acc 0.9375\n",
      "2017-11-06T08:22:31.906471: step 241, loss 0.488308, acc 0.90625\n",
      "2017-11-06T08:22:36.344706: step 242, loss 0.425838, acc 0.875\n",
      "2017-11-06T08:22:40.282506: step 243, loss 0.383709, acc 0.875\n",
      "2017-11-06T08:22:44.215300: step 244, loss 1.21895, acc 0.84375\n",
      "2017-11-06T08:22:48.166107: step 245, loss 0.813864, acc 0.90625\n",
      "2017-11-06T08:22:52.117914: step 246, loss 0.794239, acc 0.84375\n",
      "2017-11-06T08:22:56.077727: step 247, loss 0.411266, acc 0.84375\n",
      "2017-11-06T08:23:00.059557: step 248, loss 0.565145, acc 0.84375\n",
      "2017-11-06T08:23:03.967334: step 249, loss 0.220865, acc 0.9375\n",
      "2017-11-06T08:23:07.835081: step 250, loss 0.33022, acc 0.9375\n",
      "2017-11-06T08:23:11.764877: step 251, loss 0.285791, acc 0.90625\n",
      "2017-11-06T08:23:14.247638: step 252, loss 0.569891, acc 0.95\n",
      "2017-11-06T08:23:18.204449: step 253, loss 0.812857, acc 0.8125\n",
      "2017-11-06T08:23:22.203291: step 254, loss 0.438763, acc 0.90625\n",
      "2017-11-06T08:23:26.542374: step 255, loss 0.808478, acc 0.8125\n",
      "2017-11-06T08:23:30.717341: step 256, loss 0.68386, acc 0.875\n",
      "2017-11-06T08:23:34.824258: step 257, loss 0.323529, acc 0.90625\n",
      "2017-11-06T08:23:38.827103: step 258, loss 0.385135, acc 0.9375\n",
      "2017-11-06T08:23:42.753893: step 259, loss 0.68301, acc 0.84375\n",
      "2017-11-06T08:23:46.689690: step 260, loss 0.659067, acc 0.90625\n",
      "2017-11-06T08:23:50.642498: step 261, loss 0.637643, acc 0.84375\n",
      "2017-11-06T08:23:54.629331: step 262, loss 0.414074, acc 0.90625\n",
      "2017-11-06T08:23:58.597150: step 263, loss 0.994131, acc 0.84375\n",
      "2017-11-06T08:24:02.616006: step 264, loss 0.195756, acc 0.96875\n",
      "2017-11-06T08:24:06.521781: step 265, loss 0.394971, acc 0.84375\n",
      "2017-11-06T08:24:10.519621: step 266, loss 0.624394, acc 0.9375\n",
      "2017-11-06T08:24:14.487441: step 267, loss 0.152252, acc 0.9375\n",
      "2017-11-06T08:24:18.361195: step 268, loss 0.37218, acc 0.90625\n",
      "2017-11-06T08:24:22.304995: step 269, loss 0.276873, acc 0.90625\n",
      "2017-11-06T08:24:26.238792: step 270, loss 0.804902, acc 0.84375\n",
      "2017-11-06T08:24:30.170584: step 271, loss 0.040703, acc 0.96875\n",
      "2017-11-06T08:24:34.432613: step 272, loss 0.691097, acc 0.90625\n",
      "2017-11-06T08:24:38.794781: step 273, loss 0.977613, acc 0.84375\n",
      "2017-11-06T08:24:42.716568: step 274, loss 0.620971, acc 0.875\n",
      "2017-11-06T08:24:46.604330: step 275, loss 0.449537, acc 0.90625\n",
      "2017-11-06T08:24:50.563021: step 276, loss 0.184049, acc 0.96875\n",
      "2017-11-06T08:24:54.584879: step 277, loss 0.479914, acc 0.9375\n",
      "2017-11-06T08:24:58.569711: step 278, loss 0.54469, acc 0.875\n",
      "2017-11-06T08:25:02.523521: step 279, loss 0.729442, acc 0.875\n",
      "2017-11-06T08:25:06.436300: step 280, loss 0.749765, acc 0.875\n",
      "2017-11-06T08:25:10.415127: step 281, loss 0.748531, acc 0.84375\n",
      "2017-11-06T08:25:14.343918: step 282, loss 0.871803, acc 0.75\n",
      "2017-11-06T08:25:18.353767: step 283, loss 0.678385, acc 0.84375\n",
      "2017-11-06T08:25:22.295568: step 284, loss 0.446219, acc 0.9375\n",
      "2017-11-06T08:25:26.218358: step 285, loss 0.728641, acc 0.875\n",
      "2017-11-06T08:25:30.152151: step 286, loss 1.0309, acc 0.8125\n",
      "2017-11-06T08:25:34.082944: step 287, loss 0.922001, acc 0.84375\n",
      "2017-11-06T08:25:36.603737: step 288, loss 0.736811, acc 0.8\n",
      "2017-11-06T08:25:40.661619: step 289, loss 0.81646, acc 0.78125\n",
      "2017-11-06T08:25:44.962676: step 290, loss 0.364729, acc 0.9375\n",
      "2017-11-06T08:25:48.898471: step 291, loss 0.0785273, acc 0.96875\n",
      "2017-11-06T08:25:52.831266: step 292, loss 0.519742, acc 0.90625\n",
      "2017-11-06T08:25:56.793081: step 293, loss 0.298922, acc 0.90625\n",
      "2017-11-06T08:26:00.701858: step 294, loss 0.632909, acc 0.875\n",
      "2017-11-06T08:26:04.716711: step 295, loss 0.19521, acc 0.90625\n",
      "2017-11-06T08:26:08.690534: step 296, loss 0.189609, acc 0.90625\n",
      "2017-11-06T08:26:12.670362: step 297, loss 0.758909, acc 0.90625\n",
      "2017-11-06T08:26:16.624172: step 298, loss 0.0814236, acc 0.96875\n",
      "2017-11-06T08:26:20.517938: step 299, loss 0.552288, acc 0.875\n",
      "2017-11-06T08:26:24.520782: step 300, loss 0.25871, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:26:27.092609: step 300, loss 1.70833, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-300\n",
      "\n",
      "2017-11-06T08:26:32.683160: step 301, loss 0.416128, acc 0.84375\n",
      "2017-11-06T08:26:36.739042: step 302, loss 0.346011, acc 0.875\n",
      "2017-11-06T08:26:40.786918: step 303, loss 0.422256, acc 0.90625\n",
      "2017-11-06T08:26:44.711707: step 304, loss 0.239591, acc 0.96875\n",
      "2017-11-06T08:26:49.092820: step 305, loss 0.222179, acc 0.9375\n",
      "2017-11-06T08:26:53.169716: step 306, loss 0.0465458, acc 0.96875\n",
      "2017-11-06T08:26:57.162554: step 307, loss 0.56011, acc 0.875\n",
      "2017-11-06T08:27:01.263467: step 308, loss 0.56132, acc 0.875\n",
      "2017-11-06T08:27:05.225282: step 309, loss 0.469022, acc 0.875\n",
      "2017-11-06T08:27:09.279163: step 310, loss 0.366236, acc 0.96875\n",
      "2017-11-06T08:27:13.231973: step 311, loss 0.477416, acc 0.875\n",
      "2017-11-06T08:27:17.222807: step 312, loss 1.7946, acc 0.71875\n",
      "2017-11-06T08:27:21.140591: step 313, loss 0.621492, acc 0.84375\n",
      "2017-11-06T08:27:25.133429: step 314, loss 0.535993, acc 0.90625\n",
      "2017-11-06T08:27:29.050212: step 315, loss 0.482619, acc 0.90625\n",
      "2017-11-06T08:27:33.037043: step 316, loss 0.299347, acc 0.9375\n",
      "2017-11-06T08:27:36.982847: step 317, loss 0.851918, acc 0.78125\n",
      "2017-11-06T08:27:40.895628: step 318, loss 0.290874, acc 0.875\n",
      "2017-11-06T08:27:44.897471: step 319, loss 0.428367, acc 0.875\n",
      "2017-11-06T08:27:48.863289: step 320, loss 0.40516, acc 0.90625\n",
      "2017-11-06T08:27:53.146084: step 321, loss 0.451309, acc 0.84375\n",
      "2017-11-06T08:27:57.408112: step 322, loss 0.326569, acc 0.90625\n",
      "2017-11-06T08:28:01.498019: step 323, loss 0.633508, acc 0.875\n",
      "2017-11-06T08:28:04.059840: step 324, loss 0.434388, acc 0.9\n",
      "2017-11-06T08:28:08.534019: step 325, loss 0.590766, acc 0.875\n",
      "2017-11-06T08:28:12.829070: step 326, loss 0.144396, acc 0.9375\n",
      "2017-11-06T08:28:17.108110: step 327, loss 0.162314, acc 0.96875\n",
      "2017-11-06T08:28:21.342500: step 328, loss 0.203448, acc 0.875\n",
      "2017-11-06T08:28:25.585514: step 329, loss 0.114853, acc 0.9375\n",
      "2017-11-06T08:28:29.895582: step 330, loss 0.587083, acc 0.84375\n",
      "2017-11-06T08:28:34.191381: step 331, loss 0.273038, acc 0.90625\n",
      "2017-11-06T08:28:38.336324: step 332, loss 0.536425, acc 0.84375\n",
      "2017-11-06T08:28:42.711479: step 333, loss 0.751208, acc 0.9375\n",
      "2017-11-06T08:28:46.620256: step 334, loss 0.276527, acc 0.90625\n",
      "2017-11-06T08:28:50.653122: step 335, loss 0.213743, acc 0.9375\n",
      "2017-11-06T08:28:54.649961: step 336, loss 0.260972, acc 0.90625\n",
      "2017-11-06T08:28:58.844942: step 337, loss 0.176126, acc 0.96875\n",
      "2017-11-06T08:29:03.014905: step 338, loss 0.161301, acc 0.96875\n",
      "2017-11-06T08:29:06.981724: step 339, loss 0.272944, acc 0.90625\n",
      "2017-11-06T08:29:10.989571: step 340, loss 0.396354, acc 0.875\n",
      "2017-11-06T08:29:14.939378: step 341, loss 0.639538, acc 0.875\n",
      "2017-11-06T08:29:18.927212: step 342, loss 0.553037, acc 0.90625\n",
      "2017-11-06T08:29:22.949070: step 343, loss 0.825733, acc 0.78125\n",
      "2017-11-06T08:29:26.937904: step 344, loss 0.509323, acc 0.875\n",
      "2017-11-06T08:29:30.910726: step 345, loss 0.827608, acc 0.84375\n",
      "2017-11-06T08:29:34.850526: step 346, loss 0.985253, acc 0.78125\n",
      "2017-11-06T08:29:38.790325: step 347, loss 0.365158, acc 0.84375\n",
      "2017-11-06T08:29:42.701107: step 348, loss 0.587776, acc 0.84375\n",
      "2017-11-06T08:29:46.608880: step 349, loss 0.651772, acc 0.84375\n",
      "2017-11-06T08:29:50.549680: step 350, loss 0.852206, acc 0.78125\n",
      "2017-11-06T08:29:54.474469: step 351, loss 0.254537, acc 0.90625\n",
      "2017-11-06T08:29:58.365234: step 352, loss 1.04451, acc 0.84375\n",
      "2017-11-06T08:30:02.724331: step 353, loss 0.303107, acc 0.90625\n",
      "2017-11-06T08:30:07.068417: step 354, loss 0.387623, acc 0.90625\n",
      "2017-11-06T08:30:11.058252: step 355, loss 0.775987, acc 0.90625\n",
      "2017-11-06T08:30:15.023070: step 356, loss 0.181264, acc 0.90625\n",
      "2017-11-06T08:30:18.931848: step 357, loss 0.375586, acc 0.9375\n",
      "2017-11-06T08:30:22.865642: step 358, loss 0.426643, acc 0.875\n",
      "2017-11-06T08:30:26.796436: step 359, loss 0.695928, acc 0.84375\n",
      "2017-11-06T08:30:29.320229: step 360, loss 0.0222156, acc 1\n",
      "2017-11-06T08:30:33.423144: step 361, loss 0.502577, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T08:30:37.402972: step 362, loss 0.706788, acc 0.875\n",
      "2017-11-06T08:30:41.324758: step 363, loss 0.565956, acc 0.8125\n",
      "2017-11-06T08:30:45.401393: step 364, loss 0.029925, acc 0.96875\n",
      "2017-11-06T08:30:49.363208: step 365, loss 0.446199, acc 0.9375\n",
      "2017-11-06T08:30:53.280758: step 366, loss 0.494807, acc 0.875\n",
      "2017-11-06T08:30:57.194540: step 367, loss 0.679616, acc 0.9375\n",
      "2017-11-06T08:31:01.091307: step 368, loss 0.0793546, acc 0.96875\n",
      "2017-11-06T08:31:05.097154: step 369, loss 0.642598, acc 0.84375\n",
      "2017-11-06T08:31:09.279125: step 370, loss 0.558435, acc 0.90625\n",
      "2017-11-06T08:31:13.549159: step 371, loss 0.863876, acc 0.875\n",
      "2017-11-06T08:31:17.496964: step 372, loss 0.142323, acc 0.90625\n",
      "2017-11-06T08:31:21.399739: step 373, loss 0.656196, acc 0.78125\n",
      "2017-11-06T08:31:25.329530: step 374, loss 0.0634724, acc 0.96875\n",
      "2017-11-06T08:31:29.274333: step 375, loss 0.512528, acc 0.875\n",
      "2017-11-06T08:31:33.208128: step 376, loss 0.768731, acc 0.875\n",
      "2017-11-06T08:31:37.342065: step 377, loss 0.616034, acc 0.84375\n",
      "2017-11-06T08:31:41.376933: step 378, loss 0.447771, acc 0.90625\n",
      "2017-11-06T08:31:45.305724: step 379, loss 0.43605, acc 0.9375\n",
      "2017-11-06T08:31:49.227511: step 380, loss 0.376405, acc 0.9375\n",
      "2017-11-06T08:31:53.162306: step 381, loss 0.129508, acc 0.96875\n",
      "2017-11-06T08:31:57.165150: step 382, loss 0.310111, acc 0.875\n",
      "2017-11-06T08:32:01.132970: step 383, loss 0.650543, acc 0.8125\n",
      "2017-11-06T08:32:05.214870: step 384, loss 0.789229, acc 0.78125\n",
      "2017-11-06T08:32:09.193697: step 385, loss 0.190079, acc 0.96875\n",
      "2017-11-06T08:32:13.224561: step 386, loss 0.0505011, acc 0.96875\n",
      "2017-11-06T08:32:17.629691: step 387, loss 0.177126, acc 0.96875\n",
      "2017-11-06T08:32:21.587504: step 388, loss 0.428544, acc 0.84375\n",
      "2017-11-06T08:32:25.524301: step 389, loss 0.553714, acc 0.90625\n",
      "2017-11-06T08:32:29.473106: step 390, loss 0.187968, acc 0.96875\n",
      "2017-11-06T08:32:33.600039: step 391, loss 0.205147, acc 0.9375\n",
      "2017-11-06T08:32:37.607886: step 392, loss 0.0182366, acc 1\n",
      "2017-11-06T08:32:41.620738: step 393, loss 0.336225, acc 0.9375\n",
      "2017-11-06T08:32:45.652923: step 394, loss 0.928286, acc 0.8125\n",
      "2017-11-06T08:32:49.563702: step 395, loss 0.0408293, acc 0.96875\n",
      "2017-11-06T08:32:52.194573: step 396, loss 0.492765, acc 0.8\n",
      "2017-11-06T08:32:56.124364: step 397, loss 0.243644, acc 0.84375\n",
      "2017-11-06T08:33:00.031139: step 398, loss 0.117243, acc 0.9375\n",
      "2017-11-06T08:33:03.964934: step 399, loss 0.184578, acc 0.90625\n",
      "2017-11-06T08:33:07.893726: step 400, loss 0.631371, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:33:10.455546: step 400, loss 1.63264, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-400\n",
      "\n",
      "2017-11-06T08:33:16.850909: step 401, loss 0.489808, acc 0.90625\n",
      "2017-11-06T08:33:21.096926: step 402, loss 0.420865, acc 0.875\n",
      "2017-11-06T08:33:25.339941: step 403, loss 0.537629, acc 0.78125\n",
      "2017-11-06T08:33:29.515909: step 404, loss 0.296808, acc 0.9375\n",
      "2017-11-06T08:33:33.507744: step 405, loss 0.332531, acc 0.9375\n",
      "2017-11-06T08:33:37.464556: step 406, loss 0.212605, acc 0.90625\n",
      "2017-11-06T08:33:41.581481: step 407, loss 0.485097, acc 0.875\n",
      "2017-11-06T08:33:45.603339: step 408, loss 0.188028, acc 0.875\n",
      "2017-11-06T08:33:49.639206: step 409, loss 0.973573, acc 0.78125\n",
      "2017-11-06T08:33:53.840934: step 410, loss 0.243631, acc 0.875\n",
      "2017-11-06T08:33:57.806752: step 411, loss 0.569401, acc 0.875\n",
      "2017-11-06T08:34:01.839618: step 412, loss 0.177506, acc 0.9375\n",
      "2017-11-06T08:34:05.891495: step 413, loss 0.20581, acc 0.9375\n",
      "2017-11-06T08:34:09.973396: step 414, loss 0.106525, acc 0.96875\n",
      "2017-11-06T08:34:14.114338: step 415, loss 0.736147, acc 0.84375\n",
      "2017-11-06T08:34:18.216253: step 416, loss 1.04474, acc 0.84375\n",
      "2017-11-06T08:34:22.189077: step 417, loss 0.421104, acc 0.8125\n",
      "2017-11-06T08:34:26.469117: step 418, loss 0.168846, acc 0.9375\n",
      "2017-11-06T08:34:30.866241: step 419, loss 0.222135, acc 0.9375\n",
      "2017-11-06T08:34:35.072230: step 420, loss 0.545336, acc 0.84375\n",
      "2017-11-06T08:34:39.094087: step 421, loss 0.354058, acc 0.90625\n",
      "2017-11-06T08:34:43.080921: step 422, loss 0.35152, acc 0.9375\n",
      "2017-11-06T08:34:47.185837: step 423, loss 0.400524, acc 0.875\n",
      "2017-11-06T08:34:51.223706: step 424, loss 0.502476, acc 0.90625\n",
      "2017-11-06T08:34:55.437700: step 425, loss 0.434768, acc 0.9375\n",
      "2017-11-06T08:34:59.532610: step 426, loss 1.04563, acc 0.8125\n",
      "2017-11-06T08:35:03.774624: step 427, loss 0.2829, acc 0.9375\n",
      "2017-11-06T08:35:07.802486: step 428, loss 0.753908, acc 0.875\n",
      "2017-11-06T08:35:11.971469: step 429, loss 0.492708, acc 0.875\n",
      "2017-11-06T08:35:15.988302: step 430, loss 0.716432, acc 0.8125\n",
      "2017-11-06T08:35:20.026173: step 431, loss 0.32374, acc 0.84375\n",
      "2017-11-06T08:35:22.585991: step 432, loss 0.512172, acc 0.85\n",
      "2017-11-06T08:35:26.548806: step 433, loss 0.230705, acc 0.90625\n",
      "2017-11-06T08:35:30.617699: step 434, loss 0.13248, acc 0.96875\n",
      "2017-11-06T08:35:34.954779: step 435, loss 0.0449829, acc 0.96875\n",
      "2017-11-06T08:35:38.943613: step 436, loss 0.096216, acc 0.96875\n",
      "2017-11-06T08:35:42.868404: step 437, loss 0.298695, acc 0.84375\n",
      "2017-11-06T08:35:46.825213: step 438, loss 0.256514, acc 0.90625\n",
      "2017-11-06T08:35:50.776020: step 439, loss 0.0525695, acc 0.96875\n",
      "2017-11-06T08:35:54.702812: step 440, loss 0.291679, acc 0.96875\n",
      "2017-11-06T08:35:58.631602: step 441, loss 0.463523, acc 0.875\n",
      "2017-11-06T08:36:02.600422: step 442, loss 0.573747, acc 0.875\n",
      "2017-11-06T08:36:06.539221: step 443, loss 0.119019, acc 0.96875\n",
      "2017-11-06T08:36:10.583094: step 444, loss 0.756885, acc 0.875\n",
      "2017-11-06T08:36:14.643980: step 445, loss 0.0406692, acc 1\n",
      "2017-11-06T08:36:18.614802: step 446, loss 0.601015, acc 0.90625\n",
      "2017-11-06T08:36:22.647666: step 447, loss 0.251204, acc 0.9375\n",
      "2017-11-06T08:36:26.554442: step 448, loss 0.47402, acc 0.84375\n",
      "2017-11-06T08:36:30.520261: step 449, loss 0.455097, acc 0.875\n",
      "2017-11-06T08:36:34.714240: step 450, loss 0.478701, acc 0.8125\n",
      "2017-11-06T08:36:39.054324: step 451, loss 0.318088, acc 0.84375\n",
      "2017-11-06T08:36:43.117211: step 452, loss 0.824793, acc 0.84375\n",
      "2017-11-06T08:36:47.081063: step 453, loss 0.36364, acc 0.875\n",
      "2017-11-06T08:36:51.025866: step 454, loss 0.253256, acc 0.90625\n",
      "2017-11-06T08:36:54.962410: step 455, loss 0.222248, acc 0.90625\n",
      "2017-11-06T08:36:58.880193: step 456, loss 0.753744, acc 0.84375\n",
      "2017-11-06T08:37:02.869028: step 457, loss 0.8106, acc 0.8125\n",
      "2017-11-06T08:37:06.831843: step 458, loss 0.44685, acc 0.875\n",
      "2017-11-06T08:37:10.814673: step 459, loss 0.108836, acc 0.96875\n",
      "2017-11-06T08:37:14.879562: step 460, loss 0.487991, acc 0.84375\n",
      "2017-11-06T08:37:18.856388: step 461, loss 0.558783, acc 0.875\n",
      "2017-11-06T08:37:22.888252: step 462, loss 0.421078, acc 0.9375\n",
      "2017-11-06T08:37:26.828051: step 463, loss 0.490034, acc 0.8125\n",
      "2017-11-06T08:37:30.776858: step 464, loss 0.302678, acc 0.90625\n",
      "2017-11-06T08:37:34.760688: step 465, loss 0.334339, acc 0.8125\n",
      "2017-11-06T08:37:38.789551: step 466, loss 0.748938, acc 0.75\n",
      "2017-11-06T08:37:42.854439: step 467, loss 0.251392, acc 0.8125\n",
      "2017-11-06T08:37:45.717473: step 468, loss 0.0268352, acc 1\n",
      "2017-11-06T08:37:49.789366: step 469, loss 0.482464, acc 0.84375\n",
      "2017-11-06T08:37:53.802218: step 470, loss 0.551245, acc 0.84375\n",
      "2017-11-06T08:37:57.805062: step 471, loss 0.113695, acc 0.90625\n",
      "2017-11-06T08:38:01.787891: step 472, loss 0.356471, acc 0.875\n",
      "2017-11-06T08:38:05.694668: step 473, loss 0.140967, acc 0.96875\n",
      "2017-11-06T08:38:09.769563: step 474, loss 0.405059, acc 0.8125\n",
      "2017-11-06T08:38:13.756395: step 475, loss 0.763532, acc 0.84375\n",
      "2017-11-06T08:38:18.155523: step 476, loss 0.65035, acc 0.84375\n",
      "2017-11-06T08:38:22.386538: step 477, loss 0.124607, acc 0.90625\n",
      "2017-11-06T08:38:26.919764: step 478, loss 0.445499, acc 0.84375\n",
      "2017-11-06T08:38:31.043194: step 479, loss 0.480668, acc 0.90625\n",
      "2017-11-06T08:38:35.312228: step 480, loss 0.282186, acc 0.9375\n",
      "2017-11-06T08:38:39.342603: step 481, loss 0.854067, acc 0.84375\n",
      "2017-11-06T08:38:43.356455: step 482, loss 0.160123, acc 0.9375\n",
      "2017-11-06T08:38:47.654509: step 483, loss 0.386174, acc 0.875\n",
      "2017-11-06T08:38:52.061640: step 484, loss 0.201887, acc 0.9375\n",
      "2017-11-06T08:38:56.106514: step 485, loss 0.54391, acc 0.90625\n",
      "2017-11-06T08:39:00.072332: step 486, loss 0.142354, acc 0.96875\n",
      "2017-11-06T08:39:04.017136: step 487, loss 0.648345, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T08:39:08.213117: step 488, loss 0.528522, acc 0.90625\n",
      "2017-11-06T08:39:12.272001: step 489, loss 0.462285, acc 0.875\n",
      "2017-11-06T08:39:16.410941: step 490, loss 0.182719, acc 0.90625\n",
      "2017-11-06T08:39:20.440805: step 491, loss 0.288032, acc 0.875\n",
      "2017-11-06T08:39:24.434646: step 492, loss 0.326391, acc 0.9375\n",
      "2017-11-06T08:39:28.441489: step 493, loss 0.196004, acc 0.9375\n",
      "2017-11-06T08:39:32.392296: step 494, loss 0.418355, acc 0.875\n",
      "2017-11-06T08:39:36.320088: step 495, loss 0.240079, acc 0.90625\n",
      "2017-11-06T08:39:40.343947: step 496, loss 0.608809, acc 0.8125\n",
      "2017-11-06T08:39:44.328779: step 497, loss 0.478406, acc 0.875\n",
      "2017-11-06T08:39:48.254568: step 498, loss 0.433698, acc 0.90625\n",
      "2017-11-06T08:39:52.256413: step 499, loss 0.1486, acc 0.9375\n",
      "2017-11-06T08:39:56.607231: step 500, loss 0.26668, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:39:59.314155: step 500, loss 1.76038, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-500\n",
      "\n",
      "2017-11-06T08:40:04.929367: step 501, loss 0.380228, acc 0.90625\n",
      "2017-11-06T08:40:08.978244: step 502, loss 0.711817, acc 0.875\n",
      "2017-11-06T08:40:13.000102: step 503, loss 0.353043, acc 0.9375\n",
      "2017-11-06T08:40:15.623966: step 504, loss 0.722322, acc 0.85\n",
      "2017-11-06T08:40:19.654830: step 505, loss 0.148921, acc 0.90625\n",
      "2017-11-06T08:40:23.733729: step 506, loss 0.401203, acc 0.90625\n",
      "2017-11-06T08:40:27.693543: step 507, loss 0.202979, acc 0.9375\n",
      "2017-11-06T08:40:31.746422: step 508, loss 0.691522, acc 0.8125\n",
      "2017-11-06T08:40:35.982432: step 509, loss 0.163269, acc 0.9375\n",
      "2017-11-06T08:40:39.951254: step 510, loss 0.684938, acc 0.78125\n",
      "2017-11-06T08:40:43.995125: step 511, loss 0.491663, acc 0.875\n",
      "2017-11-06T08:40:47.998970: step 512, loss 0.395461, acc 0.90625\n",
      "2017-11-06T08:40:51.960785: step 513, loss 0.317784, acc 0.9375\n",
      "2017-11-06T08:40:56.008661: step 514, loss 0.36773, acc 0.90625\n",
      "2017-11-06T08:41:00.031520: step 515, loss 0.133875, acc 0.96875\n",
      "2017-11-06T08:41:04.447658: step 516, loss 0.144761, acc 0.96875\n",
      "2017-11-06T08:41:08.651645: step 517, loss 0.197727, acc 0.90625\n",
      "2017-11-06T08:41:13.522105: step 518, loss 0.343608, acc 0.875\n",
      "2017-11-06T08:41:18.650749: step 519, loss 0.349143, acc 0.875\n",
      "2017-11-06T08:41:23.700337: step 520, loss 0.188071, acc 0.875\n",
      "2017-11-06T08:41:28.571799: step 521, loss 0.433851, acc 0.875\n",
      "2017-11-06T08:41:32.723749: step 522, loss 0.43305, acc 0.875\n",
      "2017-11-06T08:41:36.870695: step 523, loss 0.0710625, acc 0.96875\n",
      "2017-11-06T08:41:40.900558: step 524, loss 0.472391, acc 0.8125\n",
      "2017-11-06T08:41:44.815342: step 525, loss 0.589043, acc 0.78125\n",
      "2017-11-06T08:41:48.808177: step 526, loss 0.272701, acc 0.90625\n",
      "2017-11-06T08:41:52.769993: step 527, loss 0.162872, acc 0.9375\n",
      "2017-11-06T08:41:56.658755: step 528, loss 0.11851, acc 0.9375\n",
      "2017-11-06T08:42:00.660600: step 529, loss 0.301941, acc 0.90625\n",
      "2017-11-06T08:42:04.538354: step 530, loss 0.509367, acc 0.875\n",
      "2017-11-06T08:42:08.776366: step 531, loss 0.878203, acc 0.84375\n",
      "2017-11-06T08:42:12.988359: step 532, loss 0.70548, acc 0.875\n",
      "2017-11-06T08:42:16.953175: step 533, loss 0.0937293, acc 0.96875\n",
      "2017-11-06T08:42:20.946013: step 534, loss 0.426596, acc 0.90625\n",
      "2017-11-06T08:42:24.904826: step 535, loss 0.471183, acc 0.90625\n",
      "2017-11-06T08:42:28.918678: step 536, loss 0.350809, acc 0.90625\n",
      "2017-11-06T08:42:32.957547: step 537, loss 0.0353489, acc 1\n",
      "2017-11-06T08:42:37.086481: step 538, loss 0.280951, acc 0.875\n",
      "2017-11-06T08:42:41.070312: step 539, loss 0.0867591, acc 0.96875\n",
      "2017-11-06T08:42:43.618122: step 540, loss 0.526407, acc 0.85\n",
      "2017-11-06T08:42:47.592948: step 541, loss 0.400481, acc 0.875\n",
      "2017-11-06T08:42:51.620475: step 542, loss 0.558376, acc 0.875\n",
      "2017-11-06T08:42:55.618075: step 543, loss 0.218653, acc 0.9375\n",
      "2017-11-06T08:42:59.590898: step 544, loss 0.214338, acc 0.90625\n",
      "2017-11-06T08:43:03.595744: step 545, loss 0.651983, acc 0.875\n",
      "2017-11-06T08:43:07.503520: step 546, loss 0.69491, acc 0.875\n",
      "2017-11-06T08:43:11.586422: step 547, loss 0.112705, acc 0.9375\n",
      "2017-11-06T08:43:15.969536: step 548, loss 0.0785433, acc 0.96875\n",
      "2017-11-06T08:43:19.995396: step 549, loss 0.26972, acc 0.96875\n",
      "2017-11-06T08:43:24.126332: step 550, loss 0.347355, acc 0.90625\n",
      "2017-11-06T08:43:28.288289: step 551, loss 0.124008, acc 0.9375\n",
      "2017-11-06T08:43:32.233091: step 552, loss 0.428293, acc 0.90625\n",
      "2017-11-06T08:43:36.184899: step 553, loss 0.389359, acc 0.875\n",
      "2017-11-06T08:43:40.185746: step 554, loss 0.260465, acc 0.9375\n",
      "2017-11-06T08:43:44.089516: step 555, loss 0.524514, acc 0.8125\n",
      "2017-11-06T08:43:48.025312: step 556, loss 0.418652, acc 0.9375\n",
      "2017-11-06T08:43:52.002138: step 557, loss 0.242952, acc 0.90625\n",
      "2017-11-06T08:43:55.977964: step 558, loss 0.337028, acc 0.90625\n",
      "2017-11-06T08:43:59.898749: step 559, loss 0.404234, acc 0.90625\n",
      "2017-11-06T08:44:03.840550: step 560, loss 0.565986, acc 0.84375\n",
      "2017-11-06T08:44:07.869412: step 561, loss 0.63325, acc 0.875\n",
      "2017-11-06T08:44:11.797204: step 562, loss 0.545128, acc 0.875\n",
      "2017-11-06T08:44:15.743008: step 563, loss 0.0740676, acc 0.9375\n",
      "2017-11-06T08:44:20.017044: step 564, loss 0.285951, acc 0.875\n",
      "2017-11-06T08:44:24.283075: step 565, loss 0.285396, acc 0.90625\n",
      "2017-11-06T08:44:28.254897: step 566, loss 0.338769, acc 0.96875\n",
      "2017-11-06T08:44:32.258743: step 567, loss 0.299115, acc 0.90625\n",
      "2017-11-06T08:44:36.409692: step 568, loss 0.344934, acc 0.875\n",
      "2017-11-06T08:44:40.405531: step 569, loss 0.31623, acc 0.90625\n",
      "2017-11-06T08:44:44.394365: step 570, loss 0.501333, acc 0.78125\n",
      "2017-11-06T08:44:48.389203: step 571, loss 0.583355, acc 0.84375\n",
      "2017-11-06T08:44:52.375036: step 572, loss 0.420715, acc 0.8125\n",
      "2017-11-06T08:44:56.404899: step 573, loss 0.616026, acc 0.90625\n",
      "2017-11-06T08:45:00.421753: step 574, loss 0.456568, acc 0.84375\n",
      "2017-11-06T08:45:04.384571: step 575, loss 0.305128, acc 0.9375\n",
      "2017-11-06T08:45:06.932379: step 576, loss 0.689618, acc 0.75\n",
      "2017-11-06T08:45:10.954237: step 577, loss 0.191997, acc 0.9375\n",
      "2017-11-06T08:45:14.896039: step 578, loss 0.258705, acc 0.84375\n",
      "2017-11-06T08:45:18.829833: step 579, loss 0.119806, acc 0.9375\n",
      "2017-11-06T08:45:23.015807: step 580, loss 0.802333, acc 0.8125\n",
      "2017-11-06T08:45:27.407928: step 581, loss 0.407046, acc 0.84375\n",
      "2017-11-06T08:45:31.441794: step 582, loss 0.400499, acc 0.9375\n",
      "2017-11-06T08:45:35.390600: step 583, loss 0.358816, acc 0.875\n",
      "2017-11-06T08:45:39.407455: step 584, loss 0.310378, acc 0.90625\n",
      "2017-11-06T08:45:43.447325: step 585, loss 0.0218694, acc 1\n",
      "2017-11-06T08:45:47.473185: step 586, loss 0.149413, acc 0.9375\n",
      "2017-11-06T08:45:51.416987: step 587, loss 0.399985, acc 0.90625\n",
      "2017-11-06T08:45:55.386847: step 588, loss 0.105218, acc 0.9375\n",
      "2017-11-06T08:45:59.416710: step 589, loss 0.480651, acc 0.84375\n",
      "2017-11-06T08:46:03.386533: step 590, loss 0.384265, acc 0.8125\n",
      "2017-11-06T08:46:07.389376: step 591, loss 0.427073, acc 0.84375\n",
      "2017-11-06T08:46:11.407229: step 592, loss 0.475415, acc 0.90625\n",
      "2017-11-06T08:46:15.443099: step 593, loss 0.352474, acc 0.9375\n",
      "2017-11-06T08:46:19.482968: step 594, loss 0.176711, acc 0.9375\n",
      "2017-11-06T08:46:23.479808: step 595, loss 0.33771, acc 0.9375\n",
      "2017-11-06T08:46:27.481651: step 596, loss 0.0586338, acc 0.96875\n",
      "2017-11-06T08:46:31.758690: step 597, loss 0.166764, acc 0.96875\n",
      "2017-11-06T08:46:36.197846: step 598, loss 0.205546, acc 0.90625\n",
      "2017-11-06T08:46:40.279745: step 599, loss 0.197969, acc 0.90625\n",
      "2017-11-06T08:46:44.314612: step 600, loss 0.297019, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:46:46.862422: step 600, loss 1.56045, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-600\n",
      "\n",
      "2017-11-06T08:46:52.518887: step 601, loss 0.590312, acc 0.875\n",
      "2017-11-06T08:46:56.477700: step 602, loss 0.515521, acc 0.90625\n",
      "2017-11-06T08:47:00.477544: step 603, loss 0.369974, acc 0.90625\n",
      "2017-11-06T08:47:04.431354: step 604, loss 0.472122, acc 0.875\n",
      "2017-11-06T08:47:08.389164: step 605, loss 0.257377, acc 0.90625\n",
      "2017-11-06T08:47:12.405018: step 606, loss 0.238669, acc 0.96875\n",
      "2017-11-06T08:47:16.392851: step 607, loss 0.604084, acc 0.8125\n",
      "2017-11-06T08:47:20.423715: step 608, loss 0.396919, acc 0.84375\n",
      "2017-11-06T08:47:24.476594: step 609, loss 0.58158, acc 0.78125\n",
      "2017-11-06T08:47:28.507458: step 610, loss 0.542448, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T08:47:32.418237: step 611, loss 0.0526823, acc 0.96875\n",
      "2017-11-06T08:47:35.129164: step 612, loss 0.601522, acc 0.9\n",
      "2017-11-06T08:47:39.493265: step 613, loss 0.569687, acc 0.875\n",
      "2017-11-06T08:47:43.533135: step 614, loss 0.258666, acc 0.90625\n",
      "2017-11-06T08:47:47.582012: step 615, loss 0.0984169, acc 0.96875\n",
      "2017-11-06T08:47:51.612876: step 616, loss 0.796997, acc 0.78125\n",
      "2017-11-06T08:47:55.558680: step 617, loss 0.420383, acc 0.90625\n",
      "2017-11-06T08:47:59.570530: step 618, loss 0.252589, acc 0.90625\n",
      "2017-11-06T08:48:03.556363: step 619, loss 0.147105, acc 0.875\n",
      "2017-11-06T08:48:07.574218: step 620, loss 0.342166, acc 0.875\n",
      "2017-11-06T08:48:11.506011: step 621, loss 0.402821, acc 0.84375\n",
      "2017-11-06T08:48:15.526868: step 622, loss 0.150322, acc 0.9375\n",
      "2017-11-06T08:48:19.540720: step 623, loss 0.221372, acc 0.9375\n",
      "2017-11-06T08:48:23.610611: step 624, loss 0.177624, acc 0.90625\n",
      "2017-11-06T08:48:27.638473: step 625, loss 0.0604146, acc 0.96875\n",
      "2017-11-06T08:48:31.616300: step 626, loss 0.311486, acc 0.90625\n",
      "2017-11-06T08:48:35.848307: step 627, loss 0.233714, acc 0.9375\n",
      "2017-11-06T08:48:39.889179: step 628, loss 0.4539, acc 0.90625\n",
      "2017-11-06T08:48:44.292307: step 629, loss 0.194997, acc 0.84375\n",
      "2017-11-06T08:48:48.440255: step 630, loss 0.538384, acc 0.84375\n",
      "2017-11-06T08:48:52.447102: step 631, loss 0.0378972, acc 1\n",
      "2017-11-06T08:48:56.473730: step 632, loss 0.37093, acc 0.875\n",
      "2017-11-06T08:49:00.512600: step 633, loss 0.305281, acc 0.875\n",
      "2017-11-06T08:49:04.557475: step 634, loss 0.344595, acc 0.90625\n",
      "2017-11-06T08:49:08.570325: step 635, loss 0.347208, acc 0.9375\n",
      "2017-11-06T08:49:12.545149: step 636, loss 0.336805, acc 0.875\n",
      "2017-11-06T08:49:16.531982: step 637, loss 0.506706, acc 0.8125\n",
      "2017-11-06T08:49:20.562846: step 638, loss 0.153903, acc 0.90625\n",
      "2017-11-06T08:49:24.718799: step 639, loss 0.552987, acc 0.78125\n",
      "2017-11-06T08:49:29.071892: step 640, loss 0.483052, acc 0.84375\n",
      "2017-11-06T08:49:33.043714: step 641, loss 0.203185, acc 0.90625\n",
      "2017-11-06T08:49:37.009533: step 642, loss 0.357815, acc 0.875\n",
      "2017-11-06T08:49:41.043399: step 643, loss 0.647089, acc 0.8125\n",
      "2017-11-06T08:49:45.042240: step 644, loss 0.182582, acc 0.9375\n",
      "2017-11-06T08:49:49.426355: step 645, loss 0.454412, acc 0.84375\n",
      "2017-11-06T08:49:53.496246: step 646, loss 0.67425, acc 0.84375\n",
      "2017-11-06T08:49:57.475074: step 647, loss 0.296789, acc 0.90625\n",
      "2017-11-06T08:50:00.031890: step 648, loss 0.107399, acc 0.95\n",
      "2017-11-06T08:50:04.206857: step 649, loss 0.116456, acc 0.96875\n",
      "2017-11-06T08:50:08.197692: step 650, loss 0.6688, acc 0.875\n",
      "2017-11-06T08:50:12.220552: step 651, loss 0.652341, acc 0.90625\n",
      "2017-11-06T08:50:16.236404: step 652, loss 0.532498, acc 0.90625\n",
      "2017-11-06T08:50:20.244253: step 653, loss 0.483355, acc 0.90625\n",
      "2017-11-06T08:50:24.230085: step 654, loss 0.257444, acc 0.9375\n",
      "2017-11-06T08:50:28.151871: step 655, loss 0.0689846, acc 0.96875\n",
      "2017-11-06T08:50:32.183736: step 656, loss 0.460876, acc 0.90625\n",
      "2017-11-06T08:50:36.385722: step 657, loss 0.319361, acc 0.84375\n",
      "2017-11-06T08:50:40.368552: step 658, loss 0.146509, acc 0.9375\n",
      "2017-11-06T08:50:44.399416: step 659, loss 0.286097, acc 0.90625\n",
      "2017-11-06T08:50:48.342217: step 660, loss 0.13398, acc 0.96875\n",
      "2017-11-06T08:50:52.444132: step 661, loss 0.576123, acc 0.84375\n",
      "2017-11-06T08:50:56.802229: step 662, loss 0.559809, acc 0.84375\n",
      "2017-11-06T08:51:00.892542: step 663, loss 0.110832, acc 0.9375\n",
      "2017-11-06T08:51:04.847351: step 664, loss 0.134994, acc 0.9375\n",
      "2017-11-06T08:51:08.878215: step 665, loss 0.356173, acc 0.875\n",
      "2017-11-06T08:51:12.805004: step 666, loss 0.767475, acc 0.8125\n",
      "2017-11-06T08:51:16.794839: step 667, loss 0.2993, acc 0.9375\n",
      "2017-11-06T08:51:20.832708: step 668, loss 0.179481, acc 0.90625\n",
      "2017-11-06T08:51:24.853566: step 669, loss 0.595971, acc 0.875\n",
      "2017-11-06T08:51:28.858411: step 670, loss 0.244169, acc 0.9375\n",
      "2017-11-06T08:51:32.869262: step 671, loss 0.0970048, acc 0.90625\n",
      "2017-11-06T08:51:36.995193: step 672, loss 0.753858, acc 0.78125\n",
      "2017-11-06T08:51:40.976021: step 673, loss 0.364111, acc 0.90625\n",
      "2017-11-06T08:51:44.989873: step 674, loss 0.318841, acc 0.875\n",
      "2017-11-06T08:51:49.108800: step 675, loss 0.374546, acc 0.875\n",
      "2017-11-06T08:51:53.086627: step 676, loss 0.668467, acc 0.84375\n",
      "2017-11-06T08:51:57.066206: step 677, loss 0.210436, acc 0.96875\n",
      "2017-11-06T08:52:01.524373: step 678, loss 0.0977367, acc 0.9375\n",
      "2017-11-06T08:52:05.530220: step 679, loss 0.367983, acc 0.875\n",
      "2017-11-06T08:52:09.505045: step 680, loss 0.204085, acc 0.9375\n",
      "2017-11-06T08:52:13.470862: step 681, loss 0.415474, acc 0.875\n",
      "2017-11-06T08:52:17.515736: step 682, loss 0.342281, acc 0.90625\n",
      "2017-11-06T08:52:21.520581: step 683, loss 0.0831686, acc 0.9375\n",
      "2017-11-06T08:52:24.108422: step 684, loss 0.342354, acc 0.8\n",
      "2017-11-06T08:52:28.074238: step 685, loss 0.533659, acc 0.875\n",
      "2017-11-06T08:52:32.097097: step 686, loss 0.267709, acc 0.90625\n",
      "2017-11-06T08:52:36.376138: step 687, loss 0.00787671, acc 1\n",
      "2017-11-06T08:52:40.345957: step 688, loss 0.442071, acc 0.90625\n",
      "2017-11-06T08:52:44.321782: step 689, loss 0.359405, acc 0.875\n",
      "2017-11-06T08:52:48.344641: step 690, loss 0.383797, acc 0.84375\n",
      "2017-11-06T08:52:52.327471: step 691, loss 0.180408, acc 0.90625\n",
      "2017-11-06T08:52:56.350330: step 692, loss 0.275394, acc 0.90625\n",
      "2017-11-06T08:53:00.305140: step 693, loss 0.170843, acc 0.9375\n",
      "2017-11-06T08:53:04.626209: step 694, loss 0.233375, acc 0.9375\n",
      "2017-11-06T08:53:08.946280: step 695, loss 0.252516, acc 0.90625\n",
      "2017-11-06T08:53:12.994157: step 696, loss 0.459567, acc 0.84375\n",
      "2017-11-06T08:53:17.033026: step 697, loss 0.184606, acc 0.875\n",
      "2017-11-06T08:53:21.030866: step 698, loss 0.405291, acc 0.875\n",
      "2017-11-06T08:53:25.145790: step 699, loss 0.12251, acc 0.96875\n",
      "2017-11-06T08:53:29.212679: step 700, loss 0.250867, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T08:53:31.889582: step 700, loss 1.15126, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-700\n",
      "\n",
      "2017-11-06T08:53:37.479908: step 701, loss 0.39063, acc 0.875\n",
      "2017-11-06T08:53:41.511772: step 702, loss 0.123648, acc 0.96875\n",
      "2017-11-06T08:53:45.518621: step 703, loss 0.502042, acc 0.84375\n",
      "2017-11-06T08:53:49.521463: step 704, loss 0.326209, acc 0.9375\n",
      "2017-11-06T08:53:53.526309: step 705, loss 0.516184, acc 0.84375\n",
      "2017-11-06T08:53:57.457104: step 706, loss 0.219156, acc 0.875\n",
      "2017-11-06T08:54:01.420919: step 707, loss 0.297566, acc 0.90625\n",
      "2017-11-06T08:54:05.400746: step 708, loss 0.333488, acc 0.9375\n",
      "2017-11-06T08:54:09.562705: step 709, loss 0.0714398, acc 0.9375\n",
      "2017-11-06T08:54:13.808720: step 710, loss 0.482843, acc 0.84375\n",
      "2017-11-06T08:54:17.832580: step 711, loss 0.0926615, acc 0.96875\n",
      "2017-11-06T08:54:21.940498: step 712, loss 0.267799, acc 0.90625\n",
      "2017-11-06T08:54:25.971362: step 713, loss 0.499186, acc 0.875\n",
      "2017-11-06T08:54:29.915165: step 714, loss 0.295722, acc 0.875\n",
      "2017-11-06T08:54:34.185199: step 715, loss 0.280376, acc 0.84375\n",
      "2017-11-06T08:54:38.304126: step 716, loss 0.465722, acc 0.84375\n",
      "2017-11-06T08:54:42.367012: step 717, loss 0.247689, acc 0.875\n",
      "2017-11-06T08:54:46.397876: step 718, loss 0.550009, acc 0.84375\n",
      "2017-11-06T08:54:50.425738: step 719, loss 0.102839, acc 0.9375\n",
      "2017-11-06T08:54:53.009574: step 720, loss 0.451531, acc 0.85\n",
      "2017-11-06T08:54:57.118288: step 721, loss 0.311908, acc 0.84375\n",
      "2017-11-06T08:55:01.111126: step 722, loss 0.0753648, acc 0.9375\n",
      "2017-11-06T08:55:05.199030: step 723, loss 0.398092, acc 0.875\n",
      "2017-11-06T08:55:09.174855: step 724, loss 0.0677268, acc 0.96875\n",
      "2017-11-06T08:55:13.301787: step 725, loss 0.619391, acc 0.8125\n",
      "2017-11-06T08:55:17.744945: step 726, loss 0.515528, acc 0.84375\n",
      "2017-11-06T08:55:21.892892: step 727, loss 0.0844856, acc 0.96875\n",
      "2017-11-06T08:55:26.092876: step 728, loss 0.025438, acc 1\n",
      "2017-11-06T08:55:30.186784: step 729, loss 0.0759908, acc 0.96875\n",
      "2017-11-06T08:55:34.208643: step 730, loss 0.319396, acc 0.875\n",
      "2017-11-06T08:55:38.322566: step 731, loss 0.0821423, acc 0.96875\n",
      "2017-11-06T08:55:42.339420: step 732, loss 0.519665, acc 0.875\n",
      "2017-11-06T08:55:46.372285: step 733, loss 0.466188, acc 0.875\n",
      "2017-11-06T08:55:50.415159: step 734, loss 0.305514, acc 0.90625\n",
      "2017-11-06T08:55:54.400991: step 735, loss 0.563553, acc 0.84375\n",
      "2017-11-06T08:55:58.354800: step 736, loss 0.345811, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T08:56:02.367651: step 737, loss 0.458715, acc 0.90625\n",
      "2017-11-06T08:56:06.367493: step 738, loss 0.225955, acc 0.9375\n",
      "2017-11-06T08:56:10.373340: step 739, loss 0.799456, acc 0.78125\n",
      "2017-11-06T08:56:14.355168: step 740, loss 0.149287, acc 0.96875\n",
      "2017-11-06T08:56:18.369020: step 741, loss 0.341021, acc 0.875\n",
      "2017-11-06T08:56:22.735123: step 742, loss 0.185332, acc 0.9375\n",
      "2017-11-06T08:56:26.948117: step 743, loss 0.312071, acc 0.875\n",
      "2017-11-06T08:56:30.954963: step 744, loss 0.34942, acc 0.875\n",
      "2017-11-06T08:56:35.109917: step 745, loss 0.454602, acc 0.84375\n",
      "2017-11-06T08:56:39.200823: step 746, loss 0.0738076, acc 0.96875\n",
      "2017-11-06T08:56:43.232688: step 747, loss 0.403756, acc 0.875\n",
      "2017-11-06T08:56:47.202508: step 748, loss 0.509347, acc 0.8125\n",
      "2017-11-06T08:56:51.230372: step 749, loss 0.28279, acc 0.90625\n",
      "2017-11-06T08:56:55.247224: step 750, loss 0.103976, acc 0.9375\n",
      "2017-11-06T08:56:59.173013: step 751, loss 0.21202, acc 0.875\n",
      "2017-11-06T08:57:03.485665: step 752, loss 0.250394, acc 0.875\n",
      "2017-11-06T08:57:07.970451: step 753, loss 0.222361, acc 0.96875\n",
      "2017-11-06T08:57:12.297525: step 754, loss 0.0464465, acc 0.96875\n",
      "2017-11-06T08:57:16.449001: step 755, loss 0.519176, acc 0.90625\n",
      "2017-11-06T08:57:19.035844: step 756, loss 0.430221, acc 0.8\n",
      "2017-11-06T08:57:23.065706: step 757, loss 0.378006, acc 0.96875\n",
      "2017-11-06T08:57:27.342745: step 758, loss 0.252355, acc 0.9375\n",
      "2017-11-06T08:57:31.626788: step 759, loss 0.515817, acc 0.84375\n",
      "2017-11-06T08:57:35.654651: step 760, loss 0.475522, acc 0.84375\n",
      "2017-11-06T08:57:39.656496: step 761, loss 0.0973625, acc 0.9375\n",
      "2017-11-06T08:57:43.689359: step 762, loss 0.146833, acc 0.9375\n",
      "2017-11-06T08:57:47.726228: step 763, loss 0.303834, acc 0.90625\n",
      "2017-11-06T08:57:51.658022: step 764, loss 0.482924, acc 0.875\n",
      "2017-11-06T08:57:55.630585: step 765, loss 0.334518, acc 0.875\n",
      "2017-11-06T08:57:59.680463: step 766, loss 0.279526, acc 0.84375\n",
      "2017-11-06T08:58:03.657288: step 767, loss 0.175399, acc 0.90625\n",
      "2017-11-06T08:58:07.727180: step 768, loss 0.276177, acc 0.875\n",
      "2017-11-06T08:58:11.732027: step 769, loss 0.262479, acc 0.8125\n",
      "2017-11-06T08:58:15.717860: step 770, loss 0.421237, acc 0.875\n",
      "2017-11-06T08:58:19.696687: step 771, loss 0.511889, acc 0.8125\n",
      "2017-11-06T08:58:23.833625: step 772, loss 0.472193, acc 0.875\n",
      "2017-11-06T08:58:28.133680: step 773, loss 0.0970214, acc 0.96875\n",
      "2017-11-06T08:58:32.356681: step 774, loss 0.0503061, acc 0.96875\n",
      "2017-11-06T08:58:36.789830: step 775, loss 0.18426, acc 0.90625\n",
      "2017-11-06T08:58:40.871730: step 776, loss 0.114314, acc 0.90625\n",
      "2017-11-06T08:58:44.849557: step 777, loss 0.276018, acc 0.90625\n",
      "2017-11-06T08:58:48.853404: step 778, loss 0.208962, acc 0.9375\n",
      "2017-11-06T08:58:52.812215: step 779, loss 0.232474, acc 0.875\n",
      "2017-11-06T08:58:56.869098: step 780, loss 0.597387, acc 0.875\n",
      "2017-11-06T08:59:00.866938: step 781, loss 0.324744, acc 0.875\n",
      "2017-11-06T08:59:04.900805: step 782, loss 0.269437, acc 0.875\n",
      "2017-11-06T08:59:08.852612: step 783, loss 0.16968, acc 0.90625\n",
      "2017-11-06T08:59:12.900489: step 784, loss 0.156296, acc 0.9375\n",
      "2017-11-06T08:59:16.948365: step 785, loss 0.100419, acc 0.9375\n",
      "2017-11-06T08:59:21.015255: step 786, loss 0.393135, acc 0.90625\n",
      "2017-11-06T08:59:25.069134: step 787, loss 0.516986, acc 0.875\n",
      "2017-11-06T08:59:29.173051: step 788, loss 0.515804, acc 0.90625\n",
      "2017-11-06T08:59:33.186902: step 789, loss 0.421304, acc 0.90625\n",
      "2017-11-06T08:59:37.339854: step 790, loss 0.473546, acc 0.90625\n",
      "2017-11-06T08:59:41.715965: step 791, loss 0.125899, acc 0.9375\n",
      "2017-11-06T08:59:44.289792: step 792, loss 0.313049, acc 0.85\n",
      "2017-11-06T08:59:48.306646: step 793, loss 0.163252, acc 0.90625\n",
      "2017-11-06T08:59:52.264460: step 794, loss 0.743446, acc 0.78125\n",
      "2017-11-06T08:59:56.210263: step 795, loss 0.339298, acc 0.90625\n",
      "2017-11-06T09:00:00.167075: step 796, loss 0.216326, acc 0.875\n",
      "2017-11-06T09:00:04.414091: step 797, loss 0.272562, acc 0.90625\n",
      "2017-11-06T09:00:08.363898: step 798, loss 0.51633, acc 0.75\n",
      "2017-11-06T09:00:12.314705: step 799, loss 0.160043, acc 0.875\n",
      "2017-11-06T09:00:16.330558: step 800, loss 0.0598156, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:00:18.951422: step 800, loss 1.23298, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-800\n",
      "\n",
      "2017-11-06T09:00:24.466875: step 801, loss 0.609039, acc 0.8125\n",
      "2017-11-06T09:00:28.440697: step 802, loss 0.510352, acc 0.8125\n",
      "2017-11-06T09:00:32.456550: step 803, loss 0.0269268, acc 1\n",
      "2017-11-06T09:00:36.544455: step 804, loss 0.293627, acc 0.90625\n",
      "2017-11-06T09:00:40.538293: step 805, loss 0.439661, acc 0.9375\n",
      "2017-11-06T09:00:44.807326: step 806, loss 0.0446306, acc 1\n",
      "2017-11-06T09:00:48.894230: step 807, loss 0.216139, acc 0.9375\n",
      "2017-11-06T09:00:52.825025: step 808, loss 0.130229, acc 0.9375\n",
      "2017-11-06T09:00:56.779683: step 809, loss 0.114771, acc 0.96875\n",
      "2017-11-06T09:01:00.754506: step 810, loss 0.149302, acc 0.90625\n",
      "2017-11-06T09:01:04.652276: step 811, loss 0.306217, acc 0.90625\n",
      "2017-11-06T09:01:08.657121: step 812, loss 0.463118, acc 0.84375\n",
      "2017-11-06T09:01:12.646956: step 813, loss 0.346954, acc 0.8125\n",
      "2017-11-06T09:01:16.593761: step 814, loss 0.522133, acc 0.8125\n",
      "2017-11-06T09:01:20.598606: step 815, loss 0.18238, acc 0.90625\n",
      "2017-11-06T09:01:24.566426: step 816, loss 0.280251, acc 0.90625\n",
      "2017-11-06T09:01:28.549256: step 817, loss 0.404605, acc 0.84375\n",
      "2017-11-06T09:01:32.485053: step 818, loss 0.389514, acc 0.90625\n",
      "2017-11-06T09:01:36.647010: step 819, loss 0.265792, acc 0.875\n",
      "2017-11-06T09:01:40.713899: step 820, loss 0.124814, acc 0.9375\n",
      "2017-11-06T09:01:44.755771: step 821, loss 0.158848, acc 0.875\n",
      "2017-11-06T09:01:48.956756: step 822, loss 0.456846, acc 0.8125\n",
      "2017-11-06T09:01:53.234796: step 823, loss 0.272964, acc 0.875\n",
      "2017-11-06T09:01:57.260657: step 824, loss 0.0893518, acc 0.96875\n",
      "2017-11-06T09:02:01.322542: step 825, loss 0.0446875, acc 0.96875\n",
      "2017-11-06T09:02:05.312378: step 826, loss 0.405726, acc 0.875\n",
      "2017-11-06T09:02:09.425300: step 827, loss 0.349054, acc 0.90625\n",
      "2017-11-06T09:02:11.918071: step 828, loss 0.313992, acc 0.85\n",
      "2017-11-06T09:02:15.861874: step 829, loss 0.294483, acc 0.875\n",
      "2017-11-06T09:02:19.845706: step 830, loss 0.251246, acc 0.84375\n",
      "2017-11-06T09:02:23.824534: step 831, loss 0.270906, acc 0.9375\n",
      "2017-11-06T09:02:27.875409: step 832, loss 0.275039, acc 0.84375\n",
      "2017-11-06T09:02:31.832223: step 833, loss 0.297518, acc 0.84375\n",
      "2017-11-06T09:02:36.066229: step 834, loss 0.174009, acc 0.9375\n",
      "2017-11-06T09:02:40.031046: step 835, loss 0.169369, acc 0.90625\n",
      "2017-11-06T09:02:44.040896: step 836, loss 0.179665, acc 0.9375\n",
      "2017-11-06T09:02:48.017721: step 837, loss 0.17294, acc 0.875\n",
      "2017-11-06T09:02:52.003553: step 838, loss 0.283737, acc 0.875\n",
      "2017-11-06T09:02:56.457718: step 839, loss 0.0983797, acc 0.9375\n",
      "2017-11-06T09:03:00.568640: step 840, loss 0.319431, acc 0.84375\n",
      "2017-11-06T09:03:04.546466: step 841, loss 0.0506674, acc 0.96875\n",
      "2017-11-06T09:03:08.564320: step 842, loss 0.203134, acc 0.90625\n",
      "2017-11-06T09:03:12.520132: step 843, loss 0.210605, acc 0.9375\n",
      "2017-11-06T09:03:16.626049: step 844, loss 0.298852, acc 0.9375\n",
      "2017-11-06T09:03:20.565849: step 845, loss 0.379759, acc 0.84375\n",
      "2017-11-06T09:03:24.660758: step 846, loss 0.299759, acc 0.84375\n",
      "2017-11-06T09:03:28.807705: step 847, loss 0.171365, acc 0.9375\n",
      "2017-11-06T09:03:32.789534: step 848, loss 0.187769, acc 0.9375\n",
      "2017-11-06T09:03:36.849418: step 849, loss 0.218863, acc 0.875\n",
      "2017-11-06T09:03:41.029389: step 850, loss 0.1034, acc 0.96875\n",
      "2017-11-06T09:03:45.084269: step 851, loss 0.32582, acc 0.875\n",
      "2017-11-06T09:03:49.198193: step 852, loss 0.420377, acc 0.84375\n",
      "2017-11-06T09:03:53.410185: step 853, loss 0.290673, acc 0.875\n",
      "2017-11-06T09:03:57.376817: step 854, loss 0.432831, acc 0.84375\n",
      "2017-11-06T09:04:01.763935: step 855, loss 0.454307, acc 0.8125\n",
      "2017-11-06T09:04:06.014956: step 856, loss 0.0523118, acc 0.96875\n",
      "2017-11-06T09:04:10.161902: step 857, loss 0.452873, acc 0.84375\n",
      "2017-11-06T09:04:14.203775: step 858, loss 0.718595, acc 0.8125\n",
      "2017-11-06T09:04:18.372736: step 859, loss 0.785107, acc 0.8125\n",
      "2017-11-06T09:04:22.419612: step 860, loss 0.0937285, acc 0.9375\n",
      "2017-11-06T09:04:26.498511: step 861, loss 0.14412, acc 0.9375\n",
      "2017-11-06T09:04:30.659467: step 862, loss 0.221321, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T09:04:34.848443: step 863, loss 0.0411552, acc 1\n",
      "2017-11-06T09:04:37.422272: step 864, loss 0.0432764, acc 1\n",
      "2017-11-06T09:04:41.598239: step 865, loss 0.201605, acc 0.9375\n",
      "2017-11-06T09:04:45.722169: step 866, loss 0.206924, acc 0.90625\n",
      "2017-11-06T09:04:49.910145: step 867, loss 0.0218623, acc 1\n",
      "2017-11-06T09:04:54.116134: step 868, loss 0.243902, acc 0.875\n",
      "2017-11-06T09:04:58.187026: step 869, loss 0.205954, acc 0.9375\n",
      "2017-11-06T09:05:02.208884: step 870, loss 0.325923, acc 0.9375\n",
      "2017-11-06T09:05:06.672055: step 871, loss 0.302085, acc 0.90625\n",
      "2017-11-06T09:05:10.963104: step 872, loss 0.23256, acc 0.96875\n",
      "2017-11-06T09:05:14.938929: step 873, loss 0.191819, acc 0.96875\n",
      "2017-11-06T09:05:18.942773: step 874, loss 0.425917, acc 0.84375\n",
      "2017-11-06T09:05:22.899587: step 875, loss 0.303578, acc 0.875\n",
      "2017-11-06T09:05:27.030522: step 876, loss 0.383379, acc 0.84375\n",
      "2017-11-06T09:05:30.991335: step 877, loss 0.251696, acc 0.90625\n",
      "2017-11-06T09:05:35.004186: step 878, loss 0.195862, acc 0.90625\n",
      "2017-11-06T09:05:38.956994: step 879, loss 0.274497, acc 0.875\n",
      "2017-11-06T09:05:42.931819: step 880, loss 0.416251, acc 0.875\n",
      "2017-11-06T09:05:46.936664: step 881, loss 0.204173, acc 0.9375\n",
      "2017-11-06T09:05:50.939508: step 882, loss 0.250849, acc 0.90625\n",
      "2017-11-06T09:05:54.906327: step 883, loss 0.13062, acc 0.96875\n",
      "2017-11-06T09:05:58.895164: step 884, loss 0.274712, acc 0.875\n",
      "2017-11-06T09:06:02.870986: step 885, loss 0.193601, acc 0.875\n",
      "2017-11-06T09:06:06.858821: step 886, loss 0.566818, acc 0.84375\n",
      "2017-11-06T09:06:11.005766: step 887, loss 0.0536079, acc 1\n",
      "2017-11-06T09:06:15.299817: step 888, loss 0.0828457, acc 0.9375\n",
      "2017-11-06T09:06:19.333684: step 889, loss 0.139219, acc 0.96875\n",
      "2017-11-06T09:06:23.310509: step 890, loss 0.273921, acc 0.90625\n",
      "2017-11-06T09:06:27.357386: step 891, loss 0.443984, acc 0.9375\n",
      "2017-11-06T09:06:31.291181: step 892, loss 0.280064, acc 0.875\n",
      "2017-11-06T09:06:35.450136: step 893, loss 0.305432, acc 0.90625\n",
      "2017-11-06T09:06:39.393938: step 894, loss 0.343593, acc 0.875\n",
      "2017-11-06T09:06:43.405788: step 895, loss 0.142934, acc 0.9375\n",
      "2017-11-06T09:06:47.449662: step 896, loss 0.212068, acc 0.875\n",
      "2017-11-06T09:06:51.377453: step 897, loss 0.316871, acc 0.875\n",
      "2017-11-06T09:06:55.404313: step 898, loss 0.367309, acc 0.8125\n",
      "2017-11-06T09:06:59.327927: step 899, loss 0.147492, acc 0.90625\n",
      "2017-11-06T09:07:01.967801: step 900, loss 0.166472, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:07:04.694738: step 900, loss 1.05565, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-900\n",
      "\n",
      "2017-11-06T09:07:10.175238: step 901, loss 0.178075, acc 0.9375\n",
      "2017-11-06T09:07:14.124045: step 902, loss 0.108009, acc 0.96875\n",
      "2017-11-06T09:07:18.393076: step 903, loss 0.220426, acc 0.875\n",
      "2017-11-06T09:07:22.613074: step 904, loss 0.220463, acc 0.9375\n",
      "2017-11-06T09:07:26.627929: step 905, loss 0.274789, acc 0.9375\n",
      "2017-11-06T09:07:30.638777: step 906, loss 0.267663, acc 0.9375\n",
      "2017-11-06T09:07:34.625610: step 907, loss 0.294935, acc 0.96875\n",
      "2017-11-06T09:07:38.649469: step 908, loss 0.179284, acc 0.9375\n",
      "2017-11-06T09:07:42.615289: step 909, loss 0.591956, acc 0.78125\n",
      "2017-11-06T09:07:46.535074: step 910, loss 0.248798, acc 0.90625\n",
      "2017-11-06T09:07:50.685021: step 911, loss 0.222162, acc 0.875\n",
      "2017-11-06T09:07:54.700876: step 912, loss 0.290661, acc 0.9375\n",
      "2017-11-06T09:07:58.663690: step 913, loss 0.458331, acc 0.8125\n",
      "2017-11-06T09:08:02.674540: step 914, loss 0.124481, acc 0.96875\n",
      "2017-11-06T09:08:06.682387: step 915, loss 0.576288, acc 0.8125\n",
      "2017-11-06T09:08:10.731265: step 916, loss 0.231814, acc 0.90625\n",
      "2017-11-06T09:08:14.706089: step 917, loss 0.296226, acc 0.875\n",
      "2017-11-06T09:08:18.692923: step 918, loss 0.210191, acc 0.90625\n",
      "2017-11-06T09:08:23.019996: step 919, loss 0.0995133, acc 0.96875\n",
      "2017-11-06T09:08:27.586241: step 920, loss 0.0341227, acc 1\n",
      "2017-11-06T09:08:31.633118: step 921, loss 0.226686, acc 0.90625\n",
      "2017-11-06T09:08:35.861120: step 922, loss 0.082722, acc 0.96875\n",
      "2017-11-06T09:08:39.882978: step 923, loss 0.407625, acc 0.84375\n",
      "2017-11-06T09:08:43.843792: step 924, loss 0.246447, acc 0.9375\n",
      "2017-11-06T09:08:47.838631: step 925, loss 0.0727428, acc 0.96875\n",
      "2017-11-06T09:08:51.881503: step 926, loss 0.147406, acc 0.90625\n",
      "2017-11-06T09:08:55.922377: step 927, loss 0.0433161, acc 0.96875\n",
      "2017-11-06T09:08:59.837156: step 928, loss 0.154152, acc 0.96875\n",
      "2017-11-06T09:09:03.864019: step 929, loss 0.688425, acc 0.84375\n",
      "2017-11-06T09:09:07.888878: step 930, loss 0.378844, acc 0.84375\n",
      "2017-11-06T09:09:11.884717: step 931, loss 0.390728, acc 0.84375\n",
      "2017-11-06T09:09:15.919584: step 932, loss 0.295736, acc 0.84375\n",
      "2017-11-06T09:09:19.899412: step 933, loss 0.178527, acc 0.90625\n",
      "2017-11-06T09:09:23.937281: step 934, loss 0.194602, acc 0.90625\n",
      "2017-11-06T09:09:28.076222: step 935, loss 0.416094, acc 0.90625\n",
      "2017-11-06T09:09:30.935253: step 936, loss 0.47544, acc 0.85\n",
      "2017-11-06T09:09:35.051178: step 937, loss 0.208902, acc 0.9375\n",
      "2017-11-06T09:09:39.129074: step 938, loss 0.141172, acc 0.90625\n",
      "2017-11-06T09:09:43.136923: step 939, loss 0.397986, acc 0.8125\n",
      "2017-11-06T09:09:47.162783: step 940, loss 0.305814, acc 0.84375\n",
      "2017-11-06T09:09:51.226671: step 941, loss 0.22255, acc 0.9375\n",
      "2017-11-06T09:09:55.267542: step 942, loss 0.452151, acc 0.8125\n",
      "2017-11-06T09:09:59.280164: step 943, loss 0.319543, acc 0.90625\n",
      "2017-11-06T09:10:03.600232: step 944, loss 0.221606, acc 0.90625\n",
      "2017-11-06T09:10:07.601076: step 945, loss 0.389594, acc 0.84375\n",
      "2017-11-06T09:10:11.688980: step 946, loss 0.223113, acc 0.9375\n",
      "2017-11-06T09:10:15.682818: step 947, loss 0.496396, acc 0.84375\n",
      "2017-11-06T09:10:19.671653: step 948, loss 0.2121, acc 0.9375\n",
      "2017-11-06T09:10:23.711523: step 949, loss 0.251664, acc 0.84375\n",
      "2017-11-06T09:10:27.662330: step 950, loss 0.285101, acc 0.8125\n",
      "2017-11-06T09:10:31.689192: step 951, loss 0.237425, acc 0.90625\n",
      "2017-11-06T09:10:36.310475: step 952, loss 0.191913, acc 0.9375\n",
      "2017-11-06T09:10:40.434405: step 953, loss 0.252083, acc 0.90625\n",
      "2017-11-06T09:10:44.486284: step 954, loss 0.122369, acc 0.9375\n",
      "2017-11-06T09:10:48.430086: step 955, loss 0.205613, acc 0.875\n",
      "2017-11-06T09:10:52.535003: step 956, loss 0.210776, acc 0.9375\n",
      "2017-11-06T09:10:56.499820: step 957, loss 0.241381, acc 0.84375\n",
      "2017-11-06T09:11:00.649769: step 958, loss 0.0351313, acc 1\n",
      "2017-11-06T09:11:04.598576: step 959, loss 0.0332541, acc 1\n",
      "2017-11-06T09:11:08.656458: step 960, loss 0.12851, acc 0.96875\n",
      "2017-11-06T09:11:12.661306: step 961, loss 0.46431, acc 0.84375\n",
      "2017-11-06T09:11:16.631124: step 962, loss 0.631541, acc 0.8125\n",
      "2017-11-06T09:11:20.589939: step 963, loss 0.24746, acc 0.90625\n",
      "2017-11-06T09:11:24.684846: step 964, loss 0.438415, acc 0.8125\n",
      "2017-11-06T09:11:28.628649: step 965, loss 0.266947, acc 0.875\n",
      "2017-11-06T09:11:32.665518: step 966, loss 0.209322, acc 0.90625\n",
      "2017-11-06T09:11:36.630334: step 967, loss 0.214692, acc 0.90625\n",
      "2017-11-06T09:11:40.931392: step 968, loss 0.185156, acc 0.90625\n",
      "2017-11-06T09:11:45.141383: step 969, loss 0.265493, acc 0.90625\n",
      "2017-11-06T09:11:49.174247: step 970, loss 0.420798, acc 0.8125\n",
      "2017-11-06T09:11:53.192102: step 971, loss 0.115823, acc 0.9375\n",
      "2017-11-06T09:11:55.730906: step 972, loss 0.0631117, acc 1\n",
      "2017-11-06T09:11:59.723745: step 973, loss 0.266124, acc 0.90625\n",
      "2017-11-06T09:12:03.784629: step 974, loss 0.142658, acc 0.875\n",
      "2017-11-06T09:12:07.783470: step 975, loss 0.228392, acc 0.90625\n",
      "2017-11-06T09:12:11.812333: step 976, loss 0.0215216, acc 1\n",
      "2017-11-06T09:12:15.951273: step 977, loss 0.572905, acc 0.875\n",
      "2017-11-06T09:12:19.941110: step 978, loss 0.137441, acc 0.9375\n",
      "2017-11-06T09:12:23.948956: step 979, loss 0.0790467, acc 0.96875\n",
      "2017-11-06T09:12:27.947797: step 980, loss 0.0855993, acc 0.96875\n",
      "2017-11-06T09:12:31.944640: step 981, loss 0.243111, acc 0.90625\n",
      "2017-11-06T09:12:36.204664: step 982, loss 0.0582739, acc 0.96875\n",
      "2017-11-06T09:12:40.173484: step 983, loss 0.235877, acc 0.90625\n",
      "2017-11-06T09:12:44.303419: step 984, loss 0.185169, acc 0.9375\n",
      "2017-11-06T09:12:48.738571: step 985, loss 0.16762, acc 0.9375\n",
      "2017-11-06T09:12:52.800456: step 986, loss 0.191515, acc 0.9375\n",
      "2017-11-06T09:12:56.862101: step 987, loss 0.242413, acc 0.90625\n",
      "2017-11-06T09:13:00.865946: step 988, loss 0.114037, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T09:13:04.894809: step 989, loss 0.173808, acc 0.90625\n",
      "2017-11-06T09:13:08.913665: step 990, loss 0.161496, acc 0.96875\n",
      "2017-11-06T09:13:12.955536: step 991, loss 0.242853, acc 0.875\n",
      "2017-11-06T09:13:16.932363: step 992, loss 0.288833, acc 0.875\n",
      "2017-11-06T09:13:20.952218: step 993, loss 0.344067, acc 0.875\n",
      "2017-11-06T09:13:25.169627: step 994, loss 0.353609, acc 0.84375\n",
      "2017-11-06T09:13:29.076403: step 995, loss 0.314631, acc 0.8125\n",
      "2017-11-06T09:13:33.142294: step 996, loss 0.102194, acc 0.96875\n",
      "2017-11-06T09:13:37.100106: step 997, loss 0.345973, acc 0.8125\n",
      "2017-11-06T09:13:41.080933: step 998, loss 0.123143, acc 0.96875\n",
      "2017-11-06T09:13:45.067766: step 999, loss 0.320273, acc 0.875\n",
      "2017-11-06T09:13:49.019574: step 1000, loss 0.277977, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:13:52.016703: step 1000, loss 0.977258, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-06T09:13:57.910146: step 1001, loss 0.213042, acc 0.90625\n",
      "2017-11-06T09:14:02.021067: step 1002, loss 0.307056, acc 0.875\n",
      "2017-11-06T09:14:05.994891: step 1003, loss 0.358149, acc 0.84375\n",
      "2017-11-06T09:14:10.026756: step 1004, loss 0.464597, acc 0.875\n",
      "2017-11-06T09:14:13.975561: step 1005, loss 0.25762, acc 0.90625\n",
      "2017-11-06T09:14:17.985411: step 1006, loss 0.233915, acc 0.90625\n",
      "2017-11-06T09:14:22.020277: step 1007, loss 0.218015, acc 0.875\n",
      "2017-11-06T09:14:24.640139: step 1008, loss 0.153474, acc 0.9\n",
      "2017-11-06T09:14:28.906170: step 1009, loss 0.288095, acc 0.875\n",
      "2017-11-06T09:14:33.027098: step 1010, loss 0.179474, acc 0.90625\n",
      "2017-11-06T09:14:37.184052: step 1011, loss 0.40788, acc 0.84375\n",
      "2017-11-06T09:14:41.208912: step 1012, loss 0.484797, acc 0.78125\n",
      "2017-11-06T09:14:45.263793: step 1013, loss 0.0292258, acc 1\n",
      "2017-11-06T09:14:49.236616: step 1014, loss 0.321409, acc 0.84375\n",
      "2017-11-06T09:14:53.263477: step 1015, loss 0.208976, acc 0.9375\n",
      "2017-11-06T09:14:57.531510: step 1016, loss 0.0871498, acc 0.96875\n",
      "2017-11-06T09:15:01.748506: step 1017, loss 0.304927, acc 0.90625\n",
      "2017-11-06T09:15:05.754353: step 1018, loss 0.0900304, acc 0.9375\n",
      "2017-11-06T09:15:09.761201: step 1019, loss 0.166791, acc 0.9375\n",
      "2017-11-06T09:15:13.806075: step 1020, loss 0.141325, acc 0.9375\n",
      "2017-11-06T09:15:17.827931: step 1021, loss 0.148562, acc 0.9375\n",
      "2017-11-06T09:15:21.798752: step 1022, loss 0.175539, acc 0.90625\n",
      "2017-11-06T09:15:25.989730: step 1023, loss 0.138008, acc 0.9375\n",
      "2017-11-06T09:15:29.939538: step 1024, loss 0.195909, acc 0.9375\n",
      "2017-11-06T09:15:34.034447: step 1025, loss 0.249467, acc 0.875\n",
      "2017-11-06T09:15:38.027285: step 1026, loss 0.124421, acc 0.9375\n",
      "2017-11-06T09:15:42.068155: step 1027, loss 0.461699, acc 0.875\n",
      "2017-11-06T09:15:46.038976: step 1028, loss 0.0584475, acc 0.96875\n",
      "2017-11-06T09:15:50.047824: step 1029, loss 0.383538, acc 0.8125\n",
      "2017-11-06T09:15:54.059676: step 1030, loss 0.31566, acc 0.90625\n",
      "2017-11-06T09:15:57.985258: step 1031, loss 0.0757476, acc 0.96875\n",
      "2017-11-06T09:16:02.093175: step 1032, loss 0.256135, acc 0.875\n",
      "2017-11-06T09:16:06.467283: step 1033, loss 0.0584395, acc 0.96875\n",
      "2017-11-06T09:16:10.420091: step 1034, loss 0.200812, acc 0.9375\n",
      "2017-11-06T09:16:14.423936: step 1035, loss 0.0338996, acc 1\n",
      "2017-11-06T09:16:18.431784: step 1036, loss 0.285493, acc 0.90625\n",
      "2017-11-06T09:16:22.478659: step 1037, loss 0.336201, acc 0.875\n",
      "2017-11-06T09:16:26.429467: step 1038, loss 0.22285, acc 0.875\n",
      "2017-11-06T09:16:30.462334: step 1039, loss 0.186881, acc 0.875\n",
      "2017-11-06T09:16:34.681330: step 1040, loss 0.0899711, acc 0.9375\n",
      "2017-11-06T09:16:38.787247: step 1041, loss 0.264683, acc 0.875\n",
      "2017-11-06T09:16:42.796095: step 1042, loss 0.390767, acc 0.90625\n",
      "2017-11-06T09:16:46.851977: step 1043, loss 0.219774, acc 0.9375\n",
      "2017-11-06T09:16:49.360761: step 1044, loss 0.164218, acc 0.95\n",
      "2017-11-06T09:16:53.360603: step 1045, loss 0.0755083, acc 0.96875\n",
      "2017-11-06T09:16:57.297399: step 1046, loss 0.324054, acc 0.9375\n",
      "2017-11-06T09:17:01.252209: step 1047, loss 0.346317, acc 0.84375\n",
      "2017-11-06T09:17:05.317098: step 1048, loss 0.224168, acc 0.90625\n",
      "2017-11-06T09:17:09.690205: step 1049, loss 0.35162, acc 0.90625\n",
      "2017-11-06T09:17:13.827144: step 1050, loss 0.243186, acc 0.90625\n",
      "2017-11-06T09:17:17.807973: step 1051, loss 0.0922356, acc 0.9375\n",
      "2017-11-06T09:17:21.750775: step 1052, loss 0.419895, acc 0.84375\n",
      "2017-11-06T09:17:25.844684: step 1053, loss 0.472627, acc 0.84375\n",
      "2017-11-06T09:17:29.885556: step 1054, loss 0.111879, acc 0.9375\n",
      "2017-11-06T09:17:33.888400: step 1055, loss 0.163625, acc 0.875\n",
      "2017-11-06T09:17:37.927268: step 1056, loss 0.255454, acc 0.875\n",
      "2017-11-06T09:17:41.926110: step 1057, loss 0.0420325, acc 1\n",
      "2017-11-06T09:17:45.932958: step 1058, loss 0.137779, acc 0.96875\n",
      "2017-11-06T09:17:49.969826: step 1059, loss 0.513088, acc 0.875\n",
      "2017-11-06T09:17:53.990683: step 1060, loss 0.0937403, acc 0.96875\n",
      "2017-11-06T09:17:58.055571: step 1061, loss 0.148488, acc 0.9375\n",
      "2017-11-06T09:18:02.079429: step 1062, loss 0.102149, acc 0.96875\n",
      "2017-11-06T09:18:06.080273: step 1063, loss 0.158717, acc 0.96875\n",
      "2017-11-06T09:18:10.152166: step 1064, loss 0.20164, acc 0.875\n",
      "2017-11-06T09:18:14.424202: step 1065, loss 0.135048, acc 0.90625\n",
      "2017-11-06T09:18:18.761283: step 1066, loss 0.143578, acc 0.9375\n",
      "2017-11-06T09:18:22.849189: step 1067, loss 0.203558, acc 0.84375\n",
      "2017-11-06T09:18:26.990130: step 1068, loss 0.223553, acc 0.90625\n",
      "2017-11-06T09:18:30.953947: step 1069, loss 0.485356, acc 0.84375\n",
      "2017-11-06T09:18:35.224982: step 1070, loss 0.17777, acc 0.9375\n",
      "2017-11-06T09:18:39.349912: step 1071, loss 0.522952, acc 0.75\n",
      "2017-11-06T09:18:43.357761: step 1072, loss 0.0639384, acc 0.9375\n",
      "2017-11-06T09:18:47.379617: step 1073, loss 0.189262, acc 0.875\n",
      "2017-11-06T09:18:51.377459: step 1074, loss 0.147883, acc 0.9375\n",
      "2017-11-06T09:18:55.399316: step 1075, loss 0.310945, acc 0.90625\n",
      "2017-11-06T09:18:59.393953: step 1076, loss 0.106243, acc 0.96875\n",
      "2017-11-06T09:19:03.344760: step 1077, loss 0.603618, acc 0.75\n",
      "2017-11-06T09:19:07.349606: step 1078, loss 0.0942968, acc 0.96875\n",
      "2017-11-06T09:19:11.353450: step 1079, loss 0.429593, acc 0.78125\n",
      "2017-11-06T09:19:13.948295: step 1080, loss 0.245902, acc 0.85\n",
      "2017-11-06T09:19:18.121260: step 1081, loss 0.313297, acc 0.9375\n",
      "2017-11-06T09:19:22.492367: step 1082, loss 0.130958, acc 0.9375\n",
      "2017-11-06T09:19:26.473194: step 1083, loss 0.24307, acc 0.875\n",
      "2017-11-06T09:19:30.544087: step 1084, loss 0.09594, acc 0.96875\n",
      "2017-11-06T09:19:34.488891: step 1085, loss 0.343749, acc 0.875\n",
      "2017-11-06T09:19:38.491733: step 1086, loss 0.226315, acc 0.9375\n",
      "2017-11-06T09:19:42.565629: step 1087, loss 0.16943, acc 0.9375\n",
      "2017-11-06T09:19:46.583483: step 1088, loss 0.327165, acc 0.8125\n",
      "2017-11-06T09:19:50.592332: step 1089, loss 0.326914, acc 0.84375\n",
      "2017-11-06T09:19:54.676234: step 1090, loss 0.430836, acc 0.84375\n",
      "2017-11-06T09:19:58.724110: step 1091, loss 0.203782, acc 0.875\n",
      "2017-11-06T09:20:02.944108: step 1092, loss 0.127884, acc 0.9375\n",
      "2017-11-06T09:20:06.973972: step 1093, loss 0.319159, acc 0.84375\n",
      "2017-11-06T09:20:11.046865: step 1094, loss 0.211573, acc 0.90625\n",
      "2017-11-06T09:20:15.040703: step 1095, loss 0.190387, acc 0.90625\n",
      "2017-11-06T09:20:19.033540: step 1096, loss 0.320866, acc 0.875\n",
      "2017-11-06T09:20:23.016370: step 1097, loss 0.292581, acc 0.875\n",
      "2017-11-06T09:20:27.433509: step 1098, loss 0.244651, acc 0.875\n",
      "2017-11-06T09:20:31.494395: step 1099, loss 0.409967, acc 0.84375\n",
      "2017-11-06T09:20:35.755424: step 1100, loss 0.282868, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:20:38.354269: step 1100, loss 1.10319, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-06T09:20:43.711484: step 1101, loss 0.0675923, acc 0.96875\n",
      "2017-11-06T09:20:47.726336: step 1102, loss 0.344979, acc 0.8125\n",
      "2017-11-06T09:20:51.708166: step 1103, loss 0.22143, acc 0.9375\n",
      "2017-11-06T09:20:55.714013: step 1104, loss 0.164654, acc 0.90625\n",
      "2017-11-06T09:20:59.649810: step 1105, loss 0.122308, acc 0.96875\n",
      "2017-11-06T09:21:03.700687: step 1106, loss 0.134743, acc 0.9375\n",
      "2017-11-06T09:21:07.720543: step 1107, loss 0.164987, acc 0.9375\n",
      "2017-11-06T09:21:11.796439: step 1108, loss 0.0804984, acc 0.96875\n",
      "2017-11-06T09:21:15.834308: step 1109, loss 0.362705, acc 0.8125\n",
      "2017-11-06T09:21:19.777110: step 1110, loss 0.16299, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T09:21:23.794965: step 1111, loss 0.153437, acc 0.90625\n",
      "2017-11-06T09:21:27.770790: step 1112, loss 0.193735, acc 0.9375\n",
      "2017-11-06T09:21:32.199077: step 1113, loss 0.228791, acc 0.875\n",
      "2017-11-06T09:21:36.411070: step 1114, loss 0.176988, acc 0.90625\n",
      "2017-11-06T09:21:40.439932: step 1115, loss 0.648608, acc 0.84375\n",
      "2017-11-06T09:21:43.031774: step 1116, loss 0.151248, acc 0.9\n",
      "2017-11-06T09:21:47.013603: step 1117, loss 0.165472, acc 0.90625\n",
      "2017-11-06T09:21:50.994432: step 1118, loss 0.090781, acc 0.9375\n",
      "2017-11-06T09:21:55.020295: step 1119, loss 0.106124, acc 0.9375\n",
      "2017-11-06T09:21:59.001966: step 1120, loss 0.181113, acc 0.90625\n",
      "2017-11-06T09:22:02.974789: step 1121, loss 0.24367, acc 0.90625\n",
      "2017-11-06T09:22:06.980636: step 1122, loss 0.236623, acc 0.90625\n",
      "2017-11-06T09:22:10.963470: step 1123, loss 0.121518, acc 0.90625\n",
      "2017-11-06T09:22:15.013343: step 1124, loss 0.326572, acc 0.875\n",
      "2017-11-06T09:22:19.054214: step 1125, loss 0.13692, acc 0.9375\n",
      "2017-11-06T09:22:22.994014: step 1126, loss 0.269179, acc 0.875\n",
      "2017-11-06T09:22:27.011868: step 1127, loss 0.444846, acc 0.84375\n",
      "2017-11-06T09:22:30.959673: step 1128, loss 0.204556, acc 0.90625\n",
      "2017-11-06T09:22:35.369807: step 1129, loss 0.172135, acc 0.9375\n",
      "2017-11-06T09:22:39.783943: step 1130, loss 0.0852733, acc 0.96875\n",
      "2017-11-06T09:22:43.815808: step 1131, loss 0.103056, acc 0.9375\n",
      "2017-11-06T09:22:47.775624: step 1132, loss 0.240739, acc 0.8125\n",
      "2017-11-06T09:22:51.809488: step 1133, loss 0.202735, acc 0.90625\n",
      "2017-11-06T09:22:55.840352: step 1134, loss 0.202497, acc 0.875\n",
      "2017-11-06T09:22:59.852203: step 1135, loss 0.324281, acc 0.84375\n",
      "2017-11-06T09:23:03.921094: step 1136, loss 0.226738, acc 0.875\n",
      "2017-11-06T09:23:07.909928: step 1137, loss 0.446165, acc 0.84375\n",
      "2017-11-06T09:23:11.943796: step 1138, loss 0.164309, acc 0.9375\n",
      "2017-11-06T09:23:15.950642: step 1139, loss 0.100709, acc 0.9375\n",
      "2017-11-06T09:23:19.955487: step 1140, loss 0.218088, acc 0.9375\n",
      "2017-11-06T09:23:23.997360: step 1141, loss 0.0333341, acc 1\n",
      "2017-11-06T09:23:28.224363: step 1142, loss 0.357455, acc 0.875\n",
      "2017-11-06T09:23:32.245267: step 1143, loss 0.187045, acc 0.96875\n",
      "2017-11-06T09:23:36.224094: step 1144, loss 0.177103, acc 0.9375\n",
      "2017-11-06T09:23:40.244952: step 1145, loss 0.586564, acc 0.84375\n",
      "2017-11-06T09:23:44.705121: step 1146, loss 0.188793, acc 0.90625\n",
      "2017-11-06T09:23:48.705963: step 1147, loss 0.32407, acc 0.90625\n",
      "2017-11-06T09:23:52.738829: step 1148, loss 0.0919557, acc 0.9375\n",
      "2017-11-06T09:23:56.797713: step 1149, loss 0.0848838, acc 0.96875\n",
      "2017-11-06T09:24:00.790550: step 1150, loss 0.267596, acc 0.8125\n",
      "2017-11-06T09:24:04.725345: step 1151, loss 0.117145, acc 0.96875\n",
      "2017-11-06T09:24:07.249139: step 1152, loss 0.292336, acc 0.85\n",
      "2017-11-06T09:24:11.271998: step 1153, loss 0.100388, acc 0.96875\n",
      "2017-11-06T09:24:15.334884: step 1154, loss 0.25772, acc 0.875\n",
      "2017-11-06T09:24:19.284691: step 1155, loss 0.188524, acc 0.875\n",
      "2017-11-06T09:24:23.276527: step 1156, loss 0.197246, acc 0.9375\n",
      "2017-11-06T09:24:27.285376: step 1157, loss 0.0956207, acc 0.96875\n",
      "2017-11-06T09:24:31.339257: step 1158, loss 0.13257, acc 0.96875\n",
      "2017-11-06T09:24:35.562257: step 1159, loss 0.269363, acc 0.90625\n",
      "2017-11-06T09:24:39.571107: step 1160, loss 0.144959, acc 0.9375\n",
      "2017-11-06T09:24:43.535922: step 1161, loss 0.127366, acc 0.90625\n",
      "2017-11-06T09:24:47.764927: step 1162, loss 0.156336, acc 0.9375\n",
      "2017-11-06T09:24:52.040966: step 1163, loss 0.263809, acc 0.90625\n",
      "2017-11-06T09:24:56.055818: step 1164, loss 0.322853, acc 0.84375\n",
      "2017-11-06T09:25:00.074449: step 1165, loss 0.146634, acc 0.9375\n",
      "2017-11-06T09:25:04.117321: step 1166, loss 0.0728876, acc 0.96875\n",
      "2017-11-06T09:25:08.126170: step 1167, loss 0.421501, acc 0.75\n",
      "2017-11-06T09:25:12.113003: step 1168, loss 0.234672, acc 0.875\n",
      "2017-11-06T09:25:16.173888: step 1169, loss 0.0862718, acc 0.96875\n",
      "2017-11-06T09:25:20.157719: step 1170, loss 0.291163, acc 0.875\n",
      "2017-11-06T09:25:24.191585: step 1171, loss 0.144892, acc 0.90625\n",
      "2017-11-06T09:25:28.287495: step 1172, loss 0.272421, acc 0.90625\n",
      "2017-11-06T09:25:32.305350: step 1173, loss 0.220463, acc 0.875\n",
      "2017-11-06T09:25:36.320203: step 1174, loss 0.178411, acc 0.90625\n",
      "2017-11-06T09:25:40.338058: step 1175, loss 0.157557, acc 0.90625\n",
      "2017-11-06T09:25:44.305877: step 1176, loss 0.107208, acc 0.9375\n",
      "2017-11-06T09:25:48.298715: step 1177, loss 0.647198, acc 0.78125\n",
      "2017-11-06T09:25:52.521715: step 1178, loss 0.0576299, acc 1\n",
      "2017-11-06T09:25:56.896823: step 1179, loss 0.457854, acc 0.875\n",
      "2017-11-06T09:26:00.898667: step 1180, loss 0.132062, acc 0.90625\n",
      "2017-11-06T09:26:04.889504: step 1181, loss 0.389992, acc 0.78125\n",
      "2017-11-06T09:26:08.865328: step 1182, loss 0.215667, acc 0.9375\n",
      "2017-11-06T09:26:12.914205: step 1183, loss 0.248376, acc 0.875\n",
      "2017-11-06T09:26:16.951073: step 1184, loss 0.213911, acc 0.90625\n",
      "2017-11-06T09:26:20.879864: step 1185, loss 0.406811, acc 0.8125\n",
      "2017-11-06T09:26:24.908727: step 1186, loss 0.260117, acc 0.84375\n",
      "2017-11-06T09:26:28.890575: step 1187, loss 0.383632, acc 0.78125\n",
      "2017-11-06T09:26:31.432362: step 1188, loss 0.0238643, acc 1\n",
      "2017-11-06T09:26:35.550288: step 1189, loss 0.188951, acc 0.9375\n",
      "2017-11-06T09:26:39.675220: step 1190, loss 0.112735, acc 0.9375\n",
      "2017-11-06T09:26:43.603010: step 1191, loss 0.268238, acc 0.875\n",
      "2017-11-06T09:26:47.617864: step 1192, loss 0.158753, acc 0.875\n",
      "2017-11-06T09:26:51.604697: step 1193, loss 0.0866805, acc 0.96875\n",
      "2017-11-06T09:26:55.603538: step 1194, loss 0.0795348, acc 0.96875\n",
      "2017-11-06T09:27:00.005665: step 1195, loss 0.144205, acc 0.9375\n",
      "2017-11-06T09:27:04.220660: step 1196, loss 0.175386, acc 0.9375\n",
      "2017-11-06T09:27:08.153454: step 1197, loss 0.182375, acc 0.90625\n",
      "2017-11-06T09:27:12.183319: step 1198, loss 0.106088, acc 0.96875\n",
      "2017-11-06T09:27:16.186162: step 1199, loss 0.177075, acc 0.90625\n",
      "2017-11-06T09:27:20.270067: step 1200, loss 0.298494, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:27:22.904936: step 1200, loss 1.02983, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-06T09:27:28.321503: step 1201, loss 0.124771, acc 0.9375\n",
      "2017-11-06T09:27:32.335440: step 1202, loss 0.395638, acc 0.78125\n",
      "2017-11-06T09:27:36.315268: step 1203, loss 0.157288, acc 0.90625\n",
      "2017-11-06T09:27:40.623928: step 1204, loss 0.396087, acc 0.84375\n",
      "2017-11-06T09:27:44.989029: step 1205, loss 0.23705, acc 0.90625\n",
      "2017-11-06T09:27:49.176614: step 1206, loss 0.16615, acc 0.875\n",
      "2017-11-06T09:27:53.359586: step 1207, loss 0.340691, acc 0.84375\n",
      "2017-11-06T09:27:57.380943: step 1208, loss 0.0591264, acc 0.96875\n",
      "2017-11-06T09:28:01.391795: step 1209, loss 0.131575, acc 0.90625\n",
      "2017-11-06T09:28:05.757898: step 1210, loss 0.226783, acc 0.90625\n",
      "2017-11-06T09:28:09.882828: step 1211, loss 0.217843, acc 0.9375\n",
      "2017-11-06T09:28:13.852649: step 1212, loss 0.287765, acc 0.875\n",
      "2017-11-06T09:28:17.958567: step 1213, loss 0.245146, acc 0.875\n",
      "2017-11-06T09:28:22.016450: step 1214, loss 0.0761838, acc 0.9375\n",
      "2017-11-06T09:28:26.003283: step 1215, loss 0.12549, acc 0.9375\n",
      "2017-11-06T09:28:29.993119: step 1216, loss 0.232154, acc 0.9375\n",
      "2017-11-06T09:28:34.228128: step 1217, loss 0.230507, acc 0.90625\n",
      "2017-11-06T09:28:38.365085: step 1218, loss 0.187578, acc 0.90625\n",
      "2017-11-06T09:28:42.367911: step 1219, loss 0.215115, acc 0.90625\n",
      "2017-11-06T09:28:46.392772: step 1220, loss 0.425544, acc 0.875\n",
      "2017-11-06T09:28:50.421633: step 1221, loss 0.180972, acc 0.9375\n",
      "2017-11-06T09:28:54.467509: step 1222, loss 0.127668, acc 0.9375\n",
      "2017-11-06T09:28:58.509381: step 1223, loss 0.150611, acc 0.9375\n",
      "2017-11-06T09:29:01.027169: step 1224, loss 0.0508265, acc 1\n",
      "2017-11-06T09:29:05.086053: step 1225, loss 0.183154, acc 0.9375\n",
      "2017-11-06T09:29:09.171956: step 1226, loss 0.19243, acc 0.84375\n",
      "2017-11-06T09:29:13.526050: step 1227, loss 0.20101, acc 0.90625\n",
      "2017-11-06T09:29:17.568922: step 1228, loss 0.133968, acc 0.90625\n",
      "2017-11-06T09:29:21.632810: step 1229, loss 0.207308, acc 0.9375\n",
      "2017-11-06T09:29:25.704703: step 1230, loss 0.0354889, acc 1\n",
      "2017-11-06T09:29:29.943717: step 1231, loss 0.243232, acc 0.90625\n",
      "2017-11-06T09:29:34.085969: step 1232, loss 0.424816, acc 0.875\n",
      "2017-11-06T09:29:38.103825: step 1233, loss 0.0595208, acc 0.96875\n",
      "2017-11-06T09:29:42.076647: step 1234, loss 0.203423, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T09:29:46.084495: step 1235, loss 0.182206, acc 0.9375\n",
      "2017-11-06T09:29:50.151384: step 1236, loss 0.299224, acc 0.875\n",
      "2017-11-06T09:29:54.177245: step 1237, loss 0.0807542, acc 0.96875\n",
      "2017-11-06T09:29:58.242133: step 1238, loss 0.297451, acc 0.84375\n",
      "2017-11-06T09:30:02.652267: step 1239, loss 0.0818058, acc 0.96875\n",
      "2017-11-06T09:30:06.791208: step 1240, loss 0.198448, acc 0.90625\n",
      "2017-11-06T09:30:10.805060: step 1241, loss 0.359063, acc 0.875\n",
      "2017-11-06T09:30:14.947003: step 1242, loss 0.176053, acc 0.9375\n",
      "2017-11-06T09:30:19.415178: step 1243, loss 0.323421, acc 0.84375\n",
      "2017-11-06T09:30:23.501081: step 1244, loss 0.171605, acc 0.90625\n",
      "2017-11-06T09:30:27.509929: step 1245, loss 0.313694, acc 0.875\n",
      "2017-11-06T09:30:31.621851: step 1246, loss 0.221866, acc 0.9375\n",
      "2017-11-06T09:30:35.799822: step 1247, loss 0.442071, acc 0.75\n",
      "2017-11-06T09:30:39.863707: step 1248, loss 0.0256039, acc 1\n",
      "2017-11-06T09:30:43.855545: step 1249, loss 0.219151, acc 0.90625\n",
      "2017-11-06T09:30:47.814358: step 1250, loss 0.353277, acc 0.84375\n",
      "2017-11-06T09:30:51.794186: step 1251, loss 0.346041, acc 0.84375\n",
      "2017-11-06T09:30:55.807035: step 1252, loss 0.129105, acc 0.90625\n",
      "2017-11-06T09:30:59.805641: step 1253, loss 0.0660245, acc 0.96875\n",
      "2017-11-06T09:31:03.837506: step 1254, loss 0.440862, acc 0.875\n",
      "2017-11-06T09:31:07.837346: step 1255, loss 0.339255, acc 0.875\n",
      "2017-11-06T09:31:11.865209: step 1256, loss 0.16396, acc 0.90625\n",
      "2017-11-06T09:31:15.913085: step 1257, loss 0.267017, acc 0.90625\n",
      "2017-11-06T09:31:20.084049: step 1258, loss 0.196274, acc 0.90625\n",
      "2017-11-06T09:31:24.514197: step 1259, loss 0.175531, acc 0.90625\n",
      "2017-11-06T09:31:27.073016: step 1260, loss 0.525853, acc 0.85\n",
      "2017-11-06T09:31:31.068873: step 1261, loss 0.337662, acc 0.90625\n",
      "2017-11-06T09:31:35.242820: step 1262, loss 0.295301, acc 0.84375\n",
      "2017-11-06T09:31:39.440804: step 1263, loss 0.0394772, acc 1\n",
      "2017-11-06T09:31:43.469665: step 1264, loss 0.154286, acc 0.96875\n",
      "2017-11-06T09:31:47.390451: step 1265, loss 0.107697, acc 0.9375\n",
      "2017-11-06T09:31:51.429321: step 1266, loss 0.351132, acc 0.78125\n",
      "2017-11-06T09:31:55.443192: step 1267, loss 0.461059, acc 0.84375\n",
      "2017-11-06T09:31:59.424004: step 1268, loss 0.247153, acc 0.84375\n",
      "2017-11-06T09:32:03.410853: step 1269, loss 0.347495, acc 0.84375\n",
      "2017-11-06T09:32:07.485731: step 1270, loss 0.363966, acc 0.84375\n",
      "2017-11-06T09:32:11.476565: step 1271, loss 0.302242, acc 0.875\n",
      "2017-11-06T09:32:15.473407: step 1272, loss 0.265152, acc 0.84375\n",
      "2017-11-06T09:32:19.528286: step 1273, loss 0.45034, acc 0.875\n",
      "2017-11-06T09:32:23.611188: step 1274, loss 0.272867, acc 0.875\n",
      "2017-11-06T09:32:27.978290: step 1275, loss 0.210245, acc 0.90625\n",
      "2017-11-06T09:32:32.143250: step 1276, loss 0.0889592, acc 0.96875\n",
      "2017-11-06T09:32:36.432297: step 1277, loss 0.237275, acc 0.875\n",
      "2017-11-06T09:32:40.408122: step 1278, loss 0.195827, acc 0.9375\n",
      "2017-11-06T09:32:44.395956: step 1279, loss 0.292971, acc 0.875\n",
      "2017-11-06T09:32:48.437828: step 1280, loss 0.0964991, acc 0.96875\n",
      "2017-11-06T09:32:52.380629: step 1281, loss 0.244254, acc 0.875\n",
      "2017-11-06T09:32:56.451522: step 1282, loss 0.142271, acc 0.90625\n",
      "2017-11-06T09:33:00.523415: step 1283, loss 0.318942, acc 0.875\n",
      "2017-11-06T09:33:04.506244: step 1284, loss 0.186944, acc 0.90625\n",
      "2017-11-06T09:33:08.561126: step 1285, loss 0.131824, acc 0.90625\n",
      "2017-11-06T09:33:12.546958: step 1286, loss 0.0726559, acc 0.96875\n",
      "2017-11-06T09:33:16.508773: step 1287, loss 0.220067, acc 0.90625\n",
      "2017-11-06T09:33:20.497608: step 1288, loss 0.310704, acc 0.90625\n",
      "2017-11-06T09:33:24.634546: step 1289, loss 0.343628, acc 0.90625\n",
      "2017-11-06T09:33:28.737462: step 1290, loss 0.205454, acc 0.90625\n",
      "2017-11-06T09:33:32.978476: step 1291, loss 0.0371375, acc 1\n",
      "2017-11-06T09:33:37.288538: step 1292, loss 0.156681, acc 0.9375\n",
      "2017-11-06T09:33:41.554569: step 1293, loss 0.116954, acc 0.9375\n",
      "2017-11-06T09:33:45.567421: step 1294, loss 0.478779, acc 0.75\n",
      "2017-11-06T09:33:49.800428: step 1295, loss 0.215684, acc 0.90625\n",
      "2017-11-06T09:33:52.343235: step 1296, loss 0.0616815, acc 1\n",
      "2017-11-06T09:33:56.475171: step 1297, loss 0.0996435, acc 0.96875\n",
      "2017-11-06T09:34:00.708030: step 1298, loss 0.0873237, acc 0.96875\n",
      "2017-11-06T09:34:04.727887: step 1299, loss 0.0500426, acc 1\n",
      "2017-11-06T09:34:08.801780: step 1300, loss 0.0936669, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:34:11.527718: step 1300, loss 1.03623, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-06T09:34:17.376893: step 1301, loss 0.152327, acc 0.90625\n",
      "2017-11-06T09:34:21.562867: step 1302, loss 0.176847, acc 0.90625\n",
      "2017-11-06T09:34:25.751845: step 1303, loss 0.128129, acc 0.9375\n",
      "2017-11-06T09:34:29.945823: step 1304, loss 0.260954, acc 0.90625\n",
      "2017-11-06T09:34:34.331940: step 1305, loss 0.298272, acc 0.875\n",
      "2017-11-06T09:34:38.698043: step 1306, loss 0.060977, acc 0.96875\n",
      "2017-11-06T09:34:43.095167: step 1307, loss 0.063598, acc 0.9375\n",
      "2017-11-06T09:34:47.125030: step 1308, loss 0.279779, acc 0.875\n",
      "2017-11-06T09:34:51.077839: step 1309, loss 0.181786, acc 0.875\n",
      "2017-11-06T09:34:55.124715: step 1310, loss 0.416206, acc 0.84375\n",
      "2017-11-06T09:34:59.168589: step 1311, loss 0.294749, acc 0.875\n",
      "2017-11-06T09:35:03.204457: step 1312, loss 0.111943, acc 0.9375\n",
      "2017-11-06T09:35:07.189286: step 1313, loss 0.13999, acc 0.90625\n",
      "2017-11-06T09:35:11.207141: step 1314, loss 0.250287, acc 0.90625\n",
      "2017-11-06T09:35:15.257019: step 1315, loss 0.302242, acc 0.84375\n",
      "2017-11-06T09:35:19.284881: step 1316, loss 0.264711, acc 0.90625\n",
      "2017-11-06T09:35:23.358776: step 1317, loss 0.244122, acc 0.90625\n",
      "2017-11-06T09:35:27.411658: step 1318, loss 0.131419, acc 0.90625\n",
      "2017-11-06T09:35:31.603634: step 1319, loss 0.155595, acc 0.9375\n",
      "2017-11-06T09:35:35.610481: step 1320, loss 0.279904, acc 0.875\n",
      "2017-11-06T09:35:39.924188: step 1321, loss 0.355034, acc 0.84375\n",
      "2017-11-06T09:35:44.244258: step 1322, loss 0.341675, acc 0.875\n",
      "2017-11-06T09:35:48.372190: step 1323, loss 0.374136, acc 0.8125\n",
      "2017-11-06T09:35:52.396049: step 1324, loss 0.31146, acc 0.90625\n",
      "2017-11-06T09:35:56.383882: step 1325, loss 0.293301, acc 0.90625\n",
      "2017-11-06T09:36:00.484796: step 1326, loss 0.143885, acc 0.96875\n",
      "2017-11-06T09:36:04.505653: step 1327, loss 0.463788, acc 0.78125\n",
      "2017-11-06T09:36:08.529512: step 1328, loss 0.274358, acc 0.90625\n",
      "2017-11-06T09:36:12.517346: step 1329, loss 0.300403, acc 0.875\n",
      "2017-11-06T09:36:16.595244: step 1330, loss 0.537779, acc 0.75\n",
      "2017-11-06T09:36:20.644121: step 1331, loss 0.385055, acc 0.84375\n",
      "2017-11-06T09:36:23.182926: step 1332, loss 0.283951, acc 0.85\n",
      "2017-11-06T09:36:27.229800: step 1333, loss 0.31439, acc 0.90625\n",
      "2017-11-06T09:36:31.238649: step 1334, loss 0.205034, acc 0.9375\n",
      "2017-11-06T09:36:35.472659: step 1335, loss 0.147628, acc 0.90625\n",
      "2017-11-06T09:36:39.471498: step 1336, loss 0.111677, acc 0.90625\n",
      "2017-11-06T09:36:43.484350: step 1337, loss 0.246946, acc 0.9375\n",
      "2017-11-06T09:36:47.537229: step 1338, loss 0.438122, acc 0.78125\n",
      "2017-11-06T09:36:52.037429: step 1339, loss 0.0719365, acc 0.96875\n",
      "2017-11-06T09:36:56.057283: step 1340, loss 0.185325, acc 0.90625\n",
      "2017-11-06T09:37:00.090911: step 1341, loss 0.348741, acc 0.84375\n",
      "2017-11-06T09:37:04.205834: step 1342, loss 0.21111, acc 0.90625\n",
      "2017-11-06T09:37:08.229693: step 1343, loss 0.118946, acc 0.9375\n",
      "2017-11-06T09:37:12.212524: step 1344, loss 0.139313, acc 0.96875\n",
      "2017-11-06T09:37:16.233380: step 1345, loss 0.0922343, acc 0.96875\n",
      "2017-11-06T09:37:20.257239: step 1346, loss 0.155285, acc 0.90625\n",
      "2017-11-06T09:37:24.203043: step 1347, loss 0.233244, acc 0.90625\n",
      "2017-11-06T09:37:28.243914: step 1348, loss 0.191227, acc 0.875\n",
      "2017-11-06T09:37:32.227745: step 1349, loss 0.159384, acc 0.90625\n",
      "2017-11-06T09:37:36.188560: step 1350, loss 0.163042, acc 0.90625\n",
      "2017-11-06T09:37:40.490408: step 1351, loss 0.159257, acc 0.9375\n",
      "2017-11-06T09:37:44.424203: step 1352, loss 0.235314, acc 0.84375\n",
      "2017-11-06T09:37:48.321972: step 1353, loss 0.178148, acc 0.90625\n",
      "2017-11-06T09:37:52.271779: step 1354, loss 0.114089, acc 0.96875\n",
      "2017-11-06T09:37:56.632879: step 1355, loss 0.223874, acc 0.875\n",
      "2017-11-06T09:38:00.730789: step 1356, loss 0.157124, acc 0.90625\n",
      "2017-11-06T09:38:04.670590: step 1357, loss 0.188975, acc 0.90625\n",
      "2017-11-06T09:38:08.615392: step 1358, loss 0.170414, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T09:38:12.563198: step 1359, loss 0.257251, acc 0.90625\n",
      "2017-11-06T09:38:16.485985: step 1360, loss 0.199202, acc 0.90625\n",
      "2017-11-06T09:38:20.560879: step 1361, loss 0.271936, acc 0.9375\n",
      "2017-11-06T09:38:24.565725: step 1362, loss 0.160314, acc 0.9375\n",
      "2017-11-06T09:38:28.592586: step 1363, loss 0.477801, acc 0.78125\n",
      "2017-11-06T09:38:32.604436: step 1364, loss 0.178254, acc 0.875\n",
      "2017-11-06T09:38:36.815429: step 1365, loss 0.335557, acc 0.9375\n",
      "2017-11-06T09:38:40.885320: step 1366, loss 0.533379, acc 0.8125\n",
      "2017-11-06T09:38:44.836128: step 1367, loss 0.27625, acc 0.875\n",
      "2017-11-06T09:38:47.440979: step 1368, loss 0.124248, acc 0.9\n",
      "2017-11-06T09:38:51.460837: step 1369, loss 0.107961, acc 0.9375\n",
      "2017-11-06T09:38:55.400634: step 1370, loss 0.204828, acc 0.84375\n",
      "2017-11-06T09:38:59.552584: step 1371, loss 0.12192, acc 0.9375\n",
      "2017-11-06T09:39:03.959715: step 1372, loss 0.409813, acc 0.84375\n",
      "2017-11-06T09:39:07.847478: step 1373, loss 0.204718, acc 0.90625\n",
      "2017-11-06T09:39:11.797287: step 1374, loss 0.36437, acc 0.8125\n",
      "2017-11-06T09:39:15.772110: step 1375, loss 0.332666, acc 0.84375\n",
      "2017-11-06T09:39:19.711909: step 1376, loss 0.206007, acc 0.90625\n",
      "2017-11-06T09:39:23.708750: step 1377, loss 0.129792, acc 0.875\n",
      "2017-11-06T09:39:27.959769: step 1378, loss 0.335754, acc 0.875\n",
      "2017-11-06T09:39:31.937596: step 1379, loss 0.104702, acc 0.96875\n",
      "2017-11-06T09:39:36.025500: step 1380, loss 0.177755, acc 0.90625\n",
      "2017-11-06T09:39:40.198465: step 1381, loss 0.0374847, acc 1\n",
      "2017-11-06T09:39:44.267356: step 1382, loss 0.21487, acc 0.84375\n",
      "2017-11-06T09:39:48.273204: step 1383, loss 0.186778, acc 0.9375\n",
      "2017-11-06T09:39:52.264038: step 1384, loss 0.222252, acc 0.90625\n",
      "2017-11-06T09:39:56.232859: step 1385, loss 0.30779, acc 0.8125\n",
      "2017-11-06T09:40:00.213446: step 1386, loss 0.283291, acc 0.90625\n",
      "2017-11-06T09:40:04.532513: step 1387, loss 0.233621, acc 0.9375\n",
      "2017-11-06T09:40:08.934641: step 1388, loss 0.393362, acc 0.875\n",
      "2017-11-06T09:40:12.881445: step 1389, loss 0.332052, acc 0.84375\n",
      "2017-11-06T09:40:16.876285: step 1390, loss 0.160541, acc 0.90625\n",
      "2017-11-06T09:40:20.927162: step 1391, loss 0.141916, acc 0.90625\n",
      "2017-11-06T09:40:24.987048: step 1392, loss 0.261598, acc 0.9375\n",
      "2017-11-06T09:40:28.989891: step 1393, loss 0.204708, acc 0.90625\n",
      "2017-11-06T09:40:33.084800: step 1394, loss 0.206097, acc 0.90625\n",
      "2017-11-06T09:40:37.323812: step 1395, loss 0.22773, acc 0.84375\n",
      "2017-11-06T09:40:41.375691: step 1396, loss 0.0496935, acc 1\n",
      "2017-11-06T09:40:45.396548: step 1397, loss 0.0418927, acc 0.96875\n",
      "2017-11-06T09:40:49.396390: step 1398, loss 0.249299, acc 0.875\n",
      "2017-11-06T09:40:53.346197: step 1399, loss 0.225273, acc 0.90625\n",
      "2017-11-06T09:40:57.353043: step 1400, loss 0.120297, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:40:59.918866: step 1400, loss 1.15254, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-06T09:41:05.893678: step 1401, loss 0.527303, acc 0.8125\n",
      "2017-11-06T09:41:09.969573: step 1402, loss 0.0689888, acc 0.9375\n",
      "2017-11-06T09:41:15.187281: step 1403, loss 0.366275, acc 0.90625\n",
      "2017-11-06T09:41:18.004282: step 1404, loss 0.405828, acc 0.85\n",
      "2017-11-06T09:41:22.188256: step 1405, loss 0.13788, acc 0.90625\n",
      "2017-11-06T09:41:26.214117: step 1406, loss 0.191095, acc 0.96875\n",
      "2017-11-06T09:41:30.143908: step 1407, loss 0.303347, acc 0.8125\n",
      "2017-11-06T09:41:34.145751: step 1408, loss 0.209643, acc 0.9375\n",
      "2017-11-06T09:41:38.166615: step 1409, loss 0.137646, acc 0.96875\n",
      "2017-11-06T09:41:42.118417: step 1410, loss 0.67751, acc 0.71875\n",
      "2017-11-06T09:41:46.247443: step 1411, loss 0.14142, acc 0.9375\n",
      "2017-11-06T09:41:50.255292: step 1412, loss 0.0845384, acc 0.96875\n",
      "2017-11-06T09:41:54.210101: step 1413, loss 0.119171, acc 0.9375\n",
      "2017-11-06T09:41:58.167913: step 1414, loss 0.0859135, acc 0.96875\n",
      "2017-11-06T09:42:02.111716: step 1415, loss 0.10772, acc 0.96875\n",
      "2017-11-06T09:42:06.089542: step 1416, loss 0.545618, acc 0.8125\n",
      "2017-11-06T09:42:10.058362: step 1417, loss 0.575988, acc 0.8125\n",
      "2017-11-06T09:42:14.067210: step 1418, loss 0.300292, acc 0.90625\n",
      "2017-11-06T09:42:18.324235: step 1419, loss 0.0864518, acc 0.9375\n",
      "2017-11-06T09:42:22.427151: step 1420, loss 0.303525, acc 0.90625\n",
      "2017-11-06T09:42:26.495042: step 1421, loss 0.135709, acc 0.96875\n",
      "2017-11-06T09:42:30.452853: step 1422, loss 0.15045, acc 0.9375\n",
      "2017-11-06T09:42:34.697869: step 1423, loss 0.25217, acc 0.875\n",
      "2017-11-06T09:42:38.730734: step 1424, loss 0.153914, acc 0.90625\n",
      "2017-11-06T09:42:42.728575: step 1425, loss 0.305781, acc 0.84375\n",
      "2017-11-06T09:42:46.691391: step 1426, loss 0.0863649, acc 0.9375\n",
      "2017-11-06T09:42:50.629189: step 1427, loss 0.196901, acc 0.84375\n",
      "2017-11-06T09:42:54.765129: step 1428, loss 0.216195, acc 0.875\n",
      "2017-11-06T09:42:58.735950: step 1429, loss 0.351157, acc 0.78125\n",
      "2017-11-06T09:43:02.729548: step 1430, loss 0.135574, acc 0.96875\n",
      "2017-11-06T09:43:06.680356: step 1431, loss 0.0416825, acc 0.96875\n",
      "2017-11-06T09:43:10.661184: step 1432, loss 0.206505, acc 0.90625\n",
      "2017-11-06T09:43:14.697052: step 1433, loss 0.193252, acc 0.90625\n",
      "2017-11-06T09:43:18.645857: step 1434, loss 0.341921, acc 0.90625\n",
      "2017-11-06T09:43:22.876864: step 1435, loss 0.373677, acc 0.78125\n",
      "2017-11-06T09:43:27.333031: step 1436, loss 0.280949, acc 0.875\n",
      "2017-11-06T09:43:31.285839: step 1437, loss 0.155133, acc 0.90625\n",
      "2017-11-06T09:43:35.260663: step 1438, loss 0.240187, acc 0.90625\n",
      "2017-11-06T09:43:39.301534: step 1439, loss 0.233343, acc 0.90625\n",
      "2017-11-06T09:43:41.890374: step 1440, loss 0.331612, acc 0.95\n",
      "2017-11-06T09:43:45.898222: step 1441, loss 0.159977, acc 0.875\n",
      "2017-11-06T09:43:49.968202: step 1442, loss 0.0824769, acc 0.96875\n",
      "2017-11-06T09:43:53.953034: step 1443, loss 0.232869, acc 0.90625\n",
      "2017-11-06T09:43:57.896837: step 1444, loss 0.226151, acc 0.9375\n",
      "2017-11-06T09:44:01.827630: step 1445, loss 0.251735, acc 0.875\n",
      "2017-11-06T09:44:05.798451: step 1446, loss 0.156282, acc 0.875\n",
      "2017-11-06T09:44:09.741252: step 1447, loss 0.0526075, acc 1\n",
      "2017-11-06T09:44:13.691059: step 1448, loss 0.18768, acc 0.84375\n",
      "2017-11-06T09:44:17.656877: step 1449, loss 0.123912, acc 0.9375\n",
      "2017-11-06T09:44:21.585668: step 1450, loss 0.351214, acc 0.875\n",
      "2017-11-06T09:44:25.622537: step 1451, loss 0.36641, acc 0.78125\n",
      "2017-11-06T09:44:29.936602: step 1452, loss 0.318886, acc 0.84375\n",
      "2017-11-06T09:44:34.292697: step 1453, loss 0.431579, acc 0.875\n",
      "2017-11-06T09:44:38.339573: step 1454, loss 0.205796, acc 0.875\n",
      "2017-11-06T09:44:42.334411: step 1455, loss 0.218135, acc 0.875\n",
      "2017-11-06T09:44:46.346262: step 1456, loss 0.34297, acc 0.8125\n",
      "2017-11-06T09:44:50.333095: step 1457, loss 0.286903, acc 0.875\n",
      "2017-11-06T09:44:54.356954: step 1458, loss 0.0786962, acc 1\n",
      "2017-11-06T09:44:58.330777: step 1459, loss 0.167872, acc 0.9375\n",
      "2017-11-06T09:45:02.368646: step 1460, loss 0.235746, acc 0.875\n",
      "2017-11-06T09:45:06.584642: step 1461, loss 0.169181, acc 0.9375\n",
      "2017-11-06T09:45:10.722582: step 1462, loss 0.285612, acc 0.875\n",
      "2017-11-06T09:45:14.847513: step 1463, loss 0.0454182, acc 1\n",
      "2017-11-06T09:45:19.271926: step 1464, loss 0.476148, acc 0.8125\n",
      "2017-11-06T09:45:23.529519: step 1465, loss 0.176777, acc 0.9375\n",
      "2017-11-06T09:45:27.854097: step 1466, loss 0.0548494, acc 1\n",
      "2017-11-06T09:45:32.054082: step 1467, loss 0.151753, acc 0.9375\n",
      "2017-11-06T09:45:36.551778: step 1468, loss 0.268538, acc 0.90625\n",
      "2017-11-06T09:45:40.607661: step 1469, loss 0.164952, acc 0.90625\n",
      "2017-11-06T09:45:44.676553: step 1470, loss 0.120402, acc 0.96875\n",
      "2017-11-06T09:45:48.689404: step 1471, loss 0.199262, acc 0.9375\n",
      "2017-11-06T09:45:52.680329: step 1472, loss 0.136251, acc 0.9375\n",
      "2017-11-06T09:45:56.710192: step 1473, loss 0.201896, acc 0.90625\n",
      "2017-11-06T09:46:00.656749: step 1474, loss 0.250035, acc 0.875\n",
      "2017-11-06T09:46:04.648585: step 1475, loss 0.039791, acc 1\n",
      "2017-11-06T09:46:07.217411: step 1476, loss 0.245784, acc 0.95\n",
      "2017-11-06T09:46:11.269290: step 1477, loss 0.21227, acc 0.90625\n",
      "2017-11-06T09:46:15.174064: step 1478, loss 0.15048, acc 0.90625\n",
      "2017-11-06T09:46:19.116866: step 1479, loss 0.0676783, acc 0.96875\n",
      "2017-11-06T09:46:23.123713: step 1480, loss 0.268588, acc 0.875\n",
      "2017-11-06T09:46:27.125556: step 1481, loss 0.114063, acc 0.9375\n",
      "2017-11-06T09:46:31.093375: step 1482, loss 0.157645, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T09:46:35.310372: step 1483, loss 0.146313, acc 0.90625\n",
      "2017-11-06T09:46:39.552386: step 1484, loss 0.0336017, acc 1\n",
      "2017-11-06T09:46:43.899475: step 1485, loss 0.260747, acc 0.8125\n",
      "2017-11-06T09:46:47.885326: step 1486, loss 0.373959, acc 0.875\n",
      "2017-11-06T09:46:51.831129: step 1487, loss 0.0973871, acc 0.96875\n",
      "2017-11-06T09:46:55.809938: step 1488, loss 0.279655, acc 0.90625\n",
      "2017-11-06T09:46:59.756742: step 1489, loss 0.0561226, acc 0.96875\n",
      "2017-11-06T09:47:03.753582: step 1490, loss 0.138508, acc 0.96875\n",
      "2017-11-06T09:47:07.785448: step 1491, loss 0.328435, acc 0.90625\n",
      "2017-11-06T09:47:11.791293: step 1492, loss 0.302316, acc 0.875\n",
      "2017-11-06T09:47:15.765117: step 1493, loss 0.191757, acc 0.875\n",
      "2017-11-06T09:47:19.753951: step 1494, loss 0.313406, acc 0.9375\n",
      "2017-11-06T09:47:23.775809: step 1495, loss 0.104277, acc 0.9375\n",
      "2017-11-06T09:47:27.817681: step 1496, loss 0.0590308, acc 0.96875\n",
      "2017-11-06T09:47:31.755479: step 1497, loss 0.117872, acc 0.9375\n",
      "2017-11-06T09:47:35.782340: step 1498, loss 0.183704, acc 0.9375\n",
      "2017-11-06T09:47:39.761167: step 1499, loss 0.271757, acc 0.84375\n",
      "2017-11-06T09:47:43.840065: step 1500, loss 0.284419, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:47:46.835193: step 1500, loss 0.879083, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-06T09:47:52.372411: step 1501, loss 0.218725, acc 0.90625\n",
      "2017-11-06T09:47:56.394268: step 1502, loss 0.370691, acc 0.8125\n",
      "2017-11-06T09:48:00.381103: step 1503, loss 0.259527, acc 0.84375\n",
      "2017-11-06T09:48:04.341915: step 1504, loss 0.264779, acc 0.90625\n",
      "2017-11-06T09:48:08.372780: step 1505, loss 0.329498, acc 0.8125\n",
      "2017-11-06T09:48:12.356612: step 1506, loss 0.163775, acc 0.90625\n",
      "2017-11-06T09:48:16.375466: step 1507, loss 0.133272, acc 0.9375\n",
      "2017-11-06T09:48:20.372306: step 1508, loss 0.181824, acc 0.90625\n",
      "2017-11-06T09:48:24.491233: step 1509, loss 0.251601, acc 0.9375\n",
      "2017-11-06T09:48:28.766270: step 1510, loss 0.173788, acc 0.9375\n",
      "2017-11-06T09:48:32.763110: step 1511, loss 0.06557, acc 0.96875\n",
      "2017-11-06T09:48:35.516066: step 1512, loss 0.204581, acc 0.9\n",
      "2017-11-06T09:48:39.525915: step 1513, loss 0.105889, acc 0.96875\n",
      "2017-11-06T09:48:43.467717: step 1514, loss 0.312385, acc 0.875\n",
      "2017-11-06T09:48:47.534606: step 1515, loss 0.28753, acc 0.875\n",
      "2017-11-06T09:48:51.774619: step 1516, loss 0.166268, acc 0.9375\n",
      "2017-11-06T09:48:55.967598: step 1517, loss 0.279933, acc 0.84375\n",
      "2017-11-06T09:48:59.979448: step 1518, loss 0.428775, acc 0.78125\n",
      "2017-11-06T09:49:03.921007: step 1519, loss 0.120009, acc 0.9375\n",
      "2017-11-06T09:49:07.983894: step 1520, loss 0.218725, acc 0.875\n",
      "2017-11-06T09:49:11.946709: step 1521, loss 0.095652, acc 0.96875\n",
      "2017-11-06T09:49:15.942549: step 1522, loss 0.242202, acc 0.90625\n",
      "2017-11-06T09:49:19.897359: step 1523, loss 0.230002, acc 0.90625\n",
      "2017-11-06T09:49:23.928223: step 1524, loss 0.335742, acc 0.8125\n",
      "2017-11-06T09:49:27.895042: step 1525, loss 0.115952, acc 0.9375\n",
      "2017-11-06T09:49:31.954926: step 1526, loss 0.566144, acc 0.75\n",
      "2017-11-06T09:49:36.015813: step 1527, loss 0.25479, acc 0.84375\n",
      "2017-11-06T09:49:39.994639: step 1528, loss 0.23759, acc 0.875\n",
      "2017-11-06T09:49:44.008491: step 1529, loss 0.0876026, acc 0.9375\n",
      "2017-11-06T09:49:48.065373: step 1530, loss 0.106546, acc 0.96875\n",
      "2017-11-06T09:49:52.115251: step 1531, loss 0.190167, acc 0.9375\n",
      "2017-11-06T09:49:56.284213: step 1532, loss 0.10833, acc 0.96875\n",
      "2017-11-06T09:50:00.810429: step 1533, loss 0.236209, acc 0.875\n",
      "2017-11-06T09:50:04.982394: step 1534, loss 0.305464, acc 0.90625\n",
      "2017-11-06T09:50:09.065295: step 1535, loss 0.176087, acc 0.9375\n",
      "2017-11-06T09:50:13.005093: step 1536, loss 0.218116, acc 0.84375\n",
      "2017-11-06T09:50:17.003935: step 1537, loss 0.180703, acc 0.875\n",
      "2017-11-06T09:50:21.022790: step 1538, loss 0.155215, acc 0.90625\n",
      "2017-11-06T09:50:25.073669: step 1539, loss 0.269224, acc 0.875\n",
      "2017-11-06T09:50:29.067507: step 1540, loss 0.176843, acc 0.9375\n",
      "2017-11-06T09:50:33.158414: step 1541, loss 0.27993, acc 0.90625\n",
      "2017-11-06T09:50:37.387419: step 1542, loss 0.16795, acc 0.90625\n",
      "2017-11-06T09:50:41.389263: step 1543, loss 0.181097, acc 0.90625\n",
      "2017-11-06T09:50:45.445145: step 1544, loss 0.149872, acc 0.96875\n",
      "2017-11-06T09:50:49.487016: step 1545, loss 0.191692, acc 0.90625\n",
      "2017-11-06T09:50:53.479853: step 1546, loss 0.281604, acc 0.875\n",
      "2017-11-06T09:50:57.446672: step 1547, loss 0.185538, acc 0.9375\n",
      "2017-11-06T09:51:00.022502: step 1548, loss 0.0264082, acc 1\n",
      "2017-11-06T09:51:04.466660: step 1549, loss 0.231432, acc 0.9375\n",
      "2017-11-06T09:51:08.568576: step 1550, loss 0.275552, acc 0.84375\n",
      "2017-11-06T09:51:12.532391: step 1551, loss 0.0640932, acc 0.96875\n",
      "2017-11-06T09:51:16.528230: step 1552, loss 0.102941, acc 0.9375\n",
      "2017-11-06T09:51:20.484040: step 1553, loss 0.206226, acc 0.9375\n",
      "2017-11-06T09:51:24.508900: step 1554, loss 0.395502, acc 0.8125\n",
      "2017-11-06T09:51:28.603810: step 1555, loss 0.0248681, acc 1\n",
      "2017-11-06T09:51:32.564624: step 1556, loss 0.390528, acc 0.875\n",
      "2017-11-06T09:51:36.521436: step 1557, loss 0.240122, acc 0.90625\n",
      "2017-11-06T09:51:40.539291: step 1558, loss 0.319312, acc 0.875\n",
      "2017-11-06T09:51:44.467100: step 1559, loss 0.628876, acc 0.78125\n",
      "2017-11-06T09:51:48.525966: step 1560, loss 0.368424, acc 0.8125\n",
      "2017-11-06T09:51:52.513801: step 1561, loss 0.136308, acc 0.90625\n",
      "2017-11-06T09:51:56.487623: step 1562, loss 0.21457, acc 0.90625\n",
      "2017-11-06T09:52:00.516485: step 1563, loss 0.353911, acc 0.9375\n",
      "2017-11-06T09:52:04.525135: step 1564, loss 0.175576, acc 0.90625\n",
      "2017-11-06T09:52:08.737129: step 1565, loss 0.0773726, acc 0.96875\n",
      "2017-11-06T09:52:12.947120: step 1566, loss 0.148977, acc 0.9375\n",
      "2017-11-06T09:52:17.037026: step 1567, loss 0.326696, acc 0.84375\n",
      "2017-11-06T09:52:20.993837: step 1568, loss 0.138384, acc 0.90625\n",
      "2017-11-06T09:52:24.991679: step 1569, loss 0.218376, acc 0.90625\n",
      "2017-11-06T09:52:29.050562: step 1570, loss 0.0961446, acc 0.90625\n",
      "2017-11-06T09:52:33.202512: step 1571, loss 0.0786468, acc 0.96875\n",
      "2017-11-06T09:52:37.357464: step 1572, loss 0.164987, acc 0.90625\n",
      "2017-11-06T09:52:41.391331: step 1573, loss 0.172174, acc 0.875\n",
      "2017-11-06T09:52:45.331130: step 1574, loss 0.10552, acc 0.96875\n",
      "2017-11-06T09:52:49.357991: step 1575, loss 0.13154, acc 0.9375\n",
      "2017-11-06T09:52:53.426883: step 1576, loss 0.19393, acc 0.9375\n",
      "2017-11-06T09:52:57.411714: step 1577, loss 0.201216, acc 0.875\n",
      "2017-11-06T09:53:01.377531: step 1578, loss 0.362203, acc 0.75\n",
      "2017-11-06T09:53:05.402391: step 1579, loss 0.321783, acc 0.8125\n",
      "2017-11-06T09:53:09.466279: step 1580, loss 0.0239627, acc 1\n",
      "2017-11-06T09:53:13.761331: step 1581, loss 0.168001, acc 0.90625\n",
      "2017-11-06T09:53:17.911279: step 1582, loss 0.178476, acc 0.90625\n",
      "2017-11-06T09:53:21.915125: step 1583, loss 0.13201, acc 0.9375\n",
      "2017-11-06T09:53:24.478946: step 1584, loss 0.0877755, acc 0.95\n",
      "2017-11-06T09:53:28.563849: step 1585, loss 0.312364, acc 0.90625\n",
      "2017-11-06T09:53:32.532669: step 1586, loss 0.303291, acc 0.875\n",
      "2017-11-06T09:53:36.522504: step 1587, loss 0.197957, acc 0.875\n",
      "2017-11-06T09:53:40.567377: step 1588, loss 0.268797, acc 0.90625\n",
      "2017-11-06T09:53:44.574224: step 1589, loss 0.127795, acc 0.90625\n",
      "2017-11-06T09:53:48.551050: step 1590, loss 0.274927, acc 0.8125\n",
      "2017-11-06T09:53:52.519870: step 1591, loss 0.442527, acc 0.78125\n",
      "2017-11-06T09:53:56.512707: step 1592, loss 0.249028, acc 0.875\n",
      "2017-11-06T09:54:00.554579: step 1593, loss 0.22209, acc 0.9375\n",
      "2017-11-06T09:54:04.556423: step 1594, loss 0.120244, acc 0.96875\n",
      "2017-11-06T09:54:08.595293: step 1595, loss 0.121342, acc 0.96875\n",
      "2017-11-06T09:54:12.600139: step 1596, loss 0.149932, acc 0.9375\n",
      "2017-11-06T09:54:16.621996: step 1597, loss 0.119025, acc 0.96875\n",
      "2017-11-06T09:54:21.055146: step 1598, loss 0.190749, acc 0.875\n",
      "2017-11-06T09:54:25.250127: step 1599, loss 0.241294, acc 0.84375\n",
      "2017-11-06T09:54:29.437101: step 1600, loss 0.175405, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T09:54:32.095991: step 1600, loss 0.894588, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-06T09:54:37.652390: step 1601, loss 0.163707, acc 0.90625\n",
      "2017-11-06T09:54:41.673247: step 1602, loss 0.0913542, acc 0.96875\n",
      "2017-11-06T09:54:45.699107: step 1603, loss 0.0998344, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T09:54:49.731974: step 1604, loss 0.16216, acc 0.9375\n",
      "2017-11-06T09:54:53.716804: step 1605, loss 0.13806, acc 0.90625\n",
      "2017-11-06T09:54:57.750671: step 1606, loss 0.315766, acc 0.875\n",
      "2017-11-06T09:55:01.745295: step 1607, loss 0.251507, acc 0.875\n",
      "2017-11-06T09:55:05.738132: step 1608, loss 0.0526097, acc 0.96875\n",
      "2017-11-06T09:55:09.831040: step 1609, loss 0.153488, acc 0.90625\n",
      "2017-11-06T09:55:13.813870: step 1610, loss 0.196359, acc 0.875\n",
      "2017-11-06T09:55:17.793698: step 1611, loss 0.0679572, acc 0.9375\n",
      "2017-11-06T09:55:21.787538: step 1612, loss 0.320453, acc 0.90625\n",
      "2017-11-06T09:55:26.151637: step 1613, loss 0.122278, acc 0.90625\n",
      "2017-11-06T09:55:30.368633: step 1614, loss 0.189524, acc 0.90625\n",
      "2017-11-06T09:55:34.510576: step 1615, loss 0.345811, acc 0.8125\n",
      "2017-11-06T09:55:38.589475: step 1616, loss 0.0964812, acc 0.96875\n",
      "2017-11-06T09:55:42.645357: step 1617, loss 0.143506, acc 0.875\n",
      "2017-11-06T09:55:46.718250: step 1618, loss 0.153054, acc 0.9375\n",
      "2017-11-06T09:55:50.774132: step 1619, loss 0.192327, acc 0.9375\n",
      "2017-11-06T09:55:53.382988: step 1620, loss 0.199572, acc 0.9\n",
      "2017-11-06T09:55:57.548946: step 1621, loss 0.193745, acc 0.90625\n",
      "2017-11-06T09:56:01.613834: step 1622, loss 0.355891, acc 0.84375\n",
      "2017-11-06T09:56:05.702740: step 1623, loss 0.333591, acc 0.8125\n",
      "2017-11-06T09:56:09.723597: step 1624, loss 0.16425, acc 0.9375\n",
      "2017-11-06T09:56:13.777478: step 1625, loss 0.196444, acc 0.9375\n",
      "2017-11-06T09:56:17.820350: step 1626, loss 0.249206, acc 0.875\n",
      "2017-11-06T09:56:21.834202: step 1627, loss 0.170169, acc 0.875\n",
      "2017-11-06T09:56:25.905094: step 1628, loss 0.202618, acc 0.875\n",
      "2017-11-06T09:56:30.281204: step 1629, loss 0.147658, acc 0.9375\n",
      "2017-11-06T09:56:34.877470: step 1630, loss 0.394416, acc 0.78125\n",
      "2017-11-06T09:56:38.937355: step 1631, loss 0.350694, acc 0.84375\n",
      "2017-11-06T09:56:42.968218: step 1632, loss 0.142158, acc 0.90625\n",
      "2017-11-06T09:56:47.008089: step 1633, loss 0.30832, acc 0.875\n",
      "2017-11-06T09:56:51.089990: step 1634, loss 0.383415, acc 0.84375\n",
      "2017-11-06T09:56:55.136865: step 1635, loss 0.0601818, acc 0.96875\n",
      "2017-11-06T09:56:59.208760: step 1636, loss 0.299002, acc 0.875\n",
      "2017-11-06T09:57:03.264640: step 1637, loss 0.13589, acc 0.9375\n",
      "2017-11-06T09:57:07.258478: step 1638, loss 0.229532, acc 0.90625\n",
      "2017-11-06T09:57:11.280336: step 1639, loss 0.0640862, acc 0.96875\n",
      "2017-11-06T09:57:15.342221: step 1640, loss 0.00522158, acc 1\n",
      "2017-11-06T09:57:19.353071: step 1641, loss 0.219491, acc 0.90625\n",
      "2017-11-06T09:57:23.443978: step 1642, loss 0.255783, acc 0.90625\n",
      "2017-11-06T09:57:27.494856: step 1643, loss 0.154872, acc 0.9375\n",
      "2017-11-06T09:57:31.590767: step 1644, loss 0.286959, acc 0.875\n",
      "2017-11-06T09:57:35.799757: step 1645, loss 0.21601, acc 0.875\n",
      "2017-11-06T09:57:40.109820: step 1646, loss 0.187586, acc 0.90625\n",
      "2017-11-06T09:57:44.172707: step 1647, loss 0.199937, acc 0.9375\n",
      "2017-11-06T09:57:48.202571: step 1648, loss 0.268547, acc 0.875\n",
      "2017-11-06T09:57:52.214422: step 1649, loss 0.337861, acc 0.78125\n",
      "2017-11-06T09:57:56.235278: step 1650, loss 0.26526, acc 0.875\n",
      "2017-11-06T09:58:00.341196: step 1651, loss 0.295908, acc 0.875\n",
      "2017-11-06T09:58:04.326808: step 1652, loss 0.133673, acc 0.96875\n",
      "2017-11-06T09:58:08.432744: step 1653, loss 0.230101, acc 0.875\n",
      "2017-11-06T09:58:12.373527: step 1654, loss 0.196136, acc 0.90625\n",
      "2017-11-06T09:58:16.410393: step 1655, loss 0.174848, acc 0.9375\n",
      "2017-11-06T09:58:19.072286: step 1656, loss 0.323326, acc 0.9\n",
      "2017-11-06T09:58:23.183207: step 1657, loss 0.149372, acc 0.9375\n",
      "2017-11-06T09:58:27.260103: step 1658, loss 0.146471, acc 0.90625\n",
      "2017-11-06T09:58:31.331996: step 1659, loss 0.318906, acc 0.875\n",
      "2017-11-06T09:58:35.590022: step 1660, loss 0.0488366, acc 0.96875\n",
      "2017-11-06T09:58:39.760985: step 1661, loss 0.305273, acc 0.84375\n",
      "2017-11-06T09:58:44.168117: step 1662, loss 0.321347, acc 0.84375\n",
      "2017-11-06T09:58:48.167958: step 1663, loss 0.26539, acc 0.90625\n",
      "2017-11-06T09:58:52.201825: step 1664, loss 0.119722, acc 0.90625\n",
      "2017-11-06T09:58:56.274719: step 1665, loss 0.249367, acc 0.90625\n",
      "2017-11-06T09:59:00.316591: step 1666, loss 0.177921, acc 0.90625\n",
      "2017-11-06T09:59:04.355461: step 1667, loss 0.0877057, acc 0.96875\n",
      "2017-11-06T09:59:08.398333: step 1668, loss 0.200671, acc 0.84375\n",
      "2017-11-06T09:59:12.474230: step 1669, loss 0.208957, acc 0.875\n",
      "2017-11-06T09:59:16.526109: step 1670, loss 0.285261, acc 0.84375\n",
      "2017-11-06T09:59:20.499932: step 1671, loss 0.0646437, acc 0.96875\n",
      "2017-11-06T09:59:24.561818: step 1672, loss 0.380663, acc 0.84375\n",
      "2017-11-06T09:59:28.586678: step 1673, loss 0.112219, acc 0.9375\n",
      "2017-11-06T09:59:32.640558: step 1674, loss 0.253304, acc 0.84375\n",
      "2017-11-06T09:59:36.619385: step 1675, loss 0.192903, acc 0.84375\n",
      "2017-11-06T09:59:40.637240: step 1676, loss 0.0786435, acc 0.96875\n",
      "2017-11-06T09:59:44.656096: step 1677, loss 0.146375, acc 0.9375\n",
      "2017-11-06T09:59:49.167301: step 1678, loss 0.0623695, acc 1\n",
      "2017-11-06T09:59:53.267214: step 1679, loss 0.263078, acc 0.875\n",
      "2017-11-06T09:59:57.274061: step 1680, loss 0.241042, acc 0.90625\n",
      "2017-11-06T10:00:01.613144: step 1681, loss 0.0824342, acc 0.9375\n",
      "2017-11-06T10:00:05.737076: step 1682, loss 0.617964, acc 0.78125\n",
      "2017-11-06T10:00:09.771941: step 1683, loss 0.185458, acc 0.90625\n",
      "2017-11-06T10:00:13.769782: step 1684, loss 0.112179, acc 0.9375\n",
      "2017-11-06T10:00:17.854685: step 1685, loss 0.329572, acc 0.875\n",
      "2017-11-06T10:00:21.865534: step 1686, loss 0.204808, acc 0.875\n",
      "2017-11-06T10:00:25.904404: step 1687, loss 0.16403, acc 0.90625\n",
      "2017-11-06T10:00:30.007320: step 1688, loss 0.208867, acc 0.84375\n",
      "2017-11-06T10:00:34.282357: step 1689, loss 0.155873, acc 0.90625\n",
      "2017-11-06T10:00:38.377267: step 1690, loss 0.0772179, acc 0.96875\n",
      "2017-11-06T10:00:42.486186: step 1691, loss 0.292849, acc 0.875\n",
      "2017-11-06T10:00:45.151080: step 1692, loss 0.149135, acc 0.95\n",
      "2017-11-06T10:00:49.194954: step 1693, loss 0.273146, acc 0.875\n",
      "2017-11-06T10:00:53.576066: step 1694, loss 0.122046, acc 0.90625\n",
      "2017-11-06T10:00:57.782055: step 1695, loss 0.163057, acc 0.9375\n",
      "2017-11-06T10:01:01.827929: step 1696, loss 0.137818, acc 0.875\n",
      "2017-11-06T10:01:05.873550: step 1697, loss 0.197214, acc 0.9375\n",
      "2017-11-06T10:01:09.964457: step 1698, loss 0.0134409, acc 1\n",
      "2017-11-06T10:01:13.987315: step 1699, loss 0.188206, acc 0.90625\n",
      "2017-11-06T10:01:17.986156: step 1700, loss 0.268331, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:01:20.666061: step 1700, loss 0.915676, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-06T10:01:25.977858: step 1701, loss 0.280168, acc 0.84375\n",
      "2017-11-06T10:01:29.995712: step 1702, loss 0.177821, acc 0.90625\n",
      "2017-11-06T10:01:34.009565: step 1703, loss 0.121719, acc 0.9375\n",
      "2017-11-06T10:01:38.237569: step 1704, loss 0.176954, acc 0.9375\n",
      "2017-11-06T10:01:42.357496: step 1705, loss 0.127609, acc 0.9375\n",
      "2017-11-06T10:01:46.403370: step 1706, loss 0.23493, acc 0.90625\n",
      "2017-11-06T10:01:50.425229: step 1707, loss 0.0582196, acc 1\n",
      "2017-11-06T10:01:54.436078: step 1708, loss 0.368247, acc 0.8125\n",
      "2017-11-06T10:01:58.734132: step 1709, loss 0.33408, acc 0.875\n",
      "2017-11-06T10:02:02.968141: step 1710, loss 0.176907, acc 0.90625\n",
      "2017-11-06T10:02:06.966982: step 1711, loss 0.15621, acc 0.90625\n",
      "2017-11-06T10:02:11.020864: step 1712, loss 0.0149365, acc 1\n",
      "2017-11-06T10:02:15.048724: step 1713, loss 0.078903, acc 0.96875\n",
      "2017-11-06T10:02:19.039560: step 1714, loss 0.158986, acc 0.9375\n",
      "2017-11-06T10:02:23.375164: step 1715, loss 0.459557, acc 0.875\n",
      "2017-11-06T10:02:27.608172: step 1716, loss 0.237755, acc 0.9375\n",
      "2017-11-06T10:02:32.030314: step 1717, loss 0.35138, acc 0.90625\n",
      "2017-11-06T10:02:36.336373: step 1718, loss 0.0849016, acc 0.96875\n",
      "2017-11-06T10:02:40.440292: step 1719, loss 0.145185, acc 0.90625\n",
      "2017-11-06T10:02:44.490170: step 1720, loss 0.280889, acc 0.90625\n",
      "2017-11-06T10:02:48.441978: step 1721, loss 0.0284623, acc 1\n",
      "2017-11-06T10:02:52.376773: step 1722, loss 0.292358, acc 0.90625\n",
      "2017-11-06T10:02:56.388624: step 1723, loss 0.533802, acc 0.6875\n",
      "2017-11-06T10:03:00.307411: step 1724, loss 0.188402, acc 0.90625\n",
      "2017-11-06T10:03:04.463363: step 1725, loss 0.158209, acc 0.90625\n",
      "2017-11-06T10:03:08.641330: step 1726, loss 0.267577, acc 0.90625\n",
      "2017-11-06T10:03:12.665189: step 1727, loss 0.350126, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T10:03:15.216002: step 1728, loss 0.323549, acc 0.85\n",
      "2017-11-06T10:03:19.162808: step 1729, loss 0.079198, acc 0.96875\n",
      "2017-11-06T10:03:23.196672: step 1730, loss 0.235685, acc 0.90625\n",
      "2017-11-06T10:03:27.212526: step 1731, loss 0.221117, acc 0.90625\n",
      "2017-11-06T10:03:31.388492: step 1732, loss 0.165021, acc 0.875\n",
      "2017-11-06T10:03:35.412352: step 1733, loss 0.11075, acc 0.90625\n",
      "2017-11-06T10:03:39.433209: step 1734, loss 0.113928, acc 0.9375\n",
      "2017-11-06T10:03:43.610177: step 1735, loss 0.235567, acc 0.90625\n",
      "2017-11-06T10:03:47.707088: step 1736, loss 0.116703, acc 0.96875\n",
      "2017-11-06T10:03:51.775979: step 1737, loss 0.23437, acc 0.875\n",
      "2017-11-06T10:03:55.801841: step 1738, loss 0.323771, acc 0.84375\n",
      "2017-11-06T10:03:59.828700: step 1739, loss 0.127028, acc 0.9375\n",
      "2017-11-06T10:04:03.950423: step 1740, loss 0.122893, acc 0.9375\n",
      "2017-11-06T10:04:08.179428: step 1741, loss 0.4655, acc 0.84375\n",
      "2017-11-06T10:04:12.624587: step 1742, loss 0.208324, acc 0.90625\n",
      "2017-11-06T10:04:16.844585: step 1743, loss 0.183378, acc 0.9375\n",
      "2017-11-06T10:04:20.902468: step 1744, loss 0.108549, acc 0.96875\n",
      "2017-11-06T10:04:25.070429: step 1745, loss 0.191021, acc 0.90625\n",
      "2017-11-06T10:04:29.082281: step 1746, loss 0.14813, acc 0.90625\n",
      "2017-11-06T10:04:33.177190: step 1747, loss 0.352738, acc 0.90625\n",
      "2017-11-06T10:04:37.477245: step 1748, loss 0.161886, acc 0.9375\n",
      "2017-11-06T10:04:41.536129: step 1749, loss 0.443243, acc 0.84375\n",
      "2017-11-06T10:04:45.727107: step 1750, loss 0.160636, acc 0.90625\n",
      "2017-11-06T10:04:49.807006: step 1751, loss 0.143429, acc 0.9375\n",
      "2017-11-06T10:04:53.981973: step 1752, loss 0.248223, acc 0.84375\n",
      "2017-11-06T10:04:58.111907: step 1753, loss 0.225122, acc 0.90625\n",
      "2017-11-06T10:05:02.094737: step 1754, loss 0.608558, acc 0.8125\n",
      "2017-11-06T10:05:06.109589: step 1755, loss 0.320593, acc 0.84375\n",
      "2017-11-06T10:05:10.078410: step 1756, loss 0.14884, acc 0.9375\n",
      "2017-11-06T10:05:14.240367: step 1757, loss 0.117762, acc 0.90625\n",
      "2017-11-06T10:05:18.520408: step 1758, loss 0.129263, acc 0.9375\n",
      "2017-11-06T10:05:22.424182: step 1759, loss 0.0817742, acc 0.96875\n",
      "2017-11-06T10:05:26.390000: step 1760, loss 0.319281, acc 0.84375\n",
      "2017-11-06T10:05:30.336804: step 1761, loss 0.18398, acc 0.875\n",
      "2017-11-06T10:05:34.520777: step 1762, loss 0.311563, acc 0.875\n",
      "2017-11-06T10:05:38.493600: step 1763, loss 0.243784, acc 0.9375\n",
      "2017-11-06T10:05:40.994376: step 1764, loss 0.142771, acc 0.95\n",
      "2017-11-06T10:05:44.930174: step 1765, loss 0.27883, acc 0.90625\n",
      "2017-11-06T10:05:48.907999: step 1766, loss 0.376177, acc 0.78125\n",
      "2017-11-06T10:05:52.877822: step 1767, loss 0.182204, acc 0.96875\n",
      "2017-11-06T10:05:56.861653: step 1768, loss 0.121963, acc 0.9375\n",
      "2017-11-06T10:06:00.898519: step 1769, loss 0.149509, acc 0.9375\n",
      "2017-11-06T10:06:04.857333: step 1770, loss 0.0980325, acc 0.96875\n",
      "2017-11-06T10:06:08.884194: step 1771, loss 0.421736, acc 0.84375\n",
      "2017-11-06T10:06:12.904050: step 1772, loss 0.207924, acc 0.875\n",
      "2017-11-06T10:06:16.925907: step 1773, loss 0.0925141, acc 0.96875\n",
      "2017-11-06T10:06:21.128894: step 1774, loss 0.272812, acc 0.90625\n",
      "2017-11-06T10:06:25.359900: step 1775, loss 0.217369, acc 0.90625\n",
      "2017-11-06T10:06:29.430793: step 1776, loss 0.21017, acc 0.875\n",
      "2017-11-06T10:06:33.487676: step 1777, loss 0.201951, acc 0.9375\n",
      "2017-11-06T10:06:37.589590: step 1778, loss 0.236238, acc 0.90625\n",
      "2017-11-06T10:06:41.588432: step 1779, loss 0.187342, acc 0.90625\n",
      "2017-11-06T10:06:45.515221: step 1780, loss 0.332421, acc 0.875\n",
      "2017-11-06T10:06:49.455021: step 1781, loss 0.184714, acc 0.875\n",
      "2017-11-06T10:06:53.381811: step 1782, loss 0.383986, acc 0.8125\n",
      "2017-11-06T10:06:57.365642: step 1783, loss 0.254531, acc 0.875\n",
      "2017-11-06T10:07:01.322453: step 1784, loss 0.224748, acc 0.875\n",
      "2017-11-06T10:07:05.269145: step 1785, loss 0.199444, acc 0.8125\n",
      "2017-11-06T10:07:09.235964: step 1786, loss 0.160171, acc 0.96875\n",
      "2017-11-06T10:07:13.250817: step 1787, loss 0.285085, acc 0.90625\n",
      "2017-11-06T10:07:17.263669: step 1788, loss 0.159911, acc 0.90625\n",
      "2017-11-06T10:07:21.250501: step 1789, loss 0.23187, acc 0.875\n",
      "2017-11-06T10:07:25.347412: step 1790, loss 0.301583, acc 0.84375\n",
      "2017-11-06T10:07:29.739533: step 1791, loss 0.23347, acc 0.90625\n",
      "2017-11-06T10:07:33.712355: step 1792, loss 0.217665, acc 0.90625\n",
      "2017-11-06T10:07:37.632141: step 1793, loss 0.172294, acc 0.9375\n",
      "2017-11-06T10:07:41.627980: step 1794, loss 0.251227, acc 0.875\n",
      "2017-11-06T10:07:45.653861: step 1795, loss 0.262234, acc 0.90625\n",
      "2017-11-06T10:07:49.610652: step 1796, loss 0.102118, acc 0.96875\n",
      "2017-11-06T10:07:53.584475: step 1797, loss 0.263122, acc 0.9375\n",
      "2017-11-06T10:07:57.584318: step 1798, loss 0.40468, acc 0.8125\n",
      "2017-11-06T10:08:01.622186: step 1799, loss 0.177078, acc 0.9375\n",
      "2017-11-06T10:08:04.194014: step 1800, loss 0.10756, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:08:06.733819: step 1800, loss 0.988442, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-06T10:08:12.075310: step 1801, loss 0.300779, acc 0.8125\n",
      "2017-11-06T10:08:16.146203: step 1802, loss 0.07559, acc 0.96875\n",
      "2017-11-06T10:08:20.085002: step 1803, loss 0.322884, acc 0.84375\n",
      "2017-11-06T10:08:24.150891: step 1804, loss 0.150904, acc 0.96875\n",
      "2017-11-06T10:08:28.230789: step 1805, loss 0.133163, acc 0.90625\n",
      "2017-11-06T10:08:32.517835: step 1806, loss 0.292303, acc 0.8125\n",
      "2017-11-06T10:08:36.804881: step 1807, loss 0.217602, acc 0.875\n",
      "2017-11-06T10:08:40.849755: step 1808, loss 0.00678314, acc 1\n",
      "2017-11-06T10:08:44.767540: step 1809, loss 0.223682, acc 0.875\n",
      "2017-11-06T10:08:48.756374: step 1810, loss 0.202333, acc 0.90625\n",
      "2017-11-06T10:08:52.672174: step 1811, loss 0.186074, acc 0.90625\n",
      "2017-11-06T10:08:56.650983: step 1812, loss 0.101597, acc 0.9375\n",
      "2017-11-06T10:09:00.605793: step 1813, loss 0.252594, acc 0.9375\n",
      "2017-11-06T10:09:04.575614: step 1814, loss 0.324071, acc 0.84375\n",
      "2017-11-06T10:09:08.556442: step 1815, loss 0.186631, acc 0.875\n",
      "2017-11-06T10:09:12.533269: step 1816, loss 0.422331, acc 0.8125\n",
      "2017-11-06T10:09:16.534113: step 1817, loss 0.289197, acc 0.875\n",
      "2017-11-06T10:09:20.476912: step 1818, loss 0.221401, acc 0.90625\n",
      "2017-11-06T10:09:24.448735: step 1819, loss 0.176135, acc 0.90625\n",
      "2017-11-06T10:09:28.404545: step 1820, loss 0.465168, acc 0.84375\n",
      "2017-11-06T10:09:32.467432: step 1821, loss 0.179924, acc 0.875\n",
      "2017-11-06T10:09:36.585358: step 1822, loss 0.111824, acc 0.96875\n",
      "2017-11-06T10:09:40.916436: step 1823, loss 0.195808, acc 0.90625\n",
      "2017-11-06T10:09:44.916278: step 1824, loss 0.108906, acc 0.9375\n",
      "2017-11-06T10:09:48.875090: step 1825, loss 0.449428, acc 0.75\n",
      "2017-11-06T10:09:52.856919: step 1826, loss 0.148808, acc 0.90625\n",
      "2017-11-06T10:09:56.892788: step 1827, loss 0.470409, acc 0.8125\n",
      "2017-11-06T10:10:01.090771: step 1828, loss 0.0857679, acc 1\n",
      "2017-11-06T10:10:05.212456: step 1829, loss 0.239397, acc 0.84375\n",
      "2017-11-06T10:10:09.223306: step 1830, loss 0.240175, acc 0.90625\n",
      "2017-11-06T10:10:13.185121: step 1831, loss 0.187157, acc 0.90625\n",
      "2017-11-06T10:10:17.275028: step 1832, loss 0.17748, acc 0.90625\n",
      "2017-11-06T10:10:21.275869: step 1833, loss 0.261862, acc 0.90625\n",
      "2017-11-06T10:10:25.204661: step 1834, loss 0.154995, acc 0.90625\n",
      "2017-11-06T10:10:29.247534: step 1835, loss 0.0812194, acc 0.9375\n",
      "2017-11-06T10:10:31.723293: step 1836, loss 0.238409, acc 0.85\n",
      "2017-11-06T10:10:36.017344: step 1837, loss 0.119903, acc 0.9375\n",
      "2017-11-06T10:10:39.976157: step 1838, loss 0.0695291, acc 0.96875\n",
      "2017-11-06T10:10:44.284218: step 1839, loss 0.270609, acc 0.875\n",
      "2017-11-06T10:10:48.405146: step 1840, loss 0.199067, acc 0.90625\n",
      "2017-11-06T10:10:52.399984: step 1841, loss 0.154072, acc 0.9375\n",
      "2017-11-06T10:10:56.491892: step 1842, loss 0.296529, acc 0.84375\n",
      "2017-11-06T10:11:00.489732: step 1843, loss 0.209216, acc 0.875\n",
      "2017-11-06T10:11:04.501583: step 1844, loss 0.159306, acc 0.9375\n",
      "2017-11-06T10:11:08.545457: step 1845, loss 0.127203, acc 0.90625\n",
      "2017-11-06T10:11:12.485256: step 1846, loss 0.0913113, acc 1\n",
      "2017-11-06T10:11:16.574162: step 1847, loss 0.122509, acc 0.9375\n",
      "2017-11-06T10:11:20.556991: step 1848, loss 0.144015, acc 0.9375\n",
      "2017-11-06T10:11:24.582852: step 1849, loss 0.159072, acc 0.90625\n",
      "2017-11-06T10:11:28.543666: step 1850, loss 0.236435, acc 0.90625\n",
      "2017-11-06T10:11:32.685609: step 1851, loss 0.280745, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T10:11:36.766509: step 1852, loss 0.175535, acc 0.90625\n",
      "2017-11-06T10:11:40.720319: step 1853, loss 0.203698, acc 0.90625\n",
      "2017-11-06T10:11:44.704149: step 1854, loss 0.127464, acc 0.9375\n",
      "2017-11-06T10:11:48.963175: step 1855, loss 0.070811, acc 0.96875\n",
      "2017-11-06T10:11:53.217197: step 1856, loss 0.16788, acc 0.90625\n",
      "2017-11-06T10:11:57.217040: step 1857, loss 0.248759, acc 0.90625\n",
      "2017-11-06T10:12:01.227890: step 1858, loss 0.239363, acc 0.90625\n",
      "2017-11-06T10:12:05.201713: step 1859, loss 0.299204, acc 0.875\n",
      "2017-11-06T10:12:09.196551: step 1860, loss 0.0865179, acc 0.96875\n",
      "2017-11-06T10:12:13.182384: step 1861, loss 0.0539863, acc 1\n",
      "2017-11-06T10:12:17.369359: step 1862, loss 0.32788, acc 0.8125\n",
      "2017-11-06T10:12:21.403225: step 1863, loss 0.132951, acc 0.90625\n",
      "2017-11-06T10:12:25.374046: step 1864, loss 0.158408, acc 0.9375\n",
      "2017-11-06T10:12:29.360879: step 1865, loss 0.302893, acc 0.90625\n",
      "2017-11-06T10:12:33.428769: step 1866, loss 0.288104, acc 0.875\n",
      "2017-11-06T10:12:37.573715: step 1867, loss 0.201608, acc 0.9375\n",
      "2017-11-06T10:12:41.496502: step 1868, loss 0.0880591, acc 0.9375\n",
      "2017-11-06T10:12:45.531370: step 1869, loss 0.172838, acc 0.9375\n",
      "2017-11-06T10:12:49.462161: step 1870, loss 0.342259, acc 0.84375\n",
      "2017-11-06T10:12:53.617114: step 1871, loss 0.186434, acc 0.9375\n",
      "2017-11-06T10:12:56.471142: step 1872, loss 0.201083, acc 0.95\n",
      "2017-11-06T10:13:00.494001: step 1873, loss 0.126431, acc 0.9375\n",
      "2017-11-06T10:13:04.477593: step 1874, loss 0.231348, acc 0.90625\n",
      "2017-11-06T10:13:08.440409: step 1875, loss 0.253017, acc 0.875\n",
      "2017-11-06T10:13:12.418255: step 1876, loss 0.0767587, acc 0.9375\n",
      "2017-11-06T10:13:16.459108: step 1877, loss 0.152764, acc 0.90625\n",
      "2017-11-06T10:13:20.427927: step 1878, loss 0.116465, acc 0.90625\n",
      "2017-11-06T10:13:24.559863: step 1879, loss 0.199961, acc 0.90625\n",
      "2017-11-06T10:13:28.732828: step 1880, loss 0.255952, acc 0.875\n",
      "2017-11-06T10:13:32.637603: step 1881, loss 0.0755771, acc 0.96875\n",
      "2017-11-06T10:13:36.630440: step 1882, loss 0.213991, acc 0.84375\n",
      "2017-11-06T10:13:40.640288: step 1883, loss 0.163822, acc 0.90625\n",
      "2017-11-06T10:13:44.573083: step 1884, loss 0.0605624, acc 0.96875\n",
      "2017-11-06T10:13:48.551910: step 1885, loss 0.136694, acc 0.9375\n",
      "2017-11-06T10:13:52.536761: step 1886, loss 0.143195, acc 0.90625\n",
      "2017-11-06T10:13:56.494554: step 1887, loss 0.124397, acc 0.96875\n",
      "2017-11-06T10:14:00.772594: step 1888, loss 0.281101, acc 0.90625\n",
      "2017-11-06T10:14:04.924545: step 1889, loss 0.12946, acc 0.90625\n",
      "2017-11-06T10:14:08.917382: step 1890, loss 0.443391, acc 0.875\n",
      "2017-11-06T10:14:12.891205: step 1891, loss 0.387441, acc 0.875\n",
      "2017-11-06T10:14:16.864027: step 1892, loss 0.294891, acc 0.875\n",
      "2017-11-06T10:14:20.852862: step 1893, loss 0.104537, acc 0.9375\n",
      "2017-11-06T10:14:24.784656: step 1894, loss 0.187582, acc 0.90625\n",
      "2017-11-06T10:14:28.746470: step 1895, loss 0.119746, acc 0.96875\n",
      "2017-11-06T10:14:32.766327: step 1896, loss 0.184771, acc 0.90625\n",
      "2017-11-06T10:14:36.930286: step 1897, loss 0.195337, acc 0.90625\n",
      "2017-11-06T10:14:40.884094: step 1898, loss 0.280975, acc 0.875\n",
      "2017-11-06T10:14:44.827898: step 1899, loss 0.129647, acc 0.9375\n",
      "2017-11-06T10:14:48.804723: step 1900, loss 0.27805, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:14:51.528658: step 1900, loss 0.910977, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-06T10:14:56.955953: step 1901, loss 0.14858, acc 0.90625\n",
      "2017-11-06T10:15:01.021842: step 1902, loss 0.437046, acc 0.8125\n",
      "2017-11-06T10:15:05.228831: step 1903, loss 0.404272, acc 0.78125\n",
      "2017-11-06T10:15:09.562910: step 1904, loss 0.328738, acc 0.875\n",
      "2017-11-06T10:15:13.533732: step 1905, loss 0.384837, acc 0.8125\n",
      "2017-11-06T10:15:17.564597: step 1906, loss 0.185376, acc 0.90625\n",
      "2017-11-06T10:15:21.603467: step 1907, loss 0.209673, acc 0.90625\n",
      "2017-11-06T10:15:24.194307: step 1908, loss 0.0528467, acc 0.95\n",
      "2017-11-06T10:15:28.202154: step 1909, loss 0.26646, acc 0.9375\n",
      "2017-11-06T10:15:32.224013: step 1910, loss 0.225143, acc 0.90625\n",
      "2017-11-06T10:15:36.294905: step 1911, loss 0.208878, acc 0.90625\n",
      "2017-11-06T10:15:40.343781: step 1912, loss 0.2196, acc 0.90625\n",
      "2017-11-06T10:15:44.264567: step 1913, loss 0.386386, acc 0.8125\n",
      "2017-11-06T10:15:48.265412: step 1914, loss 0.160787, acc 0.9375\n",
      "2017-11-06T10:15:52.275259: step 1915, loss 0.101235, acc 0.9375\n",
      "2017-11-06T10:15:56.253086: step 1916, loss 0.260909, acc 0.875\n",
      "2017-11-06T10:16:00.276945: step 1917, loss 0.218663, acc 0.875\n",
      "2017-11-06T10:16:04.336579: step 1918, loss 0.0865438, acc 0.96875\n",
      "2017-11-06T10:16:08.366442: step 1919, loss 0.0903802, acc 0.96875\n",
      "2017-11-06T10:16:12.795590: step 1920, loss 0.258521, acc 0.84375\n",
      "2017-11-06T10:16:16.906511: step 1921, loss 0.307688, acc 0.875\n",
      "2017-11-06T10:16:21.050455: step 1922, loss 0.16636, acc 0.9375\n",
      "2017-11-06T10:16:25.153370: step 1923, loss 0.106238, acc 0.90625\n",
      "2017-11-06T10:16:29.100175: step 1924, loss 0.219343, acc 0.875\n",
      "2017-11-06T10:16:33.187079: step 1925, loss 0.160452, acc 0.96875\n",
      "2017-11-06T10:16:37.430093: step 1926, loss 0.387201, acc 0.875\n",
      "2017-11-06T10:16:41.459957: step 1927, loss 0.29853, acc 0.875\n",
      "2017-11-06T10:16:45.407762: step 1928, loss 0.167696, acc 0.9375\n",
      "2017-11-06T10:16:49.450635: step 1929, loss 0.586125, acc 0.8125\n",
      "2017-11-06T10:16:53.449476: step 1930, loss 0.185123, acc 0.90625\n",
      "2017-11-06T10:16:57.451321: step 1931, loss 0.257456, acc 0.90625\n",
      "2017-11-06T10:17:01.437151: step 1932, loss 0.166596, acc 0.90625\n",
      "2017-11-06T10:17:05.412976: step 1933, loss 0.374664, acc 0.875\n",
      "2017-11-06T10:17:09.351776: step 1934, loss 0.143473, acc 0.90625\n",
      "2017-11-06T10:17:13.311589: step 1935, loss 0.16211, acc 0.9375\n",
      "2017-11-06T10:17:17.686697: step 1936, loss 0.164126, acc 0.9375\n",
      "2017-11-06T10:17:21.845652: step 1937, loss 0.0835897, acc 0.96875\n",
      "2017-11-06T10:17:25.944584: step 1938, loss 0.25103, acc 0.875\n",
      "2017-11-06T10:17:29.893371: step 1939, loss 0.268493, acc 0.8125\n",
      "2017-11-06T10:17:33.832169: step 1940, loss 0.171736, acc 0.875\n",
      "2017-11-06T10:17:37.833014: step 1941, loss 0.188432, acc 0.875\n",
      "2017-11-06T10:17:41.770810: step 1942, loss 0.307009, acc 0.8125\n",
      "2017-11-06T10:17:45.762647: step 1943, loss 0.353104, acc 0.8125\n",
      "2017-11-06T10:17:48.351486: step 1944, loss 0.168434, acc 0.95\n",
      "2017-11-06T10:17:52.369341: step 1945, loss 0.134121, acc 0.9375\n",
      "2017-11-06T10:17:56.362179: step 1946, loss 0.0150346, acc 1\n",
      "2017-11-06T10:18:00.393042: step 1947, loss 0.086264, acc 0.96875\n",
      "2017-11-06T10:18:04.330839: step 1948, loss 0.264721, acc 0.90625\n",
      "2017-11-06T10:18:08.285650: step 1949, loss 0.332424, acc 0.875\n",
      "2017-11-06T10:18:12.246464: step 1950, loss 0.243004, acc 0.90625\n",
      "2017-11-06T10:18:16.273327: step 1951, loss 0.297858, acc 0.9375\n",
      "2017-11-06T10:18:20.322202: step 1952, loss 0.381482, acc 0.90625\n",
      "2017-11-06T10:18:24.849419: step 1953, loss 0.296319, acc 0.84375\n",
      "2017-11-06T10:18:29.067418: step 1954, loss 0.187851, acc 0.875\n",
      "2017-11-06T10:18:33.145315: step 1955, loss 0.116427, acc 0.96875\n",
      "2017-11-06T10:18:37.233218: step 1956, loss 0.117725, acc 0.9375\n",
      "2017-11-06T10:18:41.237063: step 1957, loss 0.308275, acc 0.875\n",
      "2017-11-06T10:18:45.234904: step 1958, loss 0.207013, acc 0.9375\n",
      "2017-11-06T10:18:49.214731: step 1959, loss 0.139496, acc 0.9375\n",
      "2017-11-06T10:18:53.182551: step 1960, loss 0.383057, acc 0.8125\n",
      "2017-11-06T10:18:57.173388: step 1961, loss 0.12979, acc 0.96875\n",
      "2017-11-06T10:19:01.187238: step 1962, loss 0.159874, acc 0.9375\n",
      "2017-11-06T10:19:05.152836: step 1963, loss 0.33395, acc 0.875\n",
      "2017-11-06T10:19:09.039597: step 1964, loss 0.282321, acc 0.875\n",
      "2017-11-06T10:19:13.014422: step 1965, loss 0.0829376, acc 0.96875\n",
      "2017-11-06T10:19:17.006258: step 1966, loss 0.352603, acc 0.90625\n",
      "2017-11-06T10:19:20.975078: step 1967, loss 0.307712, acc 0.84375\n",
      "2017-11-06T10:19:24.957908: step 1968, loss 0.29366, acc 0.84375\n",
      "2017-11-06T10:19:29.321009: step 1969, loss 0.339691, acc 0.84375\n",
      "2017-11-06T10:19:33.547011: step 1970, loss 0.235509, acc 0.90625\n",
      "2017-11-06T10:19:37.462795: step 1971, loss 0.228262, acc 0.9375\n",
      "2017-11-06T10:19:41.506686: step 1972, loss 0.195138, acc 0.875\n",
      "2017-11-06T10:19:45.472487: step 1973, loss 0.279004, acc 0.875\n",
      "2017-11-06T10:19:49.484335: step 1974, loss 0.0559599, acc 0.96875\n",
      "2017-11-06T10:19:53.468166: step 1975, loss 0.176397, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T10:19:57.464010: step 1976, loss 0.172138, acc 0.9375\n",
      "2017-11-06T10:20:01.764060: step 1977, loss 0.260133, acc 0.8125\n",
      "2017-11-06T10:20:05.799930: step 1978, loss 0.0681928, acc 0.96875\n",
      "2017-11-06T10:20:09.795768: step 1979, loss 0.167274, acc 0.90625\n",
      "2017-11-06T10:20:12.366596: step 1980, loss 0.134882, acc 0.95\n",
      "2017-11-06T10:20:16.363435: step 1981, loss 0.164631, acc 0.90625\n",
      "2017-11-06T10:20:20.364277: step 1982, loss 0.0986532, acc 0.9375\n",
      "2017-11-06T10:20:24.390139: step 1983, loss 0.0974929, acc 0.9375\n",
      "2017-11-06T10:20:28.344948: step 1984, loss 0.124372, acc 0.9375\n",
      "2017-11-06T10:20:32.495897: step 1985, loss 0.296403, acc 0.8125\n",
      "2017-11-06T10:20:37.036124: step 1986, loss 0.200015, acc 0.90625\n",
      "2017-11-06T10:20:41.072991: step 1987, loss 0.289316, acc 0.84375\n",
      "2017-11-06T10:20:45.020796: step 1988, loss 0.189657, acc 0.9375\n",
      "2017-11-06T10:20:49.043655: step 1989, loss 0.317078, acc 0.875\n",
      "2017-11-06T10:20:53.002469: step 1990, loss 0.238936, acc 0.84375\n",
      "2017-11-06T10:20:56.939265: step 1991, loss 0.0724271, acc 0.96875\n",
      "2017-11-06T10:21:00.952116: step 1992, loss 0.0715474, acc 0.96875\n",
      "2017-11-06T10:21:04.848885: step 1993, loss 0.195643, acc 0.9375\n",
      "2017-11-06T10:21:08.872744: step 1994, loss 0.174352, acc 0.9375\n",
      "2017-11-06T10:21:12.858576: step 1995, loss 0.0715093, acc 0.96875\n",
      "2017-11-06T10:21:16.924465: step 1996, loss 0.169066, acc 0.90625\n",
      "2017-11-06T10:21:20.972341: step 1997, loss 0.384691, acc 0.875\n",
      "2017-11-06T10:21:24.975186: step 1998, loss 0.201951, acc 0.9375\n",
      "2017-11-06T10:21:29.047081: step 1999, loss 0.122657, acc 0.9375\n",
      "2017-11-06T10:21:33.050924: step 2000, loss 0.223453, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:21:35.663780: step 2000, loss 1.15787, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-06T10:21:41.572286: step 2001, loss 0.365074, acc 0.875\n",
      "2017-11-06T10:21:45.535101: step 2002, loss 0.224534, acc 0.9375\n",
      "2017-11-06T10:21:49.553955: step 2003, loss 0.179579, acc 0.9375\n",
      "2017-11-06T10:21:53.545793: step 2004, loss 0.197626, acc 0.9375\n",
      "2017-11-06T10:21:57.508609: step 2005, loss 0.221909, acc 0.90625\n",
      "2017-11-06T10:22:01.520458: step 2006, loss 0.210005, acc 0.90625\n",
      "2017-11-06T10:22:05.507058: step 2007, loss 0.124178, acc 0.9375\n",
      "2017-11-06T10:22:09.459868: step 2008, loss 0.252583, acc 0.875\n",
      "2017-11-06T10:22:13.433690: step 2009, loss 0.19949, acc 0.9375\n",
      "2017-11-06T10:22:17.522595: step 2010, loss 0.291493, acc 0.875\n",
      "2017-11-06T10:22:21.589486: step 2011, loss 0.350751, acc 0.84375\n",
      "2017-11-06T10:22:25.610342: step 2012, loss 0.211863, acc 0.9375\n",
      "2017-11-06T10:22:29.586167: step 2013, loss 0.293505, acc 0.875\n",
      "2017-11-06T10:22:33.764137: step 2014, loss 0.28446, acc 0.9375\n",
      "2017-11-06T10:22:37.805009: step 2015, loss 0.239169, acc 0.875\n",
      "2017-11-06T10:22:40.434876: step 2016, loss 0.139735, acc 0.95\n",
      "2017-11-06T10:22:44.617847: step 2017, loss 0.18972, acc 0.9375\n",
      "2017-11-06T10:22:48.923907: step 2018, loss 0.213362, acc 0.9375\n",
      "2017-11-06T10:22:52.932755: step 2019, loss 0.020217, acc 1\n",
      "2017-11-06T10:22:56.994642: step 2020, loss 0.0558586, acc 0.96875\n",
      "2017-11-06T10:23:01.013497: step 2021, loss 0.279012, acc 0.9375\n",
      "2017-11-06T10:23:04.937285: step 2022, loss 0.300069, acc 0.90625\n",
      "2017-11-06T10:23:08.897099: step 2023, loss 0.200192, acc 0.9375\n",
      "2017-11-06T10:23:12.899943: step 2024, loss 0.210996, acc 0.9375\n",
      "2017-11-06T10:23:16.898785: step 2025, loss 0.0418115, acc 0.96875\n",
      "2017-11-06T10:23:21.004702: step 2026, loss 0.120192, acc 0.90625\n",
      "2017-11-06T10:23:25.199683: step 2027, loss 0.166443, acc 0.9375\n",
      "2017-11-06T10:23:29.371647: step 2028, loss 0.0817272, acc 0.96875\n",
      "2017-11-06T10:23:33.463555: step 2029, loss 0.10133, acc 0.96875\n",
      "2017-11-06T10:23:37.454391: step 2030, loss 0.312842, acc 0.875\n",
      "2017-11-06T10:23:41.460236: step 2031, loss 0.394802, acc 0.875\n",
      "2017-11-06T10:23:45.451072: step 2032, loss 0.243968, acc 0.90625\n",
      "2017-11-06T10:23:49.630041: step 2033, loss 0.218903, acc 0.90625\n",
      "2017-11-06T10:23:53.947109: step 2034, loss 0.344804, acc 0.84375\n",
      "2017-11-06T10:23:57.949953: step 2035, loss 0.211844, acc 0.9375\n",
      "2017-11-06T10:24:02.007836: step 2036, loss 0.310706, acc 0.875\n",
      "2017-11-06T10:24:06.015685: step 2037, loss 0.322038, acc 0.8125\n",
      "2017-11-06T10:24:09.990509: step 2038, loss 0.412263, acc 0.875\n",
      "2017-11-06T10:24:13.918299: step 2039, loss 0.437298, acc 0.8125\n",
      "2017-11-06T10:24:17.892123: step 2040, loss 0.142006, acc 0.9375\n",
      "2017-11-06T10:24:21.917983: step 2041, loss 0.10775, acc 0.9375\n",
      "2017-11-06T10:24:25.978869: step 2042, loss 0.189893, acc 0.90625\n",
      "2017-11-06T10:24:29.925674: step 2043, loss 0.24801, acc 0.875\n",
      "2017-11-06T10:24:34.098639: step 2044, loss 0.249818, acc 0.90625\n",
      "2017-11-06T10:24:38.206557: step 2045, loss 0.196106, acc 0.90625\n",
      "2017-11-06T10:24:42.312474: step 2046, loss 0.411227, acc 0.84375\n",
      "2017-11-06T10:24:46.377363: step 2047, loss 0.28657, acc 0.84375\n",
      "2017-11-06T10:24:50.339178: step 2048, loss 0.166671, acc 0.875\n",
      "2017-11-06T10:24:54.364038: step 2049, loss 0.405605, acc 0.875\n",
      "2017-11-06T10:24:58.724137: step 2050, loss 0.104489, acc 1\n",
      "2017-11-06T10:25:02.799032: step 2051, loss 0.316636, acc 0.875\n",
      "2017-11-06T10:25:05.417692: step 2052, loss 0.26122, acc 0.9\n",
      "2017-11-06T10:25:09.428541: step 2053, loss 0.088654, acc 0.9375\n",
      "2017-11-06T10:25:13.425381: step 2054, loss 0.0523205, acc 0.96875\n",
      "2017-11-06T10:25:17.478261: step 2055, loss 0.289325, acc 0.875\n",
      "2017-11-06T10:25:22.097406: step 2056, loss 0.204744, acc 0.875\n",
      "2017-11-06T10:25:26.357433: step 2057, loss 0.487038, acc 0.84375\n",
      "2017-11-06T10:25:30.587439: step 2058, loss 0.128729, acc 0.9375\n",
      "2017-11-06T10:25:34.654914: step 2059, loss 0.281758, acc 0.8125\n",
      "2017-11-06T10:25:38.764833: step 2060, loss 0.2566, acc 0.90625\n",
      "2017-11-06T10:25:42.813710: step 2061, loss 0.326338, acc 0.90625\n",
      "2017-11-06T10:25:46.760515: step 2062, loss 0.0737278, acc 0.96875\n",
      "2017-11-06T10:25:50.762358: step 2063, loss 0.593008, acc 0.78125\n",
      "2017-11-06T10:25:54.866274: step 2064, loss 0.184147, acc 0.90625\n",
      "2017-11-06T10:25:58.793064: step 2065, loss 0.173084, acc 0.90625\n",
      "2017-11-06T10:26:03.290259: step 2066, loss 0.237424, acc 0.90625\n",
      "2017-11-06T10:26:07.489243: step 2067, loss 0.250656, acc 0.875\n",
      "2017-11-06T10:26:11.426040: step 2068, loss 0.379218, acc 0.84375\n",
      "2017-11-06T10:26:15.455903: step 2069, loss 0.187186, acc 0.90625\n",
      "2017-11-06T10:26:19.419720: step 2070, loss 0.172686, acc 0.90625\n",
      "2017-11-06T10:26:23.499619: step 2071, loss 0.131635, acc 0.9375\n",
      "2017-11-06T10:26:27.534486: step 2072, loss 0.261684, acc 0.84375\n",
      "2017-11-06T10:26:31.526322: step 2073, loss 0.170648, acc 0.875\n",
      "2017-11-06T10:26:35.769337: step 2074, loss 0.187154, acc 0.90625\n",
      "2017-11-06T10:26:39.732153: step 2075, loss 0.186472, acc 0.96875\n",
      "2017-11-06T10:26:43.709979: step 2076, loss 0.333724, acc 0.8125\n",
      "2017-11-06T10:26:47.755854: step 2077, loss 0.141342, acc 0.90625\n",
      "2017-11-06T10:26:51.705661: step 2078, loss 0.0818989, acc 0.96875\n",
      "2017-11-06T10:26:55.691494: step 2079, loss 0.0629295, acc 0.96875\n",
      "2017-11-06T10:26:59.671321: step 2080, loss 0.276663, acc 0.90625\n",
      "2017-11-06T10:27:03.697181: step 2081, loss 0.0528979, acc 0.96875\n",
      "2017-11-06T10:27:07.919181: step 2082, loss 0.0845431, acc 0.9375\n",
      "2017-11-06T10:27:12.087142: step 2083, loss 0.486903, acc 0.75\n",
      "2017-11-06T10:27:16.118007: step 2084, loss 0.370607, acc 0.84375\n",
      "2017-11-06T10:27:20.104839: step 2085, loss 0.153095, acc 0.9375\n",
      "2017-11-06T10:27:24.097695: step 2086, loss 0.220646, acc 0.9375\n",
      "2017-11-06T10:27:28.072502: step 2087, loss 0.0628288, acc 0.9375\n",
      "2017-11-06T10:27:30.705372: step 2088, loss 0.40541, acc 0.85\n",
      "2017-11-06T10:27:34.683198: step 2089, loss 0.220972, acc 0.875\n",
      "2017-11-06T10:27:38.750087: step 2090, loss 0.0444462, acc 1\n",
      "2017-11-06T10:27:42.733918: step 2091, loss 0.255869, acc 0.90625\n",
      "2017-11-06T10:27:46.727756: step 2092, loss 0.124643, acc 0.9375\n",
      "2017-11-06T10:27:50.709585: step 2093, loss 0.0375481, acc 1\n",
      "2017-11-06T10:27:54.684409: step 2094, loss 0.221892, acc 0.875\n",
      "2017-11-06T10:27:58.663236: step 2095, loss 0.177887, acc 0.875\n",
      "2017-11-06T10:28:02.682092: step 2096, loss 0.135867, acc 0.90625\n",
      "2017-11-06T10:28:06.662744: step 2097, loss 0.180482, acc 0.9375\n",
      "2017-11-06T10:28:10.682601: step 2098, loss 0.0799768, acc 0.96875\n",
      "2017-11-06T10:28:15.145771: step 2099, loss 0.0465407, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T10:28:19.099581: step 2100, loss 0.366862, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:28:21.776484: step 2100, loss 0.987091, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-06T10:28:27.151435: step 2101, loss 0.186182, acc 0.9375\n",
      "2017-11-06T10:28:31.142270: step 2102, loss 0.478468, acc 0.75\n",
      "2017-11-06T10:28:35.318237: step 2103, loss 0.236438, acc 0.875\n",
      "2017-11-06T10:28:39.328086: step 2104, loss 0.25236, acc 0.875\n",
      "2017-11-06T10:28:43.369958: step 2105, loss 0.240509, acc 0.90625\n",
      "2017-11-06T10:28:47.317762: step 2106, loss 0.0698591, acc 0.96875\n",
      "2017-11-06T10:28:51.318606: step 2107, loss 0.0742434, acc 0.96875\n",
      "2017-11-06T10:28:55.353472: step 2108, loss 0.229628, acc 0.875\n",
      "2017-11-06T10:28:59.311285: step 2109, loss 0.352757, acc 0.8125\n",
      "2017-11-06T10:29:03.309128: step 2110, loss 0.332134, acc 0.84375\n",
      "2017-11-06T10:29:07.322978: step 2111, loss 0.333106, acc 0.875\n",
      "2017-11-06T10:29:11.292798: step 2112, loss 0.0937212, acc 0.9375\n",
      "2017-11-06T10:29:15.320660: step 2113, loss 0.302919, acc 0.84375\n",
      "2017-11-06T10:29:19.617713: step 2114, loss 0.430827, acc 0.8125\n",
      "2017-11-06T10:29:23.787676: step 2115, loss 0.282259, acc 0.875\n",
      "2017-11-06T10:29:27.793523: step 2116, loss 0.387513, acc 0.8125\n",
      "2017-11-06T10:29:31.805375: step 2117, loss 0.248314, acc 0.875\n",
      "2017-11-06T10:29:35.823230: step 2118, loss 0.0295052, acc 1\n",
      "2017-11-06T10:29:39.795051: step 2119, loss 0.147645, acc 0.96875\n",
      "2017-11-06T10:29:43.782884: step 2120, loss 0.153391, acc 0.9375\n",
      "2017-11-06T10:29:47.750703: step 2121, loss 0.112312, acc 0.9375\n",
      "2017-11-06T10:29:51.711517: step 2122, loss 0.286004, acc 0.875\n",
      "2017-11-06T10:29:55.694347: step 2123, loss 0.246316, acc 0.875\n",
      "2017-11-06T10:29:58.301200: step 2124, loss 0.192682, acc 0.9\n",
      "2017-11-06T10:30:02.614267: step 2125, loss 0.162599, acc 0.875\n",
      "2017-11-06T10:30:06.591091: step 2126, loss 0.252505, acc 0.875\n",
      "2017-11-06T10:30:10.618952: step 2127, loss 0.212506, acc 0.90625\n",
      "2017-11-06T10:30:14.566757: step 2128, loss 0.17281, acc 0.90625\n",
      "2017-11-06T10:30:18.564599: step 2129, loss 0.22385, acc 0.875\n",
      "2017-11-06T10:30:23.085678: step 2130, loss 0.299777, acc 0.875\n",
      "2017-11-06T10:30:27.830270: step 2131, loss 0.14159, acc 0.9375\n",
      "2017-11-06T10:30:32.021247: step 2132, loss 0.388508, acc 0.84375\n",
      "2017-11-06T10:30:36.374841: step 2133, loss 0.103745, acc 0.96875\n",
      "2017-11-06T10:30:40.530325: step 2134, loss 0.290898, acc 0.84375\n",
      "2017-11-06T10:30:44.582248: step 2135, loss 0.224939, acc 0.90625\n",
      "2017-11-06T10:30:48.613112: step 2136, loss 0.169333, acc 0.90625\n",
      "2017-11-06T10:30:52.584936: step 2137, loss 0.1766, acc 0.96875\n",
      "2017-11-06T10:30:56.636814: step 2138, loss 0.144532, acc 0.9375\n",
      "2017-11-06T10:31:00.694697: step 2139, loss 0.118948, acc 0.9375\n",
      "2017-11-06T10:31:04.778599: step 2140, loss 0.191797, acc 0.9375\n",
      "2017-11-06T10:31:08.764275: step 2141, loss 0.242326, acc 0.875\n",
      "2017-11-06T10:31:12.749106: step 2142, loss 0.416329, acc 0.84375\n",
      "2017-11-06T10:31:16.746947: step 2143, loss 0.334399, acc 0.84375\n",
      "2017-11-06T10:31:20.745790: step 2144, loss 0.217444, acc 0.9375\n",
      "2017-11-06T10:31:24.825687: step 2145, loss 0.138689, acc 0.90625\n",
      "2017-11-06T10:31:28.981640: step 2146, loss 0.0502155, acc 0.96875\n",
      "2017-11-06T10:31:33.361753: step 2147, loss 0.2772, acc 0.84375\n",
      "2017-11-06T10:31:37.537719: step 2148, loss 0.230369, acc 0.84375\n",
      "2017-11-06T10:31:41.691671: step 2149, loss 0.139651, acc 0.96875\n",
      "2017-11-06T10:31:45.788582: step 2150, loss 0.398057, acc 0.78125\n",
      "2017-11-06T10:31:49.833456: step 2151, loss 0.171825, acc 0.90625\n",
      "2017-11-06T10:31:53.814285: step 2152, loss 0.290928, acc 0.90625\n",
      "2017-11-06T10:31:57.831139: step 2153, loss 0.107011, acc 0.9375\n",
      "2017-11-06T10:32:01.788951: step 2154, loss 0.192691, acc 0.875\n",
      "2017-11-06T10:32:05.918885: step 2155, loss 0.198271, acc 0.90625\n",
      "2017-11-06T10:32:09.934739: step 2156, loss 0.204131, acc 0.9375\n",
      "2017-11-06T10:32:13.919571: step 2157, loss 0.203872, acc 0.875\n",
      "2017-11-06T10:32:17.958440: step 2158, loss 0.318116, acc 0.90625\n",
      "2017-11-06T10:32:21.915252: step 2159, loss 0.132589, acc 0.96875\n",
      "2017-11-06T10:32:24.548122: step 2160, loss 0.577901, acc 0.75\n",
      "2017-11-06T10:32:28.606005: step 2161, loss 0.125175, acc 0.9375\n",
      "2017-11-06T10:32:32.651880: step 2162, loss 0.271842, acc 0.84375\n",
      "2017-11-06T10:32:37.121056: step 2163, loss 0.282308, acc 0.875\n",
      "2017-11-06T10:32:41.178939: step 2164, loss 0.0997918, acc 0.96875\n",
      "2017-11-06T10:32:45.176781: step 2165, loss 0.0886166, acc 0.96875\n",
      "2017-11-06T10:32:49.087560: step 2166, loss 0.18454, acc 0.9375\n",
      "2017-11-06T10:32:52.981327: step 2167, loss 0.181758, acc 0.90625\n",
      "2017-11-06T10:32:56.976164: step 2168, loss 0.218934, acc 0.875\n",
      "2017-11-06T10:33:00.908959: step 2169, loss 0.201763, acc 0.9375\n",
      "2017-11-06T10:33:04.843754: step 2170, loss 0.200614, acc 0.875\n",
      "2017-11-06T10:33:08.824585: step 2171, loss 0.283261, acc 0.875\n",
      "2017-11-06T10:33:12.729357: step 2172, loss 0.0747221, acc 0.96875\n",
      "2017-11-06T10:33:16.667155: step 2173, loss 0.209137, acc 0.90625\n",
      "2017-11-06T10:33:20.569928: step 2174, loss 0.304515, acc 0.90625\n",
      "2017-11-06T10:33:24.650828: step 2175, loss 0.157003, acc 0.96875\n",
      "2017-11-06T10:33:28.647668: step 2176, loss 0.384051, acc 0.875\n",
      "2017-11-06T10:33:32.603478: step 2177, loss 0.160494, acc 0.9375\n",
      "2017-11-06T10:33:36.508253: step 2178, loss 0.23291, acc 0.90625\n",
      "2017-11-06T10:33:40.628181: step 2179, loss 0.258415, acc 0.875\n",
      "2017-11-06T10:33:45.035312: step 2180, loss 0.0358274, acc 1\n",
      "2017-11-06T10:33:49.294338: step 2181, loss 0.165927, acc 0.9375\n",
      "2017-11-06T10:33:53.324201: step 2182, loss 0.125191, acc 0.9375\n",
      "2017-11-06T10:33:57.280013: step 2183, loss 0.370138, acc 0.8125\n",
      "2017-11-06T10:34:01.422956: step 2184, loss 0.340707, acc 0.875\n",
      "2017-11-06T10:34:05.417794: step 2185, loss 0.320637, acc 0.84375\n",
      "2017-11-06T10:34:09.526473: step 2186, loss 0.337049, acc 0.8125\n",
      "2017-11-06T10:34:13.483284: step 2187, loss 0.0703886, acc 0.96875\n",
      "2017-11-06T10:34:17.682268: step 2188, loss 0.194114, acc 0.9375\n",
      "2017-11-06T10:34:21.865240: step 2189, loss 0.195653, acc 0.875\n",
      "2017-11-06T10:34:26.201321: step 2190, loss 0.0322199, acc 1\n",
      "2017-11-06T10:34:30.174144: step 2191, loss 0.107102, acc 0.9375\n",
      "2017-11-06T10:34:34.494213: step 2192, loss 0.201934, acc 0.9375\n",
      "2017-11-06T10:34:38.509066: step 2193, loss 0.239991, acc 0.90625\n",
      "2017-11-06T10:34:42.429853: step 2194, loss 0.428732, acc 0.875\n",
      "2017-11-06T10:34:46.634840: step 2195, loss 0.207175, acc 0.90625\n",
      "2017-11-06T10:34:49.457846: step 2196, loss 0.461726, acc 0.8\n",
      "2017-11-06T10:34:53.417659: step 2197, loss 0.119662, acc 0.9375\n",
      "2017-11-06T10:34:57.356459: step 2198, loss 0.1363, acc 0.9375\n",
      "2017-11-06T10:35:01.348295: step 2199, loss 0.401807, acc 0.875\n",
      "2017-11-06T10:35:05.216044: step 2200, loss 0.14456, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:35:07.761852: step 2200, loss 0.926398, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-06T10:35:13.043349: step 2201, loss 0.119086, acc 0.9375\n",
      "2017-11-06T10:35:17.021176: step 2202, loss 0.131365, acc 0.9375\n",
      "2017-11-06T10:35:21.034026: step 2203, loss 0.1452, acc 0.96875\n",
      "2017-11-06T10:35:25.013854: step 2204, loss 0.118616, acc 0.90625\n",
      "2017-11-06T10:35:28.987677: step 2205, loss 0.0690638, acc 0.96875\n",
      "2017-11-06T10:35:32.971508: step 2206, loss 0.29971, acc 0.875\n",
      "2017-11-06T10:35:36.970351: step 2207, loss 0.232279, acc 0.90625\n",
      "2017-11-06T10:35:41.012221: step 2208, loss 0.318148, acc 0.84375\n",
      "2017-11-06T10:35:45.023071: step 2209, loss 0.119749, acc 0.9375\n",
      "2017-11-06T10:35:49.058939: step 2210, loss 0.105139, acc 0.9375\n",
      "2017-11-06T10:35:53.361996: step 2211, loss 0.0343893, acc 0.96875\n",
      "2017-11-06T10:35:57.546970: step 2212, loss 0.0886137, acc 0.96875\n",
      "2017-11-06T10:36:01.533802: step 2213, loss 0.250079, acc 0.875\n",
      "2017-11-06T10:36:05.542653: step 2214, loss 0.276343, acc 0.90625\n",
      "2017-11-06T10:36:09.522481: step 2215, loss 0.223031, acc 0.9375\n",
      "2017-11-06T10:36:13.464280: step 2216, loss 0.377571, acc 0.78125\n",
      "2017-11-06T10:36:17.474130: step 2217, loss 0.482691, acc 0.78125\n",
      "2017-11-06T10:36:21.406927: step 2218, loss 0.229749, acc 0.84375\n",
      "2017-11-06T10:36:25.384750: step 2219, loss 0.214555, acc 0.90625\n",
      "2017-11-06T10:36:29.395599: step 2220, loss 0.10518, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T10:36:33.557557: step 2221, loss 0.327875, acc 0.84375\n",
      "2017-11-06T10:36:37.742530: step 2222, loss 0.0809202, acc 0.96875\n",
      "2017-11-06T10:36:41.703345: step 2223, loss 0.377877, acc 0.84375\n",
      "2017-11-06T10:36:45.719199: step 2224, loss 0.201734, acc 0.90625\n",
      "2017-11-06T10:36:49.707032: step 2225, loss 0.366963, acc 0.84375\n",
      "2017-11-06T10:36:53.766916: step 2226, loss 0.125725, acc 0.9375\n",
      "2017-11-06T10:36:57.960896: step 2227, loss 0.336471, acc 0.8125\n",
      "2017-11-06T10:37:02.244940: step 2228, loss 0.290215, acc 0.8125\n",
      "2017-11-06T10:37:06.255790: step 2229, loss 0.295801, acc 0.90625\n",
      "2017-11-06T10:37:10.227387: step 2230, loss 0.253695, acc 0.9375\n",
      "2017-11-06T10:37:14.216221: step 2231, loss 0.10205, acc 0.9375\n",
      "2017-11-06T10:37:16.840086: step 2232, loss 0.238271, acc 0.9\n",
      "2017-11-06T10:37:20.861943: step 2233, loss 0.0926582, acc 0.9375\n",
      "2017-11-06T10:37:24.928832: step 2234, loss 0.130014, acc 0.9375\n",
      "2017-11-06T10:37:28.958696: step 2235, loss 0.0899851, acc 0.9375\n",
      "2017-11-06T10:37:32.967545: step 2236, loss 0.0808957, acc 0.96875\n",
      "2017-11-06T10:37:36.947373: step 2237, loss 0.00871584, acc 1\n",
      "2017-11-06T10:37:40.904184: step 2238, loss 0.209355, acc 0.875\n",
      "2017-11-06T10:37:44.879008: step 2239, loss 0.136136, acc 0.90625\n",
      "2017-11-06T10:37:48.906870: step 2240, loss 0.268794, acc 0.90625\n",
      "2017-11-06T10:37:52.942738: step 2241, loss 0.223394, acc 0.875\n",
      "2017-11-06T10:37:56.963596: step 2242, loss 0.220304, acc 0.875\n",
      "2017-11-06T10:38:00.917404: step 2243, loss 0.258002, acc 0.875\n",
      "2017-11-06T10:38:05.294515: step 2244, loss 0.414719, acc 0.75\n",
      "2017-11-06T10:38:09.440460: step 2245, loss 0.23746, acc 0.90625\n",
      "2017-11-06T10:38:13.442304: step 2246, loss 0.112624, acc 0.9375\n",
      "2017-11-06T10:38:17.450151: step 2247, loss 0.0268897, acc 1\n",
      "2017-11-06T10:38:21.418972: step 2248, loss 0.225846, acc 0.90625\n",
      "2017-11-06T10:38:25.438828: step 2249, loss 0.253128, acc 0.875\n",
      "2017-11-06T10:38:29.513724: step 2250, loss 0.127887, acc 0.9375\n",
      "2017-11-06T10:38:33.721713: step 2251, loss 0.219088, acc 0.875\n",
      "2017-11-06T10:38:37.750576: step 2252, loss 0.249978, acc 0.875\n",
      "2017-11-06T10:38:41.716393: step 2253, loss 0.142722, acc 0.9375\n",
      "2017-11-06T10:38:45.660196: step 2254, loss 0.406374, acc 0.78125\n",
      "2017-11-06T10:38:49.693061: step 2255, loss 0.0637799, acc 1\n",
      "2017-11-06T10:38:53.695906: step 2256, loss 0.132801, acc 0.9375\n",
      "2017-11-06T10:38:57.683739: step 2257, loss 0.202538, acc 0.90625\n",
      "2017-11-06T10:39:01.647557: step 2258, loss 0.205004, acc 0.875\n",
      "2017-11-06T10:39:05.622380: step 2259, loss 0.286563, acc 0.875\n",
      "2017-11-06T10:39:09.920434: step 2260, loss 0.422028, acc 0.8125\n",
      "2017-11-06T10:39:14.173456: step 2261, loss 0.241535, acc 0.8125\n",
      "2017-11-06T10:39:18.178301: step 2262, loss 0.129611, acc 0.90625\n",
      "2017-11-06T10:39:22.193154: step 2263, loss 0.167952, acc 0.9375\n",
      "2017-11-06T10:39:26.291066: step 2264, loss 0.149836, acc 0.90625\n",
      "2017-11-06T10:39:30.605132: step 2265, loss 0.100847, acc 0.96875\n",
      "2017-11-06T10:39:34.583959: step 2266, loss 0.0858064, acc 0.9375\n",
      "2017-11-06T10:39:38.537769: step 2267, loss 0.242107, acc 0.875\n",
      "2017-11-06T10:39:41.145620: step 2268, loss 0.4814, acc 0.8\n",
      "2017-11-06T10:39:45.154469: step 2269, loss 0.167785, acc 0.90625\n",
      "2017-11-06T10:39:49.216356: step 2270, loss 0.185705, acc 0.90625\n",
      "2017-11-06T10:39:53.210194: step 2271, loss 0.318264, acc 0.8125\n",
      "2017-11-06T10:39:57.232051: step 2272, loss 0.37187, acc 0.875\n",
      "2017-11-06T10:40:01.508089: step 2273, loss 0.15278, acc 0.90625\n",
      "2017-11-06T10:40:05.506930: step 2274, loss 0.156522, acc 0.96875\n",
      "2017-11-06T10:40:09.464587: step 2275, loss 0.226742, acc 0.90625\n",
      "2017-11-06T10:40:13.556493: step 2276, loss 0.13635, acc 0.96875\n",
      "2017-11-06T10:40:17.943610: step 2277, loss 0.235143, acc 0.875\n",
      "2017-11-06T10:40:21.974475: step 2278, loss 0.283172, acc 0.875\n",
      "2017-11-06T10:40:26.030356: step 2279, loss 0.231875, acc 0.875\n",
      "2017-11-06T10:40:30.033201: step 2280, loss 0.139803, acc 0.90625\n",
      "2017-11-06T10:40:34.191155: step 2281, loss 0.0496193, acc 0.96875\n",
      "2017-11-06T10:40:38.259046: step 2282, loss 0.157864, acc 0.9375\n",
      "2017-11-06T10:40:42.248880: step 2283, loss 0.223548, acc 0.875\n",
      "2017-11-06T10:40:46.183676: step 2284, loss 0.0744105, acc 0.96875\n",
      "2017-11-06T10:40:50.167507: step 2285, loss 0.145959, acc 0.90625\n",
      "2017-11-06T10:40:54.179358: step 2286, loss 0.206057, acc 0.90625\n",
      "2017-11-06T10:40:58.153180: step 2287, loss 0.146766, acc 0.96875\n",
      "2017-11-06T10:41:02.125003: step 2288, loss 0.0823623, acc 0.96875\n",
      "2017-11-06T10:41:06.107833: step 2289, loss 0.335706, acc 0.84375\n",
      "2017-11-06T10:41:10.467931: step 2290, loss 0.117211, acc 0.9375\n",
      "2017-11-06T10:41:15.421451: step 2291, loss 0.296679, acc 0.84375\n",
      "2017-11-06T10:41:19.894629: step 2292, loss 0.0733797, acc 0.96875\n",
      "2017-11-06T10:41:24.307764: step 2293, loss 0.31543, acc 0.8125\n",
      "2017-11-06T10:41:28.325620: step 2294, loss 0.242542, acc 0.90625\n",
      "2017-11-06T10:41:32.321460: step 2295, loss 0.283225, acc 0.875\n",
      "2017-11-06T10:41:36.364332: step 2296, loss 0.171404, acc 0.9375\n",
      "2017-11-06T10:41:40.310135: step 2297, loss 0.223628, acc 0.875\n",
      "2017-11-06T10:41:44.263944: step 2298, loss 0.0453406, acc 1\n",
      "2017-11-06T10:41:48.280799: step 2299, loss 0.177243, acc 0.84375\n",
      "2017-11-06T10:41:52.363700: step 2300, loss 0.273679, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:41:55.100645: step 2300, loss 0.915997, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-06T10:42:00.639257: step 2301, loss 0.0558898, acc 1\n",
      "2017-11-06T10:42:04.673124: step 2302, loss 0.208914, acc 0.9375\n",
      "2017-11-06T10:42:08.708991: step 2303, loss 0.233043, acc 0.875\n",
      "2017-11-06T10:42:11.316844: step 2304, loss 0.486145, acc 0.8\n",
      "2017-11-06T10:42:15.360717: step 2305, loss 0.187301, acc 0.9375\n",
      "2017-11-06T10:42:19.482646: step 2306, loss 0.0584315, acc 0.96875\n",
      "2017-11-06T10:42:23.876773: step 2307, loss 0.0528379, acc 1\n",
      "2017-11-06T10:42:28.793261: step 2308, loss 0.16034, acc 0.9375\n",
      "2017-11-06T10:42:33.087313: step 2309, loss 0.208226, acc 0.875\n",
      "2017-11-06T10:42:37.256275: step 2310, loss 0.168913, acc 0.9375\n",
      "2017-11-06T10:42:41.290141: step 2311, loss 0.188359, acc 0.90625\n",
      "2017-11-06T10:42:45.256961: step 2312, loss 0.151213, acc 0.9375\n",
      "2017-11-06T10:42:49.234785: step 2313, loss 0.203774, acc 0.875\n",
      "2017-11-06T10:42:53.363720: step 2314, loss 0.150144, acc 0.90625\n",
      "2017-11-06T10:42:57.619744: step 2315, loss 0.0935973, acc 0.9375\n",
      "2017-11-06T10:43:02.193995: step 2316, loss 0.0853013, acc 0.96875\n",
      "2017-11-06T10:43:06.362957: step 2317, loss 0.105744, acc 0.90625\n",
      "2017-11-06T10:43:10.419646: step 2318, loss 0.126072, acc 0.90625\n",
      "2017-11-06T10:43:14.552583: step 2319, loss 0.143204, acc 0.90625\n",
      "2017-11-06T10:43:18.688524: step 2320, loss 0.135002, acc 0.96875\n",
      "2017-11-06T10:43:23.262772: step 2321, loss 0.114039, acc 0.9375\n",
      "2017-11-06T10:43:27.332664: step 2322, loss 0.302025, acc 0.875\n",
      "2017-11-06T10:43:31.675750: step 2323, loss 0.145677, acc 0.90625\n",
      "2017-11-06T10:43:36.140922: step 2324, loss 0.436282, acc 0.78125\n",
      "2017-11-06T10:43:40.257847: step 2325, loss 0.204871, acc 0.90625\n",
      "2017-11-06T10:43:44.391785: step 2326, loss 0.199735, acc 0.90625\n",
      "2017-11-06T10:43:48.479690: step 2327, loss 0.256922, acc 0.8125\n",
      "2017-11-06T10:43:52.636644: step 2328, loss 0.11303, acc 0.9375\n",
      "2017-11-06T10:43:56.764578: step 2329, loss 0.143597, acc 0.9375\n",
      "2017-11-06T10:44:00.976570: step 2330, loss 0.151659, acc 0.90625\n",
      "2017-11-06T10:44:05.119513: step 2331, loss 0.0754381, acc 0.96875\n",
      "2017-11-06T10:44:09.252450: step 2332, loss 0.229997, acc 0.84375\n",
      "2017-11-06T10:44:13.343357: step 2333, loss 0.231665, acc 0.90625\n",
      "2017-11-06T10:44:17.496307: step 2334, loss 0.148586, acc 0.90625\n",
      "2017-11-06T10:44:21.630245: step 2335, loss 0.286344, acc 0.84375\n",
      "2017-11-06T10:44:25.961322: step 2336, loss 0.266959, acc 0.8125\n",
      "2017-11-06T10:44:30.215345: step 2337, loss 0.379392, acc 0.8125\n",
      "2017-11-06T10:44:34.483378: step 2338, loss 0.282015, acc 0.8125\n",
      "2017-11-06T10:44:39.128678: step 2339, loss 0.211989, acc 0.875\n",
      "2017-11-06T10:44:41.992662: step 2340, loss 0.0841395, acc 0.95\n",
      "2017-11-06T10:44:46.121594: step 2341, loss 0.130624, acc 0.9375\n",
      "2017-11-06T10:44:50.273544: step 2342, loss 0.196764, acc 0.875\n",
      "2017-11-06T10:44:54.393473: step 2343, loss 0.263117, acc 0.84375\n",
      "2017-11-06T10:44:58.530411: step 2344, loss 0.171115, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T10:45:02.665349: step 2345, loss 0.317216, acc 0.875\n",
      "2017-11-06T10:45:06.743246: step 2346, loss 0.205075, acc 0.84375\n",
      "2017-11-06T10:45:10.902203: step 2347, loss 0.364037, acc 0.875\n",
      "2017-11-06T10:45:15.056153: step 2348, loss 0.146036, acc 0.96875\n",
      "2017-11-06T10:45:19.204100: step 2349, loss 0.125921, acc 0.9375\n",
      "2017-11-06T10:45:23.311019: step 2350, loss 0.12037, acc 0.9375\n",
      "2017-11-06T10:45:27.403927: step 2351, loss 0.0377797, acc 0.96875\n",
      "2017-11-06T10:45:31.511846: step 2352, loss 0.14768, acc 0.9375\n",
      "2017-11-06T10:45:35.589745: step 2353, loss 0.164266, acc 0.875\n",
      "2017-11-06T10:45:39.766711: step 2354, loss 0.106822, acc 0.9375\n",
      "2017-11-06T10:45:44.251899: step 2355, loss 0.220237, acc 0.90625\n",
      "2017-11-06T10:45:48.498916: step 2356, loss 0.325139, acc 0.875\n",
      "2017-11-06T10:45:52.639858: step 2357, loss 0.107329, acc 0.9375\n",
      "2017-11-06T10:45:56.705749: step 2358, loss 0.185339, acc 0.9375\n",
      "2017-11-06T10:46:00.796655: step 2359, loss 0.148041, acc 0.9375\n",
      "2017-11-06T10:46:04.892564: step 2360, loss 0.19351, acc 0.90625\n",
      "2017-11-06T10:46:09.044287: step 2361, loss 0.260838, acc 0.84375\n",
      "2017-11-06T10:46:13.157211: step 2362, loss 0.173722, acc 0.90625\n",
      "2017-11-06T10:46:17.254119: step 2363, loss 0.108504, acc 0.96875\n",
      "2017-11-06T10:46:21.314005: step 2364, loss 0.202507, acc 0.90625\n",
      "2017-11-06T10:46:25.432932: step 2365, loss 0.203771, acc 0.90625\n",
      "2017-11-06T10:46:29.516835: step 2366, loss 0.180777, acc 0.9375\n",
      "2017-11-06T10:46:33.750842: step 2367, loss 0.108859, acc 0.96875\n",
      "2017-11-06T10:46:37.973842: step 2368, loss 0.233586, acc 0.84375\n",
      "2017-11-06T10:46:42.085763: step 2369, loss 0.22877, acc 0.90625\n",
      "2017-11-06T10:46:46.882003: step 2370, loss 0.25457, acc 0.875\n",
      "2017-11-06T10:46:51.402214: step 2371, loss 0.0245655, acc 1\n",
      "2017-11-06T10:46:55.522142: step 2372, loss 0.220394, acc 0.9375\n",
      "2017-11-06T10:46:59.589032: step 2373, loss 0.0277827, acc 1\n",
      "2017-11-06T10:47:03.675935: step 2374, loss 0.17082, acc 0.90625\n",
      "2017-11-06T10:47:07.709803: step 2375, loss 0.239207, acc 0.90625\n",
      "2017-11-06T10:47:10.327662: step 2376, loss 0.782098, acc 0.8\n",
      "2017-11-06T10:47:14.387547: step 2377, loss 0.236982, acc 0.90625\n",
      "2017-11-06T10:47:18.525489: step 2378, loss 0.112579, acc 0.90625\n",
      "2017-11-06T10:47:22.606388: step 2379, loss 0.124658, acc 0.9375\n",
      "2017-11-06T10:47:26.681282: step 2380, loss 0.152445, acc 0.9375\n",
      "2017-11-06T10:47:30.780194: step 2381, loss 0.277867, acc 0.875\n",
      "2017-11-06T10:47:34.837078: step 2382, loss 0.244899, acc 0.875\n",
      "2017-11-06T10:47:38.933988: step 2383, loss 0.182814, acc 0.9375\n",
      "2017-11-06T10:47:42.942837: step 2384, loss 0.22856, acc 0.90625\n",
      "2017-11-06T10:47:47.012729: step 2385, loss 0.220526, acc 0.90625\n",
      "2017-11-06T10:47:51.146665: step 2386, loss 0.153987, acc 0.96875\n",
      "2017-11-06T10:47:55.615841: step 2387, loss 0.177351, acc 0.9375\n",
      "2017-11-06T10:47:59.851851: step 2388, loss 0.205312, acc 0.9375\n",
      "2017-11-06T10:48:03.881715: step 2389, loss 0.241445, acc 0.9375\n",
      "2017-11-06T10:48:07.981628: step 2390, loss 0.182761, acc 0.90625\n",
      "2017-11-06T10:48:12.030504: step 2391, loss 0.204359, acc 0.9375\n",
      "2017-11-06T10:48:16.148431: step 2392, loss 0.263889, acc 0.9375\n",
      "2017-11-06T10:48:20.241339: step 2393, loss 0.205034, acc 0.875\n",
      "2017-11-06T10:48:24.443325: step 2394, loss 0.214481, acc 0.875\n",
      "2017-11-06T10:48:28.735374: step 2395, loss 0.185871, acc 0.90625\n",
      "2017-11-06T10:48:32.847296: step 2396, loss 0.225699, acc 0.90625\n",
      "2017-11-06T10:48:37.103319: step 2397, loss 0.159931, acc 0.9375\n",
      "2017-11-06T10:48:41.209237: step 2398, loss 0.171886, acc 0.9375\n",
      "2017-11-06T10:48:45.258114: step 2399, loss 0.209155, acc 0.875\n",
      "2017-11-06T10:48:49.863388: step 2400, loss 0.0914296, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:48:52.515271: step 2400, loss 0.966817, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-06T10:49:00.303730: step 2401, loss 0.319638, acc 0.875\n",
      "2017-11-06T10:49:04.620798: step 2402, loss 0.0792191, acc 0.96875\n",
      "2017-11-06T10:49:08.753511: step 2403, loss 0.148139, acc 0.9375\n",
      "2017-11-06T10:49:12.862431: step 2404, loss 0.103854, acc 0.9375\n",
      "2017-11-06T10:49:16.972352: step 2405, loss 0.359175, acc 0.84375\n",
      "2017-11-06T10:49:21.041243: step 2406, loss 0.143579, acc 0.875\n",
      "2017-11-06T10:49:25.189190: step 2407, loss 0.186669, acc 0.90625\n",
      "2017-11-06T10:49:29.262083: step 2408, loss 0.359415, acc 0.875\n",
      "2017-11-06T10:49:33.349989: step 2409, loss 0.13581, acc 0.9375\n",
      "2017-11-06T10:49:37.451903: step 2410, loss 0.280669, acc 0.875\n",
      "2017-11-06T10:49:41.545812: step 2411, loss 0.122354, acc 0.96875\n",
      "2017-11-06T10:49:44.222714: step 2412, loss 0.146626, acc 0.95\n",
      "2017-11-06T10:49:48.348645: step 2413, loss 0.153929, acc 0.9375\n",
      "2017-11-06T10:49:52.520610: step 2414, loss 0.168895, acc 0.875\n",
      "2017-11-06T10:49:56.706603: step 2415, loss 0.224338, acc 0.875\n",
      "2017-11-06T10:50:01.065686: step 2416, loss 0.312176, acc 0.875\n",
      "2017-11-06T10:50:05.449796: step 2417, loss 0.217672, acc 0.9375\n",
      "2017-11-06T10:50:09.883948: step 2418, loss 0.342502, acc 0.90625\n",
      "2017-11-06T10:50:14.083934: step 2419, loss 0.317023, acc 0.875\n",
      "2017-11-06T10:50:18.154826: step 2420, loss 0.151893, acc 0.90625\n",
      "2017-11-06T10:50:22.277753: step 2421, loss 0.335236, acc 0.84375\n",
      "2017-11-06T10:50:26.406687: step 2422, loss 0.107937, acc 0.96875\n",
      "2017-11-06T10:50:30.571646: step 2423, loss 0.149009, acc 0.90625\n",
      "2017-11-06T10:50:34.965769: step 2424, loss 0.178369, acc 0.90625\n",
      "2017-11-06T10:50:39.083694: step 2425, loss 0.275899, acc 0.90625\n",
      "2017-11-06T10:50:43.231642: step 2426, loss 0.342546, acc 0.8125\n",
      "2017-11-06T10:50:47.330554: step 2427, loss 0.091326, acc 0.96875\n",
      "2017-11-06T10:50:51.918852: step 2428, loss 0.403611, acc 0.875\n",
      "2017-11-06T10:50:55.997749: step 2429, loss 0.137315, acc 0.96875\n",
      "2017-11-06T10:51:00.143695: step 2430, loss 0.091898, acc 0.96875\n",
      "2017-11-06T10:51:04.215589: step 2431, loss 0.226548, acc 0.9375\n",
      "2017-11-06T10:51:08.541414: step 2432, loss 0.146895, acc 0.84375\n",
      "2017-11-06T10:51:13.080640: step 2433, loss 0.258058, acc 0.90625\n",
      "2017-11-06T10:51:17.208573: step 2434, loss 0.0505341, acc 0.96875\n",
      "2017-11-06T10:51:21.287471: step 2435, loss 0.117554, acc 0.90625\n",
      "2017-11-06T10:51:25.424411: step 2436, loss 0.232658, acc 0.9375\n",
      "2017-11-06T10:51:29.523323: step 2437, loss 0.567692, acc 0.8125\n",
      "2017-11-06T10:51:33.684280: step 2438, loss 0.207474, acc 0.9375\n",
      "2017-11-06T10:51:37.753170: step 2439, loss 0.179216, acc 0.9375\n",
      "2017-11-06T10:51:41.902119: step 2440, loss 0.0890756, acc 0.9375\n",
      "2017-11-06T10:51:46.013040: step 2441, loss 0.148827, acc 0.90625\n",
      "2017-11-06T10:51:50.202016: step 2442, loss 0.286312, acc 0.90625\n",
      "2017-11-06T10:51:54.302931: step 2443, loss 0.0966904, acc 0.96875\n",
      "2017-11-06T10:51:58.432864: step 2444, loss 0.447206, acc 0.8125\n",
      "2017-11-06T10:52:02.629846: step 2445, loss 0.376819, acc 0.875\n",
      "2017-11-06T10:52:06.848844: step 2446, loss 0.249267, acc 0.90625\n",
      "2017-11-06T10:52:11.007799: step 2447, loss 0.23568, acc 0.90625\n",
      "2017-11-06T10:52:13.738740: step 2448, loss 0.207256, acc 0.9\n",
      "2017-11-06T10:52:18.345013: step 2449, loss 0.315317, acc 0.90625\n",
      "2017-11-06T10:52:22.731129: step 2450, loss 0.142307, acc 0.9375\n",
      "2017-11-06T10:52:26.918104: step 2451, loss 0.0881383, acc 0.96875\n",
      "2017-11-06T10:52:30.982992: step 2452, loss 0.233824, acc 0.90625\n",
      "2017-11-06T10:52:35.587859: step 2453, loss 0.269372, acc 0.90625\n",
      "2017-11-06T10:52:39.753820: step 2454, loss 0.180103, acc 0.9375\n",
      "2017-11-06T10:52:43.929786: step 2455, loss 0.16622, acc 0.9375\n",
      "2017-11-06T10:52:48.008684: step 2456, loss 0.251442, acc 0.90625\n",
      "2017-11-06T10:52:52.861259: step 2457, loss 0.0970272, acc 0.9375\n",
      "2017-11-06T10:52:57.043230: step 2458, loss 0.106652, acc 0.96875\n",
      "2017-11-06T10:53:01.180169: step 2459, loss 0.326047, acc 0.8125\n",
      "2017-11-06T10:53:05.201026: step 2460, loss 0.0565267, acc 1\n",
      "2017-11-06T10:53:09.357981: step 2461, loss 0.288579, acc 0.90625\n",
      "2017-11-06T10:53:13.462896: step 2462, loss 0.320673, acc 0.875\n",
      "2017-11-06T10:53:17.585827: step 2463, loss 0.160484, acc 0.9375\n",
      "2017-11-06T10:53:21.904895: step 2464, loss 0.33379, acc 0.8125\n",
      "2017-11-06T10:53:26.375071: step 2465, loss 0.260296, acc 0.90625\n",
      "2017-11-06T10:53:30.696142: step 2466, loss 0.119922, acc 0.9375\n",
      "2017-11-06T10:53:34.802059: step 2467, loss 0.124901, acc 0.9375\n",
      "2017-11-06T10:53:38.895968: step 2468, loss 0.26773, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T10:53:43.019898: step 2469, loss 0.166358, acc 0.9375\n",
      "2017-11-06T10:53:47.086787: step 2470, loss 0.0828359, acc 0.96875\n",
      "2017-11-06T10:53:51.213721: step 2471, loss 0.204807, acc 0.84375\n",
      "2017-11-06T10:53:55.353662: step 2472, loss 0.260842, acc 0.90625\n",
      "2017-11-06T10:53:59.450573: step 2473, loss 0.0926057, acc 0.96875\n",
      "2017-11-06T10:54:03.574503: step 2474, loss 0.316115, acc 0.875\n",
      "2017-11-06T10:54:07.692430: step 2475, loss 0.0607978, acc 0.96875\n",
      "2017-11-06T10:54:11.769325: step 2476, loss 0.252109, acc 0.875\n",
      "2017-11-06T10:54:15.903263: step 2477, loss 0.142059, acc 0.96875\n",
      "2017-11-06T10:54:19.980160: step 2478, loss 0.389048, acc 0.8125\n",
      "2017-11-06T10:54:24.097086: step 2479, loss 0.48564, acc 0.78125\n",
      "2017-11-06T10:54:28.452180: step 2480, loss 0.217213, acc 0.90625\n",
      "2017-11-06T10:54:32.836295: step 2481, loss 0.228856, acc 0.875\n",
      "2017-11-06T10:54:37.055292: step 2482, loss 0.289605, acc 0.8125\n",
      "2017-11-06T10:54:41.286298: step 2483, loss 0.0701081, acc 0.96875\n",
      "2017-11-06T10:54:43.963202: step 2484, loss 0.232233, acc 0.9\n",
      "2017-11-06T10:54:48.094136: step 2485, loss 0.285087, acc 0.84375\n",
      "2017-11-06T10:54:52.363150: step 2486, loss 0.274814, acc 0.84375\n",
      "2017-11-06T10:54:56.412027: step 2487, loss 0.0851857, acc 0.96875\n",
      "2017-11-06T10:55:00.458903: step 2488, loss 0.132358, acc 0.9375\n",
      "2017-11-06T10:55:04.553812: step 2489, loss 0.109846, acc 0.96875\n",
      "2017-11-06T10:55:08.688750: step 2490, loss 0.162128, acc 0.90625\n",
      "2017-11-06T10:55:12.736410: step 2491, loss 0.291039, acc 0.84375\n",
      "2017-11-06T10:55:16.918382: step 2492, loss 0.225886, acc 0.875\n",
      "2017-11-06T10:55:21.059326: step 2493, loss 0.175238, acc 0.90625\n",
      "2017-11-06T10:55:25.163242: step 2494, loss 0.1829, acc 0.9375\n",
      "2017-11-06T10:55:29.294176: step 2495, loss 0.29955, acc 0.8125\n",
      "2017-11-06T10:55:33.605238: step 2496, loss 0.0773454, acc 0.96875\n",
      "2017-11-06T10:55:38.021376: step 2497, loss 0.192853, acc 0.9375\n",
      "2017-11-06T10:55:42.201347: step 2498, loss 0.149103, acc 0.90625\n",
      "2017-11-06T10:55:46.425348: step 2499, loss 0.314789, acc 0.8125\n",
      "2017-11-06T10:55:50.507249: step 2500, loss 0.213603, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T10:55:53.229182: step 2500, loss 0.943629, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-06T10:55:59.943877: step 2501, loss 0.188218, acc 0.875\n",
      "2017-11-06T10:56:04.032783: step 2502, loss 0.212895, acc 0.84375\n",
      "2017-11-06T10:56:08.126692: step 2503, loss 0.350444, acc 0.84375\n",
      "2017-11-06T10:56:12.229607: step 2504, loss 0.120075, acc 0.90625\n",
      "2017-11-06T10:56:16.370550: step 2505, loss 0.114414, acc 0.96875\n",
      "2017-11-06T10:56:20.474465: step 2506, loss 0.42428, acc 0.8125\n",
      "2017-11-06T10:56:24.597395: step 2507, loss 0.00770294, acc 1\n",
      "2017-11-06T10:56:28.655278: step 2508, loss 0.308288, acc 0.875\n",
      "2017-11-06T10:56:32.842253: step 2509, loss 0.224486, acc 0.9375\n",
      "2017-11-06T10:56:37.091272: step 2510, loss 0.0247602, acc 1\n",
      "2017-11-06T10:56:41.625494: step 2511, loss 0.152669, acc 0.9375\n",
      "2017-11-06T10:56:45.771440: step 2512, loss 0.349148, acc 0.875\n",
      "2017-11-06T10:56:49.863348: step 2513, loss 0.196999, acc 0.875\n",
      "2017-11-06T10:56:54.275710: step 2514, loss 0.208559, acc 0.90625\n",
      "2017-11-06T10:56:58.368617: step 2515, loss 0.286195, acc 0.90625\n",
      "2017-11-06T10:57:02.443512: step 2516, loss 0.186037, acc 0.90625\n",
      "2017-11-06T10:57:06.477377: step 2517, loss 0.267632, acc 0.875\n",
      "2017-11-06T10:57:10.572287: step 2518, loss 0.115196, acc 0.9375\n",
      "2017-11-06T10:57:14.715231: step 2519, loss 0.194049, acc 0.90625\n",
      "2017-11-06T10:57:17.374120: step 2520, loss 0.276783, acc 0.9\n",
      "2017-11-06T10:57:21.479037: step 2521, loss 0.0969452, acc 0.9375\n",
      "2017-11-06T10:57:25.649002: step 2522, loss 0.245503, acc 0.8125\n",
      "2017-11-06T10:57:29.798949: step 2523, loss 0.0784184, acc 0.9375\n",
      "2017-11-06T10:57:34.000935: step 2524, loss 0.229119, acc 0.90625\n",
      "2017-11-06T10:57:38.168896: step 2525, loss 0.240247, acc 0.90625\n",
      "2017-11-06T10:57:42.307837: step 2526, loss 0.11158, acc 0.9375\n",
      "2017-11-06T10:57:46.856069: step 2527, loss 0.300129, acc 0.84375\n",
      "2017-11-06T10:57:51.108090: step 2528, loss 0.181757, acc 0.90625\n",
      "2017-11-06T10:57:55.276051: step 2529, loss 0.130562, acc 0.90625\n",
      "2017-11-06T10:57:59.371962: step 2530, loss 0.252167, acc 0.90625\n",
      "2017-11-06T10:58:03.537923: step 2531, loss 0.323022, acc 0.875\n",
      "2017-11-06T10:58:07.650845: step 2532, loss 0.215793, acc 0.875\n",
      "2017-11-06T10:58:12.040893: step 2533, loss 0.0328825, acc 1\n",
      "2017-11-06T10:58:16.121793: step 2534, loss 0.0840633, acc 0.9375\n",
      "2017-11-06T10:58:20.254730: step 2535, loss 0.147456, acc 0.90625\n",
      "2017-11-06T10:58:24.556787: step 2536, loss 0.292828, acc 0.84375\n",
      "2017-11-06T10:58:28.910880: step 2537, loss 0.121402, acc 0.96875\n",
      "2017-11-06T10:58:33.170908: step 2538, loss 0.150449, acc 0.9375\n",
      "2017-11-06T10:58:37.427932: step 2539, loss 0.234697, acc 0.84375\n",
      "2017-11-06T10:58:41.567874: step 2540, loss 0.0854583, acc 0.96875\n",
      "2017-11-06T10:58:45.703814: step 2541, loss 0.425755, acc 0.8125\n",
      "2017-11-06T10:58:49.891788: step 2542, loss 0.141988, acc 0.9375\n",
      "2017-11-06T10:58:54.424008: step 2543, loss 0.177712, acc 0.9375\n",
      "2017-11-06T10:58:58.624993: step 2544, loss 0.0471607, acc 1\n",
      "2017-11-06T10:59:02.764937: step 2545, loss 0.215319, acc 0.90625\n",
      "2017-11-06T10:59:06.843834: step 2546, loss 0.104841, acc 0.9375\n",
      "2017-11-06T10:59:10.933740: step 2547, loss 0.183337, acc 0.9375\n",
      "2017-11-06T10:59:15.054667: step 2548, loss 0.191801, acc 0.9375\n",
      "2017-11-06T10:59:19.148577: step 2549, loss 0.308524, acc 0.84375\n",
      "2017-11-06T10:59:23.317538: step 2550, loss 0.103904, acc 0.96875\n",
      "2017-11-06T10:59:27.394436: step 2551, loss 0.157913, acc 0.90625\n",
      "2017-11-06T10:59:31.539381: step 2552, loss 0.311426, acc 0.78125\n",
      "2017-11-06T10:59:35.729358: step 2553, loss 0.0413774, acc 0.96875\n",
      "2017-11-06T10:59:39.908327: step 2554, loss 0.161478, acc 0.90625\n",
      "2017-11-06T10:59:43.948198: step 2555, loss 0.267339, acc 0.84375\n",
      "2017-11-06T10:59:46.582069: step 2556, loss 0.0723908, acc 1\n",
      "2017-11-06T10:59:50.719008: step 2557, loss 0.295486, acc 0.84375\n",
      "2017-11-06T10:59:54.929000: step 2558, loss 0.0381471, acc 1\n",
      "2017-11-06T10:59:59.392173: step 2559, loss 0.247161, acc 0.9375\n",
      "2017-11-06T11:00:03.735257: step 2560, loss 0.144314, acc 0.9375\n",
      "2017-11-06T11:00:07.948251: step 2561, loss 0.081431, acc 0.96875\n",
      "2017-11-06T11:00:12.036155: step 2562, loss 0.216186, acc 0.875\n",
      "2017-11-06T11:00:16.093039: step 2563, loss 0.171694, acc 0.90625\n",
      "2017-11-06T11:00:20.256997: step 2564, loss 0.0972395, acc 0.9375\n",
      "2017-11-06T11:00:24.406945: step 2565, loss 0.266249, acc 0.875\n",
      "2017-11-06T11:00:28.541883: step 2566, loss 0.275762, acc 0.875\n",
      "2017-11-06T11:00:32.755878: step 2567, loss 0.199692, acc 0.875\n",
      "2017-11-06T11:00:37.023911: step 2568, loss 0.13293, acc 0.96875\n",
      "2017-11-06T11:00:41.074788: step 2569, loss 0.110129, acc 0.96875\n",
      "2017-11-06T11:00:45.167697: step 2570, loss 0.132824, acc 0.9375\n",
      "2017-11-06T11:00:49.297631: step 2571, loss 0.28123, acc 0.84375\n",
      "2017-11-06T11:00:53.830176: step 2572, loss 0.199608, acc 0.90625\n",
      "2017-11-06T11:00:58.084251: step 2573, loss 0.343197, acc 0.8125\n",
      "2017-11-06T11:01:02.425336: step 2574, loss 0.234196, acc 0.875\n",
      "2017-11-06T11:01:06.719387: step 2575, loss 0.0842024, acc 0.96875\n",
      "2017-11-06T11:01:10.825259: step 2576, loss 0.238955, acc 0.875\n",
      "2017-11-06T11:01:14.876139: step 2577, loss 0.226001, acc 0.875\n",
      "2017-11-06T11:01:18.944028: step 2578, loss 0.122436, acc 0.90625\n",
      "2017-11-06T11:01:23.121996: step 2579, loss 0.208265, acc 0.90625\n",
      "2017-11-06T11:01:27.237920: step 2580, loss 0.124468, acc 0.9375\n",
      "2017-11-06T11:01:31.243767: step 2581, loss 0.0656838, acc 0.96875\n",
      "2017-11-06T11:01:35.423737: step 2582, loss 0.0501295, acc 0.96875\n",
      "2017-11-06T11:01:39.712785: step 2583, loss 0.225973, acc 0.9375\n",
      "2017-11-06T11:01:43.881747: step 2584, loss 0.277295, acc 0.90625\n",
      "2017-11-06T11:01:47.922618: step 2585, loss 0.141344, acc 0.9375\n",
      "2017-11-06T11:01:52.074588: step 2586, loss 0.33658, acc 0.90625\n",
      "2017-11-06T11:01:56.207506: step 2587, loss 0.304732, acc 0.90625\n",
      "2017-11-06T11:02:00.369462: step 2588, loss 0.197281, acc 0.875\n",
      "2017-11-06T11:02:04.593464: step 2589, loss 0.3175, acc 0.8125\n",
      "2017-11-06T11:02:09.101666: step 2590, loss 0.124191, acc 0.9375\n",
      "2017-11-06T11:02:13.314660: step 2591, loss 0.127061, acc 0.9375\n",
      "2017-11-06T11:02:15.983557: step 2592, loss 0.310827, acc 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T11:02:20.130503: step 2593, loss 0.082546, acc 0.96875\n",
      "2017-11-06T11:02:24.329486: step 2594, loss 0.135737, acc 0.9375\n",
      "2017-11-06T11:02:28.496448: step 2595, loss 0.0453052, acc 0.96875\n",
      "2017-11-06T11:02:32.636389: step 2596, loss 0.176634, acc 0.9375\n",
      "2017-11-06T11:02:36.888411: step 2597, loss 0.245641, acc 0.90625\n",
      "2017-11-06T11:02:41.026350: step 2598, loss 0.262358, acc 0.90625\n",
      "2017-11-06T11:02:45.167293: step 2599, loss 0.2228, acc 0.90625\n",
      "2017-11-06T11:02:49.225178: step 2600, loss 0.217607, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:02:51.880063: step 2600, loss 0.981117, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-06T11:02:59.526227: step 2601, loss 0.266915, acc 0.84375\n",
      "2017-11-06T11:03:03.780841: step 2602, loss 0.12599, acc 0.9375\n",
      "2017-11-06T11:03:07.889760: step 2603, loss 0.278997, acc 0.78125\n",
      "2017-11-06T11:03:12.151788: step 2604, loss 0.21264, acc 0.90625\n",
      "2017-11-06T11:03:16.670000: step 2605, loss 0.320297, acc 0.90625\n",
      "2017-11-06T11:03:20.719877: step 2606, loss 0.188937, acc 0.9375\n",
      "2017-11-06T11:03:25.027937: step 2607, loss 0.118109, acc 0.9375\n",
      "2017-11-06T11:03:29.136857: step 2608, loss 0.109654, acc 0.9375\n",
      "2017-11-06T11:03:33.226763: step 2609, loss 0.239399, acc 0.875\n",
      "2017-11-06T11:03:37.349693: step 2610, loss 0.190743, acc 0.90625\n",
      "2017-11-06T11:03:42.142214: step 2611, loss 0.201662, acc 0.9375\n",
      "2017-11-06T11:03:46.306171: step 2612, loss 0.0736993, acc 0.9375\n",
      "2017-11-06T11:03:50.572202: step 2613, loss 0.129176, acc 0.90625\n",
      "2017-11-06T11:03:54.643095: step 2614, loss 0.265664, acc 0.875\n",
      "2017-11-06T11:03:58.798047: step 2615, loss 0.229289, acc 0.90625\n",
      "2017-11-06T11:04:02.954000: step 2616, loss 0.264569, acc 0.875\n",
      "2017-11-06T11:04:07.059918: step 2617, loss 0.51209, acc 0.8125\n",
      "2017-11-06T11:04:11.133622: step 2618, loss 0.1171, acc 0.9375\n",
      "2017-11-06T11:04:15.210519: step 2619, loss 0.385315, acc 0.84375\n",
      "2017-11-06T11:04:19.684698: step 2620, loss 0.0769146, acc 0.96875\n",
      "2017-11-06T11:04:24.199907: step 2621, loss 0.232658, acc 0.8125\n",
      "2017-11-06T11:04:28.345852: step 2622, loss 0.376853, acc 0.78125\n",
      "2017-11-06T11:04:32.624892: step 2623, loss 0.159045, acc 0.96875\n",
      "2017-11-06T11:04:36.974983: step 2624, loss 0.146359, acc 0.90625\n",
      "2017-11-06T11:04:41.175968: step 2625, loss 0.186592, acc 0.90625\n",
      "2017-11-06T11:04:45.400971: step 2626, loss 0.337329, acc 0.8125\n",
      "2017-11-06T11:04:49.609961: step 2627, loss 0.132797, acc 0.9375\n",
      "2017-11-06T11:04:52.404947: step 2628, loss 0.280461, acc 0.9\n",
      "2017-11-06T11:04:56.836095: step 2629, loss 0.101664, acc 0.9375\n",
      "2017-11-06T11:05:01.057095: step 2630, loss 0.248149, acc 0.875\n",
      "2017-11-06T11:05:05.308115: step 2631, loss 0.179713, acc 0.9375\n",
      "2017-11-06T11:05:09.443053: step 2632, loss 0.202359, acc 0.875\n",
      "2017-11-06T11:05:13.539965: step 2633, loss 0.122838, acc 0.96875\n",
      "2017-11-06T11:05:17.587840: step 2634, loss 0.121512, acc 0.90625\n",
      "2017-11-06T11:05:21.635719: step 2635, loss 0.146195, acc 0.9375\n",
      "2017-11-06T11:05:26.155929: step 2636, loss 0.400412, acc 0.8125\n",
      "2017-11-06T11:05:30.381931: step 2637, loss 0.190008, acc 0.9375\n",
      "2017-11-06T11:05:34.443819: step 2638, loss 0.171806, acc 0.90625\n",
      "2017-11-06T11:05:38.589763: step 2639, loss 0.175412, acc 0.875\n",
      "2017-11-06T11:05:42.601614: step 2640, loss 0.0734984, acc 0.96875\n",
      "2017-11-06T11:05:46.707532: step 2641, loss 0.303027, acc 0.875\n",
      "2017-11-06T11:05:50.906515: step 2642, loss 0.0248722, acc 1\n",
      "2017-11-06T11:05:55.060466: step 2643, loss 0.234778, acc 0.875\n",
      "2017-11-06T11:05:59.137364: step 2644, loss 0.283541, acc 0.90625\n",
      "2017-11-06T11:06:03.261293: step 2645, loss 0.437455, acc 0.8125\n",
      "2017-11-06T11:06:07.374216: step 2646, loss 0.206612, acc 0.90625\n",
      "2017-11-06T11:06:11.440105: step 2647, loss 0.176688, acc 0.90625\n",
      "2017-11-06T11:06:15.537016: step 2648, loss 0.15578, acc 0.9375\n",
      "2017-11-06T11:06:19.661947: step 2649, loss 0.305248, acc 0.9375\n",
      "2017-11-06T11:06:23.655785: step 2650, loss 0.290502, acc 0.90625\n",
      "2017-11-06T11:06:27.802731: step 2651, loss 0.143004, acc 0.9375\n",
      "2017-11-06T11:06:32.341957: step 2652, loss 0.181908, acc 0.90625\n",
      "2017-11-06T11:06:36.662026: step 2653, loss 0.11377, acc 0.9375\n",
      "2017-11-06T11:06:40.767943: step 2654, loss 0.273829, acc 0.8125\n",
      "2017-11-06T11:06:44.822824: step 2655, loss 0.118538, acc 0.9375\n",
      "2017-11-06T11:06:48.832674: step 2656, loss 0.0619858, acc 0.9375\n",
      "2017-11-06T11:06:53.193766: step 2657, loss 0.410278, acc 0.78125\n",
      "2017-11-06T11:06:57.169591: step 2658, loss 0.288546, acc 0.90625\n",
      "2017-11-06T11:07:01.168432: step 2659, loss 0.169368, acc 0.90625\n",
      "2017-11-06T11:07:05.233404: step 2660, loss 0.15874, acc 0.90625\n",
      "2017-11-06T11:07:09.216234: step 2661, loss 0.093847, acc 0.96875\n",
      "2017-11-06T11:07:13.217847: step 2662, loss 0.168486, acc 0.9375\n",
      "2017-11-06T11:07:17.211684: step 2663, loss 0.174062, acc 0.9375\n",
      "2017-11-06T11:07:19.775506: step 2664, loss 0.172791, acc 0.9\n",
      "2017-11-06T11:07:23.865412: step 2665, loss 0.0965774, acc 0.96875\n",
      "2017-11-06T11:07:27.856248: step 2666, loss 0.197986, acc 0.875\n",
      "2017-11-06T11:07:31.943153: step 2667, loss 0.092738, acc 0.90625\n",
      "2017-11-06T11:07:36.244208: step 2668, loss 0.151238, acc 0.9375\n",
      "2017-11-06T11:07:40.447194: step 2669, loss 0.059974, acc 1\n",
      "2017-11-06T11:07:44.440033: step 2670, loss 0.134004, acc 0.9375\n",
      "2017-11-06T11:07:48.444877: step 2671, loss 0.189885, acc 0.84375\n",
      "2017-11-06T11:07:52.486749: step 2672, loss 0.278435, acc 0.90625\n",
      "2017-11-06T11:07:56.466576: step 2673, loss 0.118957, acc 0.9375\n",
      "2017-11-06T11:08:00.471423: step 2674, loss 0.257089, acc 0.84375\n",
      "2017-11-06T11:08:04.439242: step 2675, loss 0.0770162, acc 0.96875\n",
      "2017-11-06T11:08:08.430078: step 2676, loss 0.119301, acc 0.9375\n",
      "2017-11-06T11:08:12.462944: step 2677, loss 0.254417, acc 0.8125\n",
      "2017-11-06T11:08:16.470791: step 2678, loss 0.175376, acc 0.90625\n",
      "2017-11-06T11:08:20.475636: step 2679, loss 0.19172, acc 0.875\n",
      "2017-11-06T11:08:24.677622: step 2680, loss 0.233208, acc 0.90625\n",
      "2017-11-06T11:08:28.842581: step 2681, loss 0.203153, acc 0.90625\n",
      "2017-11-06T11:08:32.967512: step 2682, loss 0.2599, acc 0.875\n",
      "2017-11-06T11:08:37.161493: step 2683, loss 0.0906549, acc 0.9375\n",
      "2017-11-06T11:08:41.397502: step 2684, loss 0.172832, acc 0.90625\n",
      "2017-11-06T11:08:45.656528: step 2685, loss 0.250361, acc 0.875\n",
      "2017-11-06T11:08:49.645362: step 2686, loss 0.271172, acc 0.84375\n",
      "2017-11-06T11:08:53.726262: step 2687, loss 0.0452475, acc 1\n",
      "2017-11-06T11:08:57.703088: step 2688, loss 0.0408952, acc 0.96875\n",
      "2017-11-06T11:09:01.725946: step 2689, loss 0.174165, acc 0.875\n",
      "2017-11-06T11:09:05.929120: step 2690, loss 0.19437, acc 0.90625\n",
      "2017-11-06T11:09:09.985020: step 2691, loss 0.369794, acc 0.8125\n",
      "2017-11-06T11:09:13.953822: step 2692, loss 0.174764, acc 0.875\n",
      "2017-11-06T11:09:17.968674: step 2693, loss 0.0437591, acc 1\n",
      "2017-11-06T11:09:21.930489: step 2694, loss 0.350989, acc 0.875\n",
      "2017-11-06T11:09:26.001382: step 2695, loss 0.107534, acc 0.9375\n",
      "2017-11-06T11:09:30.007247: step 2696, loss 0.156192, acc 0.9375\n",
      "2017-11-06T11:09:34.077120: step 2697, loss 0.216241, acc 0.875\n",
      "2017-11-06T11:09:38.070957: step 2698, loss 0.0629375, acc 0.96875\n",
      "2017-11-06T11:09:42.100821: step 2699, loss 0.347907, acc 0.71875\n",
      "2017-11-06T11:09:44.646630: step 2700, loss 0.0804999, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:09:47.637755: step 2700, loss 1.02646, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-06T11:09:54.470629: step 2701, loss 0.322197, acc 0.8125\n",
      "2017-11-06T11:09:58.528512: step 2702, loss 0.0773186, acc 0.96875\n",
      "2017-11-06T11:10:02.771527: step 2703, loss 0.0726171, acc 0.9375\n",
      "2017-11-06T11:10:06.763363: step 2704, loss 0.1775, acc 0.90625\n",
      "2017-11-06T11:10:10.771115: step 2705, loss 0.0434143, acc 0.96875\n",
      "2017-11-06T11:10:14.864025: step 2706, loss 0.254652, acc 0.875\n",
      "2017-11-06T11:10:18.881878: step 2707, loss 0.0545519, acc 1\n",
      "2017-11-06T11:10:22.878719: step 2708, loss 0.0624975, acc 0.96875\n",
      "2017-11-06T11:10:26.894572: step 2709, loss 0.0972299, acc 0.96875\n",
      "2017-11-06T11:10:30.919433: step 2710, loss 0.153273, acc 0.90625\n",
      "2017-11-06T11:10:35.242503: step 2711, loss 0.164656, acc 0.9375\n",
      "2017-11-06T11:10:39.271366: step 2712, loss 0.280639, acc 0.875\n",
      "2017-11-06T11:10:43.380285: step 2713, loss 0.470207, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T11:10:47.450178: step 2714, loss 0.149703, acc 0.9375\n",
      "2017-11-06T11:10:51.583114: step 2715, loss 0.0495071, acc 1\n",
      "2017-11-06T11:10:56.041282: step 2716, loss 0.126437, acc 0.9375\n",
      "2017-11-06T11:11:00.133189: step 2717, loss 0.251394, acc 0.90625\n",
      "2017-11-06T11:11:04.244110: step 2718, loss 0.327746, acc 0.875\n",
      "2017-11-06T11:11:08.463627: step 2719, loss 0.202427, acc 0.90625\n",
      "2017-11-06T11:11:12.526514: step 2720, loss 0.222645, acc 0.9375\n",
      "2017-11-06T11:11:16.551374: step 2721, loss 0.213958, acc 0.875\n",
      "2017-11-06T11:11:20.658292: step 2722, loss 0.148758, acc 0.90625\n",
      "2017-11-06T11:11:24.805240: step 2723, loss 0.316709, acc 0.84375\n",
      "2017-11-06T11:11:29.301005: step 2724, loss 0.178381, acc 0.875\n",
      "2017-11-06T11:11:33.271826: step 2725, loss 0.164051, acc 0.90625\n",
      "2017-11-06T11:11:37.341719: step 2726, loss 0.185162, acc 0.875\n",
      "2017-11-06T11:11:41.378587: step 2727, loss 0.286435, acc 0.84375\n",
      "2017-11-06T11:11:45.445476: step 2728, loss 0.156655, acc 0.9375\n",
      "2017-11-06T11:11:49.441315: step 2729, loss 0.318989, acc 0.84375\n",
      "2017-11-06T11:11:53.646302: step 2730, loss 0.0831846, acc 0.9375\n",
      "2017-11-06T11:11:57.811262: step 2731, loss 0.151132, acc 0.90625\n",
      "2017-11-06T11:12:02.246413: step 2732, loss 0.23957, acc 0.9375\n",
      "2017-11-06T11:12:06.347328: step 2733, loss 0.212796, acc 0.875\n",
      "2017-11-06T11:12:10.425225: step 2734, loss 0.080304, acc 0.96875\n",
      "2017-11-06T11:12:14.523136: step 2735, loss 0.14926, acc 0.90625\n",
      "2017-11-06T11:12:17.073950: step 2736, loss 0.342057, acc 0.8\n",
      "2017-11-06T11:12:21.137836: step 2737, loss 0.199689, acc 0.9375\n",
      "2017-11-06T11:12:25.215735: step 2738, loss 0.273038, acc 0.90625\n",
      "2017-11-06T11:12:29.335662: step 2739, loss 0.199349, acc 0.875\n",
      "2017-11-06T11:12:33.465596: step 2740, loss 0.168793, acc 0.90625\n",
      "2017-11-06T11:12:37.701606: step 2741, loss 0.191988, acc 0.90625\n",
      "2017-11-06T11:12:41.789511: step 2742, loss 0.348006, acc 0.90625\n",
      "2017-11-06T11:12:45.853398: step 2743, loss 0.093493, acc 0.9375\n",
      "2017-11-06T11:12:49.950309: step 2744, loss 0.121252, acc 0.90625\n",
      "2017-11-06T11:12:54.052223: step 2745, loss 0.0767526, acc 0.96875\n",
      "2017-11-06T11:12:58.204174: step 2746, loss 0.23048, acc 0.875\n",
      "2017-11-06T11:13:02.374139: step 2747, loss 0.375709, acc 0.75\n",
      "2017-11-06T11:13:06.882341: step 2748, loss 0.172321, acc 0.90625\n",
      "2017-11-06T11:13:11.783644: step 2749, loss 0.136069, acc 0.96875\n",
      "2017-11-06T11:13:15.837525: step 2750, loss 0.116644, acc 0.9375\n",
      "2017-11-06T11:13:19.877395: step 2751, loss 0.127712, acc 0.90625\n",
      "2017-11-06T11:13:24.017338: step 2752, loss 0.306095, acc 0.875\n",
      "2017-11-06T11:13:28.067215: step 2753, loss 0.327875, acc 0.875\n",
      "2017-11-06T11:13:32.125098: step 2754, loss 0.160249, acc 0.90625\n",
      "2017-11-06T11:13:36.188987: step 2755, loss 0.145549, acc 0.9375\n",
      "2017-11-06T11:13:40.304909: step 2756, loss 0.300164, acc 0.90625\n",
      "2017-11-06T11:13:44.349784: step 2757, loss 0.176163, acc 0.90625\n",
      "2017-11-06T11:13:48.401664: step 2758, loss 0.190476, acc 0.90625\n",
      "2017-11-06T11:13:52.397503: step 2759, loss 0.173202, acc 0.9375\n",
      "2017-11-06T11:13:56.509423: step 2760, loss 0.0857915, acc 0.96875\n",
      "2017-11-06T11:14:00.548295: step 2761, loss 0.21441, acc 0.84375\n",
      "2017-11-06T11:14:04.584162: step 2762, loss 0.248714, acc 0.875\n",
      "2017-11-06T11:14:08.800157: step 2763, loss 0.077804, acc 0.96875\n",
      "2017-11-06T11:14:13.249319: step 2764, loss 0.121648, acc 0.9375\n",
      "2017-11-06T11:14:17.297194: step 2765, loss 0.209241, acc 0.875\n",
      "2017-11-06T11:14:21.335064: step 2766, loss 0.244543, acc 0.90625\n",
      "2017-11-06T11:14:25.443984: step 2767, loss 0.289546, acc 0.90625\n",
      "2017-11-06T11:14:29.494861: step 2768, loss 0.274984, acc 0.84375\n",
      "2017-11-06T11:14:33.598777: step 2769, loss 0.0664131, acc 0.96875\n",
      "2017-11-06T11:14:37.837789: step 2770, loss 0.251779, acc 0.875\n",
      "2017-11-06T11:14:41.968726: step 2771, loss 0.235888, acc 0.875\n",
      "2017-11-06T11:14:44.544554: step 2772, loss 0.330533, acc 0.85\n",
      "2017-11-06T11:14:48.644469: step 2773, loss 0.10041, acc 0.9375\n",
      "2017-11-06T11:14:52.739378: step 2774, loss 0.225842, acc 0.875\n",
      "2017-11-06T11:14:56.713201: step 2775, loss 0.224727, acc 0.84375\n",
      "2017-11-06T11:15:00.720048: step 2776, loss 0.167935, acc 0.9375\n",
      "2017-11-06T11:15:04.863993: step 2777, loss 0.111722, acc 0.9375\n",
      "2017-11-06T11:15:08.914871: step 2778, loss 0.206067, acc 0.875\n",
      "2017-11-06T11:15:13.336463: step 2779, loss 0.207807, acc 0.90625\n",
      "2017-11-06T11:15:17.753601: step 2780, loss 0.196538, acc 0.90625\n",
      "2017-11-06T11:15:21.879533: step 2781, loss 0.253579, acc 0.875\n",
      "2017-11-06T11:15:25.930411: step 2782, loss 0.288878, acc 0.84375\n",
      "2017-11-06T11:15:29.939259: step 2783, loss 0.156246, acc 0.96875\n",
      "2017-11-06T11:15:33.990138: step 2784, loss 0.0867146, acc 0.96875\n",
      "2017-11-06T11:15:38.014998: step 2785, loss 0.119474, acc 0.9375\n",
      "2017-11-06T11:15:42.132924: step 2786, loss 0.118164, acc 0.96875\n",
      "2017-11-06T11:15:46.174798: step 2787, loss 0.377718, acc 0.875\n",
      "2017-11-06T11:15:50.227676: step 2788, loss 0.10637, acc 0.9375\n",
      "2017-11-06T11:15:54.410648: step 2789, loss 0.193461, acc 0.875\n",
      "2017-11-06T11:15:58.473535: step 2790, loss 0.0507882, acc 0.96875\n",
      "2017-11-06T11:16:02.561439: step 2791, loss 0.166585, acc 0.90625\n",
      "2017-11-06T11:16:06.628329: step 2792, loss 0.171631, acc 0.875\n",
      "2017-11-06T11:16:10.755262: step 2793, loss 0.157069, acc 0.9375\n",
      "2017-11-06T11:16:14.819925: step 2794, loss 0.261402, acc 0.90625\n",
      "2017-11-06T11:16:18.935848: step 2795, loss 0.160756, acc 0.90625\n",
      "2017-11-06T11:16:23.496088: step 2796, loss 0.187917, acc 0.90625\n",
      "2017-11-06T11:16:27.546967: step 2797, loss 0.117767, acc 0.90625\n",
      "2017-11-06T11:16:31.677901: step 2798, loss 0.38113, acc 0.84375\n",
      "2017-11-06T11:16:35.953939: step 2799, loss 0.119704, acc 0.96875\n",
      "2017-11-06T11:16:40.036840: step 2800, loss 0.261434, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:16:42.761777: step 2800, loss 0.900137, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-06T11:16:50.859004: step 2801, loss 0.221859, acc 0.90625\n",
      "2017-11-06T11:16:54.909883: step 2802, loss 0.185275, acc 0.9375\n",
      "2017-11-06T11:16:58.978773: step 2803, loss 0.217065, acc 0.875\n",
      "2017-11-06T11:17:03.014643: step 2804, loss 0.255016, acc 0.875\n",
      "2017-11-06T11:17:07.089537: step 2805, loss 0.231461, acc 0.875\n",
      "2017-11-06T11:17:11.139414: step 2806, loss 0.135926, acc 0.90625\n",
      "2017-11-06T11:17:16.114600: step 2807, loss 0.197298, acc 0.90625\n",
      "2017-11-06T11:17:18.660409: step 2808, loss 0.367268, acc 0.8\n",
      "2017-11-06T11:17:22.703283: step 2809, loss 0.248937, acc 0.90625\n",
      "2017-11-06T11:17:27.016347: step 2810, loss 0.197067, acc 0.90625\n",
      "2017-11-06T11:17:31.320405: step 2811, loss 0.156941, acc 0.875\n",
      "2017-11-06T11:17:35.389297: step 2812, loss 0.0740916, acc 0.96875\n",
      "2017-11-06T11:17:39.480203: step 2813, loss 0.082398, acc 0.96875\n",
      "2017-11-06T11:17:43.498059: step 2814, loss 0.1136, acc 0.9375\n",
      "2017-11-06T11:17:47.578958: step 2815, loss 0.252843, acc 0.84375\n",
      "2017-11-06T11:17:51.610822: step 2816, loss 0.211594, acc 0.875\n",
      "2017-11-06T11:17:55.661700: step 2817, loss 0.213761, acc 0.875\n",
      "2017-11-06T11:17:59.772621: step 2818, loss 0.0803323, acc 1\n",
      "2017-11-06T11:18:03.802485: step 2819, loss 0.111398, acc 0.90625\n",
      "2017-11-06T11:18:07.899398: step 2820, loss 0.379417, acc 0.8125\n",
      "2017-11-06T11:18:11.943270: step 2821, loss 0.301128, acc 0.84375\n",
      "2017-11-06T11:18:15.991145: step 2822, loss 0.0982507, acc 0.9375\n",
      "2017-11-06T11:18:20.084053: step 2823, loss 0.0569189, acc 0.96875\n",
      "2017-11-06T11:18:24.141937: step 2824, loss 0.333212, acc 0.875\n",
      "2017-11-06T11:18:28.215832: step 2825, loss 0.152388, acc 0.9375\n",
      "2017-11-06T11:18:32.652984: step 2826, loss 0.193859, acc 0.90625\n",
      "2017-11-06T11:18:37.084133: step 2827, loss 0.320868, acc 0.90625\n",
      "2017-11-06T11:18:41.156028: step 2828, loss 0.492299, acc 0.84375\n",
      "2017-11-06T11:18:45.244931: step 2829, loss 0.0922009, acc 0.9375\n",
      "2017-11-06T11:18:49.311821: step 2830, loss 0.161516, acc 0.9375\n",
      "2017-11-06T11:18:53.444757: step 2831, loss 0.158023, acc 0.9375\n",
      "2017-11-06T11:18:57.530662: step 2832, loss 0.113982, acc 0.9375\n",
      "2017-11-06T11:19:01.629573: step 2833, loss 0.120664, acc 0.90625\n",
      "2017-11-06T11:19:05.775520: step 2834, loss 0.132107, acc 0.9375\n",
      "2017-11-06T11:19:09.915461: step 2835, loss 0.330944, acc 0.78125\n",
      "2017-11-06T11:19:14.045150: step 2836, loss 0.0988042, acc 0.9375\n",
      "2017-11-06T11:19:18.303740: step 2837, loss 0.216238, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T11:19:22.420666: step 2838, loss 0.0254854, acc 1\n",
      "2017-11-06T11:19:26.910856: step 2839, loss 0.194958, acc 0.90625\n",
      "2017-11-06T11:19:31.100833: step 2840, loss 0.243632, acc 0.9375\n",
      "2017-11-06T11:19:35.222762: step 2841, loss 0.299585, acc 0.84375\n",
      "2017-11-06T11:19:39.772995: step 2842, loss 0.150508, acc 0.90625\n",
      "2017-11-06T11:19:43.772837: step 2843, loss 0.175058, acc 0.90625\n",
      "2017-11-06T11:19:46.317645: step 2844, loss 0.530107, acc 0.75\n",
      "2017-11-06T11:19:50.405550: step 2845, loss 0.139777, acc 0.96875\n",
      "2017-11-06T11:19:54.376372: step 2846, loss 0.155496, acc 0.875\n",
      "2017-11-06T11:19:58.452268: step 2847, loss 0.193428, acc 0.90625\n",
      "2017-11-06T11:20:02.699286: step 2848, loss 0.236452, acc 0.875\n",
      "2017-11-06T11:20:06.741157: step 2849, loss 0.445638, acc 0.84375\n",
      "2017-11-06T11:20:10.723987: step 2850, loss 0.11354, acc 0.9375\n",
      "2017-11-06T11:20:14.722829: step 2851, loss 0.179926, acc 0.9375\n",
      "2017-11-06T11:20:18.712665: step 2852, loss 0.10938, acc 0.96875\n",
      "2017-11-06T11:20:22.648460: step 2853, loss 0.196725, acc 0.875\n",
      "2017-11-06T11:20:26.689351: step 2854, loss 0.108086, acc 0.96875\n",
      "2017-11-06T11:20:30.672161: step 2855, loss 0.0864347, acc 0.96875\n",
      "2017-11-06T11:20:34.967213: step 2856, loss 0.20063, acc 0.875\n",
      "2017-11-06T11:20:39.004082: step 2857, loss 0.151031, acc 0.875\n",
      "2017-11-06T11:20:43.322150: step 2858, loss 0.215882, acc 0.8125\n",
      "2017-11-06T11:20:47.628209: step 2859, loss 0.112857, acc 0.9375\n",
      "2017-11-06T11:20:51.612040: step 2860, loss 0.163887, acc 0.9375\n",
      "2017-11-06T11:20:55.612883: step 2861, loss 0.120074, acc 0.9375\n",
      "2017-11-06T11:21:00.259884: step 2862, loss 0.299008, acc 0.84375\n",
      "2017-11-06T11:21:04.298755: step 2863, loss 0.383002, acc 0.84375\n",
      "2017-11-06T11:21:08.259568: step 2864, loss 0.12689, acc 0.90625\n",
      "2017-11-06T11:21:12.257410: step 2865, loss 0.215666, acc 0.90625\n",
      "2017-11-06T11:21:16.242239: step 2866, loss 0.123746, acc 0.9375\n",
      "2017-11-06T11:21:20.474187: step 2867, loss 0.0716177, acc 0.96875\n",
      "2017-11-06T11:21:24.459019: step 2868, loss 0.212621, acc 0.875\n",
      "2017-11-06T11:21:28.500891: step 2869, loss 0.176228, acc 0.90625\n",
      "2017-11-06T11:21:32.464707: step 2870, loss 0.173302, acc 0.9375\n",
      "2017-11-06T11:21:36.454542: step 2871, loss 0.330977, acc 0.875\n",
      "2017-11-06T11:21:40.490412: step 2872, loss 0.208869, acc 0.875\n",
      "2017-11-06T11:21:44.488250: step 2873, loss 0.109498, acc 0.9375\n",
      "2017-11-06T11:21:48.660215: step 2874, loss 0.057462, acc 0.96875\n",
      "2017-11-06T11:21:52.965274: step 2875, loss 0.199657, acc 0.875\n",
      "2017-11-06T11:21:56.996137: step 2876, loss 0.0559554, acc 1\n",
      "2017-11-06T11:22:01.055022: step 2877, loss 0.178155, acc 0.90625\n",
      "2017-11-06T11:22:05.065872: step 2878, loss 0.249922, acc 0.9375\n",
      "2017-11-06T11:22:09.047701: step 2879, loss 0.31802, acc 0.875\n",
      "2017-11-06T11:22:11.658556: step 2880, loss 0.11819, acc 0.95\n",
      "2017-11-06T11:22:15.661200: step 2881, loss 0.131557, acc 0.9375\n",
      "2017-11-06T11:22:19.661043: step 2882, loss 0.10581, acc 0.9375\n",
      "2017-11-06T11:22:23.679898: step 2883, loss 0.237946, acc 0.875\n",
      "2017-11-06T11:22:27.650720: step 2884, loss 0.234893, acc 0.84375\n",
      "2017-11-06T11:22:31.648560: step 2885, loss 0.07104, acc 0.96875\n",
      "2017-11-06T11:22:36.285547: step 2886, loss 0.0670621, acc 1\n",
      "2017-11-06T11:22:40.293395: step 2887, loss 0.122089, acc 0.9375\n",
      "2017-11-06T11:22:44.313251: step 2888, loss 0.139048, acc 0.96875\n",
      "2017-11-06T11:22:48.332107: step 2889, loss 0.125684, acc 0.9375\n",
      "2017-11-06T11:22:52.345959: step 2890, loss 0.217648, acc 0.84375\n",
      "2017-11-06T11:22:56.876178: step 2891, loss 0.279073, acc 0.8125\n",
      "2017-11-06T11:23:01.056148: step 2892, loss 0.428308, acc 0.78125\n",
      "2017-11-06T11:23:05.033974: step 2893, loss 0.203461, acc 0.875\n",
      "2017-11-06T11:23:09.140893: step 2894, loss 0.112288, acc 0.9375\n",
      "2017-11-06T11:23:13.154745: step 2895, loss 0.0624092, acc 0.9375\n",
      "2017-11-06T11:23:17.202621: step 2896, loss 0.201418, acc 0.9375\n",
      "2017-11-06T11:23:22.232600: step 2897, loss 0.0922627, acc 0.96875\n",
      "2017-11-06T11:23:26.375544: step 2898, loss 0.0809069, acc 0.96875\n",
      "2017-11-06T11:23:30.338361: step 2899, loss 0.159296, acc 0.90625\n",
      "2017-11-06T11:23:34.336200: step 2900, loss 0.165963, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:23:36.989085: step 2900, loss 1.08915, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-06T11:23:45.311345: step 2901, loss 0.325593, acc 0.90625\n",
      "2017-11-06T11:23:49.317192: step 2902, loss 0.356958, acc 0.84375\n",
      "2017-11-06T11:23:53.428113: step 2903, loss 0.322422, acc 0.875\n",
      "2017-11-06T11:23:57.406940: step 2904, loss 0.134039, acc 0.90625\n",
      "2017-11-06T11:24:01.794057: step 2905, loss 0.243568, acc 0.90625\n",
      "2017-11-06T11:24:05.944006: step 2906, loss 0.145294, acc 0.875\n",
      "2017-11-06T11:24:09.962862: step 2907, loss 0.251829, acc 0.875\n",
      "2017-11-06T11:24:13.984719: step 2908, loss 0.193782, acc 0.90625\n",
      "2017-11-06T11:24:17.996569: step 2909, loss 0.329773, acc 0.78125\n",
      "2017-11-06T11:24:22.099485: step 2910, loss 0.172494, acc 0.9375\n",
      "2017-11-06T11:24:26.116339: step 2911, loss 0.415155, acc 0.78125\n",
      "2017-11-06T11:24:30.195238: step 2912, loss 0.365764, acc 0.8125\n",
      "2017-11-06T11:24:34.277137: step 2913, loss 0.197873, acc 0.84375\n",
      "2017-11-06T11:24:38.477122: step 2914, loss 0.287342, acc 0.90625\n",
      "2017-11-06T11:24:42.496979: step 2915, loss 0.244146, acc 0.9375\n",
      "2017-11-06T11:24:45.089820: step 2916, loss 0.380145, acc 0.85\n",
      "2017-11-06T11:24:49.205745: step 2917, loss 0.202259, acc 0.90625\n",
      "2017-11-06T11:24:53.226602: step 2918, loss 0.195055, acc 0.9375\n",
      "2017-11-06T11:24:57.274478: step 2919, loss 0.201129, acc 0.90625\n",
      "2017-11-06T11:25:01.346371: step 2920, loss 0.123734, acc 0.9375\n",
      "2017-11-06T11:25:05.593389: step 2921, loss 0.104264, acc 0.96875\n",
      "2017-11-06T11:25:10.016532: step 2922, loss 0.117958, acc 0.9375\n",
      "2017-11-06T11:25:14.049164: step 2923, loss 0.109509, acc 0.9375\n",
      "2017-11-06T11:25:18.099041: step 2924, loss 0.149246, acc 0.90625\n",
      "2017-11-06T11:25:22.477152: step 2925, loss 0.240089, acc 0.875\n",
      "2017-11-06T11:25:26.538037: step 2926, loss 0.0897901, acc 0.96875\n",
      "2017-11-06T11:25:30.568902: step 2927, loss 0.144324, acc 0.9375\n",
      "2017-11-06T11:25:34.647802: step 2928, loss 0.368722, acc 0.84375\n",
      "2017-11-06T11:25:38.750716: step 2929, loss 0.348615, acc 0.78125\n",
      "2017-11-06T11:25:42.803595: step 2930, loss 0.259417, acc 0.84375\n",
      "2017-11-06T11:25:46.879492: step 2931, loss 0.331237, acc 0.84375\n",
      "2017-11-06T11:25:50.885337: step 2932, loss 0.398077, acc 0.84375\n",
      "2017-11-06T11:25:54.976244: step 2933, loss 0.439638, acc 0.8125\n",
      "2017-11-06T11:25:59.147208: step 2934, loss 0.18758, acc 0.9375\n",
      "2017-11-06T11:26:03.274142: step 2935, loss 0.0180243, acc 1\n",
      "2017-11-06T11:26:07.393067: step 2936, loss 0.091157, acc 0.96875\n",
      "2017-11-06T11:26:11.682115: step 2937, loss 0.0221311, acc 1\n",
      "2017-11-06T11:26:16.030204: step 2938, loss 0.128204, acc 0.9375\n",
      "2017-11-06T11:26:20.163140: step 2939, loss 0.147461, acc 0.90625\n",
      "2017-11-06T11:26:24.188000: step 2940, loss 0.137694, acc 0.9375\n",
      "2017-11-06T11:26:28.228872: step 2941, loss 0.176204, acc 0.90625\n",
      "2017-11-06T11:26:32.322781: step 2942, loss 0.408865, acc 0.78125\n",
      "2017-11-06T11:26:36.579805: step 2943, loss 0.430319, acc 0.8125\n",
      "2017-11-06T11:26:40.612670: step 2944, loss 0.296733, acc 0.875\n",
      "2017-11-06T11:26:44.680561: step 2945, loss 0.214452, acc 0.875\n",
      "2017-11-06T11:26:48.746450: step 2946, loss 0.299738, acc 0.8125\n",
      "2017-11-06T11:26:52.819344: step 2947, loss 0.317434, acc 0.84375\n",
      "2017-11-06T11:26:56.973297: step 2948, loss 0.114371, acc 0.96875\n",
      "2017-11-06T11:27:01.017169: step 2949, loss 0.169696, acc 0.9375\n",
      "2017-11-06T11:27:05.075053: step 2950, loss 0.207922, acc 0.875\n",
      "2017-11-06T11:27:09.135939: step 2951, loss 0.0483371, acc 1\n",
      "2017-11-06T11:27:11.741790: step 2952, loss 0.0924131, acc 0.95\n",
      "2017-11-06T11:27:15.903746: step 2953, loss 0.23268, acc 0.84375\n",
      "2017-11-06T11:27:20.328891: step 2954, loss 0.169219, acc 0.875\n",
      "2017-11-06T11:27:25.148439: step 2955, loss 0.0279464, acc 1\n",
      "2017-11-06T11:27:29.220332: step 2956, loss 0.126261, acc 0.96875\n",
      "2017-11-06T11:27:33.218173: step 2957, loss 0.136971, acc 0.96875\n",
      "2017-11-06T11:27:37.328093: step 2958, loss 0.371099, acc 0.8125\n",
      "2017-11-06T11:27:41.362961: step 2959, loss 0.460971, acc 0.84375\n",
      "2017-11-06T11:27:45.378813: step 2960, loss 0.169378, acc 0.9375\n",
      "2017-11-06T11:27:49.443701: step 2961, loss 0.10535, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T11:27:53.468561: step 2962, loss 0.0814785, acc 0.9375\n",
      "2017-11-06T11:27:57.609503: step 2963, loss 0.110436, acc 0.9375\n",
      "2017-11-06T11:28:01.602341: step 2964, loss 0.277573, acc 0.875\n",
      "2017-11-06T11:28:05.616192: step 2965, loss 0.151163, acc 0.90625\n",
      "2017-11-06T11:28:09.637049: step 2966, loss 0.0325929, acc 1\n",
      "2017-11-06T11:28:13.691706: step 2967, loss 0.0441559, acc 0.96875\n",
      "2017-11-06T11:28:17.708560: step 2968, loss 0.183427, acc 0.9375\n",
      "2017-11-06T11:28:21.817480: step 2969, loss 0.0674942, acc 0.96875\n",
      "2017-11-06T11:28:26.467785: step 2970, loss 0.303973, acc 0.90625\n",
      "2017-11-06T11:28:30.694788: step 2971, loss 0.346655, acc 0.84375\n",
      "2017-11-06T11:28:34.939804: step 2972, loss 0.104656, acc 0.9375\n",
      "2017-11-06T11:28:39.066737: step 2973, loss 0.177635, acc 0.875\n",
      "2017-11-06T11:28:43.124620: step 2974, loss 0.131934, acc 0.9375\n",
      "2017-11-06T11:28:47.194511: step 2975, loss 0.0886558, acc 0.96875\n",
      "2017-11-06T11:28:51.228379: step 2976, loss 0.198554, acc 0.875\n",
      "2017-11-06T11:28:55.336318: step 2977, loss 0.245582, acc 0.875\n",
      "2017-11-06T11:28:59.416196: step 2978, loss 0.138218, acc 0.90625\n",
      "2017-11-06T11:29:03.533123: step 2979, loss 0.194036, acc 0.875\n",
      "2017-11-06T11:29:07.573992: step 2980, loss 0.172385, acc 0.90625\n",
      "2017-11-06T11:29:11.599853: step 2981, loss 0.343722, acc 0.78125\n",
      "2017-11-06T11:29:15.711775: step 2982, loss 0.23426, acc 0.875\n",
      "2017-11-06T11:29:19.738636: step 2983, loss 0.359101, acc 0.8125\n",
      "2017-11-06T11:29:23.824538: step 2984, loss 0.135158, acc 0.9375\n",
      "2017-11-06T11:29:28.221153: step 2985, loss 0.128411, acc 0.90625\n",
      "2017-11-06T11:29:32.779392: step 2986, loss 0.223373, acc 0.875\n",
      "2017-11-06T11:29:36.909329: step 2987, loss 0.366134, acc 0.84375\n",
      "2017-11-06T11:29:39.584227: step 2988, loss 0.335814, acc 0.85\n",
      "2017-11-06T11:29:43.693146: step 2989, loss 0.142368, acc 0.90625\n",
      "2017-11-06T11:29:47.739021: step 2990, loss 0.168783, acc 0.9375\n",
      "2017-11-06T11:29:51.817919: step 2991, loss 0.156925, acc 0.90625\n",
      "2017-11-06T11:29:55.843780: step 2992, loss 0.0431859, acc 1\n",
      "2017-11-06T11:29:59.973714: step 2993, loss 0.128262, acc 0.9375\n",
      "2017-11-06T11:30:04.330811: step 2994, loss 0.118593, acc 0.9375\n",
      "2017-11-06T11:30:08.364677: step 2995, loss 0.123688, acc 0.90625\n",
      "2017-11-06T11:30:12.399544: step 2996, loss 0.298642, acc 0.90625\n",
      "2017-11-06T11:30:16.471437: step 2997, loss 0.0757888, acc 0.9375\n",
      "2017-11-06T11:30:20.523316: step 2998, loss 0.126864, acc 0.9375\n",
      "2017-11-06T11:30:24.582200: step 2999, loss 0.281672, acc 0.84375\n",
      "2017-11-06T11:30:28.781183: step 3000, loss 0.201381, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:30:31.437071: step 3000, loss 1.03439, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-06T11:30:38.068170: step 3001, loss 0.109188, acc 0.9375\n",
      "2017-11-06T11:30:42.083024: step 3002, loss 0.241296, acc 0.875\n",
      "2017-11-06T11:30:46.109887: step 3003, loss 0.333571, acc 0.8125\n",
      "2017-11-06T11:30:50.137746: step 3004, loss 0.197147, acc 0.875\n",
      "2017-11-06T11:30:54.153600: step 3005, loss 0.0769457, acc 1\n",
      "2017-11-06T11:30:58.238502: step 3006, loss 0.352681, acc 0.78125\n",
      "2017-11-06T11:31:02.313397: step 3007, loss 0.197597, acc 0.90625\n",
      "2017-11-06T11:31:06.366277: step 3008, loss 0.359537, acc 0.8125\n",
      "2017-11-06T11:31:10.479201: step 3009, loss 0.235347, acc 0.90625\n",
      "2017-11-06T11:31:14.657978: step 3010, loss 0.285752, acc 0.8125\n",
      "2017-11-06T11:31:18.678834: step 3011, loss 0.134204, acc 0.96875\n",
      "2017-11-06T11:31:22.817775: step 3012, loss 0.199556, acc 0.875\n",
      "2017-11-06T11:31:26.875659: step 3013, loss 0.145336, acc 0.90625\n",
      "2017-11-06T11:31:30.917565: step 3014, loss 0.571987, acc 0.78125\n",
      "2017-11-06T11:31:35.026485: step 3015, loss 0.401917, acc 0.8125\n",
      "2017-11-06T11:31:39.375575: step 3016, loss 0.260077, acc 0.875\n",
      "2017-11-06T11:31:43.878775: step 3017, loss 0.324536, acc 0.90625\n",
      "2017-11-06T11:31:47.980689: step 3018, loss 0.0176672, acc 1\n",
      "2017-11-06T11:31:51.990540: step 3019, loss 0.126525, acc 0.9375\n",
      "2017-11-06T11:31:56.069437: step 3020, loss 0.199612, acc 0.90625\n",
      "2017-11-06T11:32:00.102302: step 3021, loss 0.0651661, acc 0.96875\n",
      "2017-11-06T11:32:04.206218: step 3022, loss 0.298457, acc 0.90625\n",
      "2017-11-06T11:32:08.365173: step 3023, loss 0.377492, acc 0.78125\n",
      "2017-11-06T11:32:10.926994: step 3024, loss 0.0950309, acc 0.95\n",
      "2017-11-06T11:32:14.997888: step 3025, loss 0.219825, acc 0.9375\n",
      "2017-11-06T11:32:19.103804: step 3026, loss 0.126798, acc 0.9375\n",
      "2017-11-06T11:32:23.173695: step 3027, loss 0.212824, acc 0.90625\n",
      "2017-11-06T11:32:27.225576: step 3028, loss 0.0649945, acc 0.96875\n",
      "2017-11-06T11:32:31.206403: step 3029, loss 0.709618, acc 0.75\n",
      "2017-11-06T11:32:35.438411: step 3030, loss 0.177331, acc 0.96875\n",
      "2017-11-06T11:32:39.453264: step 3031, loss 0.372, acc 0.84375\n",
      "2017-11-06T11:32:43.414078: step 3032, loss 0.276677, acc 0.875\n",
      "2017-11-06T11:32:47.918277: step 3033, loss 0.150086, acc 0.9375\n",
      "2017-11-06T11:32:51.947140: step 3034, loss 0.12281, acc 0.90625\n",
      "2017-11-06T11:32:55.958991: step 3035, loss 0.0802163, acc 0.96875\n",
      "2017-11-06T11:33:00.005866: step 3036, loss 0.157686, acc 0.90625\n",
      "2017-11-06T11:33:04.109782: step 3037, loss 0.162127, acc 0.90625\n",
      "2017-11-06T11:33:08.105621: step 3038, loss 0.175459, acc 0.90625\n",
      "2017-11-06T11:33:12.153498: step 3039, loss 0.184112, acc 0.90625\n",
      "2017-11-06T11:33:16.350480: step 3040, loss 0.325021, acc 0.78125\n",
      "2017-11-06T11:33:20.366333: step 3041, loss 0.249564, acc 0.90625\n",
      "2017-11-06T11:33:24.404202: step 3042, loss 0.240693, acc 0.90625\n",
      "2017-11-06T11:33:28.374023: step 3043, loss 0.143979, acc 0.9375\n",
      "2017-11-06T11:33:32.732119: step 3044, loss 0.145299, acc 0.9375\n",
      "2017-11-06T11:33:36.807017: step 3045, loss 0.0947669, acc 0.9375\n",
      "2017-11-06T11:33:40.914933: step 3046, loss 0.230521, acc 0.9375\n",
      "2017-11-06T11:33:45.072889: step 3047, loss 0.478523, acc 0.875\n",
      "2017-11-06T11:33:49.140779: step 3048, loss 0.180263, acc 0.9375\n",
      "2017-11-06T11:33:53.629970: step 3049, loss 0.1709, acc 0.9375\n",
      "2017-11-06T11:33:57.835957: step 3050, loss 0.202805, acc 0.9375\n",
      "2017-11-06T11:34:01.981903: step 3051, loss 0.215467, acc 0.90625\n",
      "2017-11-06T11:34:06.193895: step 3052, loss 0.306244, acc 0.875\n",
      "2017-11-06T11:34:10.210750: step 3053, loss 0.263399, acc 0.90625\n",
      "2017-11-06T11:34:14.284421: step 3054, loss 0.37387, acc 0.78125\n",
      "2017-11-06T11:34:18.373327: step 3055, loss 0.259295, acc 0.8125\n",
      "2017-11-06T11:34:22.590323: step 3056, loss 0.329239, acc 0.8125\n",
      "2017-11-06T11:34:26.898385: step 3057, loss 0.152028, acc 0.9375\n",
      "2017-11-06T11:34:31.038326: step 3058, loss 0.18323, acc 0.90625\n",
      "2017-11-06T11:34:35.348389: step 3059, loss 0.18954, acc 0.875\n",
      "2017-11-06T11:34:38.009279: step 3060, loss 0.133755, acc 0.9\n",
      "2017-11-06T11:34:42.184245: step 3061, loss 0.174108, acc 0.875\n",
      "2017-11-06T11:34:46.264144: step 3062, loss 0.196695, acc 0.875\n",
      "2017-11-06T11:34:50.692291: step 3063, loss 0.221794, acc 0.875\n",
      "2017-11-06T11:34:54.861253: step 3064, loss 0.263851, acc 0.84375\n",
      "2017-11-06T11:34:59.447512: step 3065, loss 0.157832, acc 0.90625\n",
      "2017-11-06T11:35:03.772585: step 3066, loss 0.160039, acc 0.90625\n",
      "2017-11-06T11:35:07.924537: step 3067, loss 0.209823, acc 0.875\n",
      "2017-11-06T11:35:11.888353: step 3068, loss 0.0965826, acc 0.9375\n",
      "2017-11-06T11:35:16.000273: step 3069, loss 0.103115, acc 0.9375\n",
      "2017-11-06T11:35:19.960087: step 3070, loss 0.205646, acc 0.875\n",
      "2017-11-06T11:35:23.988949: step 3071, loss 0.16466, acc 0.96875\n",
      "2017-11-06T11:35:27.970779: step 3072, loss 0.267456, acc 0.875\n",
      "2017-11-06T11:35:32.000642: step 3073, loss 0.0800872, acc 0.96875\n",
      "2017-11-06T11:35:36.197202: step 3074, loss 0.22361, acc 0.875\n",
      "2017-11-06T11:35:40.258089: step 3075, loss 0.330515, acc 0.875\n",
      "2017-11-06T11:35:44.226907: step 3076, loss 0.139074, acc 0.90625\n",
      "2017-11-06T11:35:48.220745: step 3077, loss 0.234802, acc 0.90625\n",
      "2017-11-06T11:35:52.317657: step 3078, loss 0.11102, acc 0.9375\n",
      "2017-11-06T11:35:56.326505: step 3079, loss 0.141736, acc 0.96875\n",
      "2017-11-06T11:36:00.406403: step 3080, loss 0.263554, acc 0.8125\n",
      "2017-11-06T11:36:04.965643: step 3081, loss 0.281055, acc 0.84375\n",
      "2017-11-06T11:36:09.042539: step 3082, loss 0.192044, acc 0.90625\n",
      "2017-11-06T11:36:13.037379: step 3083, loss 0.202244, acc 0.875\n",
      "2017-11-06T11:36:17.166313: step 3084, loss 0.00743304, acc 1\n",
      "2017-11-06T11:36:21.144139: step 3085, loss 0.0700682, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T11:36:25.239048: step 3086, loss 0.320838, acc 0.90625\n",
      "2017-11-06T11:36:29.267911: step 3087, loss 0.386544, acc 0.84375\n",
      "2017-11-06T11:36:33.356816: step 3088, loss 0.132792, acc 0.96875\n",
      "2017-11-06T11:36:37.520775: step 3089, loss 0.435889, acc 0.8125\n",
      "2017-11-06T11:36:41.550638: step 3090, loss 0.199087, acc 0.9375\n",
      "2017-11-06T11:36:45.545477: step 3091, loss 0.178997, acc 0.90625\n",
      "2017-11-06T11:36:49.612368: step 3092, loss 0.185247, acc 0.9375\n",
      "2017-11-06T11:36:53.611207: step 3093, loss 0.184033, acc 0.90625\n",
      "2017-11-06T11:36:57.627061: step 3094, loss 0.402208, acc 0.875\n",
      "2017-11-06T11:37:01.566861: step 3095, loss 0.242114, acc 0.875\n",
      "2017-11-06T11:37:04.146694: step 3096, loss 0.0799367, acc 0.95\n",
      "2017-11-06T11:37:08.450751: step 3097, loss 0.245515, acc 0.875\n",
      "2017-11-06T11:37:12.705775: step 3098, loss 0.162426, acc 0.9375\n",
      "2017-11-06T11:37:16.796472: step 3099, loss 0.227673, acc 0.84375\n",
      "2017-11-06T11:37:20.774295: step 3100, loss 0.0733741, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:37:23.402161: step 3100, loss 1.03829, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-06T11:37:31.134339: step 3101, loss 0.105541, acc 0.96875\n",
      "2017-11-06T11:37:35.174209: step 3102, loss 0.111735, acc 0.9375\n",
      "2017-11-06T11:37:39.326160: step 3103, loss 0.241293, acc 0.875\n",
      "2017-11-06T11:37:43.331005: step 3104, loss 0.193894, acc 0.90625\n",
      "2017-11-06T11:37:47.373879: step 3105, loss 0.118738, acc 0.9375\n",
      "2017-11-06T11:37:51.420753: step 3106, loss 0.170492, acc 0.875\n",
      "2017-11-06T11:37:55.442611: step 3107, loss 0.356657, acc 0.875\n",
      "2017-11-06T11:37:59.417437: step 3108, loss 0.165676, acc 0.9375\n",
      "2017-11-06T11:38:03.460308: step 3109, loss 0.205108, acc 0.90625\n",
      "2017-11-06T11:38:07.523195: step 3110, loss 0.208482, acc 0.875\n",
      "2017-11-06T11:38:11.572072: step 3111, loss 0.237111, acc 0.875\n",
      "2017-11-06T11:38:16.032240: step 3112, loss 0.24607, acc 0.875\n",
      "2017-11-06T11:38:20.101132: step 3113, loss 0.3491, acc 0.875\n",
      "2017-11-06T11:38:24.258086: step 3114, loss 0.218399, acc 0.875\n",
      "2017-11-06T11:38:28.509106: step 3115, loss 0.450714, acc 0.84375\n",
      "2017-11-06T11:38:32.638041: step 3116, loss 0.175688, acc 0.90625\n",
      "2017-11-06T11:38:36.837023: step 3117, loss 0.307075, acc 0.875\n",
      "2017-11-06T11:38:40.887901: step 3118, loss 0.15117, acc 0.90625\n",
      "2017-11-06T11:38:44.923769: step 3119, loss 0.229819, acc 0.9375\n",
      "2017-11-06T11:38:48.965642: step 3120, loss 0.303033, acc 0.84375\n",
      "2017-11-06T11:38:53.061552: step 3121, loss 0.129232, acc 0.96875\n",
      "2017-11-06T11:38:57.059392: step 3122, loss 0.317779, acc 0.90625\n",
      "2017-11-06T11:39:01.089255: step 3123, loss 0.179888, acc 0.90625\n",
      "2017-11-06T11:39:05.156146: step 3124, loss 0.212428, acc 0.875\n",
      "2017-11-06T11:39:09.160991: step 3125, loss 0.24664, acc 0.875\n",
      "2017-11-06T11:39:13.200862: step 3126, loss 0.28872, acc 0.875\n",
      "2017-11-06T11:39:17.271754: step 3127, loss 0.168527, acc 0.875\n",
      "2017-11-06T11:39:21.744932: step 3128, loss 0.213722, acc 0.90625\n",
      "2017-11-06T11:39:25.880871: step 3129, loss 0.202795, acc 0.9375\n",
      "2017-11-06T11:39:29.940755: step 3130, loss 0.250117, acc 0.875\n",
      "2017-11-06T11:39:33.948603: step 3131, loss 0.149408, acc 0.96875\n",
      "2017-11-06T11:39:36.563462: step 3132, loss 0.260905, acc 0.85\n",
      "2017-11-06T11:39:41.200371: step 3133, loss 0.207952, acc 0.84375\n",
      "2017-11-06T11:39:45.250248: step 3134, loss 0.22274, acc 0.84375\n",
      "2017-11-06T11:39:49.296142: step 3135, loss 0.23635, acc 0.90625\n",
      "2017-11-06T11:39:53.302970: step 3136, loss 0.12923, acc 0.9375\n",
      "2017-11-06T11:39:57.314821: step 3137, loss 0.114597, acc 0.9375\n",
      "2017-11-06T11:40:01.619880: step 3138, loss 0.268783, acc 0.90625\n",
      "2017-11-06T11:40:05.777834: step 3139, loss 0.101773, acc 0.9375\n",
      "2017-11-06T11:40:09.816703: step 3140, loss 0.266107, acc 0.875\n",
      "2017-11-06T11:40:13.896603: step 3141, loss 0.179147, acc 0.9375\n",
      "2017-11-06T11:40:17.997271: step 3142, loss 0.094444, acc 0.96875\n",
      "2017-11-06T11:40:22.061158: step 3143, loss 0.104115, acc 0.96875\n",
      "2017-11-06T11:40:26.545345: step 3144, loss 0.00989127, acc 1\n",
      "2017-11-06T11:40:30.737323: step 3145, loss 0.237068, acc 0.90625\n",
      "2017-11-06T11:40:35.025370: step 3146, loss 0.308535, acc 0.875\n",
      "2017-11-06T11:40:39.093261: step 3147, loss 0.344403, acc 0.875\n",
      "2017-11-06T11:40:43.131129: step 3148, loss 0.180937, acc 0.875\n",
      "2017-11-06T11:40:47.185011: step 3149, loss 0.300232, acc 0.90625\n",
      "2017-11-06T11:40:51.167840: step 3150, loss 0.143371, acc 0.9375\n",
      "2017-11-06T11:40:55.174687: step 3151, loss 0.31217, acc 0.84375\n",
      "2017-11-06T11:40:59.226566: step 3152, loss 0.0529591, acc 0.96875\n",
      "2017-11-06T11:41:03.250425: step 3153, loss 0.218063, acc 0.875\n",
      "2017-11-06T11:41:07.367351: step 3154, loss 0.216216, acc 0.90625\n",
      "2017-11-06T11:41:11.914581: step 3155, loss 0.196512, acc 0.875\n",
      "2017-11-06T11:41:16.925143: step 3156, loss 0.177293, acc 0.90625\n",
      "2017-11-06T11:41:20.987028: step 3157, loss 0.17071, acc 0.9375\n",
      "2017-11-06T11:41:25.179006: step 3158, loss 0.17511, acc 0.875\n",
      "2017-11-06T11:41:29.288927: step 3159, loss 0.349417, acc 0.8125\n",
      "2017-11-06T11:41:33.883191: step 3160, loss 0.506493, acc 0.75\n",
      "2017-11-06T11:41:37.895042: step 3161, loss 0.157405, acc 0.90625\n",
      "2017-11-06T11:41:42.350338: step 3162, loss 0.208914, acc 0.90625\n",
      "2017-11-06T11:41:46.393210: step 3163, loss 0.105378, acc 1\n",
      "2017-11-06T11:41:50.489121: step 3164, loss 0.277061, acc 0.875\n",
      "2017-11-06T11:41:54.501971: step 3165, loss 0.334992, acc 0.84375\n",
      "2017-11-06T11:41:58.565858: step 3166, loss 0.153428, acc 0.9375\n",
      "2017-11-06T11:42:02.544685: step 3167, loss 0.240031, acc 0.84375\n",
      "2017-11-06T11:42:05.160545: step 3168, loss 0.128363, acc 0.95\n",
      "2017-11-06T11:42:09.204417: step 3169, loss 0.0430761, acc 1\n",
      "2017-11-06T11:42:13.290321: step 3170, loss 0.162672, acc 0.875\n",
      "2017-11-06T11:42:17.369220: step 3171, loss 0.35038, acc 0.75\n",
      "2017-11-06T11:42:21.444115: step 3172, loss 0.132713, acc 0.96875\n",
      "2017-11-06T11:42:25.572048: step 3173, loss 0.202897, acc 0.9375\n",
      "2017-11-06T11:42:29.616921: step 3174, loss 0.0840178, acc 0.96875\n",
      "2017-11-06T11:42:33.813904: step 3175, loss 0.22566, acc 0.90625\n",
      "2017-11-06T11:42:38.473214: step 3176, loss 0.292395, acc 0.84375\n",
      "2017-11-06T11:42:42.616158: step 3177, loss 0.180543, acc 0.9375\n",
      "2017-11-06T11:42:46.639016: step 3178, loss 0.228523, acc 0.90625\n",
      "2017-11-06T11:42:50.685893: step 3179, loss 0.245069, acc 0.84375\n",
      "2017-11-06T11:42:54.759786: step 3180, loss 0.157362, acc 0.875\n",
      "2017-11-06T11:42:58.813667: step 3181, loss 0.0839806, acc 0.96875\n",
      "2017-11-06T11:43:02.957613: step 3182, loss 0.534486, acc 0.875\n",
      "2017-11-06T11:43:07.002487: step 3183, loss 0.173797, acc 0.9375\n",
      "2017-11-06T11:43:11.058368: step 3184, loss 0.330139, acc 0.90625\n",
      "2017-11-06T11:43:15.133011: step 3185, loss 0.104275, acc 0.9375\n",
      "2017-11-06T11:43:19.247936: step 3186, loss 0.0873826, acc 0.96875\n",
      "2017-11-06T11:43:23.412895: step 3187, loss 0.313631, acc 0.8125\n",
      "2017-11-06T11:43:27.631892: step 3188, loss 0.0856865, acc 0.96875\n",
      "2017-11-06T11:43:31.763829: step 3189, loss 0.129749, acc 0.90625\n",
      "2017-11-06T11:43:35.858738: step 3190, loss 0.0618534, acc 0.96875\n",
      "2017-11-06T11:43:40.019695: step 3191, loss 0.308494, acc 0.90625\n",
      "2017-11-06T11:43:44.667998: step 3192, loss 0.202392, acc 0.96875\n",
      "2017-11-06T11:43:48.792928: step 3193, loss 0.352133, acc 0.90625\n",
      "2017-11-06T11:43:52.925866: step 3194, loss 0.344898, acc 0.90625\n",
      "2017-11-06T11:43:57.041789: step 3195, loss 0.272438, acc 0.875\n",
      "2017-11-06T11:44:01.235770: step 3196, loss 0.457806, acc 0.84375\n",
      "2017-11-06T11:44:05.260629: step 3197, loss 0.0154033, acc 1\n",
      "2017-11-06T11:44:09.338527: step 3198, loss 0.130337, acc 0.9375\n",
      "2017-11-06T11:44:13.369392: step 3199, loss 0.191809, acc 0.9375\n",
      "2017-11-06T11:44:17.479311: step 3200, loss 0.190184, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:44:20.175227: step 3200, loss 0.949343, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-06T11:44:26.389705: step 3201, loss 0.176367, acc 0.875\n",
      "2017-11-06T11:44:30.419569: step 3202, loss 0.175763, acc 0.90625\n",
      "2017-11-06T11:44:34.745642: step 3203, loss 0.277431, acc 0.875\n",
      "2017-11-06T11:44:37.359500: step 3204, loss 0.376147, acc 0.85\n",
      "2017-11-06T11:44:41.437398: step 3205, loss 0.175143, acc 0.90625\n",
      "2017-11-06T11:44:45.571335: step 3206, loss 0.231856, acc 0.90625\n",
      "2017-11-06T11:44:50.005485: step 3207, loss 0.41741, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T11:44:54.128415: step 3208, loss 0.0521076, acc 0.96875\n",
      "2017-11-06T11:44:58.151274: step 3209, loss 0.107073, acc 0.96875\n",
      "2017-11-06T11:45:02.160122: step 3210, loss 0.215239, acc 0.90625\n",
      "2017-11-06T11:45:06.238020: step 3211, loss 0.314857, acc 0.84375\n",
      "2017-11-06T11:45:10.298905: step 3212, loss 0.293851, acc 0.90625\n",
      "2017-11-06T11:45:14.390812: step 3213, loss 0.15058, acc 0.90625\n",
      "2017-11-06T11:45:18.457702: step 3214, loss 0.152791, acc 0.9375\n",
      "2017-11-06T11:45:22.490568: step 3215, loss 0.256144, acc 0.84375\n",
      "2017-11-06T11:45:26.560459: step 3216, loss 0.116758, acc 0.90625\n",
      "2017-11-06T11:45:30.541288: step 3217, loss 0.110427, acc 0.9375\n",
      "2017-11-06T11:45:34.565147: step 3218, loss 0.194533, acc 0.9375\n",
      "2017-11-06T11:45:38.628034: step 3219, loss 0.263952, acc 0.84375\n",
      "2017-11-06T11:45:43.194103: step 3220, loss 0.230812, acc 0.875\n",
      "2017-11-06T11:45:47.540989: step 3221, loss 0.142012, acc 0.96875\n",
      "2017-11-06T11:45:51.919609: step 3222, loss 0.287807, acc 0.875\n",
      "2017-11-06T11:45:56.395422: step 3223, loss 0.0573234, acc 1\n",
      "2017-11-06T11:46:00.446813: step 3224, loss 0.175983, acc 0.875\n",
      "2017-11-06T11:46:04.422639: step 3225, loss 0.294368, acc 0.8125\n",
      "2017-11-06T11:46:08.534560: step 3226, loss 0.206532, acc 0.90625\n",
      "2017-11-06T11:46:12.566425: step 3227, loss 0.216061, acc 0.90625\n",
      "2017-11-06T11:46:16.572111: step 3228, loss 0.315916, acc 0.875\n",
      "2017-11-06T11:46:20.560945: step 3229, loss 0.0924596, acc 0.96875\n",
      "2017-11-06T11:46:24.578800: step 3230, loss 0.196165, acc 0.9375\n",
      "2017-11-06T11:46:28.583645: step 3231, loss 0.38689, acc 0.875\n",
      "2017-11-06T11:46:32.560471: step 3232, loss 0.130823, acc 0.90625\n",
      "2017-11-06T11:46:36.802486: step 3233, loss 0.072114, acc 0.96875\n",
      "2017-11-06T11:46:40.836351: step 3234, loss 0.313385, acc 0.8125\n",
      "2017-11-06T11:46:44.759139: step 3235, loss 0.064165, acc 0.96875\n",
      "2017-11-06T11:46:48.742969: step 3236, loss 0.257432, acc 0.90625\n",
      "2017-11-06T11:46:52.736827: step 3237, loss 0.242955, acc 0.9375\n",
      "2017-11-06T11:46:56.760666: step 3238, loss 0.266468, acc 0.875\n",
      "2017-11-06T11:47:01.312901: step 3239, loss 0.099439, acc 0.96875\n",
      "2017-11-06T11:47:03.955779: step 3240, loss 0.0304043, acc 1\n",
      "2017-11-06T11:47:07.986643: step 3241, loss 0.27589, acc 0.875\n",
      "2017-11-06T11:47:12.033519: step 3242, loss 0.105641, acc 0.9375\n",
      "2017-11-06T11:47:16.061381: step 3243, loss 0.163012, acc 0.90625\n",
      "2017-11-06T11:47:20.063224: step 3244, loss 0.142332, acc 0.9375\n",
      "2017-11-06T11:47:24.197161: step 3245, loss 0.169803, acc 0.90625\n",
      "2017-11-06T11:47:28.144967: step 3246, loss 0.214511, acc 0.875\n",
      "2017-11-06T11:47:32.135802: step 3247, loss 0.166806, acc 0.90625\n",
      "2017-11-06T11:47:36.047582: step 3248, loss 0.0687073, acc 0.9375\n",
      "2017-11-06T11:47:40.053428: step 3249, loss 0.141155, acc 0.9375\n",
      "2017-11-06T11:47:43.963206: step 3250, loss 0.12296, acc 0.9375\n",
      "2017-11-06T11:47:47.959126: step 3251, loss 0.215221, acc 0.90625\n",
      "2017-11-06T11:47:51.926945: step 3252, loss 0.314831, acc 0.84375\n",
      "2017-11-06T11:47:55.891765: step 3253, loss 0.00530542, acc 1\n",
      "2017-11-06T11:47:59.832562: step 3254, loss 0.0933032, acc 0.9375\n",
      "2017-11-06T11:48:03.917465: step 3255, loss 0.128185, acc 0.90625\n",
      "2017-11-06T11:48:08.099436: step 3256, loss 0.166146, acc 0.875\n",
      "2017-11-06T11:48:12.065255: step 3257, loss 0.153358, acc 0.90625\n",
      "2017-11-06T11:48:16.005054: step 3258, loss 0.198854, acc 0.875\n",
      "2017-11-06T11:48:19.990886: step 3259, loss 0.381825, acc 0.78125\n",
      "2017-11-06T11:48:24.131829: step 3260, loss 0.254253, acc 0.875\n",
      "2017-11-06T11:48:28.234744: step 3261, loss 0.128208, acc 0.9375\n",
      "2017-11-06T11:48:32.163535: step 3262, loss 0.425607, acc 0.78125\n",
      "2017-11-06T11:48:36.329495: step 3263, loss 0.0891329, acc 0.96875\n",
      "2017-11-06T11:48:40.320331: step 3264, loss 0.256531, acc 0.90625\n",
      "2017-11-06T11:48:44.304163: step 3265, loss 0.21992, acc 0.90625\n",
      "2017-11-06T11:48:48.282989: step 3266, loss 0.203569, acc 0.90625\n",
      "2017-11-06T11:48:52.247806: step 3267, loss 0.276858, acc 0.875\n",
      "2017-11-06T11:48:56.229635: step 3268, loss 0.165443, acc 0.90625\n",
      "2017-11-06T11:49:00.183444: step 3269, loss 0.190788, acc 0.90625\n",
      "2017-11-06T11:49:04.145279: step 3270, loss 0.350042, acc 0.875\n",
      "2017-11-06T11:49:08.113079: step 3271, loss 0.133638, acc 0.9375\n",
      "2017-11-06T11:49:12.461169: step 3272, loss 0.264862, acc 0.90625\n",
      "2017-11-06T11:49:16.545950: step 3273, loss 0.207166, acc 0.90625\n",
      "2017-11-06T11:49:20.442720: step 3274, loss 0.190877, acc 0.90625\n",
      "2017-11-06T11:49:24.445563: step 3275, loss 0.147936, acc 0.90625\n",
      "2017-11-06T11:49:27.017390: step 3276, loss 0.327451, acc 0.85\n",
      "2017-11-06T11:49:30.947182: step 3277, loss 0.096747, acc 0.9375\n",
      "2017-11-06T11:49:34.956031: step 3278, loss 0.348781, acc 0.875\n",
      "2017-11-06T11:49:38.887824: step 3279, loss 0.0127788, acc 1\n",
      "2017-11-06T11:49:42.848640: step 3280, loss 0.406856, acc 0.875\n",
      "2017-11-06T11:49:46.770425: step 3281, loss 0.274443, acc 0.90625\n",
      "2017-11-06T11:49:50.927267: step 3282, loss 0.402473, acc 0.875\n",
      "2017-11-06T11:49:54.867066: step 3283, loss 0.178064, acc 0.90625\n",
      "2017-11-06T11:49:58.818874: step 3284, loss 0.157063, acc 0.90625\n",
      "2017-11-06T11:50:03.029866: step 3285, loss 0.129989, acc 0.90625\n",
      "2017-11-06T11:50:07.060730: step 3286, loss 0.188973, acc 0.9375\n",
      "2017-11-06T11:50:11.033552: step 3287, loss 0.16067, acc 0.9375\n",
      "2017-11-06T11:50:15.226532: step 3288, loss 0.202143, acc 0.875\n",
      "2017-11-06T11:50:19.566616: step 3289, loss 0.0585474, acc 1\n",
      "2017-11-06T11:50:23.580470: step 3290, loss 0.0254981, acc 1\n",
      "2017-11-06T11:50:27.577307: step 3291, loss 0.233131, acc 0.9375\n",
      "2017-11-06T11:50:31.525113: step 3292, loss 0.219778, acc 0.875\n",
      "2017-11-06T11:50:35.674061: step 3293, loss 0.212516, acc 0.90625\n",
      "2017-11-06T11:50:39.659893: step 3294, loss 0.504072, acc 0.8125\n",
      "2017-11-06T11:50:43.577676: step 3295, loss 0.240658, acc 0.875\n",
      "2017-11-06T11:50:47.546498: step 3296, loss 0.108163, acc 0.9375\n",
      "2017-11-06T11:50:51.533330: step 3297, loss 0.527066, acc 0.84375\n",
      "2017-11-06T11:50:55.521163: step 3298, loss 0.121141, acc 0.90625\n",
      "2017-11-06T11:50:59.450956: step 3299, loss 0.274218, acc 0.8125\n",
      "2017-11-06T11:51:03.419784: step 3300, loss 0.0907837, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:51:06.165727: step 3300, loss 0.96049, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-06T11:51:11.780290: step 3301, loss 0.183244, acc 0.875\n",
      "2017-11-06T11:51:15.829167: step 3302, loss 0.311959, acc 0.84375\n",
      "2017-11-06T11:51:19.864034: step 3303, loss 0.124799, acc 0.9375\n",
      "2017-11-06T11:51:24.209122: step 3304, loss 0.203259, acc 0.90625\n",
      "2017-11-06T11:51:28.145918: step 3305, loss 0.28156, acc 0.84375\n",
      "2017-11-06T11:51:32.128748: step 3306, loss 0.0773492, acc 0.96875\n",
      "2017-11-06T11:51:36.182629: step 3307, loss 0.181398, acc 0.9375\n",
      "2017-11-06T11:51:40.118425: step 3308, loss 0.36247, acc 0.84375\n",
      "2017-11-06T11:51:44.107261: step 3309, loss 0.216331, acc 0.90625\n",
      "2017-11-06T11:51:48.009034: step 3310, loss 0.319304, acc 0.84375\n",
      "2017-11-06T11:51:52.267838: step 3311, loss 0.0771584, acc 0.96875\n",
      "2017-11-06T11:51:54.790631: step 3312, loss 0.0795665, acc 0.95\n",
      "2017-11-06T11:51:58.751444: step 3313, loss 0.119847, acc 0.9375\n",
      "2017-11-06T11:52:02.755289: step 3314, loss 0.192629, acc 0.90625\n",
      "2017-11-06T11:52:06.731115: step 3315, loss 0.166443, acc 0.90625\n",
      "2017-11-06T11:52:10.695933: step 3316, loss 0.0554471, acc 0.96875\n",
      "2017-11-06T11:52:14.649741: step 3317, loss 0.102745, acc 0.96875\n",
      "2017-11-06T11:52:18.692417: step 3318, loss 0.180702, acc 0.9375\n",
      "2017-11-06T11:52:22.627211: step 3319, loss 0.0948801, acc 0.9375\n",
      "2017-11-06T11:52:26.884237: step 3320, loss 0.34546, acc 0.84375\n",
      "2017-11-06T11:52:31.197302: step 3321, loss 0.152301, acc 0.9375\n",
      "2017-11-06T11:52:35.361262: step 3322, loss 0.357574, acc 0.875\n",
      "2017-11-06T11:52:39.403132: step 3323, loss 0.173434, acc 0.90625\n",
      "2017-11-06T11:52:43.363946: step 3324, loss 0.361679, acc 0.75\n",
      "2017-11-06T11:52:47.349779: step 3325, loss 0.208239, acc 0.875\n",
      "2017-11-06T11:52:51.371638: step 3326, loss 0.0912062, acc 0.96875\n",
      "2017-11-06T11:52:55.381485: step 3327, loss 0.183436, acc 0.90625\n",
      "2017-11-06T11:52:59.408346: step 3328, loss 0.135156, acc 0.90625\n",
      "2017-11-06T11:53:03.521269: step 3329, loss 0.0966204, acc 0.96875\n",
      "2017-11-06T11:53:07.574149: step 3330, loss 0.171841, acc 0.90625\n",
      "2017-11-06T11:53:11.503941: step 3331, loss 0.22672, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T11:53:15.548815: step 3332, loss 0.140181, acc 0.90625\n",
      "2017-11-06T11:53:19.498623: step 3333, loss 0.133856, acc 0.9375\n",
      "2017-11-06T11:53:23.605540: step 3334, loss 0.13159, acc 0.90625\n",
      "2017-11-06T11:53:27.621392: step 3335, loss 0.138612, acc 0.96875\n",
      "2017-11-06T11:53:31.710299: step 3336, loss 0.136468, acc 0.90625\n",
      "2017-11-06T11:53:36.088409: step 3337, loss 0.120502, acc 0.96875\n",
      "2017-11-06T11:53:40.069238: step 3338, loss 0.251376, acc 0.84375\n",
      "2017-11-06T11:53:44.078086: step 3339, loss 0.132808, acc 0.96875\n",
      "2017-11-06T11:53:48.096942: step 3340, loss 0.286849, acc 0.84375\n",
      "2017-11-06T11:53:52.211865: step 3341, loss 0.289456, acc 0.84375\n",
      "2017-11-06T11:53:56.212708: step 3342, loss 0.386459, acc 0.84375\n",
      "2017-11-06T11:54:00.212550: step 3343, loss 0.363969, acc 0.78125\n",
      "2017-11-06T11:54:04.344486: step 3344, loss 0.206336, acc 0.9375\n",
      "2017-11-06T11:54:08.411376: step 3345, loss 0.368225, acc 0.8125\n",
      "2017-11-06T11:54:12.540310: step 3346, loss 0.215611, acc 0.875\n",
      "2017-11-06T11:54:16.575177: step 3347, loss 0.154719, acc 0.96875\n",
      "2017-11-06T11:54:19.161014: step 3348, loss 0.386054, acc 0.9\n",
      "2017-11-06T11:54:23.199884: step 3349, loss 0.161391, acc 0.9375\n",
      "2017-11-06T11:54:27.198726: step 3350, loss 0.122793, acc 0.9375\n",
      "2017-11-06T11:54:31.195565: step 3351, loss 0.0859304, acc 0.96875\n",
      "2017-11-06T11:54:35.470603: step 3352, loss 0.244017, acc 0.875\n",
      "2017-11-06T11:54:39.872730: step 3353, loss 0.245008, acc 0.84375\n",
      "2017-11-06T11:54:44.004667: step 3354, loss 0.312581, acc 0.875\n",
      "2017-11-06T11:54:47.995501: step 3355, loss 0.0802965, acc 0.96875\n",
      "2017-11-06T11:54:52.000348: step 3356, loss 0.062816, acc 0.96875\n",
      "2017-11-06T11:54:55.992185: step 3357, loss 0.222281, acc 0.875\n",
      "2017-11-06T11:55:00.008037: step 3358, loss 0.176227, acc 0.90625\n",
      "2017-11-06T11:55:03.985867: step 3359, loss 0.0956279, acc 0.9375\n",
      "2017-11-06T11:55:08.049751: step 3360, loss 0.0342059, acc 1\n",
      "2017-11-06T11:55:12.075612: step 3361, loss 0.0933417, acc 0.96875\n",
      "2017-11-06T11:55:16.080458: step 3362, loss 0.537204, acc 0.78125\n",
      "2017-11-06T11:55:19.988978: step 3363, loss 0.209868, acc 0.9375\n",
      "2017-11-06T11:55:24.026847: step 3364, loss 0.0771743, acc 0.9375\n",
      "2017-11-06T11:55:28.083729: step 3365, loss 0.101878, acc 0.96875\n",
      "2017-11-06T11:55:32.133607: step 3366, loss 0.170977, acc 0.90625\n",
      "2017-11-06T11:55:36.243527: step 3367, loss 0.179558, acc 0.9375\n",
      "2017-11-06T11:55:40.330431: step 3368, loss 0.0704972, acc 0.96875\n",
      "2017-11-06T11:55:44.543425: step 3369, loss 0.332115, acc 0.84375\n",
      "2017-11-06T11:55:48.830471: step 3370, loss 0.357717, acc 0.84375\n",
      "2017-11-06T11:55:52.875744: step 3371, loss 0.263838, acc 0.875\n",
      "2017-11-06T11:55:56.922620: step 3372, loss 0.0996367, acc 0.96875\n",
      "2017-11-06T11:56:00.942476: step 3373, loss 0.0146186, acc 1\n",
      "2017-11-06T11:56:04.947321: step 3374, loss 0.194512, acc 0.9375\n",
      "2017-11-06T11:56:09.098271: step 3375, loss 0.338695, acc 0.84375\n",
      "2017-11-06T11:56:13.082101: step 3376, loss 0.122045, acc 0.9375\n",
      "2017-11-06T11:56:17.164002: step 3377, loss 0.280856, acc 0.90625\n",
      "2017-11-06T11:56:21.171849: step 3378, loss 0.427797, acc 0.78125\n",
      "2017-11-06T11:56:25.183700: step 3379, loss 0.205377, acc 0.875\n",
      "2017-11-06T11:56:29.200554: step 3380, loss 0.11439, acc 0.96875\n",
      "2017-11-06T11:56:33.336494: step 3381, loss 0.340344, acc 0.84375\n",
      "2017-11-06T11:56:37.529472: step 3382, loss 0.101407, acc 0.90625\n",
      "2017-11-06T11:56:41.562338: step 3383, loss 0.307788, acc 0.875\n",
      "2017-11-06T11:56:44.139168: step 3384, loss 0.140377, acc 0.9\n",
      "2017-11-06T11:56:48.252091: step 3385, loss 0.171957, acc 0.875\n",
      "2017-11-06T11:56:52.691245: step 3386, loss 0.114208, acc 0.96875\n",
      "2017-11-06T11:56:56.713103: step 3387, loss 0.106312, acc 0.9375\n",
      "2017-11-06T11:57:00.787998: step 3388, loss 0.353091, acc 0.84375\n",
      "2017-11-06T11:57:04.745810: step 3389, loss 0.0935706, acc 0.96875\n",
      "2017-11-06T11:57:08.770670: step 3390, loss 0.0366536, acc 0.96875\n",
      "2017-11-06T11:57:12.730484: step 3391, loss 0.24283, acc 0.84375\n",
      "2017-11-06T11:57:16.800376: step 3392, loss 0.179135, acc 0.90625\n",
      "2017-11-06T11:57:20.744179: step 3393, loss 0.110759, acc 0.9375\n",
      "2017-11-06T11:57:24.769040: step 3394, loss 0.0610583, acc 0.96875\n",
      "2017-11-06T11:57:28.711840: step 3395, loss 0.10734, acc 0.96875\n",
      "2017-11-06T11:57:32.680659: step 3396, loss 0.288461, acc 0.84375\n",
      "2017-11-06T11:57:36.721530: step 3397, loss 0.186337, acc 0.90625\n",
      "2017-11-06T11:57:40.720373: step 3398, loss 0.214059, acc 0.90625\n",
      "2017-11-06T11:57:44.724217: step 3399, loss 0.255429, acc 0.84375\n",
      "2017-11-06T11:57:48.685031: step 3400, loss 0.122687, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T11:57:51.234843: step 3400, loss 0.991964, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-06T11:57:57.210509: step 3401, loss 0.226211, acc 0.875\n",
      "2017-11-06T11:58:01.248378: step 3402, loss 0.190161, acc 0.90625\n",
      "2017-11-06T11:58:05.227207: step 3403, loss 0.186529, acc 0.90625\n",
      "2017-11-06T11:58:09.249063: step 3404, loss 0.23179, acc 0.8125\n",
      "2017-11-06T11:58:13.274924: step 3405, loss 0.420585, acc 0.78125\n",
      "2017-11-06T11:58:17.299548: step 3406, loss 0.359136, acc 0.84375\n",
      "2017-11-06T11:58:21.277372: step 3407, loss 0.230254, acc 0.875\n",
      "2017-11-06T11:58:25.377286: step 3408, loss 0.216861, acc 0.84375\n",
      "2017-11-06T11:58:29.662330: step 3409, loss 0.0372008, acc 1\n",
      "2017-11-06T11:58:33.796266: step 3410, loss 0.320933, acc 0.78125\n",
      "2017-11-06T11:58:37.873163: step 3411, loss 0.0909348, acc 0.96875\n",
      "2017-11-06T11:58:41.931047: step 3412, loss 0.308283, acc 0.875\n",
      "2017-11-06T11:58:45.841826: step 3413, loss 0.19594, acc 0.84375\n",
      "2017-11-06T11:58:49.835664: step 3414, loss 0.193147, acc 0.9375\n",
      "2017-11-06T11:58:53.820495: step 3415, loss 0.305411, acc 0.875\n",
      "2017-11-06T11:58:57.823339: step 3416, loss 0.336013, acc 0.8125\n",
      "2017-11-06T11:59:02.133402: step 3417, loss 0.17656, acc 0.90625\n",
      "2017-11-06T11:59:06.234316: step 3418, loss 0.0814698, acc 0.96875\n",
      "2017-11-06T11:59:10.250169: step 3419, loss 0.0790347, acc 0.96875\n",
      "2017-11-06T11:59:12.799980: step 3420, loss 0.172439, acc 0.85\n",
      "2017-11-06T11:59:16.841853: step 3421, loss 0.0606687, acc 0.96875\n",
      "2017-11-06T11:59:20.907742: step 3422, loss 0.112071, acc 0.96875\n",
      "2017-11-06T11:59:24.941608: step 3423, loss 0.352919, acc 0.84375\n",
      "2017-11-06T11:59:28.937447: step 3424, loss 0.0605295, acc 0.96875\n",
      "2017-11-06T11:59:33.011342: step 3425, loss 0.0701378, acc 1\n",
      "2017-11-06T11:59:37.012186: step 3426, loss 0.137995, acc 0.96875\n",
      "2017-11-06T11:59:41.053056: step 3427, loss 0.177256, acc 0.90625\n",
      "2017-11-06T11:59:45.083920: step 3428, loss 0.190615, acc 0.875\n",
      "2017-11-06T11:59:49.147807: step 3429, loss 0.314037, acc 0.875\n",
      "2017-11-06T11:59:53.120630: step 3430, loss 0.174678, acc 0.875\n",
      "2017-11-06T11:59:57.106462: step 3431, loss 0.15299, acc 0.9375\n",
      "2017-11-06T12:00:01.357483: step 3432, loss 0.136581, acc 0.90625\n",
      "2017-11-06T12:00:05.544458: step 3433, loss 0.138113, acc 0.96875\n",
      "2017-11-06T12:00:09.891546: step 3434, loss 0.161006, acc 0.9375\n",
      "2017-11-06T12:00:13.867372: step 3435, loss 0.015902, acc 1\n",
      "2017-11-06T12:00:17.898236: step 3436, loss 0.418286, acc 0.84375\n",
      "2017-11-06T12:00:21.884068: step 3437, loss 0.145904, acc 0.96875\n",
      "2017-11-06T12:00:25.834875: step 3438, loss 0.461763, acc 0.90625\n",
      "2017-11-06T12:00:29.893759: step 3439, loss 0.296608, acc 0.875\n",
      "2017-11-06T12:00:34.057718: step 3440, loss 0.281351, acc 0.875\n",
      "2017-11-06T12:00:38.089583: step 3441, loss 0.302504, acc 0.84375\n",
      "2017-11-06T12:00:42.085422: step 3442, loss 0.286935, acc 0.90625\n",
      "2017-11-06T12:00:46.094271: step 3443, loss 0.219257, acc 0.84375\n",
      "2017-11-06T12:00:50.115127: step 3444, loss 0.343234, acc 0.875\n",
      "2017-11-06T12:00:54.145991: step 3445, loss 0.430413, acc 0.875\n",
      "2017-11-06T12:00:58.147835: step 3446, loss 0.390857, acc 0.875\n",
      "2017-11-06T12:01:02.143674: step 3447, loss 0.15395, acc 0.9375\n",
      "2017-11-06T12:01:06.164533: step 3448, loss 0.148192, acc 0.9375\n",
      "2017-11-06T12:01:10.227418: step 3449, loss 0.250446, acc 0.90625\n",
      "2017-11-06T12:01:14.696593: step 3450, loss 0.235625, acc 0.875\n",
      "2017-11-06T12:01:18.757240: step 3451, loss 0.217845, acc 0.84375\n",
      "2017-11-06T12:01:22.762088: step 3452, loss 0.285893, acc 0.875\n",
      "2017-11-06T12:01:26.713895: step 3453, loss 0.121798, acc 0.90625\n",
      "2017-11-06T12:01:30.699727: step 3454, loss 0.13892, acc 0.90625\n",
      "2017-11-06T12:01:34.691563: step 3455, loss 0.277231, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:01:37.367464: step 3456, loss 0.088039, acc 0.95\n",
      "2017-11-06T12:01:41.578456: step 3457, loss 0.0648653, acc 1\n",
      "2017-11-06T12:01:45.527263: step 3458, loss 0.187474, acc 0.875\n",
      "2017-11-06T12:01:49.585145: step 3459, loss 0.190418, acc 0.90625\n",
      "2017-11-06T12:01:53.568976: step 3460, loss 0.16933, acc 0.875\n",
      "2017-11-06T12:01:57.567817: step 3461, loss 0.116854, acc 0.96875\n",
      "2017-11-06T12:02:01.520626: step 3462, loss 0.0913483, acc 0.96875\n",
      "2017-11-06T12:02:05.639749: step 3463, loss 0.120856, acc 0.90625\n",
      "2017-11-06T12:02:09.587554: step 3464, loss 0.174629, acc 0.90625\n",
      "2017-11-06T12:02:13.548368: step 3465, loss 0.150825, acc 0.9375\n",
      "2017-11-06T12:02:17.800389: step 3466, loss 0.120715, acc 0.96875\n",
      "2017-11-06T12:02:21.976356: step 3467, loss 0.23908, acc 0.875\n",
      "2017-11-06T12:02:25.984204: step 3468, loss 0.179527, acc 0.875\n",
      "2017-11-06T12:02:29.995055: step 3469, loss 0.266451, acc 0.875\n",
      "2017-11-06T12:02:34.152008: step 3470, loss 0.273982, acc 0.875\n",
      "2017-11-06T12:02:38.225903: step 3471, loss 0.239044, acc 0.8125\n",
      "2017-11-06T12:02:42.199726: step 3472, loss 0.0788136, acc 0.96875\n",
      "2017-11-06T12:02:46.195566: step 3473, loss 0.0985374, acc 0.9375\n",
      "2017-11-06T12:02:50.179397: step 3474, loss 0.253474, acc 0.875\n",
      "2017-11-06T12:02:54.214265: step 3475, loss 0.12778, acc 0.96875\n",
      "2017-11-06T12:02:58.177078: step 3476, loss 0.0558636, acc 0.96875\n",
      "2017-11-06T12:03:02.172918: step 3477, loss 0.106712, acc 0.9375\n",
      "2017-11-06T12:03:06.224798: step 3478, loss 0.389694, acc 0.78125\n",
      "2017-11-06T12:03:10.146603: step 3479, loss 0.209723, acc 0.90625\n",
      "2017-11-06T12:03:14.128412: step 3480, loss 0.174253, acc 0.875\n",
      "2017-11-06T12:03:18.104238: step 3481, loss 0.115129, acc 0.9375\n",
      "2017-11-06T12:03:22.322234: step 3482, loss 0.12388, acc 0.9375\n",
      "2017-11-06T12:03:26.690339: step 3483, loss 0.194431, acc 0.90625\n",
      "2017-11-06T12:03:30.682177: step 3484, loss 0.259298, acc 0.875\n",
      "2017-11-06T12:03:34.664004: step 3485, loss 0.181689, acc 0.96875\n",
      "2017-11-06T12:03:38.640830: step 3486, loss 0.179269, acc 0.9375\n",
      "2017-11-06T12:03:42.909863: step 3487, loss 0.510288, acc 0.8125\n",
      "2017-11-06T12:03:47.040798: step 3488, loss 0.220523, acc 0.9375\n",
      "2017-11-06T12:03:51.153720: step 3489, loss 0.254827, acc 0.84375\n",
      "2017-11-06T12:03:55.238623: step 3490, loss 0.0994206, acc 0.9375\n",
      "2017-11-06T12:03:59.343540: step 3491, loss 0.0824836, acc 0.9375\n",
      "2017-11-06T12:04:02.026446: step 3492, loss 0.365502, acc 0.8\n",
      "2017-11-06T12:04:06.173394: step 3493, loss 0.0379756, acc 1\n",
      "2017-11-06T12:04:10.350360: step 3494, loss 0.383386, acc 0.84375\n",
      "2017-11-06T12:04:14.358209: step 3495, loss 0.356648, acc 0.78125\n",
      "2017-11-06T12:04:18.465903: step 3496, loss 0.22077, acc 0.90625\n",
      "2017-11-06T12:04:22.577826: step 3497, loss 0.148387, acc 0.875\n",
      "2017-11-06T12:04:26.825843: step 3498, loss 0.134695, acc 0.90625\n",
      "2017-11-06T12:04:31.500164: step 3499, loss 0.234451, acc 0.875\n",
      "2017-11-06T12:04:35.858261: step 3500, loss 0.252947, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:04:38.632232: step 3500, loss 0.89822, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-06T12:04:44.504404: step 3501, loss 0.0994733, acc 0.96875\n",
      "2017-11-06T12:04:48.622330: step 3502, loss 0.265934, acc 0.84375\n",
      "2017-11-06T12:04:52.808305: step 3503, loss 0.130734, acc 0.90625\n",
      "2017-11-06T12:04:56.772122: step 3504, loss 0.241066, acc 0.875\n",
      "2017-11-06T12:05:00.746945: step 3505, loss 0.116986, acc 0.9375\n",
      "2017-11-06T12:05:04.669733: step 3506, loss 0.160627, acc 0.9375\n",
      "2017-11-06T12:05:08.698597: step 3507, loss 0.0624298, acc 0.96875\n",
      "2017-11-06T12:05:12.721453: step 3508, loss 0.179749, acc 0.9375\n",
      "2017-11-06T12:05:16.731303: step 3509, loss 0.328374, acc 0.84375\n",
      "2017-11-06T12:05:20.725141: step 3510, loss 0.143186, acc 0.90625\n",
      "2017-11-06T12:05:24.699965: step 3511, loss 0.303349, acc 0.84375\n",
      "2017-11-06T12:05:28.714817: step 3512, loss 0.122557, acc 0.96875\n",
      "2017-11-06T12:05:32.711657: step 3513, loss 0.0934868, acc 0.96875\n",
      "2017-11-06T12:05:37.124793: step 3514, loss 0.0658152, acc 0.96875\n",
      "2017-11-06T12:05:41.187680: step 3515, loss 0.389608, acc 0.75\n",
      "2017-11-06T12:05:45.233554: step 3516, loss 0.424232, acc 0.84375\n",
      "2017-11-06T12:05:49.271423: step 3517, loss 0.328686, acc 0.90625\n",
      "2017-11-06T12:05:53.293283: step 3518, loss 0.0685663, acc 1\n",
      "2017-11-06T12:05:57.289120: step 3519, loss 0.210004, acc 0.875\n",
      "2017-11-06T12:06:01.334995: step 3520, loss 0.138435, acc 0.90625\n",
      "2017-11-06T12:06:05.239770: step 3521, loss 0.234328, acc 0.875\n",
      "2017-11-06T12:06:09.383715: step 3522, loss 0.141555, acc 0.9375\n",
      "2017-11-06T12:06:13.397568: step 3523, loss 0.203926, acc 0.9375\n",
      "2017-11-06T12:06:17.396407: step 3524, loss 0.367591, acc 0.875\n",
      "2017-11-06T12:06:21.400252: step 3525, loss 0.406755, acc 0.84375\n",
      "2017-11-06T12:06:25.434120: step 3526, loss 0.278544, acc 0.90625\n",
      "2017-11-06T12:06:29.418950: step 3527, loss 0.128459, acc 0.9375\n",
      "2017-11-06T12:06:32.042817: step 3528, loss 0.271294, acc 0.9\n",
      "2017-11-06T12:06:36.282829: step 3529, loss 0.389662, acc 0.84375\n",
      "2017-11-06T12:06:40.556864: step 3530, loss 0.207005, acc 0.90625\n",
      "2017-11-06T12:06:44.759851: step 3531, loss 0.175651, acc 0.875\n",
      "2017-11-06T12:06:48.715661: step 3532, loss 0.17983, acc 0.875\n",
      "2017-11-06T12:06:52.769544: step 3533, loss 0.21123, acc 0.84375\n",
      "2017-11-06T12:06:56.704340: step 3534, loss 0.240899, acc 0.875\n",
      "2017-11-06T12:07:00.767225: step 3535, loss 0.102502, acc 0.9375\n",
      "2017-11-06T12:07:04.701020: step 3536, loss 0.21679, acc 0.875\n",
      "2017-11-06T12:07:08.755900: step 3537, loss 0.0911457, acc 0.96875\n",
      "2017-11-06T12:07:12.760746: step 3538, loss 0.0986408, acc 0.9375\n",
      "2017-11-06T12:07:16.714555: step 3539, loss 0.23547, acc 0.875\n",
      "2017-11-06T12:07:20.751229: step 3540, loss 0.250918, acc 0.9375\n",
      "2017-11-06T12:07:24.800105: step 3541, loss 0.189281, acc 0.875\n",
      "2017-11-06T12:07:28.870996: step 3542, loss 0.278933, acc 0.90625\n",
      "2017-11-06T12:07:32.833812: step 3543, loss 0.173922, acc 0.875\n",
      "2017-11-06T12:07:36.836656: step 3544, loss 0.0814376, acc 0.96875\n",
      "2017-11-06T12:07:40.803475: step 3545, loss 0.141395, acc 0.9375\n",
      "2017-11-06T12:07:44.897384: step 3546, loss 0.155914, acc 0.90625\n",
      "2017-11-06T12:07:49.315523: step 3547, loss 0.053489, acc 0.96875\n",
      "2017-11-06T12:07:53.360397: step 3548, loss 0.171677, acc 0.9375\n",
      "2017-11-06T12:07:57.349233: step 3549, loss 0.108772, acc 0.9375\n",
      "2017-11-06T12:08:01.352076: step 3550, loss 0.226914, acc 0.84375\n",
      "2017-11-06T12:08:05.372933: step 3551, loss 0.16634, acc 0.875\n",
      "2017-11-06T12:08:09.475850: step 3552, loss 0.0540077, acc 0.96875\n",
      "2017-11-06T12:08:13.423652: step 3553, loss 0.102, acc 0.96875\n",
      "2017-11-06T12:08:17.430501: step 3554, loss 0.227728, acc 0.84375\n",
      "2017-11-06T12:08:21.429341: step 3555, loss 0.0590155, acc 0.96875\n",
      "2017-11-06T12:08:25.500234: step 3556, loss 0.202839, acc 0.9375\n",
      "2017-11-06T12:08:29.643178: step 3557, loss 0.245613, acc 0.9375\n",
      "2017-11-06T12:08:33.787122: step 3558, loss 0.353715, acc 0.84375\n",
      "2017-11-06T12:08:37.921059: step 3559, loss 0.0297519, acc 1\n",
      "2017-11-06T12:08:41.952924: step 3560, loss 0.255895, acc 0.84375\n",
      "2017-11-06T12:08:45.906734: step 3561, loss 0.254481, acc 0.84375\n",
      "2017-11-06T12:08:50.025660: step 3562, loss 0.228856, acc 0.84375\n",
      "2017-11-06T12:08:54.494836: step 3563, loss 0.25592, acc 0.875\n",
      "2017-11-06T12:08:57.007621: step 3564, loss 0.470104, acc 0.85\n",
      "2017-11-06T12:09:01.025476: step 3565, loss 0.0595921, acc 1\n",
      "2017-11-06T12:09:05.027320: step 3566, loss 0.258912, acc 0.90625\n",
      "2017-11-06T12:09:09.078198: step 3567, loss 0.397412, acc 0.875\n",
      "2017-11-06T12:09:12.996982: step 3568, loss 0.250186, acc 0.90625\n",
      "2017-11-06T12:09:17.021842: step 3569, loss 0.480857, acc 0.78125\n",
      "2017-11-06T12:09:20.990664: step 3570, loss 0.145247, acc 0.90625\n",
      "2017-11-06T12:09:24.965488: step 3571, loss 0.245665, acc 0.90625\n",
      "2017-11-06T12:09:28.967330: step 3572, loss 0.254691, acc 0.875\n",
      "2017-11-06T12:09:32.967173: step 3573, loss 0.238226, acc 0.90625\n",
      "2017-11-06T12:09:36.984026: step 3574, loss 0.398732, acc 0.84375\n",
      "2017-11-06T12:09:41.069929: step 3575, loss 0.24245, acc 0.90625\n",
      "2017-11-06T12:09:45.021739: step 3576, loss 0.688961, acc 0.75\n",
      "2017-11-06T12:09:49.041593: step 3577, loss 0.255117, acc 0.875\n",
      "2017-11-06T12:09:53.039434: step 3578, loss 0.109295, acc 0.9375\n",
      "2017-11-06T12:09:57.309468: step 3579, loss 0.20882, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:10:01.910738: step 3580, loss 0.225361, acc 0.9375\n",
      "2017-11-06T12:10:05.942602: step 3581, loss 0.128924, acc 0.96875\n",
      "2017-11-06T12:10:09.966461: step 3582, loss 0.171351, acc 0.9375\n",
      "2017-11-06T12:10:13.947291: step 3583, loss 0.459487, acc 0.875\n",
      "2017-11-06T12:10:18.055995: step 3584, loss 0.435732, acc 0.84375\n",
      "2017-11-06T12:10:22.001798: step 3585, loss 0.24486, acc 0.875\n",
      "2017-11-06T12:10:25.961612: step 3586, loss 0.154418, acc 0.9375\n",
      "2017-11-06T12:10:29.948444: step 3587, loss 0.138339, acc 0.90625\n",
      "2017-11-06T12:10:34.178450: step 3588, loss 0.168917, acc 0.90625\n",
      "2017-11-06T12:10:38.321396: step 3589, loss 0.176073, acc 0.90625\n",
      "2017-11-06T12:10:42.249185: step 3590, loss 0.22923, acc 0.90625\n",
      "2017-11-06T12:10:46.215003: step 3591, loss 0.194989, acc 0.9375\n",
      "2017-11-06T12:10:50.189828: step 3592, loss 0.456742, acc 0.84375\n",
      "2017-11-06T12:10:54.258719: step 3593, loss 0.184579, acc 0.9375\n",
      "2017-11-06T12:10:58.224536: step 3594, loss 0.267189, acc 0.9375\n",
      "2017-11-06T12:11:02.320446: step 3595, loss 0.235269, acc 0.90625\n",
      "2017-11-06T12:11:06.569465: step 3596, loss 0.32033, acc 0.84375\n",
      "2017-11-06T12:11:10.513267: step 3597, loss 0.202071, acc 0.90625\n",
      "2017-11-06T12:11:14.436055: step 3598, loss 0.026045, acc 1\n",
      "2017-11-06T12:11:18.378856: step 3599, loss 0.256872, acc 0.90625\n",
      "2017-11-06T12:11:20.882635: step 3600, loss 0.161161, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:11:23.458466: step 3600, loss 0.993343, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\6\\1509930377\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C9310B70>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\n",
      "\n",
      "2017-11-06T12:11:33.246325: step 1, loss 1.867, acc 0.53125\n",
      "2017-11-06T12:11:37.190126: step 2, loss 1.39403, acc 0.78125\n",
      "2017-11-06T12:11:41.188967: step 3, loss 1.1905, acc 0.71875\n",
      "2017-11-06T12:11:45.109755: step 4, loss 0.439929, acc 0.90625\n",
      "2017-11-06T12:11:49.020532: step 5, loss 0.000273009, acc 1\n",
      "2017-11-06T12:11:52.915301: step 6, loss 0.49532, acc 0.96875\n",
      "2017-11-06T12:11:56.857100: step 7, loss 0.810705, acc 0.9375\n",
      "2017-11-06T12:12:00.827921: step 8, loss 1.89352, acc 0.875\n",
      "2017-11-06T12:12:04.763717: step 9, loss 0.851979, acc 0.90625\n",
      "2017-11-06T12:12:08.966704: step 10, loss 2.7013, acc 0.8125\n",
      "2017-11-06T12:12:13.099640: step 11, loss 0.444377, acc 0.96875\n",
      "2017-11-06T12:12:17.009418: step 12, loss 2.95254, acc 0.8125\n",
      "2017-11-06T12:12:20.920197: step 13, loss 1.02212, acc 0.90625\n",
      "2017-11-06T12:12:24.937051: step 14, loss 2.18505, acc 0.78125\n",
      "2017-11-06T12:12:28.884856: step 15, loss 1.50338, acc 0.90625\n",
      "2017-11-06T12:12:32.881697: step 16, loss 1.34869, acc 0.84375\n",
      "2017-11-06T12:12:36.941584: step 17, loss 0.962958, acc 0.875\n",
      "2017-11-06T12:12:40.827343: step 18, loss 1.98375, acc 0.6875\n",
      "2017-11-06T12:12:44.788158: step 19, loss 0.311683, acc 0.9375\n",
      "2017-11-06T12:12:48.706944: step 20, loss 1.69811, acc 0.71875\n",
      "2017-11-06T12:12:52.615719: step 21, loss 0.678711, acc 0.71875\n",
      "2017-11-06T12:12:56.613561: step 22, loss 0.862549, acc 0.78125\n",
      "2017-11-06T12:13:00.614403: step 23, loss 1.5907, acc 0.65625\n",
      "2017-11-06T12:13:04.560205: step 24, loss 0.754289, acc 0.75\n",
      "2017-11-06T12:13:08.502007: step 25, loss 1.02245, acc 0.71875\n",
      "2017-11-06T12:13:12.542879: step 26, loss 2.17963, acc 0.75\n",
      "2017-11-06T12:13:16.884964: step 27, loss 1.29209, acc 0.84375\n",
      "2017-11-06T12:13:20.842620: step 28, loss 1.36177, acc 0.8125\n",
      "2017-11-06T12:13:24.989567: step 29, loss 1.54746, acc 0.875\n",
      "2017-11-06T12:13:29.013425: step 30, loss 1.85956, acc 0.71875\n",
      "2017-11-06T12:13:33.034025: step 31, loss 0.753439, acc 0.84375\n",
      "2017-11-06T12:13:36.964818: step 32, loss 0.549592, acc 0.90625\n",
      "2017-11-06T12:13:40.955654: step 33, loss 0.245727, acc 0.9375\n",
      "2017-11-06T12:13:44.902458: step 34, loss 0.888948, acc 0.90625\n",
      "2017-11-06T12:13:48.829248: step 35, loss 3.15931, acc 0.75\n",
      "2017-11-06T12:13:51.332027: step 36, loss 0.474899, acc 0.9\n",
      "2017-11-06T12:13:55.295843: step 37, loss 1.21182, acc 0.84375\n",
      "2017-11-06T12:13:59.294684: step 38, loss 0.569327, acc 0.9375\n",
      "2017-11-06T12:14:03.247493: step 39, loss 1.26585, acc 0.875\n",
      "2017-11-06T12:14:07.171281: step 40, loss 0.788493, acc 0.90625\n",
      "2017-11-06T12:14:11.066048: step 41, loss 0.760598, acc 0.875\n",
      "2017-11-06T12:14:15.104918: step 42, loss 1.33181, acc 0.84375\n",
      "2017-11-06T12:14:19.241858: step 43, loss 0.651517, acc 0.84375\n",
      "2017-11-06T12:14:23.538911: step 44, loss 1.07966, acc 0.90625\n",
      "2017-11-06T12:14:27.556766: step 45, loss 0.884127, acc 0.875\n",
      "2017-11-06T12:14:31.582627: step 46, loss 0.336362, acc 0.875\n",
      "2017-11-06T12:14:35.789616: step 47, loss 0.0790545, acc 0.96875\n",
      "2017-11-06T12:14:39.798464: step 48, loss 0.714568, acc 0.875\n",
      "2017-11-06T12:14:43.736263: step 49, loss 0.434223, acc 0.9375\n",
      "2017-11-06T12:14:47.697076: step 50, loss 1.73144, acc 0.75\n",
      "2017-11-06T12:14:51.698920: step 51, loss 0.662017, acc 0.8125\n",
      "2017-11-06T12:14:55.667740: step 52, loss 1.05861, acc 0.875\n",
      "2017-11-06T12:14:59.672585: step 53, loss 1.14158, acc 0.78125\n",
      "2017-11-06T12:15:03.624394: step 54, loss 0.597402, acc 0.84375\n",
      "2017-11-06T12:15:07.513156: step 55, loss 0.822714, acc 0.90625\n",
      "2017-11-06T12:15:11.468968: step 56, loss 0.483276, acc 0.90625\n",
      "2017-11-06T12:15:15.370740: step 57, loss 0.625219, acc 0.8125\n",
      "2017-11-06T12:15:19.312541: step 58, loss 0.703726, acc 0.875\n",
      "2017-11-06T12:15:23.264349: step 59, loss 0.23957, acc 0.9375\n",
      "2017-11-06T12:15:27.691495: step 60, loss 0.847832, acc 0.78125\n",
      "2017-11-06T12:15:31.639299: step 61, loss 1.53006, acc 0.75\n",
      "2017-11-06T12:15:35.660418: step 62, loss 1.24366, acc 0.75\n",
      "2017-11-06T12:15:39.584206: step 63, loss 0.784611, acc 0.90625\n",
      "2017-11-06T12:15:43.505993: step 64, loss 1.28456, acc 0.78125\n",
      "2017-11-06T12:15:47.463807: step 65, loss 1.73939, acc 0.8125\n",
      "2017-11-06T12:15:51.426621: step 66, loss 0.998426, acc 0.84375\n",
      "2017-11-06T12:15:55.332398: step 67, loss 1.09253, acc 0.8125\n",
      "2017-11-06T12:15:59.317229: step 68, loss 0.58775, acc 0.8125\n",
      "2017-11-06T12:16:03.322073: step 69, loss 1.21309, acc 0.75\n",
      "2017-11-06T12:16:07.316912: step 70, loss 1.24871, acc 0.875\n",
      "2017-11-06T12:16:11.397811: step 71, loss 0.256635, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:16:13.918602: step 72, loss 0.488034, acc 0.85\n",
      "2017-11-06T12:16:17.885423: step 73, loss 0.28328, acc 0.90625\n",
      "2017-11-06T12:16:21.838041: step 74, loss 0.587091, acc 0.84375\n",
      "2017-11-06T12:16:25.793852: step 75, loss 1.15347, acc 0.78125\n",
      "2017-11-06T12:16:29.859741: step 76, loss 0.605961, acc 0.84375\n",
      "2017-11-06T12:16:34.429988: step 77, loss 1.74807, acc 0.84375\n",
      "2017-11-06T12:16:38.456851: step 78, loss 1.17613, acc 0.78125\n",
      "2017-11-06T12:16:42.514733: step 79, loss 1.16658, acc 0.78125\n",
      "2017-11-06T12:16:46.479550: step 80, loss 0.529754, acc 0.84375\n",
      "2017-11-06T12:16:50.394333: step 81, loss 0.829508, acc 0.78125\n",
      "2017-11-06T12:16:54.369156: step 82, loss 0.653, acc 0.8125\n",
      "2017-11-06T12:16:58.267928: step 83, loss 0.15745, acc 0.96875\n",
      "2017-11-06T12:17:02.230742: step 84, loss 1.35368, acc 0.8125\n",
      "2017-11-06T12:17:06.209569: step 85, loss 0.982895, acc 0.8125\n",
      "2017-11-06T12:17:10.130355: step 86, loss 0.65669, acc 0.90625\n",
      "2017-11-06T12:17:14.056145: step 87, loss 0.659913, acc 0.84375\n",
      "2017-11-06T12:17:18.012955: step 88, loss 0.155437, acc 0.96875\n",
      "2017-11-06T12:17:21.964765: step 89, loss 0.690377, acc 0.875\n",
      "2017-11-06T12:17:26.009638: step 90, loss 0.679538, acc 0.875\n",
      "2017-11-06T12:17:29.966450: step 91, loss 0.470937, acc 0.9375\n",
      "2017-11-06T12:17:33.947279: step 92, loss 0.971345, acc 0.78125\n",
      "2017-11-06T12:17:38.337397: step 93, loss 1.58747, acc 0.65625\n",
      "2017-11-06T12:17:42.424303: step 94, loss 0.8806, acc 0.84375\n",
      "2017-11-06T12:17:46.405129: step 95, loss 0.24982, acc 0.9375\n",
      "2017-11-06T12:17:50.395966: step 96, loss 0.746919, acc 0.8125\n",
      "2017-11-06T12:17:54.313749: step 97, loss 0.515848, acc 0.875\n",
      "2017-11-06T12:17:58.256550: step 98, loss 0.554312, acc 0.84375\n",
      "2017-11-06T12:18:02.201355: step 99, loss 1.02906, acc 0.875\n",
      "2017-11-06T12:18:06.179180: step 100, loss 0.237823, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:18:08.836068: step 100, loss 1.70059, acc 0.816667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-100\n",
      "\n",
      "2017-11-06T12:18:14.272778: step 101, loss 0.412308, acc 0.9375\n",
      "2017-11-06T12:18:18.322656: step 102, loss 0.942989, acc 0.90625\n",
      "2017-11-06T12:18:22.351519: step 103, loss 0.3188, acc 0.9375\n",
      "2017-11-06T12:18:26.362368: step 104, loss 0.314303, acc 0.90625\n",
      "2017-11-06T12:18:30.339195: step 105, loss 0.753924, acc 0.84375\n",
      "2017-11-06T12:18:34.530172: step 106, loss 0.527389, acc 0.84375\n",
      "2017-11-06T12:18:38.504998: step 107, loss 0.541276, acc 0.9375\n",
      "2017-11-06T12:18:41.256952: step 108, loss 1.89096, acc 0.75\n",
      "2017-11-06T12:18:45.560009: step 109, loss 0.175603, acc 0.96875\n",
      "2017-11-06T12:18:49.552846: step 110, loss 0.96496, acc 0.78125\n",
      "2017-11-06T12:18:53.557692: step 111, loss 0.413712, acc 0.8125\n",
      "2017-11-06T12:18:57.503495: step 112, loss 0.871365, acc 0.75\n",
      "2017-11-06T12:19:01.481322: step 113, loss 0.916176, acc 0.84375\n",
      "2017-11-06T12:19:05.480163: step 114, loss 0.670737, acc 0.84375\n",
      "2017-11-06T12:19:09.470999: step 115, loss 0.484066, acc 0.84375\n",
      "2017-11-06T12:19:13.468840: step 116, loss 0.5028, acc 0.875\n",
      "2017-11-06T12:19:17.453671: step 117, loss 0.083063, acc 0.9375\n",
      "2017-11-06T12:19:21.553489: step 118, loss 0.648021, acc 0.78125\n",
      "2017-11-06T12:19:25.684425: step 119, loss 0.402696, acc 0.875\n",
      "2017-11-06T12:19:29.862393: step 120, loss 0.510577, acc 0.90625\n",
      "2017-11-06T12:19:33.893258: step 121, loss 0.529866, acc 0.90625\n",
      "2017-11-06T12:19:37.887096: step 122, loss 0.628031, acc 0.875\n",
      "2017-11-06T12:19:41.990075: step 123, loss 0.276004, acc 0.96875\n",
      "2017-11-06T12:19:46.093991: step 124, loss 0.152127, acc 0.90625\n",
      "2017-11-06T12:19:50.576175: step 125, loss 0.276367, acc 0.90625\n",
      "2017-11-06T12:19:54.671085: step 126, loss 0.648351, acc 0.8125\n",
      "2017-11-06T12:19:58.630899: step 127, loss 0.684703, acc 0.875\n",
      "2017-11-06T12:20:02.910939: step 128, loss 1.7308, acc 0.6875\n",
      "2017-11-06T12:20:06.941804: step 129, loss 0.706134, acc 0.90625\n",
      "2017-11-06T12:20:10.965663: step 130, loss 0.589288, acc 0.875\n",
      "2017-11-06T12:20:14.939486: step 131, loss 0.262521, acc 0.9375\n",
      "2017-11-06T12:20:18.905304: step 132, loss 0.539246, acc 0.90625\n",
      "2017-11-06T12:20:22.904148: step 133, loss 0.262761, acc 0.96875\n",
      "2017-11-06T12:20:26.878970: step 134, loss 0.892874, acc 0.84375\n",
      "2017-11-06T12:20:30.857797: step 135, loss 0.794839, acc 0.9375\n",
      "2017-11-06T12:20:35.015752: step 136, loss 1.70211, acc 0.78125\n",
      "2017-11-06T12:20:39.023600: step 137, loss 0.489604, acc 0.9375\n",
      "2017-11-06T12:20:43.028445: step 138, loss 0.183858, acc 0.875\n",
      "2017-11-06T12:20:46.979252: step 139, loss 0.349236, acc 0.9375\n",
      "2017-11-06T12:20:50.946070: step 140, loss 0.00897681, acc 1\n",
      "2017-11-06T12:20:55.354203: step 141, loss 0.739452, acc 0.875\n",
      "2017-11-06T12:20:59.410084: step 142, loss 0.44018, acc 0.9375\n",
      "2017-11-06T12:21:03.403922: step 143, loss 1.00024, acc 0.8125\n",
      "2017-11-06T12:21:06.021802: step 144, loss 0.186319, acc 0.9\n",
      "2017-11-06T12:21:10.054649: step 145, loss 0.981204, acc 0.75\n",
      "2017-11-06T12:21:14.034476: step 146, loss 0.994724, acc 0.78125\n",
      "2017-11-06T12:21:18.059335: step 147, loss 0.647212, acc 0.84375\n",
      "2017-11-06T12:21:22.017148: step 148, loss 0.43701, acc 0.875\n",
      "2017-11-06T12:21:26.016990: step 149, loss 0.827552, acc 0.8125\n",
      "2017-11-06T12:21:29.980807: step 150, loss 0.730298, acc 0.8125\n",
      "2017-11-06T12:21:33.939619: step 151, loss 0.163503, acc 0.96875\n",
      "2017-11-06T12:21:37.984494: step 152, loss 0.945015, acc 0.6875\n",
      "2017-11-06T12:21:42.051383: step 153, loss 0.554951, acc 0.9375\n",
      "2017-11-06T12:21:46.050224: step 154, loss 0.734812, acc 0.875\n",
      "2017-11-06T12:21:50.086093: step 155, loss 0.00364262, acc 1\n",
      "2017-11-06T12:21:54.152981: step 156, loss 0.18706, acc 0.90625\n",
      "2017-11-06T12:21:58.317941: step 157, loss 0.4017, acc 0.90625\n",
      "2017-11-06T12:22:02.686044: step 158, loss 0.781922, acc 0.875\n",
      "2017-11-06T12:22:06.689890: step 159, loss 0.68966, acc 0.90625\n",
      "2017-11-06T12:22:10.720754: step 160, loss 0.787961, acc 0.8125\n",
      "2017-11-06T12:22:14.678566: step 161, loss 1.09593, acc 0.875\n",
      "2017-11-06T12:22:18.701424: step 162, loss 0.716832, acc 0.84375\n",
      "2017-11-06T12:22:22.660008: step 163, loss 1.05609, acc 0.875\n",
      "2017-11-06T12:22:26.628830: step 164, loss 0.915896, acc 0.8125\n",
      "2017-11-06T12:22:30.599651: step 165, loss 0.412902, acc 0.9375\n",
      "2017-11-06T12:22:34.870686: step 166, loss 0.166369, acc 0.9375\n",
      "2017-11-06T12:22:38.950584: step 167, loss 1.00737, acc 0.78125\n",
      "2017-11-06T12:22:42.992455: step 168, loss 0.905133, acc 0.8125\n",
      "2017-11-06T12:22:46.939260: step 169, loss 0.754867, acc 0.8125\n",
      "2017-11-06T12:22:50.900074: step 170, loss 0.538282, acc 0.84375\n",
      "2017-11-06T12:22:54.879902: step 171, loss 0.430688, acc 0.875\n",
      "2017-11-06T12:22:58.900760: step 172, loss 0.287785, acc 0.9375\n",
      "2017-11-06T12:23:03.005676: step 173, loss 0.42768, acc 0.875\n",
      "2017-11-06T12:23:07.553909: step 174, loss 0.674874, acc 0.875\n",
      "2017-11-06T12:23:11.606787: step 175, loss 0.310566, acc 0.90625\n",
      "2017-11-06T12:23:15.662668: step 176, loss 0.705214, acc 0.9375\n",
      "2017-11-06T12:23:19.692534: step 177, loss 0.1327, acc 0.9375\n",
      "2017-11-06T12:23:23.839479: step 178, loss 0.834606, acc 0.875\n",
      "2017-11-06T12:23:27.872344: step 179, loss 0.235556, acc 0.9375\n",
      "2017-11-06T12:23:30.389133: step 180, loss 0.0390804, acc 1\n",
      "2017-11-06T12:23:34.377969: step 181, loss 0.591131, acc 0.9375\n",
      "2017-11-06T12:23:38.357795: step 182, loss 0.380752, acc 0.90625\n",
      "2017-11-06T12:23:42.339624: step 183, loss 0.699822, acc 0.875\n",
      "2017-11-06T12:23:46.315448: step 184, loss 0.00645287, acc 1\n",
      "2017-11-06T12:23:50.371331: step 185, loss 0.282679, acc 0.9375\n",
      "2017-11-06T12:23:54.399193: step 186, loss 0.204352, acc 0.96875\n",
      "2017-11-06T12:23:58.349000: step 187, loss 1.45241, acc 0.71875\n",
      "2017-11-06T12:24:02.342837: step 188, loss 0.176324, acc 0.96875\n",
      "2017-11-06T12:24:06.414730: step 189, loss 0.469716, acc 0.9375\n",
      "2017-11-06T12:24:10.670755: step 190, loss 0.555345, acc 0.875\n",
      "2017-11-06T12:24:14.947794: step 191, loss 0.396461, acc 0.90625\n",
      "2017-11-06T12:24:18.927622: step 192, loss 0.536448, acc 0.9375\n",
      "2017-11-06T12:24:22.894441: step 193, loss 0.0322888, acc 1\n",
      "2017-11-06T12:24:26.977342: step 194, loss 0.808424, acc 0.84375\n",
      "2017-11-06T12:24:30.904131: step 195, loss 0.607545, acc 0.875\n",
      "2017-11-06T12:24:35.130134: step 196, loss 0.317488, acc 0.90625\n",
      "2017-11-06T12:24:39.206030: step 197, loss 0.807782, acc 0.84375\n",
      "2017-11-06T12:24:43.282927: step 198, loss 0.851976, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:24:47.265757: step 199, loss 0.176751, acc 0.9375\n",
      "2017-11-06T12:24:51.289616: step 200, loss 0.403976, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:24:53.928491: step 200, loss 1.93997, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-200\n",
      "\n",
      "2017-11-06T12:24:59.404626: step 201, loss 0.19741, acc 0.9375\n",
      "2017-11-06T12:25:03.387456: step 202, loss 0.562385, acc 0.875\n",
      "2017-11-06T12:25:07.381293: step 203, loss 0.971574, acc 0.84375\n",
      "2017-11-06T12:25:11.343109: step 204, loss 1.31418, acc 0.75\n",
      "2017-11-06T12:25:15.534086: step 205, loss 0.331878, acc 0.9375\n",
      "2017-11-06T12:25:19.775099: step 206, loss 0.611263, acc 0.875\n",
      "2017-11-06T12:25:23.741674: step 207, loss 0.961904, acc 0.8125\n",
      "2017-11-06T12:25:27.736513: step 208, loss 1.02704, acc 0.84375\n",
      "2017-11-06T12:25:31.702331: step 209, loss 0.0716375, acc 0.96875\n",
      "2017-11-06T12:25:35.719185: step 210, loss 0.775947, acc 0.90625\n",
      "2017-11-06T12:25:39.736040: step 211, loss 0.824209, acc 0.8125\n",
      "2017-11-06T12:25:43.747286: step 212, loss 0.915656, acc 0.90625\n",
      "2017-11-06T12:25:47.650059: step 213, loss 0.417084, acc 0.96875\n",
      "2017-11-06T12:25:51.592860: step 214, loss 0.758714, acc 0.84375\n",
      "2017-11-06T12:25:55.495632: step 215, loss 0.612719, acc 0.78125\n",
      "2017-11-06T12:25:58.019427: step 216, loss 0.267557, acc 0.95\n",
      "2017-11-06T12:26:02.009261: step 217, loss 0.812507, acc 0.84375\n",
      "2017-11-06T12:26:05.993091: step 218, loss 0.43668, acc 0.90625\n",
      "2017-11-06T12:26:09.993935: step 219, loss 0.153263, acc 0.9375\n",
      "2017-11-06T12:26:14.094848: step 220, loss 0.533034, acc 0.9375\n",
      "2017-11-06T12:26:18.049658: step 221, loss 0.48955, acc 0.84375\n",
      "2017-11-06T12:26:22.290672: step 222, loss 0.11396, acc 0.9375\n",
      "2017-11-06T12:26:26.494658: step 223, loss 0.429461, acc 0.96875\n",
      "2017-11-06T12:26:30.447467: step 224, loss 0.751311, acc 0.8125\n",
      "2017-11-06T12:26:34.639446: step 225, loss 0.396425, acc 0.90625\n",
      "2017-11-06T12:26:38.592254: step 226, loss 0.129433, acc 0.9375\n",
      "2017-11-06T12:26:42.550066: step 227, loss 0.62012, acc 0.84375\n",
      "2017-11-06T12:26:46.459845: step 228, loss 0.0879363, acc 0.96875\n",
      "2017-11-06T12:26:50.411652: step 229, loss 0.538962, acc 0.90625\n",
      "2017-11-06T12:26:54.387479: step 230, loss 0.722555, acc 0.875\n",
      "2017-11-06T12:26:58.286248: step 231, loss 0.578013, acc 0.84375\n",
      "2017-11-06T12:27:02.239058: step 232, loss 0.279425, acc 0.84375\n",
      "2017-11-06T12:27:06.167850: step 233, loss 0.741574, acc 0.84375\n",
      "2017-11-06T12:27:10.118657: step 234, loss 0.892412, acc 0.84375\n",
      "2017-11-06T12:27:14.069462: step 235, loss 0.723329, acc 0.84375\n",
      "2017-11-06T12:27:18.025273: step 236, loss 0.420044, acc 0.8125\n",
      "2017-11-06T12:27:21.980083: step 237, loss 0.982499, acc 0.90625\n",
      "2017-11-06T12:27:26.091004: step 238, loss 0.310977, acc 0.9375\n",
      "2017-11-06T12:27:30.389059: step 239, loss 0.291887, acc 0.875\n",
      "2017-11-06T12:27:34.338865: step 240, loss 0.146599, acc 0.96875\n",
      "2017-11-06T12:27:38.266656: step 241, loss 0.433037, acc 0.84375\n",
      "2017-11-06T12:27:42.203453: step 242, loss 0.141551, acc 0.96875\n",
      "2017-11-06T12:27:46.234799: step 243, loss 0.527231, acc 0.875\n",
      "2017-11-06T12:27:50.196614: step 244, loss 0.275151, acc 0.9375\n",
      "2017-11-06T12:27:54.140418: step 245, loss 0.406073, acc 0.9375\n",
      "2017-11-06T12:27:58.115260: step 246, loss 0.523409, acc 0.90625\n",
      "2017-11-06T12:28:02.030022: step 247, loss 0.680934, acc 0.84375\n",
      "2017-11-06T12:28:05.959815: step 248, loss 0.277096, acc 0.90625\n",
      "2017-11-06T12:28:10.012694: step 249, loss 0.958202, acc 0.8125\n",
      "2017-11-06T12:28:13.980514: step 250, loss 0.422806, acc 0.90625\n",
      "2017-11-06T12:28:17.992364: step 251, loss 0.613285, acc 0.90625\n",
      "2017-11-06T12:28:20.556979: step 252, loss 0.431857, acc 0.95\n",
      "2017-11-06T12:28:24.760966: step 253, loss 0.772478, acc 0.875\n",
      "2017-11-06T12:28:29.106053: step 254, loss 0.802675, acc 0.84375\n",
      "2017-11-06T12:28:33.460148: step 255, loss 0.749729, acc 0.875\n",
      "2017-11-06T12:28:37.795229: step 256, loss 0.98395, acc 0.8125\n",
      "2017-11-06T12:28:41.808078: step 257, loss 0.647241, acc 0.84375\n",
      "2017-11-06T12:28:45.790909: step 258, loss 0.488531, acc 0.9375\n",
      "2017-11-06T12:28:49.759729: step 259, loss 0.445991, acc 0.875\n",
      "2017-11-06T12:28:53.679513: step 260, loss 0.0166922, acc 1\n",
      "2017-11-06T12:28:57.634323: step 261, loss 0.270668, acc 0.875\n",
      "2017-11-06T12:29:01.653179: step 262, loss 0.504065, acc 0.84375\n",
      "2017-11-06T12:29:05.641013: step 263, loss 0.132377, acc 0.96875\n",
      "2017-11-06T12:29:09.601827: step 264, loss 0.411776, acc 0.90625\n",
      "2017-11-06T12:29:13.606673: step 265, loss 0.167411, acc 0.90625\n",
      "2017-11-06T12:29:17.619524: step 266, loss 0.19556, acc 0.96875\n",
      "2017-11-06T12:29:21.608359: step 267, loss 0.373212, acc 0.90625\n",
      "2017-11-06T12:29:25.606199: step 268, loss 0.746921, acc 0.84375\n",
      "2017-11-06T12:29:29.551002: step 269, loss 0.166233, acc 0.90625\n",
      "2017-11-06T12:29:33.488800: step 270, loss 0.935374, acc 0.8125\n",
      "2017-11-06T12:29:37.558691: step 271, loss 0.212722, acc 0.96875\n",
      "2017-11-06T12:29:41.885766: step 272, loss 0.280883, acc 0.9375\n",
      "2017-11-06T12:29:45.856587: step 273, loss 0.185576, acc 0.9375\n",
      "2017-11-06T12:29:49.949537: step 274, loss 0.409341, acc 0.9375\n",
      "2017-11-06T12:29:53.917357: step 275, loss 0.174051, acc 0.96875\n",
      "2017-11-06T12:29:57.879172: step 276, loss 0.276646, acc 0.90625\n",
      "2017-11-06T12:30:02.131192: step 277, loss 0.00908998, acc 1\n",
      "2017-11-06T12:30:06.077998: step 278, loss 0.444545, acc 0.96875\n",
      "2017-11-06T12:30:10.018797: step 279, loss 0.73516, acc 0.875\n",
      "2017-11-06T12:30:13.955596: step 280, loss 0.780496, acc 0.875\n",
      "2017-11-06T12:30:17.962442: step 281, loss 0.602983, acc 0.84375\n",
      "2017-11-06T12:30:21.941268: step 282, loss 0.813014, acc 0.84375\n",
      "2017-11-06T12:30:25.956121: step 283, loss 0.71361, acc 0.90625\n",
      "2017-11-06T12:30:29.983983: step 284, loss 0.220917, acc 0.96875\n",
      "2017-11-06T12:30:34.180965: step 285, loss 0.572699, acc 0.875\n",
      "2017-11-06T12:30:38.226840: step 286, loss 0.0438724, acc 1\n",
      "2017-11-06T12:30:42.211672: step 287, loss 0.552081, acc 0.875\n",
      "2017-11-06T12:30:45.093719: step 288, loss 1.2592, acc 0.75\n",
      "2017-11-06T12:30:49.337735: step 289, loss 0.68285, acc 0.8125\n",
      "2017-11-06T12:30:53.328570: step 290, loss 0.40696, acc 0.9375\n",
      "2017-11-06T12:30:57.293390: step 291, loss 0.614703, acc 0.90625\n",
      "2017-11-06T12:31:01.302237: step 292, loss 0.509503, acc 0.84375\n",
      "2017-11-06T12:31:05.262050: step 293, loss 0.22449, acc 0.9375\n",
      "2017-11-06T12:31:09.256888: step 294, loss 0.725989, acc 0.875\n",
      "2017-11-06T12:31:13.249725: step 295, loss 0.355107, acc 0.9375\n",
      "2017-11-06T12:31:17.223549: step 296, loss 0.638864, acc 0.90625\n",
      "2017-11-06T12:31:21.185117: step 297, loss 0.336001, acc 0.90625\n",
      "2017-11-06T12:31:25.226990: step 298, loss 0.442035, acc 0.84375\n",
      "2017-11-06T12:31:29.205817: step 299, loss 0.335882, acc 0.9375\n",
      "2017-11-06T12:31:33.135609: step 300, loss 0.390435, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:31:35.805507: step 300, loss 2.07959, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-300\n",
      "\n",
      "2017-11-06T12:31:41.335681: step 301, loss 0.120148, acc 0.9375\n",
      "2017-11-06T12:31:45.433592: step 302, loss 0.574276, acc 0.9375\n",
      "2017-11-06T12:31:49.630575: step 303, loss 0.241931, acc 0.90625\n",
      "2017-11-06T12:31:53.931631: step 304, loss 0.552475, acc 0.8125\n",
      "2017-11-06T12:31:57.904454: step 305, loss 0.540704, acc 0.9375\n",
      "2017-11-06T12:32:01.890288: step 306, loss 0.755554, acc 0.9375\n",
      "2017-11-06T12:32:05.939163: step 307, loss 0.661488, acc 0.90625\n",
      "2017-11-06T12:32:10.018062: step 308, loss 0.175659, acc 0.9375\n",
      "2017-11-06T12:32:13.963866: step 309, loss 0.892315, acc 0.875\n",
      "2017-11-06T12:32:17.997731: step 310, loss 0.584144, acc 0.78125\n",
      "2017-11-06T12:32:22.013586: step 311, loss 1.27129, acc 0.8125\n",
      "2017-11-06T12:32:26.037443: step 312, loss 1.00281, acc 0.8125\n",
      "2017-11-06T12:32:30.143362: step 313, loss 0.118498, acc 0.96875\n",
      "2017-11-06T12:32:34.341344: step 314, loss 0.805209, acc 0.84375\n",
      "2017-11-06T12:32:38.369207: step 315, loss 0.345025, acc 0.90625\n",
      "2017-11-06T12:32:42.323015: step 316, loss 0.47748, acc 0.875\n",
      "2017-11-06T12:32:46.329862: step 317, loss 0.639222, acc 0.8125\n",
      "2017-11-06T12:32:50.330705: step 318, loss 0.961362, acc 0.75\n",
      "2017-11-06T12:32:54.451632: step 319, loss 0.321564, acc 0.90625\n",
      "2017-11-06T12:32:58.797720: step 320, loss 0.556157, acc 0.875\n",
      "2017-11-06T12:33:02.811573: step 321, loss 0.95348, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:33:06.838434: step 322, loss 0.179227, acc 0.9375\n",
      "2017-11-06T12:33:10.786239: step 323, loss 1.11905, acc 0.8125\n",
      "2017-11-06T12:33:13.365072: step 324, loss 0.276696, acc 0.9\n",
      "2017-11-06T12:33:17.414951: step 325, loss 0.283834, acc 0.90625\n",
      "2017-11-06T12:33:21.403784: step 326, loss 0.125585, acc 0.96875\n",
      "2017-11-06T12:33:25.622781: step 327, loss 0.378398, acc 0.9375\n",
      "2017-11-06T12:33:29.609614: step 328, loss 0.142459, acc 0.90625\n",
      "2017-11-06T12:33:33.592444: step 329, loss 0.390533, acc 0.9375\n",
      "2017-11-06T12:33:37.692357: step 330, loss 0.130067, acc 0.96875\n",
      "2017-11-06T12:33:41.844308: step 331, loss 0.230969, acc 0.90625\n",
      "2017-11-06T12:33:45.915199: step 332, loss 0.725844, acc 0.875\n",
      "2017-11-06T12:33:50.012110: step 333, loss 0.359868, acc 0.9375\n",
      "2017-11-06T12:33:54.129036: step 334, loss 0.274695, acc 0.9375\n",
      "2017-11-06T12:33:58.233953: step 335, loss 0.220577, acc 0.9375\n",
      "2017-11-06T12:34:02.751162: step 336, loss 0.0941331, acc 0.96875\n",
      "2017-11-06T12:34:06.908116: step 337, loss 0.13617, acc 0.9375\n",
      "2017-11-06T12:34:11.123111: step 338, loss 0.0514463, acc 1\n",
      "2017-11-06T12:34:15.198006: step 339, loss 1.07412, acc 0.8125\n",
      "2017-11-06T12:34:19.350957: step 340, loss 0.300447, acc 0.9375\n",
      "2017-11-06T12:34:23.442840: step 341, loss 0.488723, acc 0.875\n",
      "2017-11-06T12:34:27.687855: step 342, loss 1.2714, acc 0.6875\n",
      "2017-11-06T12:34:31.805781: step 343, loss 0.750115, acc 0.875\n",
      "2017-11-06T12:34:36.307982: step 344, loss 0.110457, acc 0.90625\n",
      "2017-11-06T12:34:40.423904: step 345, loss 0.342479, acc 0.78125\n",
      "2017-11-06T12:34:44.669922: step 346, loss 0.248111, acc 0.9375\n",
      "2017-11-06T12:34:48.753825: step 347, loss 1.17751, acc 0.84375\n",
      "2017-11-06T12:34:52.919783: step 348, loss 1.16467, acc 0.78125\n",
      "2017-11-06T12:34:56.993678: step 349, loss 0.383015, acc 0.875\n",
      "2017-11-06T12:35:01.025543: step 350, loss 0.401806, acc 0.90625\n",
      "2017-11-06T12:35:05.124455: step 351, loss 0.184718, acc 0.96875\n",
      "2017-11-06T12:35:09.597634: step 352, loss 0.776998, acc 0.84375\n",
      "2017-11-06T12:35:13.657518: step 353, loss 0.175925, acc 0.9375\n",
      "2017-11-06T12:35:17.632343: step 354, loss 0.166373, acc 0.9375\n",
      "2017-11-06T12:35:21.649197: step 355, loss 0.233527, acc 0.875\n",
      "2017-11-06T12:35:25.729096: step 356, loss 0.499243, acc 0.875\n",
      "2017-11-06T12:35:29.724935: step 357, loss 0.795696, acc 0.84375\n",
      "2017-11-06T12:35:33.721775: step 358, loss 0.709103, acc 0.84375\n",
      "2017-11-06T12:35:37.754641: step 359, loss 0.56587, acc 0.875\n",
      "2017-11-06T12:35:40.261421: step 360, loss 0.00187022, acc 1\n",
      "2017-11-06T12:35:44.217234: step 361, loss 0.208244, acc 0.9375\n",
      "2017-11-06T12:35:48.242094: step 362, loss 0.29384, acc 0.9375\n",
      "2017-11-06T12:35:52.260948: step 363, loss 0.637574, acc 0.9375\n",
      "2017-11-06T12:35:56.261790: step 364, loss 0.13175, acc 0.96875\n",
      "2017-11-06T12:36:00.295657: step 365, loss 0.331548, acc 0.9375\n",
      "2017-11-06T12:36:04.361546: step 366, loss 0.421989, acc 0.9375\n",
      "2017-11-06T12:36:08.361388: step 367, loss 0.772727, acc 0.84375\n",
      "2017-11-06T12:36:12.656440: step 368, loss 0.0289431, acc 1\n",
      "2017-11-06T12:36:17.056567: step 369, loss 0.880525, acc 0.875\n",
      "2017-11-06T12:36:21.082426: step 370, loss 0.416152, acc 0.875\n",
      "2017-11-06T12:36:25.153319: step 371, loss 0.0152991, acc 1\n",
      "2017-11-06T12:36:29.182182: step 372, loss 0.176378, acc 0.96875\n",
      "2017-11-06T12:36:33.365154: step 373, loss 1.0484, acc 0.75\n",
      "2017-11-06T12:36:37.539120: step 374, loss 0.0579208, acc 0.96875\n",
      "2017-11-06T12:36:41.641035: step 375, loss 1.0652, acc 0.78125\n",
      "2017-11-06T12:36:45.665895: step 376, loss 0.544214, acc 0.875\n",
      "2017-11-06T12:36:49.790826: step 377, loss 1.07623, acc 0.875\n",
      "2017-11-06T12:36:53.842705: step 378, loss 0.219746, acc 0.90625\n",
      "2017-11-06T12:36:57.932610: step 379, loss 0.916597, acc 0.875\n",
      "2017-11-06T12:37:01.942460: step 380, loss 0.429463, acc 0.90625\n",
      "2017-11-06T12:37:05.957313: step 381, loss 0.248182, acc 0.9375\n",
      "2017-11-06T12:37:10.021200: step 382, loss 0.221442, acc 0.90625\n",
      "2017-11-06T12:37:14.054067: step 383, loss 0.549106, acc 0.84375\n",
      "2017-11-06T12:37:18.329103: step 384, loss 0.537202, acc 0.9375\n",
      "2017-11-06T12:37:22.700960: step 385, loss 0.516414, acc 0.8125\n",
      "2017-11-06T12:37:26.763847: step 386, loss 0.447007, acc 0.90625\n",
      "2017-11-06T12:37:30.820729: step 387, loss 0.0696727, acc 0.96875\n",
      "2017-11-06T12:37:34.815568: step 388, loss 0.947153, acc 0.8125\n",
      "2017-11-06T12:37:38.855438: step 389, loss 0.83882, acc 0.90625\n",
      "2017-11-06T12:37:42.886302: step 390, loss 0.213615, acc 0.9375\n",
      "2017-11-06T12:37:46.914164: step 391, loss 0.330558, acc 0.9375\n",
      "2017-11-06T12:37:50.982054: step 392, loss 0.344206, acc 0.96875\n",
      "2017-11-06T12:37:54.994906: step 393, loss 0.216171, acc 0.9375\n",
      "2017-11-06T12:37:59.215113: step 394, loss 0.710176, acc 0.8125\n",
      "2017-11-06T12:38:03.407093: step 395, loss 0.344368, acc 0.90625\n",
      "2017-11-06T12:38:06.003938: step 396, loss 1.12188, acc 0.8\n",
      "2017-11-06T12:38:10.158890: step 397, loss 0.391389, acc 0.9375\n",
      "2017-11-06T12:38:14.273815: step 398, loss 0.251913, acc 0.96875\n",
      "2017-11-06T12:38:18.277658: step 399, loss 0.599229, acc 0.875\n",
      "2017-11-06T12:38:22.384577: step 400, loss 0.222909, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:38:25.376703: step 400, loss 2.06895, acc 0.7\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-400\n",
      "\n",
      "2017-11-06T12:38:31.075002: step 401, loss 0.351717, acc 0.90625\n",
      "2017-11-06T12:38:35.276988: step 402, loss 0.271393, acc 0.875\n",
      "2017-11-06T12:38:39.300847: step 403, loss 0.373511, acc 0.84375\n",
      "2017-11-06T12:38:43.358731: step 404, loss 0.406578, acc 0.90625\n",
      "2017-11-06T12:38:47.425620: step 405, loss 0.20369, acc 0.96875\n",
      "2017-11-06T12:38:51.531537: step 406, loss 0.411147, acc 0.875\n",
      "2017-11-06T12:38:55.545390: step 407, loss 0.134498, acc 0.96875\n",
      "2017-11-06T12:38:59.556241: step 408, loss 0.396587, acc 0.875\n",
      "2017-11-06T12:39:03.520056: step 409, loss 0.00623715, acc 1\n",
      "2017-11-06T12:39:07.492878: step 410, loss 0.106388, acc 0.9375\n",
      "2017-11-06T12:39:11.461700: step 411, loss 0.301912, acc 0.875\n",
      "2017-11-06T12:39:15.437524: step 412, loss 0.325328, acc 0.90625\n",
      "2017-11-06T12:39:19.465386: step 413, loss 0.212301, acc 0.9375\n",
      "2017-11-06T12:39:23.573305: step 414, loss 0.734342, acc 0.875\n",
      "2017-11-06T12:39:27.972430: step 415, loss 0.206976, acc 0.90625\n",
      "2017-11-06T12:39:32.416588: step 416, loss 0.995798, acc 0.75\n",
      "2017-11-06T12:39:36.429441: step 417, loss 1.27589, acc 0.84375\n",
      "2017-11-06T12:39:40.493326: step 418, loss 0.129592, acc 0.96875\n",
      "2017-11-06T12:39:44.447136: step 419, loss 0.550155, acc 0.875\n",
      "2017-11-06T12:39:48.456985: step 420, loss 0.356051, acc 0.875\n",
      "2017-11-06T12:39:52.481845: step 421, loss 0.162584, acc 0.96875\n",
      "2017-11-06T12:39:56.446662: step 422, loss 0.260312, acc 0.96875\n",
      "2017-11-06T12:40:00.645646: step 423, loss 0.20624, acc 0.9375\n",
      "2017-11-06T12:40:04.839627: step 424, loss 1.06845, acc 0.71875\n",
      "2017-11-06T12:40:08.817471: step 425, loss 0.420191, acc 0.9375\n",
      "2017-11-06T12:40:12.844314: step 426, loss 0.21589, acc 0.9375\n",
      "2017-11-06T12:40:16.851161: step 427, loss 0.0508095, acc 0.96875\n",
      "2017-11-06T12:40:20.881024: step 428, loss 0.224087, acc 0.9375\n",
      "2017-11-06T12:40:24.895641: step 429, loss 0.416466, acc 0.875\n",
      "2017-11-06T12:40:28.947520: step 430, loss 0.77735, acc 0.8125\n",
      "2017-11-06T12:40:33.190535: step 431, loss 0.299848, acc 0.90625\n",
      "2017-11-06T12:40:36.091598: step 432, loss 0.725581, acc 0.85\n",
      "2017-11-06T12:40:40.255555: step 433, loss 0.51189, acc 0.84375\n",
      "2017-11-06T12:40:44.291422: step 434, loss 0.154071, acc 0.96875\n",
      "2017-11-06T12:40:48.283259: step 435, loss 0.0888286, acc 0.96875\n",
      "2017-11-06T12:40:52.276115: step 436, loss 0.840873, acc 0.90625\n",
      "2017-11-06T12:40:56.265931: step 437, loss 0.228549, acc 0.90625\n",
      "2017-11-06T12:41:00.309804: step 438, loss 0.241315, acc 0.875\n",
      "2017-11-06T12:41:04.371690: step 439, loss 0.127542, acc 0.9375\n",
      "2017-11-06T12:41:08.382540: step 440, loss 0.00565896, acc 1\n",
      "2017-11-06T12:41:13.289027: step 441, loss 1.11273, acc 0.78125\n",
      "2017-11-06T12:41:17.776215: step 442, loss 0.934762, acc 0.84375\n",
      "2017-11-06T12:41:21.830097: step 443, loss 0.127863, acc 0.9375\n",
      "2017-11-06T12:41:25.854955: step 444, loss 0.290997, acc 0.9375\n",
      "2017-11-06T12:41:29.919843: step 445, loss 0.000727546, acc 1\n",
      "2017-11-06T12:41:33.966719: step 446, loss 0.741304, acc 0.84375\n",
      "2017-11-06T12:41:38.048619: step 447, loss 0.155285, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:41:42.438738: step 448, loss 0.545058, acc 0.875\n",
      "2017-11-06T12:41:46.530648: step 449, loss 0.46651, acc 0.875\n",
      "2017-11-06T12:41:50.545499: step 450, loss 0.182397, acc 0.9375\n",
      "2017-11-06T12:41:54.572360: step 451, loss 0.149303, acc 0.96875\n",
      "2017-11-06T12:41:58.501151: step 452, loss 0.619628, acc 0.84375\n",
      "2017-11-06T12:42:02.514003: step 453, loss 0.334898, acc 0.875\n",
      "2017-11-06T12:42:06.517848: step 454, loss 0.156356, acc 0.9375\n",
      "2017-11-06T12:42:10.480663: step 455, loss 0.530122, acc 0.84375\n",
      "2017-11-06T12:42:14.467496: step 456, loss 0.33231, acc 0.875\n",
      "2017-11-06T12:42:18.465337: step 457, loss 0.712076, acc 0.90625\n",
      "2017-11-06T12:42:22.477188: step 458, loss 0.588612, acc 0.84375\n",
      "2017-11-06T12:42:26.516057: step 459, loss 0.139239, acc 0.9375\n",
      "2017-11-06T12:42:30.514898: step 460, loss 0.227034, acc 0.9375\n",
      "2017-11-06T12:42:34.755911: step 461, loss 0.338203, acc 0.875\n",
      "2017-11-06T12:42:38.817798: step 462, loss 0.437154, acc 0.90625\n",
      "2017-11-06T12:42:42.827647: step 463, loss 0.654824, acc 0.90625\n",
      "2017-11-06T12:42:47.178739: step 464, loss 0.0389084, acc 1\n",
      "2017-11-06T12:42:51.392734: step 465, loss 0.251588, acc 0.96875\n",
      "2017-11-06T12:42:55.400581: step 466, loss 0.953321, acc 0.8125\n",
      "2017-11-06T12:42:59.395419: step 467, loss 0.221077, acc 0.9375\n",
      "2017-11-06T12:43:01.945231: step 468, loss 0.024043, acc 1\n",
      "2017-11-06T12:43:05.958083: step 469, loss 0.178686, acc 0.90625\n",
      "2017-11-06T12:43:09.952921: step 470, loss 0.0573758, acc 1\n",
      "2017-11-06T12:43:13.994794: step 471, loss 0.338411, acc 0.84375\n",
      "2017-11-06T12:43:18.030660: step 472, loss 0.45007, acc 0.84375\n",
      "2017-11-06T12:43:22.141328: step 473, loss 0.465757, acc 0.875\n",
      "2017-11-06T12:43:26.192206: step 474, loss 0.496754, acc 0.875\n",
      "2017-11-06T12:43:30.213063: step 475, loss 0.583767, acc 0.78125\n",
      "2017-11-06T12:43:34.201897: step 476, loss 0.258496, acc 0.90625\n",
      "2017-11-06T12:43:38.244770: step 477, loss 0.0442481, acc 0.96875\n",
      "2017-11-06T12:43:42.273633: step 478, loss 0.24741, acc 0.90625\n",
      "2017-11-06T12:43:46.267471: step 479, loss 0.728161, acc 0.875\n",
      "2017-11-06T12:43:50.402409: step 480, loss 0.444272, acc 0.9375\n",
      "2017-11-06T12:43:54.791527: step 481, loss 0.441125, acc 0.90625\n",
      "2017-11-06T12:43:58.826394: step 482, loss 0.0798336, acc 0.9375\n",
      "2017-11-06T12:44:02.868266: step 483, loss 0.82154, acc 0.84375\n",
      "2017-11-06T12:44:06.901132: step 484, loss 0.212981, acc 0.9375\n",
      "2017-11-06T12:44:10.899974: step 485, loss 0.527469, acc 0.90625\n",
      "2017-11-06T12:44:14.948850: step 486, loss 0.0491895, acc 0.96875\n",
      "2017-11-06T12:44:18.920672: step 487, loss 0.301413, acc 0.90625\n",
      "2017-11-06T12:44:22.937528: step 488, loss 0.440806, acc 0.90625\n",
      "2017-11-06T12:44:27.008419: step 489, loss 0.379803, acc 0.90625\n",
      "2017-11-06T12:44:31.035280: step 490, loss 0.0845639, acc 0.9375\n",
      "2017-11-06T12:44:35.297308: step 491, loss 0.122606, acc 0.9375\n",
      "2017-11-06T12:44:39.334176: step 492, loss 0.448505, acc 0.875\n",
      "2017-11-06T12:44:43.327013: step 493, loss 0.409074, acc 0.9375\n",
      "2017-11-06T12:44:47.300837: step 494, loss 0.220742, acc 0.96875\n",
      "2017-11-06T12:44:51.228628: step 495, loss 0.487748, acc 0.90625\n",
      "2017-11-06T12:44:55.291515: step 496, loss 0.236349, acc 0.9375\n",
      "2017-11-06T12:44:59.689640: step 497, loss 0.359846, acc 0.90625\n",
      "2017-11-06T12:45:03.801562: step 498, loss 1.05824, acc 0.8125\n",
      "2017-11-06T12:45:07.803405: step 499, loss 0.292818, acc 0.9375\n",
      "2017-11-06T12:45:11.834270: step 500, loss 0.0207951, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:45:14.406096: step 500, loss 1.9084, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-500\n",
      "\n",
      "2017-11-06T12:45:19.621788: step 501, loss 0.60323, acc 0.84375\n",
      "2017-11-06T12:45:23.613622: step 502, loss 0.677629, acc 0.84375\n",
      "2017-11-06T12:45:27.626473: step 503, loss 0.256863, acc 0.9375\n",
      "2017-11-06T12:45:30.230323: step 504, loss 0.173275, acc 0.9\n",
      "2017-11-06T12:45:34.285205: step 505, loss 0.29791, acc 0.9375\n",
      "2017-11-06T12:45:38.283045: step 506, loss 0.317015, acc 0.90625\n",
      "2017-11-06T12:45:42.346932: step 507, loss 0.408065, acc 0.90625\n",
      "2017-11-06T12:45:46.331764: step 508, loss 0.719082, acc 0.875\n",
      "2017-11-06T12:45:50.352621: step 509, loss 0.174457, acc 0.9375\n",
      "2017-11-06T12:45:54.378482: step 510, loss 0.403532, acc 0.84375\n",
      "2017-11-06T12:45:58.330290: step 511, loss 0.429454, acc 0.90625\n",
      "2017-11-06T12:46:02.501253: step 512, loss 0.205579, acc 0.9375\n",
      "2017-11-06T12:46:06.960421: step 513, loss 0.0157754, acc 1\n",
      "2017-11-06T12:46:10.975274: step 514, loss 0.246401, acc 0.9375\n",
      "2017-11-06T12:46:14.985123: step 515, loss 0.0111025, acc 1\n",
      "2017-11-06T12:46:18.959947: step 516, loss 0.226088, acc 0.9375\n",
      "2017-11-06T12:46:23.023629: step 517, loss 0.337227, acc 0.9375\n",
      "2017-11-06T12:46:26.994451: step 518, loss 0.289846, acc 0.84375\n",
      "2017-11-06T12:46:30.961270: step 519, loss 0.605741, acc 0.78125\n",
      "2017-11-06T12:46:35.125228: step 520, loss 0.295547, acc 0.90625\n",
      "2017-11-06T12:46:39.139080: step 521, loss 0.123774, acc 0.9375\n",
      "2017-11-06T12:46:43.088886: step 522, loss 0.285017, acc 0.9375\n",
      "2017-11-06T12:46:46.985655: step 523, loss 0.104046, acc 0.9375\n",
      "2017-11-06T12:46:50.953476: step 524, loss 0.0806384, acc 0.96875\n",
      "2017-11-06T12:46:54.906284: step 525, loss 0.0618506, acc 1\n",
      "2017-11-06T12:46:58.910128: step 526, loss 0.318419, acc 0.90625\n",
      "2017-11-06T12:47:02.893960: step 527, loss 0.0150385, acc 1\n",
      "2017-11-06T12:47:06.959848: step 528, loss 0.224548, acc 0.875\n",
      "2017-11-06T12:47:11.366979: step 529, loss 0.391019, acc 0.875\n",
      "2017-11-06T12:47:15.326793: step 530, loss 0.266402, acc 0.9375\n",
      "2017-11-06T12:47:19.305620: step 531, loss 0.57988, acc 0.84375\n",
      "2017-11-06T12:47:23.253425: step 532, loss 0.661099, acc 0.875\n",
      "2017-11-06T12:47:27.176215: step 533, loss 0.220239, acc 0.96875\n",
      "2017-11-06T12:47:31.164047: step 534, loss 0.573519, acc 0.84375\n",
      "2017-11-06T12:47:35.116856: step 535, loss 0.294714, acc 0.90625\n",
      "2017-11-06T12:47:39.105689: step 536, loss 0.120679, acc 0.9375\n",
      "2017-11-06T12:47:43.059499: step 537, loss 0.307488, acc 0.90625\n",
      "2017-11-06T12:47:46.984287: step 538, loss 0.458267, acc 0.90625\n",
      "2017-11-06T12:47:50.911077: step 539, loss 0.123208, acc 0.96875\n",
      "2017-11-06T12:47:53.436873: step 540, loss 0.707165, acc 0.85\n",
      "2017-11-06T12:47:57.376673: step 541, loss 0.192919, acc 0.9375\n",
      "2017-11-06T12:48:01.304463: step 542, loss 0.0663444, acc 0.96875\n",
      "2017-11-06T12:48:05.262274: step 543, loss 0.413202, acc 0.875\n",
      "2017-11-06T12:48:09.243398: step 544, loss 0.398781, acc 0.84375\n",
      "2017-11-06T12:48:13.341310: step 545, loss 0.509229, acc 0.84375\n",
      "2017-11-06T12:48:17.718420: step 546, loss 0.283459, acc 0.9375\n",
      "2017-11-06T12:48:21.681237: step 547, loss 0.120454, acc 0.9375\n",
      "2017-11-06T12:48:25.800163: step 548, loss 0.208511, acc 0.96875\n",
      "2017-11-06T12:48:29.927096: step 549, loss 0.063395, acc 0.96875\n",
      "2017-11-06T12:48:34.101062: step 550, loss 0.144893, acc 0.9375\n",
      "2017-11-06T12:48:38.109909: step 551, loss 0.329715, acc 0.9375\n",
      "2017-11-06T12:48:42.121760: step 552, loss 0.442962, acc 0.90625\n",
      "2017-11-06T12:48:46.137613: step 553, loss 0.774751, acc 0.84375\n",
      "2017-11-06T12:48:50.123445: step 554, loss 0.35917, acc 0.90625\n",
      "2017-11-06T12:48:54.095268: step 555, loss 0.427464, acc 0.90625\n",
      "2017-11-06T12:48:58.118125: step 556, loss 0.151713, acc 0.9375\n",
      "2017-11-06T12:49:02.081943: step 557, loss 0.107647, acc 0.96875\n",
      "2017-11-06T12:49:06.050762: step 558, loss 0.279681, acc 0.875\n",
      "2017-11-06T12:49:10.050605: step 559, loss 0.272115, acc 0.90625\n",
      "2017-11-06T12:49:14.108488: step 560, loss 0.189564, acc 0.875\n",
      "2017-11-06T12:49:18.143355: step 561, loss 0.399733, acc 0.90625\n",
      "2017-11-06T12:49:22.547484: step 562, loss 0.455632, acc 0.875\n",
      "2017-11-06T12:49:26.570109: step 563, loss 0.206237, acc 0.9375\n",
      "2017-11-06T12:49:30.502902: step 564, loss 0.0825271, acc 0.9375\n",
      "2017-11-06T12:49:34.470722: step 565, loss 0.0623938, acc 1\n",
      "2017-11-06T12:49:38.423529: step 566, loss 0.296307, acc 0.84375\n",
      "2017-11-06T12:49:42.390348: step 567, loss 0.0722709, acc 0.96875\n",
      "2017-11-06T12:49:46.392192: step 568, loss 0.659083, acc 0.84375\n",
      "2017-11-06T12:49:50.362014: step 569, loss 0.112542, acc 0.96875\n",
      "2017-11-06T12:49:54.297810: step 570, loss 0.0252466, acc 1\n",
      "2017-11-06T12:49:58.278637: step 571, loss 0.996282, acc 0.8125\n",
      "2017-11-06T12:50:02.515650: step 572, loss 0.343117, acc 0.90625\n",
      "2017-11-06T12:50:06.482467: step 573, loss 0.571904, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:50:10.491315: step 574, loss 0.379103, acc 0.90625\n",
      "2017-11-06T12:50:14.431114: step 575, loss 0.434691, acc 0.90625\n",
      "2017-11-06T12:50:16.965916: step 576, loss 0.485979, acc 0.85\n",
      "2017-11-06T12:50:20.984771: step 577, loss 0.307115, acc 0.84375\n",
      "2017-11-06T12:50:25.299837: step 578, loss 0.184941, acc 0.90625\n",
      "2017-11-06T12:50:29.578878: step 579, loss 0.244008, acc 0.9375\n",
      "2017-11-06T12:50:33.750844: step 580, loss 0.183884, acc 0.96875\n",
      "2017-11-06T12:50:37.836746: step 581, loss 0.25131, acc 0.96875\n",
      "2017-11-06T12:50:41.782548: step 582, loss 0.152698, acc 0.90625\n",
      "2017-11-06T12:50:45.835429: step 583, loss 0.562547, acc 0.875\n",
      "2017-11-06T12:50:49.823262: step 584, loss 0.267706, acc 0.9375\n",
      "2017-11-06T12:50:53.830109: step 585, loss 0.398047, acc 0.90625\n",
      "2017-11-06T12:50:57.837957: step 586, loss 0.922834, acc 0.84375\n",
      "2017-11-06T12:51:01.750737: step 587, loss 0.415215, acc 0.84375\n",
      "2017-11-06T12:51:05.646505: step 588, loss 0.111478, acc 0.96875\n",
      "2017-11-06T12:51:09.589306: step 589, loss 0.16267, acc 0.90625\n",
      "2017-11-06T12:51:13.543116: step 590, loss 0.414877, acc 0.84375\n",
      "2017-11-06T12:51:17.506932: step 591, loss 0.629286, acc 0.90625\n",
      "2017-11-06T12:51:21.494766: step 592, loss 0.226204, acc 0.9375\n",
      "2017-11-06T12:51:25.455581: step 593, loss 0.3919, acc 0.875\n",
      "2017-11-06T12:51:29.452422: step 594, loss 0.125079, acc 0.9375\n",
      "2017-11-06T12:51:33.826528: step 595, loss 0.540791, acc 0.90625\n",
      "2017-11-06T12:51:37.831377: step 596, loss 0.0484685, acc 1\n",
      "2017-11-06T12:51:41.767175: step 597, loss 0.117815, acc 0.9375\n",
      "2017-11-06T12:51:45.776020: step 598, loss 0.309824, acc 0.875\n",
      "2017-11-06T12:51:49.762852: step 599, loss 0.851035, acc 0.84375\n",
      "2017-11-06T12:51:53.707655: step 600, loss 0.317533, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:51:56.254466: step 600, loss 1.80376, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-600\n",
      "\n",
      "2017-11-06T12:52:02.269860: step 601, loss 0.605751, acc 0.875\n",
      "2017-11-06T12:52:06.208659: step 602, loss 0.478399, acc 0.90625\n",
      "2017-11-06T12:52:10.215506: step 603, loss 0.326951, acc 0.9375\n",
      "2017-11-06T12:52:14.351704: step 604, loss 0.171631, acc 0.9375\n",
      "2017-11-06T12:52:18.305534: step 605, loss 0.336915, acc 0.90625\n",
      "2017-11-06T12:52:22.216293: step 606, loss 0.790012, acc 0.875\n",
      "2017-11-06T12:52:26.199933: step 607, loss 0.29205, acc 0.90625\n",
      "2017-11-06T12:52:30.156746: step 608, loss 0.251137, acc 0.90625\n",
      "2017-11-06T12:52:34.335714: step 609, loss 0.0952185, acc 0.96875\n",
      "2017-11-06T12:52:38.728835: step 610, loss 0.30358, acc 0.90625\n",
      "2017-11-06T12:52:42.772708: step 611, loss 0.0607415, acc 0.96875\n",
      "2017-11-06T12:52:45.283494: step 612, loss 0.863206, acc 0.8\n",
      "2017-11-06T12:52:49.239303: step 613, loss 0.505526, acc 0.8125\n",
      "2017-11-06T12:52:53.193112: step 614, loss 0.287658, acc 0.90625\n",
      "2017-11-06T12:52:57.199960: step 615, loss 0.38496, acc 0.875\n",
      "2017-11-06T12:53:01.139758: step 616, loss 0.337473, acc 0.90625\n",
      "2017-11-06T12:53:05.158615: step 617, loss 0.205938, acc 0.9375\n",
      "2017-11-06T12:53:09.181472: step 618, loss 0.389897, acc 0.9375\n",
      "2017-11-06T12:53:13.142287: step 619, loss 0.376963, acc 0.90625\n",
      "2017-11-06T12:53:17.138126: step 620, loss 0.324355, acc 0.9375\n",
      "2017-11-06T12:53:21.116954: step 621, loss 0.17562, acc 0.9375\n",
      "2017-11-06T12:53:25.249891: step 622, loss 0.394216, acc 0.90625\n",
      "2017-11-06T12:53:29.330789: step 623, loss 0.30239, acc 0.9375\n",
      "2017-11-06T12:53:33.284599: step 624, loss 0.503625, acc 0.90625\n",
      "2017-11-06T12:53:37.271432: step 625, loss 0.0899498, acc 0.9375\n",
      "2017-11-06T12:53:41.462410: step 626, loss 0.0853739, acc 0.9375\n",
      "2017-11-06T12:53:45.828512: step 627, loss 0.261013, acc 0.90625\n",
      "2017-11-06T12:53:49.844367: step 628, loss 0.391111, acc 0.875\n",
      "2017-11-06T12:53:53.794172: step 629, loss 0.396338, acc 0.90625\n",
      "2017-11-06T12:53:57.826037: step 630, loss 0.930222, acc 0.8125\n",
      "2017-11-06T12:54:01.762834: step 631, loss 0.726834, acc 0.84375\n",
      "2017-11-06T12:54:05.708637: step 632, loss 0.23912, acc 0.9375\n",
      "2017-11-06T12:54:09.752511: step 633, loss 0.145691, acc 0.90625\n",
      "2017-11-06T12:54:13.668293: step 634, loss 0.146487, acc 0.90625\n",
      "2017-11-06T12:54:17.740556: step 635, loss 0.243782, acc 0.90625\n",
      "2017-11-06T12:54:21.692364: step 636, loss 0.27448, acc 0.90625\n",
      "2017-11-06T12:54:25.728230: step 637, loss 0.273655, acc 0.9375\n",
      "2017-11-06T12:54:29.844154: step 638, loss 0.58122, acc 0.875\n",
      "2017-11-06T12:54:34.052143: step 639, loss 0.313595, acc 0.875\n",
      "2017-11-06T12:54:38.165068: step 640, loss 0.740762, acc 0.8125\n",
      "2017-11-06T12:54:42.138892: step 641, loss 0.403994, acc 0.90625\n",
      "2017-11-06T12:54:46.205779: step 642, loss 0.366984, acc 0.875\n",
      "2017-11-06T12:54:50.556871: step 643, loss 0.0861638, acc 0.96875\n",
      "2017-11-06T12:54:54.560717: step 644, loss 0.0275523, acc 1\n",
      "2017-11-06T12:54:58.502516: step 645, loss 0.382199, acc 0.875\n",
      "2017-11-06T12:55:02.454324: step 646, loss 0.187078, acc 0.90625\n",
      "2017-11-06T12:55:06.422144: step 647, loss 0.0178079, acc 1\n",
      "2017-11-06T12:55:08.933930: step 648, loss 0.179033, acc 0.95\n",
      "2017-11-06T12:55:12.879733: step 649, loss 0.100372, acc 0.96875\n",
      "2017-11-06T12:55:16.892583: step 650, loss 0.0345378, acc 0.96875\n",
      "2017-11-06T12:55:20.877415: step 651, loss 0.482073, acc 0.84375\n",
      "2017-11-06T12:55:24.902037: step 652, loss 0.276603, acc 0.90625\n",
      "2017-11-06T12:55:28.842837: step 653, loss 0.0785407, acc 0.96875\n",
      "2017-11-06T12:55:32.844680: step 654, loss 0.119568, acc 0.9375\n",
      "2017-11-06T12:55:36.941591: step 655, loss 0.520803, acc 0.8125\n",
      "2017-11-06T12:55:40.963450: step 656, loss 0.276696, acc 0.875\n",
      "2017-11-06T12:55:45.016333: step 657, loss 0.350308, acc 0.90625\n",
      "2017-11-06T12:55:48.985150: step 658, loss 0.400574, acc 0.875\n",
      "2017-11-06T12:55:53.167120: step 659, loss 0.385339, acc 0.90625\n",
      "2017-11-06T12:55:57.486190: step 660, loss 0.103482, acc 0.9375\n",
      "2017-11-06T12:56:01.468019: step 661, loss 0.193393, acc 0.90625\n",
      "2017-11-06T12:56:05.393808: step 662, loss 0.229821, acc 0.90625\n",
      "2017-11-06T12:56:09.470705: step 663, loss 0.147827, acc 0.96875\n",
      "2017-11-06T12:56:13.433520: step 664, loss 0.0971944, acc 0.96875\n",
      "2017-11-06T12:56:17.542440: step 665, loss 0.232559, acc 0.9375\n",
      "2017-11-06T12:56:21.611485: step 666, loss 0.282108, acc 0.875\n",
      "2017-11-06T12:56:25.587309: step 667, loss 0.511977, acc 0.875\n",
      "2017-11-06T12:56:29.559132: step 668, loss 0.187316, acc 0.9375\n",
      "2017-11-06T12:56:33.655043: step 669, loss 0.574336, acc 0.90625\n",
      "2017-11-06T12:56:37.725934: step 670, loss 0.272736, acc 0.875\n",
      "2017-11-06T12:56:41.710766: step 671, loss 0.209709, acc 0.9375\n",
      "2017-11-06T12:56:45.725637: step 672, loss 0.326478, acc 0.90625\n",
      "2017-11-06T12:56:49.687435: step 673, loss 0.0136525, acc 1\n",
      "2017-11-06T12:56:53.703287: step 674, loss 0.147106, acc 0.9375\n",
      "2017-11-06T12:56:57.844230: step 675, loss 0.43307, acc 0.875\n",
      "2017-11-06T12:57:02.215335: step 676, loss 0.664376, acc 0.84375\n",
      "2017-11-06T12:57:06.245199: step 677, loss 0.350265, acc 0.90625\n",
      "2017-11-06T12:57:10.249044: step 678, loss 0.298206, acc 0.875\n",
      "2017-11-06T12:57:14.247885: step 679, loss 0.448162, acc 0.875\n",
      "2017-11-06T12:57:18.260737: step 680, loss 0.159109, acc 0.96875\n",
      "2017-11-06T12:57:22.198534: step 681, loss 0.337998, acc 0.875\n",
      "2017-11-06T12:57:26.196375: step 682, loss 0.0407373, acc 0.96875\n",
      "2017-11-06T12:57:30.175202: step 683, loss 0.0483274, acc 1\n",
      "2017-11-06T12:57:32.696994: step 684, loss 0.622888, acc 0.85\n",
      "2017-11-06T12:57:36.688831: step 685, loss 0.167073, acc 0.90625\n",
      "2017-11-06T12:57:40.636637: step 686, loss 0.453927, acc 0.8125\n",
      "2017-11-06T12:57:44.586443: step 687, loss 0.311503, acc 0.875\n",
      "2017-11-06T12:57:48.561266: step 688, loss 0.875762, acc 0.84375\n",
      "2017-11-06T12:57:52.506069: step 689, loss 0.289953, acc 0.9375\n",
      "2017-11-06T12:57:56.507912: step 690, loss 0.56937, acc 0.78125\n",
      "2017-11-06T12:58:00.450714: step 691, loss 0.126358, acc 0.9375\n",
      "2017-11-06T12:58:04.690727: step 692, loss 0.274436, acc 0.90625\n",
      "2017-11-06T12:58:08.936745: step 693, loss 0.199104, acc 0.96875\n",
      "2017-11-06T12:58:12.894557: step 694, loss 0.166072, acc 0.96875\n",
      "2017-11-06T12:58:16.858373: step 695, loss 0.0727426, acc 0.96875\n",
      "2017-11-06T12:58:20.813183: step 696, loss 0.674301, acc 0.8125\n",
      "2017-11-06T12:58:25.086068: step 697, loss 0.189748, acc 0.9375\n",
      "2017-11-06T12:58:29.170990: step 698, loss 0.226045, acc 0.875\n",
      "2017-11-06T12:58:33.260876: step 699, loss 0.261386, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T12:58:37.319760: step 700, loss 0.345198, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T12:58:39.886584: step 700, loss 1.61954, acc 0.666667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-700\n",
      "\n",
      "2017-11-06T12:58:45.542901: step 701, loss 0.364421, acc 0.875\n",
      "2017-11-06T12:58:49.552749: step 702, loss 0.316837, acc 0.875\n",
      "2017-11-06T12:58:53.584615: step 703, loss 0.188953, acc 0.9375\n",
      "2017-11-06T12:58:57.627487: step 704, loss 0.426178, acc 0.90625\n",
      "2017-11-06T12:59:01.624327: step 705, loss 0.463854, acc 0.84375\n",
      "2017-11-06T12:59:05.652189: step 706, loss 0.34057, acc 0.875\n",
      "2017-11-06T12:59:09.872187: step 707, loss 0.0574932, acc 0.96875\n",
      "2017-11-06T12:59:14.188256: step 708, loss 0.257907, acc 0.9375\n",
      "2017-11-06T12:59:18.235129: step 709, loss 0.00404374, acc 1\n",
      "2017-11-06T12:59:22.422105: step 710, loss 0.434008, acc 0.875\n",
      "2017-11-06T12:59:26.422947: step 711, loss 0.0579059, acc 0.96875\n",
      "2017-11-06T12:59:30.440803: step 712, loss 0.218069, acc 0.90625\n",
      "2017-11-06T12:59:34.415626: step 713, loss 0.491234, acc 0.90625\n",
      "2017-11-06T12:59:38.498527: step 714, loss 0.147719, acc 0.9375\n",
      "2017-11-06T12:59:42.521387: step 715, loss 0.417168, acc 0.875\n",
      "2017-11-06T12:59:46.624302: step 716, loss 0.516341, acc 0.90625\n",
      "2017-11-06T12:59:50.695194: step 717, loss 0.394004, acc 0.875\n",
      "2017-11-06T12:59:54.729060: step 718, loss 0.231003, acc 0.84375\n",
      "2017-11-06T12:59:58.775936: step 719, loss 0.211085, acc 0.9375\n",
      "2017-11-06T13:00:01.536898: step 720, loss 0.902138, acc 0.7\n",
      "2017-11-06T13:00:05.626804: step 721, loss 0.150946, acc 0.96875\n",
      "2017-11-06T13:00:09.590620: step 722, loss 0.181291, acc 0.9375\n",
      "2017-11-06T13:00:13.570448: step 723, loss 0.0515967, acc 0.96875\n",
      "2017-11-06T13:00:18.011604: step 724, loss 0.498073, acc 0.875\n",
      "2017-11-06T13:00:22.100509: step 725, loss 0.629838, acc 0.8125\n",
      "2017-11-06T13:00:26.263605: step 726, loss 0.488294, acc 0.875\n",
      "2017-11-06T13:00:30.307480: step 727, loss 0.167468, acc 0.875\n",
      "2017-11-06T13:00:34.491452: step 728, loss 0.0053579, acc 1\n",
      "2017-11-06T13:00:38.470279: step 729, loss 0.00166975, acc 1\n",
      "2017-11-06T13:00:42.454109: step 730, loss 0.315316, acc 0.90625\n",
      "2017-11-06T13:00:46.460956: step 731, loss 0.0665218, acc 0.96875\n",
      "2017-11-06T13:00:50.493822: step 732, loss 0.898451, acc 0.875\n",
      "2017-11-06T13:00:54.533693: step 733, loss 0.356006, acc 0.875\n",
      "2017-11-06T13:00:58.561554: step 734, loss 0.198646, acc 0.875\n",
      "2017-11-06T13:01:02.614434: step 735, loss 0.199772, acc 0.9375\n",
      "2017-11-06T13:01:06.757377: step 736, loss 0.292347, acc 0.96875\n",
      "2017-11-06T13:01:10.785239: step 737, loss 0.409558, acc 0.84375\n",
      "2017-11-06T13:01:14.783083: step 738, loss 0.043814, acc 0.96875\n",
      "2017-11-06T13:01:18.791929: step 739, loss 0.348944, acc 0.875\n",
      "2017-11-06T13:01:23.195058: step 740, loss 0.130903, acc 0.96875\n",
      "2017-11-06T13:01:27.345791: step 741, loss 0.326672, acc 0.875\n",
      "2017-11-06T13:01:31.318614: step 742, loss 0.505872, acc 0.84375\n",
      "2017-11-06T13:01:35.511593: step 743, loss 0.203201, acc 0.9375\n",
      "2017-11-06T13:01:39.698569: step 744, loss 0.186626, acc 0.90625\n",
      "2017-11-06T13:01:43.944585: step 745, loss 0.0868314, acc 0.96875\n",
      "2017-11-06T13:01:47.905399: step 746, loss 0.39992, acc 0.875\n",
      "2017-11-06T13:01:51.955278: step 747, loss 0.0318733, acc 1\n",
      "2017-11-06T13:01:55.902081: step 748, loss 0.446989, acc 0.90625\n",
      "2017-11-06T13:01:59.939950: step 749, loss 0.482891, acc 0.875\n",
      "2017-11-06T13:02:04.191972: step 750, loss 0.502239, acc 0.875\n",
      "2017-11-06T13:02:08.319905: step 751, loss 0.236016, acc 0.90625\n",
      "2017-11-06T13:02:12.333757: step 752, loss 0.219529, acc 0.90625\n",
      "2017-11-06T13:02:16.318588: step 753, loss 0.262296, acc 0.90625\n",
      "2017-11-06T13:02:20.262391: step 754, loss 0.0404974, acc 0.96875\n",
      "2017-11-06T13:02:24.228208: step 755, loss 0.808608, acc 0.90625\n",
      "2017-11-06T13:02:26.978162: step 756, loss 0.378862, acc 0.85\n",
      "2017-11-06T13:02:31.187153: step 757, loss 0.0478972, acc 0.96875\n",
      "2017-11-06T13:02:35.418160: step 758, loss 0.238988, acc 0.9375\n",
      "2017-11-06T13:02:39.405993: step 759, loss 0.0910678, acc 0.96875\n",
      "2017-11-06T13:02:43.382818: step 760, loss 0.357543, acc 0.90625\n",
      "2017-11-06T13:02:47.347635: step 761, loss 0.141115, acc 0.875\n",
      "2017-11-06T13:02:51.298443: step 762, loss 0.228092, acc 0.875\n",
      "2017-11-06T13:02:55.227234: step 763, loss 0.377609, acc 0.8125\n",
      "2017-11-06T13:02:59.229078: step 764, loss 0.287354, acc 0.875\n",
      "2017-11-06T13:03:03.291965: step 765, loss 0.226891, acc 0.875\n",
      "2017-11-06T13:03:07.352850: step 766, loss 0.133355, acc 0.90625\n",
      "2017-11-06T13:03:11.550833: step 767, loss 0.296642, acc 0.875\n",
      "2017-11-06T13:03:15.938951: step 768, loss 0.0378613, acc 0.96875\n",
      "2017-11-06T13:03:19.964811: step 769, loss 0.151122, acc 0.9375\n",
      "2017-11-06T13:03:24.154789: step 770, loss 0.462807, acc 0.875\n",
      "2017-11-06T13:03:28.165639: step 771, loss 0.344432, acc 0.875\n",
      "2017-11-06T13:03:32.318589: step 772, loss 0.315238, acc 0.9375\n",
      "2017-11-06T13:03:36.664680: step 773, loss 0.0442168, acc 1\n",
      "2017-11-06T13:03:40.764590: step 774, loss 0.133971, acc 0.9375\n",
      "2017-11-06T13:03:44.827477: step 775, loss 0.245013, acc 0.875\n",
      "2017-11-06T13:03:48.987433: step 776, loss 0.0752337, acc 0.9375\n",
      "2017-11-06T13:03:53.142386: step 777, loss 0.453041, acc 0.875\n",
      "2017-11-06T13:03:57.200269: step 778, loss 0.250175, acc 0.875\n",
      "2017-11-06T13:04:01.396250: step 779, loss 0.384891, acc 0.875\n",
      "2017-11-06T13:04:05.551202: step 780, loss 0.260449, acc 0.9375\n",
      "2017-11-06T13:04:09.790214: step 781, loss 0.289469, acc 0.875\n",
      "2017-11-06T13:04:13.883122: step 782, loss 0.345544, acc 0.875\n",
      "2017-11-06T13:04:18.112127: step 783, loss 0.0565258, acc 0.96875\n",
      "2017-11-06T13:04:22.175014: step 784, loss 0.0194515, acc 1\n",
      "2017-11-06T13:04:26.373822: step 785, loss 0.313848, acc 0.90625\n",
      "2017-11-06T13:04:30.529775: step 786, loss 0.219871, acc 0.9375\n",
      "2017-11-06T13:04:34.893875: step 787, loss 0.848652, acc 0.84375\n",
      "2017-11-06T13:04:39.256976: step 788, loss 0.354415, acc 0.875\n",
      "2017-11-06T13:04:43.762176: step 789, loss 0.0733338, acc 0.96875\n",
      "2017-11-06T13:04:48.000187: step 790, loss 0.654158, acc 0.78125\n",
      "2017-11-06T13:04:52.008035: step 791, loss 0.14436, acc 0.9375\n",
      "2017-11-06T13:04:54.623894: step 792, loss 0.253185, acc 0.9\n",
      "2017-11-06T13:04:58.606724: step 793, loss 0.194146, acc 0.90625\n",
      "2017-11-06T13:05:02.621577: step 794, loss 0.1561, acc 0.9375\n",
      "2017-11-06T13:05:06.620418: step 795, loss 0.261495, acc 0.90625\n",
      "2017-11-06T13:05:10.676300: step 796, loss 0.241624, acc 0.9375\n",
      "2017-11-06T13:05:14.678143: step 797, loss 0.382899, acc 0.8125\n",
      "2017-11-06T13:05:18.664976: step 798, loss 0.0571363, acc 1\n",
      "2017-11-06T13:05:22.608779: step 799, loss 0.29915, acc 0.875\n",
      "2017-11-06T13:05:26.560587: step 800, loss 0.229182, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:05:29.128411: step 800, loss 1.43375, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-800\n",
      "\n",
      "2017-11-06T13:05:34.473209: step 801, loss 0.345826, acc 0.78125\n",
      "2017-11-06T13:05:38.419013: step 802, loss 0.330657, acc 0.90625\n",
      "2017-11-06T13:05:42.491906: step 803, loss 0.0505666, acc 1\n",
      "2017-11-06T13:05:46.856009: step 804, loss 0.0706449, acc 0.96875\n",
      "2017-11-06T13:05:50.830831: step 805, loss 0.28146, acc 0.90625\n",
      "2017-11-06T13:05:54.738610: step 806, loss 0.0912805, acc 0.96875\n",
      "2017-11-06T13:05:58.652389: step 807, loss 0.162711, acc 0.9375\n",
      "2017-11-06T13:06:02.609201: step 808, loss 0.166853, acc 0.9375\n",
      "2017-11-06T13:06:06.528985: step 809, loss 0.205162, acc 0.9375\n",
      "2017-11-06T13:06:10.491801: step 810, loss 0.119837, acc 0.96875\n",
      "2017-11-06T13:06:14.446611: step 811, loss 0.19078, acc 0.90625\n",
      "2017-11-06T13:06:18.544524: step 812, loss 0.709273, acc 0.8125\n",
      "2017-11-06T13:06:22.470313: step 813, loss 0.280675, acc 0.90625\n",
      "2017-11-06T13:06:26.424122: step 814, loss 0.5215, acc 0.90625\n",
      "2017-11-06T13:06:30.440661: step 815, loss 0.245253, acc 0.9375\n",
      "2017-11-06T13:06:34.642648: step 816, loss 0.212757, acc 0.90625\n",
      "2017-11-06T13:06:38.582447: step 817, loss 0.0748819, acc 0.9375\n",
      "2017-11-06T13:06:42.541260: step 818, loss 0.226495, acc 0.875\n",
      "2017-11-06T13:06:46.503073: step 819, loss 0.339511, acc 0.875\n",
      "2017-11-06T13:06:50.725073: step 820, loss 0.135729, acc 0.96875\n",
      "2017-11-06T13:06:54.875024: step 821, loss 0.153017, acc 0.96875\n",
      "2017-11-06T13:06:58.849846: step 822, loss 0.417552, acc 0.84375\n",
      "2017-11-06T13:07:02.820668: step 823, loss 0.181289, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T13:07:06.781482: step 824, loss 0.183041, acc 0.90625\n",
      "2017-11-06T13:07:10.742297: step 825, loss 0.139322, acc 0.9375\n",
      "2017-11-06T13:07:14.655077: step 826, loss 0.473349, acc 0.84375\n",
      "2017-11-06T13:07:18.588871: step 827, loss 0.251589, acc 0.9375\n",
      "2017-11-06T13:07:21.119671: step 828, loss 0.277497, acc 0.9\n",
      "2017-11-06T13:07:25.121227: step 829, loss 0.553433, acc 0.78125\n",
      "2017-11-06T13:07:29.098053: step 830, loss 0.0795769, acc 0.9375\n",
      "2017-11-06T13:07:33.024843: step 831, loss 0.452341, acc 0.8125\n",
      "2017-11-06T13:07:36.958638: step 832, loss 0.0725436, acc 0.96875\n",
      "2017-11-06T13:07:40.872419: step 833, loss 0.350335, acc 0.84375\n",
      "2017-11-06T13:07:44.818223: step 834, loss 0.166946, acc 0.875\n",
      "2017-11-06T13:07:48.769030: step 835, loss 0.184273, acc 0.90625\n",
      "2017-11-06T13:07:52.681811: step 836, loss 0.0429603, acc 0.96875\n",
      "2017-11-06T13:07:56.935832: step 837, loss 0.200675, acc 0.875\n",
      "2017-11-06T13:08:01.023739: step 838, loss 0.267189, acc 0.90625\n",
      "2017-11-06T13:08:04.981550: step 839, loss 0.0221766, acc 1\n",
      "2017-11-06T13:08:08.894332: step 840, loss 0.244203, acc 0.9375\n",
      "2017-11-06T13:08:12.857146: step 841, loss 0.164685, acc 0.9375\n",
      "2017-11-06T13:08:16.861991: step 842, loss 0.579095, acc 0.875\n",
      "2017-11-06T13:08:20.872841: step 843, loss 0.429498, acc 0.84375\n",
      "2017-11-06T13:08:25.025795: step 844, loss 0.4542, acc 0.84375\n",
      "2017-11-06T13:08:29.227777: step 845, loss 0.407504, acc 0.90625\n",
      "2017-11-06T13:08:33.398835: step 846, loss 0.395126, acc 0.84375\n",
      "2017-11-06T13:08:37.436704: step 847, loss 0.279934, acc 0.90625\n",
      "2017-11-06T13:08:41.359493: step 848, loss 0.435887, acc 0.84375\n",
      "2017-11-06T13:08:45.321307: step 849, loss 0.0835505, acc 1\n",
      "2017-11-06T13:08:49.254101: step 850, loss 0.425622, acc 0.84375\n",
      "2017-11-06T13:08:53.246938: step 851, loss 0.117743, acc 0.9375\n",
      "2017-11-06T13:08:57.203749: step 852, loss 0.478953, acc 0.84375\n",
      "2017-11-06T13:09:01.372711: step 853, loss 0.44191, acc 0.90625\n",
      "2017-11-06T13:09:05.522660: step 854, loss 0.193031, acc 0.90625\n",
      "2017-11-06T13:09:09.468464: step 855, loss 0.193896, acc 0.875\n",
      "2017-11-06T13:09:13.451294: step 856, loss 0.18333, acc 0.875\n",
      "2017-11-06T13:09:17.419114: step 857, loss 0.219047, acc 0.9375\n",
      "2017-11-06T13:09:21.397940: step 858, loss 0.308574, acc 0.875\n",
      "2017-11-06T13:09:25.402786: step 859, loss 0.467391, acc 0.84375\n",
      "2017-11-06T13:09:29.354594: step 860, loss 0.158423, acc 0.9375\n",
      "2017-11-06T13:09:33.304401: step 861, loss 0.557692, acc 0.78125\n",
      "2017-11-06T13:09:37.238195: step 862, loss 0.0528687, acc 0.96875\n",
      "2017-11-06T13:09:41.258052: step 863, loss 0.100817, acc 0.90625\n",
      "2017-11-06T13:09:43.800859: step 864, loss 0.046368, acc 0.95\n",
      "2017-11-06T13:09:47.794696: step 865, loss 0.0461855, acc 1\n",
      "2017-11-06T13:09:51.794540: step 866, loss 0.286533, acc 0.875\n",
      "2017-11-06T13:09:55.762357: step 867, loss 0.0888659, acc 0.9375\n",
      "2017-11-06T13:09:59.696153: step 868, loss 0.0376546, acc 1\n",
      "2017-11-06T13:10:03.872120: step 869, loss 0.43326, acc 0.9375\n",
      "2017-11-06T13:10:08.241224: step 870, loss 0.130674, acc 0.9375\n",
      "2017-11-06T13:10:12.482238: step 871, loss 0.377433, acc 0.875\n",
      "2017-11-06T13:10:16.519106: step 872, loss 0.158989, acc 0.96875\n",
      "2017-11-06T13:10:20.461908: step 873, loss 0.0694519, acc 0.96875\n",
      "2017-11-06T13:10:24.484766: step 874, loss 0.185581, acc 0.9375\n",
      "2017-11-06T13:10:28.396347: step 875, loss 0.175342, acc 0.9375\n",
      "2017-11-06T13:10:32.346153: step 876, loss 0.465186, acc 0.8125\n",
      "2017-11-06T13:10:36.640292: step 877, loss 0.202382, acc 0.9375\n",
      "2017-11-06T13:10:40.559077: step 878, loss 0.289427, acc 0.875\n",
      "2017-11-06T13:10:44.521893: step 879, loss 0.173206, acc 0.9375\n",
      "2017-11-06T13:10:48.468697: step 880, loss 0.247794, acc 0.875\n",
      "2017-11-06T13:10:52.436517: step 881, loss 0.0835747, acc 0.96875\n",
      "2017-11-06T13:10:56.355301: step 882, loss 0.178001, acc 0.90625\n",
      "2017-11-06T13:11:00.293099: step 883, loss 0.312266, acc 0.875\n",
      "2017-11-06T13:11:04.235901: step 884, loss 0.273152, acc 0.84375\n",
      "2017-11-06T13:11:08.231740: step 885, loss 0.412444, acc 0.90625\n",
      "2017-11-06T13:11:12.492768: step 886, loss 0.290889, acc 0.875\n",
      "2017-11-06T13:11:16.710765: step 887, loss 0.297577, acc 0.875\n",
      "2017-11-06T13:11:20.693596: step 888, loss 0.223599, acc 0.9375\n",
      "2017-11-06T13:11:24.652407: step 889, loss 0.269132, acc 0.90625\n",
      "2017-11-06T13:11:28.611220: step 890, loss 0.120107, acc 0.9375\n",
      "2017-11-06T13:11:32.574036: step 891, loss 0.103868, acc 0.9375\n",
      "2017-11-06T13:11:36.499826: step 892, loss 0.162656, acc 0.9375\n",
      "2017-11-06T13:11:40.445629: step 893, loss 0.757265, acc 0.875\n",
      "2017-11-06T13:11:44.385430: step 894, loss 0.155547, acc 0.9375\n",
      "2017-11-06T13:11:48.274191: step 895, loss 0.286456, acc 0.90625\n",
      "2017-11-06T13:11:52.249016: step 896, loss 0.396923, acc 0.90625\n",
      "2017-11-06T13:11:56.233847: step 897, loss 0.184437, acc 0.96875\n",
      "2017-11-06T13:12:00.294734: step 898, loss 0.21571, acc 0.90625\n",
      "2017-11-06T13:12:04.302581: step 899, loss 0.409012, acc 0.84375\n",
      "2017-11-06T13:12:06.815366: step 900, loss 0.0983207, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:12:09.368180: step 900, loss 1.3956, acc 0.683333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-900\n",
      "\n",
      "2017-11-06T13:12:14.711338: step 901, loss 0.0813506, acc 1\n",
      "2017-11-06T13:12:19.412679: step 902, loss 0.190057, acc 0.96875\n",
      "2017-11-06T13:12:23.537609: step 903, loss 0.41286, acc 0.8125\n",
      "2017-11-06T13:12:27.564470: step 904, loss 0.370054, acc 0.90625\n",
      "2017-11-06T13:12:31.587329: step 905, loss 0.15048, acc 0.9375\n",
      "2017-11-06T13:12:35.890386: step 906, loss 0.198372, acc 0.9375\n",
      "2017-11-06T13:12:39.893231: step 907, loss 0.0145291, acc 1\n",
      "2017-11-06T13:12:43.882064: step 908, loss 0.547221, acc 0.875\n",
      "2017-11-06T13:12:47.863894: step 909, loss 0.750659, acc 0.8125\n",
      "2017-11-06T13:12:51.890755: step 910, loss 0.26588, acc 0.90625\n",
      "2017-11-06T13:12:55.871585: step 911, loss 0.178697, acc 0.90625\n",
      "2017-11-06T13:12:59.861420: step 912, loss 0.269568, acc 0.875\n",
      "2017-11-06T13:13:03.926307: step 913, loss 0.37778, acc 0.84375\n",
      "2017-11-06T13:13:07.905134: step 914, loss 0.136968, acc 0.90625\n",
      "2017-11-06T13:13:11.947006: step 915, loss 0.39903, acc 0.875\n",
      "2017-11-06T13:13:16.047920: step 916, loss 0.135771, acc 0.9375\n",
      "2017-11-06T13:13:20.057769: step 917, loss 0.287599, acc 0.90625\n",
      "2017-11-06T13:13:24.406859: step 918, loss 0.100949, acc 0.96875\n",
      "2017-11-06T13:13:28.681695: step 919, loss 0.257623, acc 0.90625\n",
      "2017-11-06T13:13:32.616491: step 920, loss 0.063557, acc 0.96875\n",
      "2017-11-06T13:13:36.626341: step 921, loss 0.177591, acc 0.9375\n",
      "2017-11-06T13:13:40.736260: step 922, loss 0.199904, acc 0.84375\n",
      "2017-11-06T13:13:44.790141: step 923, loss 0.406379, acc 0.8125\n",
      "2017-11-06T13:13:48.798989: step 924, loss 0.105153, acc 0.96875\n",
      "2017-11-06T13:13:52.795830: step 925, loss 0.127571, acc 0.96875\n",
      "2017-11-06T13:13:56.903748: step 926, loss 0.305996, acc 0.9375\n",
      "2017-11-06T13:14:00.962633: step 927, loss 0.118139, acc 0.96875\n",
      "2017-11-06T13:14:04.988493: step 928, loss 0.296198, acc 0.9375\n",
      "2017-11-06T13:14:09.007349: step 929, loss 0.146829, acc 0.9375\n",
      "2017-11-06T13:14:13.044216: step 930, loss 0.189481, acc 0.96875\n",
      "2017-11-06T13:14:17.120113: step 931, loss 0.163432, acc 0.875\n",
      "2017-11-06T13:14:21.180999: step 932, loss 0.356198, acc 0.875\n",
      "2017-11-06T13:14:25.517079: step 933, loss 0.50608, acc 0.875\n",
      "2017-11-06T13:14:30.056304: step 934, loss 0.448612, acc 0.875\n",
      "2017-11-06T13:14:34.548497: step 935, loss 0.260628, acc 0.84375\n",
      "2017-11-06T13:14:37.161353: step 936, loss 0.0964884, acc 0.95\n",
      "2017-11-06T13:14:41.256412: step 937, loss 0.198174, acc 0.875\n",
      "2017-11-06T13:14:45.209221: step 938, loss 0.372761, acc 0.875\n",
      "2017-11-06T13:14:49.162030: step 939, loss 0.340434, acc 0.8125\n",
      "2017-11-06T13:14:53.152865: step 940, loss 0.190268, acc 0.90625\n",
      "2017-11-06T13:14:57.170721: step 941, loss 0.205139, acc 0.96875\n",
      "2017-11-06T13:15:01.188575: step 942, loss 0.311736, acc 0.78125\n",
      "2017-11-06T13:15:05.150390: step 943, loss 0.179035, acc 0.9375\n",
      "2017-11-06T13:15:09.170247: step 944, loss 0.081344, acc 0.9375\n",
      "2017-11-06T13:15:13.107043: step 945, loss 0.279802, acc 0.9375\n",
      "2017-11-06T13:15:17.151918: step 946, loss 0.277676, acc 0.875\n",
      "2017-11-06T13:15:21.224813: step 947, loss 0.109635, acc 0.96875\n",
      "2017-11-06T13:15:25.321723: step 948, loss 0.16505, acc 0.9375\n",
      "2017-11-06T13:15:29.341579: step 949, loss 0.22375, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T13:15:33.459505: step 950, loss 0.333925, acc 0.875\n",
      "2017-11-06T13:15:37.931683: step 951, loss 0.119, acc 0.9375\n",
      "2017-11-06T13:15:41.920517: step 952, loss 0.0570347, acc 0.96875\n",
      "2017-11-06T13:15:45.929366: step 953, loss 0.456266, acc 0.875\n",
      "2017-11-06T13:15:49.997257: step 954, loss 0.137395, acc 0.90625\n",
      "2017-11-06T13:15:54.051136: step 955, loss 0.140037, acc 0.9375\n",
      "2017-11-06T13:15:58.023959: step 956, loss 0.437275, acc 0.84375\n",
      "2017-11-06T13:16:02.065831: step 957, loss 0.0574447, acc 0.96875\n",
      "2017-11-06T13:16:06.114708: step 958, loss 0.0720554, acc 0.96875\n",
      "2017-11-06T13:16:10.108546: step 959, loss 0.0653693, acc 0.9375\n",
      "2017-11-06T13:16:14.110389: step 960, loss 0.0248212, acc 1\n",
      "2017-11-06T13:16:18.105227: step 961, loss 0.368447, acc 0.84375\n",
      "2017-11-06T13:16:22.243167: step 962, loss 0.280248, acc 0.90625\n",
      "2017-11-06T13:16:26.301807: step 963, loss 0.0701121, acc 0.96875\n",
      "2017-11-06T13:16:30.295645: step 964, loss 0.523473, acc 0.8125\n",
      "2017-11-06T13:16:34.536659: step 965, loss 0.246738, acc 0.9375\n",
      "2017-11-06T13:16:38.631568: step 966, loss 0.0576445, acc 1\n",
      "2017-11-06T13:16:43.145776: step 967, loss 0.196537, acc 0.9375\n",
      "2017-11-06T13:16:47.192651: step 968, loss 0.15996, acc 0.90625\n",
      "2017-11-06T13:16:51.201500: step 969, loss 0.618137, acc 0.78125\n",
      "2017-11-06T13:16:55.204346: step 970, loss 0.106459, acc 0.9375\n",
      "2017-11-06T13:16:59.188175: step 971, loss 0.0740283, acc 0.96875\n",
      "2017-11-06T13:17:01.785021: step 972, loss 0.10191, acc 0.95\n",
      "2017-11-06T13:17:05.800873: step 973, loss 0.203081, acc 0.9375\n",
      "2017-11-06T13:17:09.776698: step 974, loss 0.173971, acc 0.9375\n",
      "2017-11-06T13:17:13.800560: step 975, loss 0.184395, acc 0.96875\n",
      "2017-11-06T13:17:17.788391: step 976, loss 0.0123863, acc 1\n",
      "2017-11-06T13:17:21.766219: step 977, loss 0.269598, acc 0.9375\n",
      "2017-11-06T13:17:25.895153: step 978, loss 0.144585, acc 0.9375\n",
      "2017-11-06T13:17:29.911004: step 979, loss 0.108927, acc 0.96875\n",
      "2017-11-06T13:17:33.876823: step 980, loss 0.212923, acc 0.9375\n",
      "2017-11-06T13:17:37.864657: step 981, loss 0.224017, acc 0.90625\n",
      "2017-11-06T13:17:41.885513: step 982, loss 0.0172405, acc 1\n",
      "2017-11-06T13:17:46.106512: step 983, loss 0.212833, acc 0.9375\n",
      "2017-11-06T13:17:50.425583: step 984, loss 0.0259807, acc 1\n",
      "2017-11-06T13:17:54.466453: step 985, loss 0.129785, acc 0.90625\n",
      "2017-11-06T13:17:58.434272: step 986, loss 0.341794, acc 0.875\n",
      "2017-11-06T13:18:02.401092: step 987, loss 0.247228, acc 0.9375\n",
      "2017-11-06T13:18:06.359903: step 988, loss 0.220973, acc 0.90625\n",
      "2017-11-06T13:18:10.340732: step 989, loss 0.197888, acc 0.96875\n",
      "2017-11-06T13:18:14.290538: step 990, loss 0.186853, acc 0.90625\n",
      "2017-11-06T13:18:18.320402: step 991, loss 0.313112, acc 0.8125\n",
      "2017-11-06T13:18:22.431323: step 992, loss 0.169862, acc 0.875\n",
      "2017-11-06T13:18:26.551269: step 993, loss 0.284982, acc 0.9375\n",
      "2017-11-06T13:18:30.563101: step 994, loss 0.120643, acc 0.96875\n",
      "2017-11-06T13:18:34.762085: step 995, loss 0.494928, acc 0.8125\n",
      "2017-11-06T13:18:38.755922: step 996, loss 0.213493, acc 0.96875\n",
      "2017-11-06T13:18:42.851832: step 997, loss 0.326773, acc 0.875\n",
      "2017-11-06T13:18:46.821653: step 998, loss 0.0705489, acc 0.96875\n",
      "2017-11-06T13:18:50.998621: step 999, loss 0.791593, acc 0.75\n",
      "2017-11-06T13:18:55.349712: step 1000, loss 0.146264, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:18:57.981582: step 1000, loss 1.30489, acc 0.683333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-06T13:19:03.332891: step 1001, loss 0.170729, acc 0.9375\n",
      "2017-11-06T13:19:07.351746: step 1002, loss 0.0531363, acc 1\n",
      "2017-11-06T13:19:11.401624: step 1003, loss 0.0914547, acc 0.96875\n",
      "2017-11-06T13:19:15.436491: step 1004, loss 0.258617, acc 0.90625\n",
      "2017-11-06T13:19:19.564425: step 1005, loss 0.363925, acc 0.90625\n",
      "2017-11-06T13:19:23.691356: step 1006, loss 0.293968, acc 0.90625\n",
      "2017-11-06T13:19:27.781015: step 1007, loss 0.248511, acc 0.90625\n",
      "2017-11-06T13:19:30.339833: step 1008, loss 0.327768, acc 0.85\n",
      "2017-11-06T13:19:34.332670: step 1009, loss 0.399548, acc 0.875\n",
      "2017-11-06T13:19:38.355529: step 1010, loss 0.0420547, acc 0.96875\n",
      "2017-11-06T13:19:42.408409: step 1011, loss 0.334924, acc 0.90625\n",
      "2017-11-06T13:19:46.467293: step 1012, loss 0.286798, acc 0.90625\n",
      "2017-11-06T13:19:50.506162: step 1013, loss 0.261403, acc 0.9375\n",
      "2017-11-06T13:19:54.558041: step 1014, loss 0.280993, acc 0.875\n",
      "2017-11-06T13:19:59.093264: step 1015, loss 0.235205, acc 0.9375\n",
      "2017-11-06T13:20:03.457365: step 1016, loss 0.259337, acc 0.875\n",
      "2017-11-06T13:20:07.484225: step 1017, loss 0.327396, acc 0.90625\n",
      "2017-11-06T13:20:11.529101: step 1018, loss 0.332341, acc 0.875\n",
      "2017-11-06T13:20:15.502923: step 1019, loss 0.418124, acc 0.84375\n",
      "2017-11-06T13:20:19.516777: step 1020, loss 0.17157, acc 0.9375\n",
      "2017-11-06T13:20:23.537633: step 1021, loss 0.128996, acc 0.9375\n",
      "2017-11-06T13:20:27.572500: step 1022, loss 0.0706072, acc 0.96875\n",
      "2017-11-06T13:20:31.597359: step 1023, loss 0.306824, acc 0.875\n",
      "2017-11-06T13:20:35.846378: step 1024, loss 0.286109, acc 0.9375\n",
      "2017-11-06T13:20:39.919274: step 1025, loss 0.40424, acc 0.84375\n",
      "2017-11-06T13:20:43.887092: step 1026, loss 0.0284889, acc 0.96875\n",
      "2017-11-06T13:20:48.005017: step 1027, loss 0.171975, acc 0.96875\n",
      "2017-11-06T13:20:51.955826: step 1028, loss 0.155399, acc 0.96875\n",
      "2017-11-06T13:20:55.964673: step 1029, loss 0.145069, acc 0.9375\n",
      "2017-11-06T13:21:00.006545: step 1030, loss 0.255652, acc 0.8125\n",
      "2017-11-06T13:21:04.482726: step 1031, loss 0.17422, acc 0.9375\n",
      "2017-11-06T13:21:08.609658: step 1032, loss 0.191568, acc 0.875\n",
      "2017-11-06T13:21:12.610501: step 1033, loss 0.0681889, acc 0.96875\n",
      "2017-11-06T13:21:16.672387: step 1034, loss 0.169682, acc 0.90625\n",
      "2017-11-06T13:21:20.700249: step 1035, loss 0.301879, acc 0.8125\n",
      "2017-11-06T13:21:24.709097: step 1036, loss 0.241763, acc 0.90625\n",
      "2017-11-06T13:21:28.789997: step 1037, loss 0.0161441, acc 1\n",
      "2017-11-06T13:21:32.811855: step 1038, loss 0.190358, acc 0.90625\n",
      "2017-11-06T13:21:36.796686: step 1039, loss 0.294769, acc 0.875\n",
      "2017-11-06T13:21:40.877586: step 1040, loss 0.409783, acc 0.875\n",
      "2017-11-06T13:21:44.945476: step 1041, loss 0.458704, acc 0.875\n",
      "2017-11-06T13:21:49.025375: step 1042, loss 0.408359, acc 0.84375\n",
      "2017-11-06T13:21:53.025217: step 1043, loss 0.212366, acc 0.90625\n",
      "2017-11-06T13:21:55.649082: step 1044, loss 0.0214822, acc 1\n",
      "2017-11-06T13:21:59.809037: step 1045, loss 0.0576997, acc 1\n",
      "2017-11-06T13:22:03.889936: step 1046, loss 0.130866, acc 0.9375\n",
      "2017-11-06T13:22:08.088921: step 1047, loss 0.220917, acc 0.90625\n",
      "2017-11-06T13:22:12.428004: step 1048, loss 0.363351, acc 0.875\n",
      "2017-11-06T13:22:16.519911: step 1049, loss 0.151665, acc 0.9375\n",
      "2017-11-06T13:22:20.534764: step 1050, loss 0.115132, acc 0.96875\n",
      "2017-11-06T13:22:24.623669: step 1051, loss 0.123482, acc 0.96875\n",
      "2017-11-06T13:22:28.759387: step 1052, loss 0.089374, acc 0.96875\n",
      "2017-11-06T13:22:32.880316: step 1053, loss 0.0818876, acc 1\n",
      "2017-11-06T13:22:37.113324: step 1054, loss 0.148756, acc 0.9375\n",
      "2017-11-06T13:22:41.156196: step 1055, loss 0.180584, acc 0.90625\n",
      "2017-11-06T13:22:45.148118: step 1056, loss 0.251514, acc 0.875\n",
      "2017-11-06T13:22:49.160969: step 1057, loss 0.251549, acc 0.9375\n",
      "2017-11-06T13:22:53.220854: step 1058, loss 0.223417, acc 0.875\n",
      "2017-11-06T13:22:57.203684: step 1059, loss 0.17452, acc 0.90625\n",
      "2017-11-06T13:23:01.166500: step 1060, loss 0.231245, acc 0.9375\n",
      "2017-11-06T13:23:05.263411: step 1061, loss 0.0167852, acc 1\n",
      "2017-11-06T13:23:09.220222: step 1062, loss 0.386665, acc 0.875\n",
      "2017-11-06T13:23:13.407197: step 1063, loss 0.0714965, acc 0.96875\n",
      "2017-11-06T13:23:17.750284: step 1064, loss 0.215721, acc 0.9375\n",
      "2017-11-06T13:23:21.809169: step 1065, loss 0.238584, acc 0.90625\n",
      "2017-11-06T13:23:25.933098: step 1066, loss 0.00978974, acc 1\n",
      "2017-11-06T13:23:30.125076: step 1067, loss 0.145623, acc 0.90625\n",
      "2017-11-06T13:23:34.129922: step 1068, loss 0.202517, acc 0.875\n",
      "2017-11-06T13:23:38.130765: step 1069, loss 0.133085, acc 0.90625\n",
      "2017-11-06T13:23:42.234681: step 1070, loss 0.114273, acc 0.96875\n",
      "2017-11-06T13:23:46.253536: step 1071, loss 0.103726, acc 0.96875\n",
      "2017-11-06T13:23:50.206345: step 1072, loss 0.0926131, acc 0.96875\n",
      "2017-11-06T13:23:54.278239: step 1073, loss 0.213226, acc 0.90625\n",
      "2017-11-06T13:23:58.287086: step 1074, loss 0.31235, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T13:24:02.284927: step 1075, loss 0.502611, acc 0.8125\n",
      "2017-11-06T13:24:06.271760: step 1076, loss 0.157972, acc 0.90625\n",
      "2017-11-06T13:24:10.305627: step 1077, loss 0.4821, acc 0.8125\n",
      "2017-11-06T13:24:14.214403: step 1078, loss 0.304511, acc 0.90625\n",
      "2017-11-06T13:24:18.378362: step 1079, loss 0.191091, acc 0.90625\n",
      "2017-11-06T13:24:21.232390: step 1080, loss 0.340459, acc 0.9\n",
      "2017-11-06T13:24:25.417365: step 1081, loss 0.179165, acc 0.9375\n",
      "2017-11-06T13:24:29.478249: step 1082, loss 0.135529, acc 0.9375\n",
      "2017-11-06T13:24:33.682236: step 1083, loss 0.320212, acc 0.90625\n",
      "2017-11-06T13:24:37.726110: step 1084, loss 0.123408, acc 0.96875\n",
      "2017-11-06T13:24:41.788996: step 1085, loss 0.213881, acc 0.875\n",
      "2017-11-06T13:24:46.029058: step 1086, loss 0.331797, acc 0.9375\n",
      "2017-11-06T13:24:49.966875: step 1087, loss 0.32024, acc 0.90625\n",
      "2017-11-06T13:24:53.912660: step 1088, loss 0.148731, acc 0.96875\n",
      "2017-11-06T13:24:57.928515: step 1089, loss 0.191089, acc 0.9375\n",
      "2017-11-06T13:25:01.931358: step 1090, loss 0.373304, acc 0.875\n",
      "2017-11-06T13:25:05.880163: step 1091, loss 0.221119, acc 0.90625\n",
      "2017-11-06T13:25:09.850985: step 1092, loss 0.109638, acc 0.9375\n",
      "2017-11-06T13:25:13.848826: step 1093, loss 0.196862, acc 0.9375\n",
      "2017-11-06T13:25:17.818646: step 1094, loss 0.18317, acc 0.9375\n",
      "2017-11-06T13:25:21.821491: step 1095, loss 0.0798949, acc 0.96875\n",
      "2017-11-06T13:25:26.128551: step 1096, loss 0.378622, acc 0.84375\n",
      "2017-11-06T13:25:30.317306: step 1097, loss 0.119552, acc 0.96875\n",
      "2017-11-06T13:25:34.368184: step 1098, loss 0.33311, acc 0.875\n",
      "2017-11-06T13:25:38.401049: step 1099, loss 0.314374, acc 0.875\n",
      "2017-11-06T13:25:42.437919: step 1100, loss 0.251554, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:25:44.987730: step 1100, loss 1.19762, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-06T13:25:50.509128: step 1101, loss 0.361713, acc 0.84375\n",
      "2017-11-06T13:25:54.506969: step 1102, loss 0.144363, acc 0.9375\n",
      "2017-11-06T13:25:58.538834: step 1103, loss 0.228589, acc 0.90625\n",
      "2017-11-06T13:26:02.700791: step 1104, loss 0.121255, acc 0.9375\n",
      "2017-11-06T13:26:06.743665: step 1105, loss 0.226091, acc 0.90625\n",
      "2017-11-06T13:26:10.841576: step 1106, loss 0.193513, acc 0.90625\n",
      "2017-11-06T13:26:14.870438: step 1107, loss 0.268898, acc 0.9375\n",
      "2017-11-06T13:26:18.955341: step 1108, loss 0.22072, acc 0.84375\n",
      "2017-11-06T13:26:23.108291: step 1109, loss 0.130039, acc 0.9375\n",
      "2017-11-06T13:26:27.118140: step 1110, loss 0.223731, acc 0.875\n",
      "2017-11-06T13:26:31.349146: step 1111, loss 0.180577, acc 0.9375\n",
      "2017-11-06T13:26:35.920396: step 1112, loss 0.171112, acc 0.9375\n",
      "2017-11-06T13:26:39.968271: step 1113, loss 0.370319, acc 0.875\n",
      "2017-11-06T13:26:44.048170: step 1114, loss 0.147397, acc 0.96875\n",
      "2017-11-06T13:26:48.047012: step 1115, loss 0.546314, acc 0.8125\n",
      "2017-11-06T13:26:50.573806: step 1116, loss 0.438456, acc 0.9\n",
      "2017-11-06T13:26:54.622683: step 1117, loss 0.138251, acc 0.9375\n",
      "2017-11-06T13:26:58.596508: step 1118, loss 0.134634, acc 0.96875\n",
      "2017-11-06T13:27:02.575336: step 1119, loss 0.360911, acc 0.875\n",
      "2017-11-06T13:27:06.610203: step 1120, loss 0.133031, acc 0.90625\n",
      "2017-11-06T13:27:10.634061: step 1121, loss 0.182583, acc 0.9375\n",
      "2017-11-06T13:27:14.619894: step 1122, loss 0.135831, acc 0.9375\n",
      "2017-11-06T13:27:18.652760: step 1123, loss 0.174436, acc 0.9375\n",
      "2017-11-06T13:27:22.698633: step 1124, loss 0.148958, acc 0.9375\n",
      "2017-11-06T13:27:26.885608: step 1125, loss 0.323202, acc 0.84375\n",
      "2017-11-06T13:27:30.888452: step 1126, loss 0.229193, acc 0.875\n",
      "2017-11-06T13:27:35.000374: step 1127, loss 0.473594, acc 0.875\n",
      "2017-11-06T13:27:39.377484: step 1128, loss 0.299014, acc 0.90625\n",
      "2017-11-06T13:27:43.407349: step 1129, loss 0.11623, acc 0.9375\n",
      "2017-11-06T13:27:47.398183: step 1130, loss 0.0877335, acc 0.9375\n",
      "2017-11-06T13:27:51.417039: step 1131, loss 0.270028, acc 0.875\n",
      "2017-11-06T13:27:55.492935: step 1132, loss 0.197029, acc 0.875\n",
      "2017-11-06T13:27:59.526803: step 1133, loss 0.110965, acc 0.9375\n",
      "2017-11-06T13:28:03.553662: step 1134, loss 0.0424033, acc 1\n",
      "2017-11-06T13:28:07.599538: step 1135, loss 0.548522, acc 0.78125\n",
      "2017-11-06T13:28:11.600380: step 1136, loss 0.13111, acc 0.9375\n",
      "2017-11-06T13:28:15.654260: step 1137, loss 0.177359, acc 0.875\n",
      "2017-11-06T13:28:19.655103: step 1138, loss 0.0561725, acc 0.96875\n",
      "2017-11-06T13:28:23.945151: step 1139, loss 0.200322, acc 0.90625\n",
      "2017-11-06T13:28:28.099877: step 1140, loss 0.167503, acc 0.9375\n",
      "2017-11-06T13:28:32.062692: step 1141, loss 0.402462, acc 0.84375\n",
      "2017-11-06T13:28:36.325722: step 1142, loss 0.160016, acc 0.9375\n",
      "2017-11-06T13:28:40.417629: step 1143, loss 0.0661018, acc 0.96875\n",
      "2017-11-06T13:28:44.755711: step 1144, loss 0.0919606, acc 0.9375\n",
      "2017-11-06T13:28:48.739544: step 1145, loss 0.208127, acc 0.90625\n",
      "2017-11-06T13:28:52.763401: step 1146, loss 0.291301, acc 0.90625\n",
      "2017-11-06T13:28:56.790262: step 1147, loss 0.399009, acc 0.84375\n",
      "2017-11-06T13:29:00.806116: step 1148, loss 0.162923, acc 0.90625\n",
      "2017-11-06T13:29:04.807959: step 1149, loss 0.185167, acc 0.9375\n",
      "2017-11-06T13:29:08.787789: step 1150, loss 0.232848, acc 0.90625\n",
      "2017-11-06T13:29:12.818650: step 1151, loss 0.141236, acc 0.90625\n",
      "2017-11-06T13:29:15.321430: step 1152, loss 0.284365, acc 0.85\n",
      "2017-11-06T13:29:19.301257: step 1153, loss 0.23005, acc 0.90625\n",
      "2017-11-06T13:29:23.368147: step 1154, loss 0.0904622, acc 0.96875\n",
      "2017-11-06T13:29:27.419027: step 1155, loss 0.053163, acc 1\n",
      "2017-11-06T13:29:31.379839: step 1156, loss 0.294953, acc 0.84375\n",
      "2017-11-06T13:29:35.424714: step 1157, loss 0.103873, acc 0.96875\n",
      "2017-11-06T13:29:39.420553: step 1158, loss 0.182699, acc 0.9375\n",
      "2017-11-06T13:29:43.366376: step 1159, loss 0.128252, acc 0.90625\n",
      "2017-11-06T13:29:47.645397: step 1160, loss 0.240534, acc 0.9375\n",
      "2017-11-06T13:29:51.921436: step 1161, loss 0.169478, acc 0.9375\n",
      "2017-11-06T13:29:55.910271: step 1162, loss 0.412349, acc 0.84375\n",
      "2017-11-06T13:29:59.908110: step 1163, loss 0.282051, acc 0.84375\n",
      "2017-11-06T13:30:04.172140: step 1164, loss 0.269095, acc 0.84375\n",
      "2017-11-06T13:30:08.211010: step 1165, loss 0.133918, acc 0.90625\n",
      "2017-11-06T13:30:12.227864: step 1166, loss 0.165888, acc 0.90625\n",
      "2017-11-06T13:30:16.205690: step 1167, loss 0.383764, acc 0.84375\n",
      "2017-11-06T13:30:20.160501: step 1168, loss 0.220447, acc 0.9375\n",
      "2017-11-06T13:30:24.135324: step 1169, loss 0.0874448, acc 0.96875\n",
      "2017-11-06T13:30:28.205218: step 1170, loss 0.314961, acc 0.875\n",
      "2017-11-06T13:30:32.248090: step 1171, loss 0.49352, acc 0.78125\n",
      "2017-11-06T13:30:36.481097: step 1172, loss 0.139798, acc 0.9375\n",
      "2017-11-06T13:30:40.434906: step 1173, loss 0.247838, acc 0.9375\n",
      "2017-11-06T13:30:44.472776: step 1174, loss 0.279678, acc 0.90625\n",
      "2017-11-06T13:30:48.497637: step 1175, loss 0.113587, acc 0.9375\n",
      "2017-11-06T13:30:52.706626: step 1176, loss 0.0420848, acc 1\n",
      "2017-11-06T13:30:57.071728: step 1177, loss 0.434895, acc 0.84375\n",
      "2017-11-06T13:31:01.012527: step 1178, loss 0.108757, acc 0.9375\n",
      "2017-11-06T13:31:05.021376: step 1179, loss 0.382456, acc 0.84375\n",
      "2017-11-06T13:31:09.027222: step 1180, loss 0.0714455, acc 0.96875\n",
      "2017-11-06T13:31:13.013054: step 1181, loss 0.191486, acc 0.875\n",
      "2017-11-06T13:31:17.028908: step 1182, loss 0.27565, acc 0.90625\n",
      "2017-11-06T13:31:21.065778: step 1183, loss 0.158363, acc 0.96875\n",
      "2017-11-06T13:31:25.021588: step 1184, loss 0.249489, acc 0.90625\n",
      "2017-11-06T13:31:29.051225: step 1185, loss 0.150878, acc 0.9375\n",
      "2017-11-06T13:31:33.090095: step 1186, loss 0.326164, acc 0.875\n",
      "2017-11-06T13:31:37.299086: step 1187, loss 0.415049, acc 0.84375\n",
      "2017-11-06T13:31:40.074058: step 1188, loss 0.386119, acc 0.85\n",
      "2017-11-06T13:31:44.186980: step 1189, loss 0.126325, acc 0.9375\n",
      "2017-11-06T13:31:48.187822: step 1190, loss 0.346672, acc 0.84375\n",
      "2017-11-06T13:31:52.170655: step 1191, loss 0.117682, acc 0.9375\n",
      "2017-11-06T13:31:56.187507: step 1192, loss 0.200836, acc 0.90625\n",
      "2017-11-06T13:32:00.571621: step 1193, loss 0.180095, acc 0.96875\n",
      "2017-11-06T13:32:04.793622: step 1194, loss 0.199959, acc 0.90625\n",
      "2017-11-06T13:32:08.831490: step 1195, loss 0.287577, acc 0.9375\n",
      "2017-11-06T13:32:12.863357: step 1196, loss 0.215057, acc 0.9375\n",
      "2017-11-06T13:32:16.866200: step 1197, loss 0.138701, acc 0.96875\n",
      "2017-11-06T13:32:20.891059: step 1198, loss 0.0136758, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T13:32:24.899908: step 1199, loss 0.211157, acc 0.96875\n",
      "2017-11-06T13:32:28.906755: step 1200, loss 0.346024, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:32:31.458568: step 1200, loss 1.17487, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-06T13:32:36.904758: step 1201, loss 0.158912, acc 0.875\n",
      "2017-11-06T13:32:40.891592: step 1202, loss 0.216889, acc 0.9375\n",
      "2017-11-06T13:32:44.897438: step 1203, loss 0.176178, acc 0.9375\n",
      "2017-11-06T13:32:48.810217: step 1204, loss 0.363167, acc 0.84375\n",
      "2017-11-06T13:32:52.862281: step 1205, loss 0.217516, acc 0.90625\n",
      "2017-11-06T13:32:56.870111: step 1206, loss 0.0652515, acc 1\n",
      "2017-11-06T13:33:00.739858: step 1207, loss 0.246375, acc 0.9375\n",
      "2017-11-06T13:33:05.003889: step 1208, loss 0.132849, acc 0.9375\n",
      "2017-11-06T13:33:09.263915: step 1209, loss 0.181255, acc 0.9375\n",
      "2017-11-06T13:33:13.193709: step 1210, loss 0.25577, acc 0.90625\n",
      "2017-11-06T13:33:17.269603: step 1211, loss 0.332822, acc 0.90625\n",
      "2017-11-06T13:33:21.351504: step 1212, loss 0.096598, acc 0.96875\n",
      "2017-11-06T13:33:25.527471: step 1213, loss 0.265786, acc 0.875\n",
      "2017-11-06T13:33:29.782495: step 1214, loss 0.207224, acc 0.90625\n",
      "2017-11-06T13:33:33.768327: step 1215, loss 0.179177, acc 0.90625\n",
      "2017-11-06T13:33:37.722136: step 1216, loss 0.145666, acc 0.90625\n",
      "2017-11-06T13:33:41.970155: step 1217, loss 0.106709, acc 0.9375\n",
      "2017-11-06T13:33:46.201161: step 1218, loss 0.198186, acc 0.9375\n",
      "2017-11-06T13:33:50.270052: step 1219, loss 0.179724, acc 0.9375\n",
      "2017-11-06T13:33:54.406992: step 1220, loss 0.540925, acc 0.8125\n",
      "2017-11-06T13:33:58.573954: step 1221, loss 0.122182, acc 0.90625\n",
      "2017-11-06T13:34:02.794952: step 1222, loss 0.0200489, acc 1\n",
      "2017-11-06T13:34:06.859840: step 1223, loss 0.439261, acc 0.75\n",
      "2017-11-06T13:34:09.708864: step 1224, loss 0.231173, acc 0.85\n",
      "2017-11-06T13:34:14.089977: step 1225, loss 0.20769, acc 0.9375\n",
      "2017-11-06T13:34:18.298968: step 1226, loss 0.0880135, acc 0.9375\n",
      "2017-11-06T13:34:22.480939: step 1227, loss 0.105643, acc 0.9375\n",
      "2017-11-06T13:34:26.645899: step 1228, loss 0.197967, acc 0.9375\n",
      "2017-11-06T13:34:30.778738: step 1229, loss 0.119981, acc 0.90625\n",
      "2017-11-06T13:34:35.102811: step 1230, loss 0.0378874, acc 1\n",
      "2017-11-06T13:34:39.193718: step 1231, loss 0.147821, acc 0.9375\n",
      "2017-11-06T13:34:43.344668: step 1232, loss 0.684808, acc 0.84375\n",
      "2017-11-06T13:34:47.425567: step 1233, loss 0.0258743, acc 1\n",
      "2017-11-06T13:34:51.379377: step 1234, loss 0.24168, acc 0.875\n",
      "2017-11-06T13:34:55.431255: step 1235, loss 0.1977, acc 0.9375\n",
      "2017-11-06T13:34:59.415086: step 1236, loss 0.204019, acc 0.875\n",
      "2017-11-06T13:35:03.343878: step 1237, loss 0.0246535, acc 1\n",
      "2017-11-06T13:35:07.330710: step 1238, loss 0.143934, acc 0.9375\n",
      "2017-11-06T13:35:11.312540: step 1239, loss 0.0673704, acc 1\n",
      "2017-11-06T13:35:15.448478: step 1240, loss 0.183027, acc 0.9375\n",
      "2017-11-06T13:35:19.739527: step 1241, loss 0.17465, acc 0.9375\n",
      "2017-11-06T13:35:23.726360: step 1242, loss 0.200908, acc 0.90625\n",
      "2017-11-06T13:35:27.666159: step 1243, loss 0.236611, acc 0.90625\n",
      "2017-11-06T13:35:31.605959: step 1244, loss 0.157838, acc 0.90625\n",
      "2017-11-06T13:35:35.602799: step 1245, loss 0.301771, acc 0.8125\n",
      "2017-11-06T13:35:39.580625: step 1246, loss 0.439075, acc 0.84375\n",
      "2017-11-06T13:35:43.577467: step 1247, loss 0.143542, acc 0.96875\n",
      "2017-11-06T13:35:47.538280: step 1248, loss 0.302274, acc 0.9375\n",
      "2017-11-06T13:35:51.531117: step 1249, loss 0.114947, acc 0.9375\n",
      "2017-11-06T13:35:55.560980: step 1250, loss 0.441591, acc 0.84375\n",
      "2017-11-06T13:35:59.532802: step 1251, loss 0.325605, acc 0.8125\n",
      "2017-11-06T13:36:03.557662: step 1252, loss 0.414235, acc 0.84375\n",
      "2017-11-06T13:36:07.527483: step 1253, loss 0.119701, acc 0.96875\n",
      "2017-11-06T13:36:11.575359: step 1254, loss 0.208743, acc 0.9375\n",
      "2017-11-06T13:36:15.568196: step 1255, loss 0.189833, acc 0.9375\n",
      "2017-11-06T13:36:19.580046: step 1256, loss 0.0664742, acc 0.96875\n",
      "2017-11-06T13:36:24.100259: step 1257, loss 0.337727, acc 0.875\n",
      "2017-11-06T13:36:28.084090: step 1258, loss 0.104626, acc 0.9375\n",
      "2017-11-06T13:36:32.055911: step 1259, loss 0.293314, acc 0.84375\n",
      "2017-11-06T13:36:34.776845: step 1260, loss 0.071636, acc 1\n",
      "2017-11-06T13:36:38.772683: step 1261, loss 0.281572, acc 0.875\n",
      "2017-11-06T13:36:42.707481: step 1262, loss 0.0598773, acc 0.96875\n",
      "2017-11-06T13:36:46.659287: step 1263, loss 0.242959, acc 0.90625\n",
      "2017-11-06T13:36:50.610094: step 1264, loss 0.0924696, acc 0.96875\n",
      "2017-11-06T13:36:54.565905: step 1265, loss 0.25615, acc 0.90625\n",
      "2017-11-06T13:36:58.530798: step 1266, loss 0.0697941, acc 0.96875\n",
      "2017-11-06T13:37:02.499618: step 1267, loss 0.262719, acc 0.875\n",
      "2017-11-06T13:37:06.443420: step 1268, loss 0.0489478, acc 0.96875\n",
      "2017-11-06T13:37:10.371212: step 1269, loss 0.309557, acc 0.90625\n",
      "2017-11-06T13:37:14.361046: step 1270, loss 0.296136, acc 0.84375\n",
      "2017-11-06T13:37:18.305850: step 1271, loss 0.243806, acc 0.875\n",
      "2017-11-06T13:37:22.270666: step 1272, loss 0.231277, acc 0.90625\n",
      "2017-11-06T13:37:26.384590: step 1273, loss 0.26903, acc 0.84375\n",
      "2017-11-06T13:37:30.682444: step 1274, loss 0.2279, acc 0.90625\n",
      "2017-11-06T13:37:34.685286: step 1275, loss 0.136677, acc 0.9375\n",
      "2017-11-06T13:37:38.651105: step 1276, loss 0.0977862, acc 0.96875\n",
      "2017-11-06T13:37:42.633937: step 1277, loss 0.0487314, acc 1\n",
      "2017-11-06T13:37:46.607760: step 1278, loss 0.234984, acc 0.875\n",
      "2017-11-06T13:37:50.585584: step 1279, loss 0.249255, acc 0.875\n",
      "2017-11-06T13:37:54.556406: step 1280, loss 0.0930679, acc 0.96875\n",
      "2017-11-06T13:37:58.539236: step 1281, loss 0.120831, acc 0.9375\n",
      "2017-11-06T13:38:02.524067: step 1282, loss 0.0943422, acc 0.96875\n",
      "2017-11-06T13:38:06.462866: step 1283, loss 0.445802, acc 0.8125\n",
      "2017-11-06T13:38:10.443694: step 1284, loss 0.0243132, acc 1\n",
      "2017-11-06T13:38:14.433529: step 1285, loss 0.0962295, acc 0.9375\n",
      "2017-11-06T13:38:18.416361: step 1286, loss 0.0337428, acc 1\n",
      "2017-11-06T13:38:22.505265: step 1287, loss 0.25175, acc 0.90625\n",
      "2017-11-06T13:38:26.637200: step 1288, loss 0.123985, acc 0.9375\n",
      "2017-11-06T13:38:30.653054: step 1289, loss 0.286788, acc 0.90625\n",
      "2017-11-06T13:38:35.494494: step 1290, loss 0.193864, acc 0.9375\n",
      "2017-11-06T13:38:39.632434: step 1291, loss 0.0627994, acc 1\n",
      "2017-11-06T13:38:43.670303: step 1292, loss 0.272502, acc 0.90625\n",
      "2017-11-06T13:38:47.672148: step 1293, loss 0.174288, acc 0.9375\n",
      "2017-11-06T13:38:51.623955: step 1294, loss 0.34828, acc 0.875\n",
      "2017-11-06T13:38:55.544742: step 1295, loss 0.155812, acc 0.90625\n",
      "2017-11-06T13:38:58.088549: step 1296, loss 0.0934286, acc 0.95\n",
      "2017-11-06T13:39:02.004330: step 1297, loss 0.246577, acc 0.875\n",
      "2017-11-06T13:39:05.955139: step 1298, loss 0.136434, acc 0.96875\n",
      "2017-11-06T13:39:09.932964: step 1299, loss 0.0672159, acc 0.96875\n",
      "2017-11-06T13:39:14.039882: step 1300, loss 0.114419, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:39:16.688764: step 1300, loss 1.19538, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-06T13:39:22.643096: step 1301, loss 0.448586, acc 0.875\n",
      "2017-11-06T13:39:26.632930: step 1302, loss 0.208061, acc 0.90625\n",
      "2017-11-06T13:39:30.559721: step 1303, loss 0.195759, acc 0.90625\n",
      "2017-11-06T13:39:34.509528: step 1304, loss 0.623931, acc 0.78125\n",
      "2017-11-06T13:39:38.754543: step 1305, loss 0.211901, acc 0.90625\n",
      "2017-11-06T13:39:42.962534: step 1306, loss 0.103031, acc 0.9375\n",
      "2017-11-06T13:39:46.879316: step 1307, loss 0.0618477, acc 0.96875\n",
      "2017-11-06T13:39:50.871153: step 1308, loss 0.214697, acc 0.9375\n",
      "2017-11-06T13:39:54.836970: step 1309, loss 0.163965, acc 0.90625\n",
      "2017-11-06T13:39:58.852823: step 1310, loss 0.247716, acc 0.8125\n",
      "2017-11-06T13:40:03.056811: step 1311, loss 0.279712, acc 0.84375\n",
      "2017-11-06T13:40:07.083672: step 1312, loss 0.113419, acc 0.9375\n",
      "2017-11-06T13:40:11.094522: step 1313, loss 0.197133, acc 0.90625\n",
      "2017-11-06T13:40:15.065343: step 1314, loss 0.212748, acc 0.90625\n",
      "2017-11-06T13:40:19.048173: step 1315, loss 0.36923, acc 0.90625\n",
      "2017-11-06T13:40:23.024999: step 1316, loss 0.306438, acc 0.875\n",
      "2017-11-06T13:40:27.039852: step 1317, loss 0.0978851, acc 0.9375\n",
      "2017-11-06T13:40:30.996438: step 1318, loss 0.163987, acc 0.90625\n",
      "2017-11-06T13:40:35.228445: step 1319, loss 0.341145, acc 0.90625\n",
      "2017-11-06T13:40:39.204269: step 1320, loss 0.195484, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T13:40:43.339207: step 1321, loss 0.209745, acc 0.875\n",
      "2017-11-06T13:40:47.642266: step 1322, loss 0.0637999, acc 1\n",
      "2017-11-06T13:40:51.660138: step 1323, loss 0.144141, acc 0.9375\n",
      "2017-11-06T13:40:55.643950: step 1324, loss 0.252147, acc 0.90625\n",
      "2017-11-06T13:40:59.686823: step 1325, loss 0.175921, acc 0.96875\n",
      "2017-11-06T13:41:03.647637: step 1326, loss 0.128715, acc 0.90625\n",
      "2017-11-06T13:41:07.606450: step 1327, loss 0.221486, acc 0.90625\n",
      "2017-11-06T13:41:12.346818: step 1328, loss 0.0459865, acc 0.96875\n",
      "2017-11-06T13:41:17.009131: step 1329, loss 0.318377, acc 0.875\n",
      "2017-11-06T13:41:20.989960: step 1330, loss 0.296333, acc 0.84375\n",
      "2017-11-06T13:41:25.131903: step 1331, loss 0.221978, acc 0.84375\n",
      "2017-11-06T13:41:27.663702: step 1332, loss 0.319811, acc 0.9\n",
      "2017-11-06T13:41:31.701571: step 1333, loss 0.143427, acc 0.9375\n",
      "2017-11-06T13:41:35.661384: step 1334, loss 0.145584, acc 0.90625\n",
      "2017-11-06T13:41:39.656225: step 1335, loss 0.203318, acc 0.875\n",
      "2017-11-06T13:41:43.668076: step 1336, loss 0.215313, acc 0.90625\n",
      "2017-11-06T13:41:47.788001: step 1337, loss 0.165149, acc 0.9375\n",
      "2017-11-06T13:41:52.215146: step 1338, loss 0.21067, acc 0.875\n",
      "2017-11-06T13:41:56.238005: step 1339, loss 0.0676248, acc 1\n",
      "2017-11-06T13:42:00.296889: step 1340, loss 0.442385, acc 0.84375\n",
      "2017-11-06T13:42:04.282721: step 1341, loss 0.320691, acc 0.875\n",
      "2017-11-06T13:42:08.282564: step 1342, loss 0.19532, acc 0.84375\n",
      "2017-11-06T13:42:12.266394: step 1343, loss 0.11857, acc 0.9375\n",
      "2017-11-06T13:42:16.278247: step 1344, loss 0.212836, acc 0.90625\n",
      "2017-11-06T13:42:20.276085: step 1345, loss 0.254552, acc 0.9375\n",
      "2017-11-06T13:42:24.298943: step 1346, loss 0.306671, acc 0.875\n",
      "2017-11-06T13:42:28.433881: step 1347, loss 0.172712, acc 0.9375\n",
      "2017-11-06T13:42:32.465746: step 1348, loss 0.153069, acc 0.90625\n",
      "2017-11-06T13:42:36.666732: step 1349, loss 0.385913, acc 0.875\n",
      "2017-11-06T13:42:40.758639: step 1350, loss 0.174023, acc 0.90625\n",
      "2017-11-06T13:42:44.828530: step 1351, loss 0.140626, acc 0.9375\n",
      "2017-11-06T13:42:48.876407: step 1352, loss 0.139243, acc 0.90625\n",
      "2017-11-06T13:42:52.879251: step 1353, loss 0.0968133, acc 0.96875\n",
      "2017-11-06T13:42:57.301393: step 1354, loss 0.0635938, acc 0.96875\n",
      "2017-11-06T13:43:01.439333: step 1355, loss 0.282534, acc 0.875\n",
      "2017-11-06T13:43:05.476201: step 1356, loss 0.192031, acc 0.90625\n",
      "2017-11-06T13:43:09.518075: step 1357, loss 0.121294, acc 0.96875\n",
      "2017-11-06T13:43:13.549938: step 1358, loss 0.243605, acc 0.90625\n",
      "2017-11-06T13:43:17.515756: step 1359, loss 0.109079, acc 0.96875\n",
      "2017-11-06T13:43:21.537614: step 1360, loss 0.111977, acc 0.96875\n",
      "2017-11-06T13:43:25.551466: step 1361, loss 0.41366, acc 0.84375\n",
      "2017-11-06T13:43:29.576147: step 1362, loss 0.0433761, acc 0.96875\n",
      "2017-11-06T13:43:33.646038: step 1363, loss 0.226338, acc 0.875\n",
      "2017-11-06T13:43:37.674901: step 1364, loss 0.12964, acc 0.9375\n",
      "2017-11-06T13:43:41.694757: step 1365, loss 0.258548, acc 0.84375\n",
      "2017-11-06T13:43:45.689596: step 1366, loss 0.185375, acc 0.90625\n",
      "2017-11-06T13:43:49.750482: step 1367, loss 0.345292, acc 0.8125\n",
      "2017-11-06T13:43:52.259264: step 1368, loss 0.52797, acc 0.75\n",
      "2017-11-06T13:43:56.336162: step 1369, loss 0.11537, acc 0.96875\n",
      "2017-11-06T13:44:00.548154: step 1370, loss 0.0579453, acc 1\n",
      "2017-11-06T13:44:04.921261: step 1371, loss 0.189434, acc 0.875\n",
      "2017-11-06T13:44:08.974141: step 1372, loss 0.152801, acc 0.9375\n",
      "2017-11-06T13:44:13.017015: step 1373, loss 0.315654, acc 0.84375\n",
      "2017-11-06T13:44:17.073896: step 1374, loss 0.249545, acc 0.90625\n",
      "2017-11-06T13:44:21.089749: step 1375, loss 0.242205, acc 0.90625\n",
      "2017-11-06T13:44:25.324761: step 1376, loss 0.22031, acc 0.90625\n",
      "2017-11-06T13:44:29.541755: step 1377, loss 0.116329, acc 0.96875\n",
      "2017-11-06T13:44:33.815791: step 1378, loss 0.227409, acc 0.90625\n",
      "2017-11-06T13:44:37.959736: step 1379, loss 0.200805, acc 0.9375\n",
      "2017-11-06T13:44:41.974589: step 1380, loss 0.13074, acc 0.9375\n",
      "2017-11-06T13:44:46.046482: step 1381, loss 0.153467, acc 0.9375\n",
      "2017-11-06T13:44:50.044323: step 1382, loss 0.283603, acc 0.9375\n",
      "2017-11-06T13:44:54.052171: step 1383, loss 0.120235, acc 0.96875\n",
      "2017-11-06T13:44:58.162091: step 1384, loss 0.143296, acc 0.90625\n",
      "2017-11-06T13:45:02.236165: step 1385, loss 0.17671, acc 0.90625\n",
      "2017-11-06T13:45:06.535220: step 1386, loss 0.284447, acc 0.90625\n",
      "2017-11-06T13:45:10.882309: step 1387, loss 0.142355, acc 0.96875\n",
      "2017-11-06T13:45:14.911171: step 1388, loss 0.242912, acc 0.875\n",
      "2017-11-06T13:45:19.038104: step 1389, loss 0.375861, acc 0.875\n",
      "2017-11-06T13:45:23.079976: step 1390, loss 0.136713, acc 0.9375\n",
      "2017-11-06T13:45:27.133858: step 1391, loss 0.226884, acc 0.90625\n",
      "2017-11-06T13:45:31.185735: step 1392, loss 0.463092, acc 0.84375\n",
      "2017-11-06T13:45:35.223604: step 1393, loss 0.0905748, acc 0.96875\n",
      "2017-11-06T13:45:39.192425: step 1394, loss 0.0816205, acc 1\n",
      "2017-11-06T13:45:43.281330: step 1395, loss 0.219024, acc 0.9375\n",
      "2017-11-06T13:45:47.352222: step 1396, loss 0.127658, acc 0.9375\n",
      "2017-11-06T13:45:51.415109: step 1397, loss 0.0929627, acc 0.96875\n",
      "2017-11-06T13:45:55.528031: step 1398, loss 0.200468, acc 0.90625\n",
      "2017-11-06T13:45:59.513863: step 1399, loss 0.157461, acc 0.9375\n",
      "2017-11-06T13:46:03.492691: step 1400, loss 0.0426684, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:46:06.089536: step 1400, loss 1.20369, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-06T13:46:11.709454: step 1401, loss 0.456189, acc 0.75\n",
      "2017-11-06T13:46:16.046536: step 1402, loss 0.133035, acc 0.96875\n",
      "2017-11-06T13:46:20.042374: step 1403, loss 0.219354, acc 0.875\n",
      "2017-11-06T13:46:22.662237: step 1404, loss 0.175107, acc 0.85\n",
      "2017-11-06T13:46:26.701106: step 1405, loss 0.217879, acc 0.875\n",
      "2017-11-06T13:46:30.665675: step 1406, loss 0.206568, acc 0.9375\n",
      "2017-11-06T13:46:34.850648: step 1407, loss 0.142547, acc 0.9375\n",
      "2017-11-06T13:46:38.830477: step 1408, loss 0.367042, acc 0.84375\n",
      "2017-11-06T13:46:42.791289: step 1409, loss 0.196987, acc 0.90625\n",
      "2017-11-06T13:46:46.757108: step 1410, loss 0.0731809, acc 0.96875\n",
      "2017-11-06T13:46:50.706914: step 1411, loss 0.113615, acc 0.96875\n",
      "2017-11-06T13:46:54.676734: step 1412, loss 0.121653, acc 0.9375\n",
      "2017-11-06T13:46:58.642553: step 1413, loss 0.121172, acc 0.9375\n",
      "2017-11-06T13:47:02.692881: step 1414, loss 0.0762107, acc 0.96875\n",
      "2017-11-06T13:47:06.626675: step 1415, loss 0.136517, acc 0.90625\n",
      "2017-11-06T13:47:10.564476: step 1416, loss 0.137352, acc 0.9375\n",
      "2017-11-06T13:47:14.488261: step 1417, loss 0.292014, acc 0.875\n",
      "2017-11-06T13:47:18.697253: step 1418, loss 0.0989045, acc 0.9375\n",
      "2017-11-06T13:47:22.854206: step 1419, loss 0.0922592, acc 0.96875\n",
      "2017-11-06T13:47:26.895077: step 1420, loss 0.148617, acc 0.9375\n",
      "2017-11-06T13:47:30.848886: step 1421, loss 0.263562, acc 0.875\n",
      "2017-11-06T13:47:34.866742: step 1422, loss 0.175718, acc 0.9375\n",
      "2017-11-06T13:47:38.860579: step 1423, loss 0.231784, acc 0.875\n",
      "2017-11-06T13:47:42.857419: step 1424, loss 0.241946, acc 0.90625\n",
      "2017-11-06T13:47:46.813230: step 1425, loss 0.23081, acc 0.875\n",
      "2017-11-06T13:47:50.796061: step 1426, loss 0.317555, acc 0.875\n",
      "2017-11-06T13:47:54.780891: step 1427, loss 0.319012, acc 0.875\n",
      "2017-11-06T13:47:58.758717: step 1428, loss 0.193028, acc 0.875\n",
      "2017-11-06T13:48:02.750554: step 1429, loss 0.268243, acc 0.90625\n",
      "2017-11-06T13:48:06.724378: step 1430, loss 0.135921, acc 0.9375\n",
      "2017-11-06T13:48:10.702205: step 1431, loss 0.0198988, acc 1\n",
      "2017-11-06T13:48:14.674026: step 1432, loss 0.234634, acc 0.9375\n",
      "2017-11-06T13:48:18.654854: step 1433, loss 0.22966, acc 0.90625\n",
      "2017-11-06T13:48:22.853838: step 1434, loss 0.379673, acc 0.875\n",
      "2017-11-06T13:48:27.296013: step 1435, loss 0.439285, acc 0.84375\n",
      "2017-11-06T13:48:31.220783: step 1436, loss 0.249934, acc 0.90625\n",
      "2017-11-06T13:48:35.393749: step 1437, loss 0.22155, acc 0.90625\n",
      "2017-11-06T13:48:39.398594: step 1438, loss 0.238609, acc 0.9375\n",
      "2017-11-06T13:48:43.404440: step 1439, loss 0.17305, acc 0.9375\n",
      "2017-11-06T13:48:45.938241: step 1440, loss 0.209103, acc 0.95\n",
      "2017-11-06T13:48:49.943086: step 1441, loss 0.112843, acc 0.96875\n",
      "2017-11-06T13:48:53.903901: step 1442, loss 0.1651, acc 0.9375\n",
      "2017-11-06T13:48:57.866717: step 1443, loss 0.183999, acc 0.9375\n",
      "2017-11-06T13:49:01.838539: step 1444, loss 0.273387, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T13:49:05.885416: step 1445, loss 0.172464, acc 0.90625\n",
      "2017-11-06T13:49:09.946299: step 1446, loss 0.0981732, acc 0.96875\n",
      "2017-11-06T13:49:13.904112: step 1447, loss 0.0538564, acc 0.96875\n",
      "2017-11-06T13:49:17.899951: step 1448, loss 0.115639, acc 0.9375\n",
      "2017-11-06T13:49:21.904797: step 1449, loss 0.174943, acc 0.96875\n",
      "2017-11-06T13:49:25.903637: step 1450, loss 0.102795, acc 0.96875\n",
      "2017-11-06T13:49:30.235421: step 1451, loss 0.434584, acc 0.75\n",
      "2017-11-06T13:49:34.441428: step 1452, loss 0.216698, acc 0.9375\n",
      "2017-11-06T13:49:38.409229: step 1453, loss 0.310909, acc 0.8125\n",
      "2017-11-06T13:49:42.399065: step 1454, loss 0.145464, acc 0.9375\n",
      "2017-11-06T13:49:46.388899: step 1455, loss 0.247379, acc 0.875\n",
      "2017-11-06T13:49:50.430771: step 1456, loss 0.262227, acc 0.84375\n",
      "2017-11-06T13:49:54.376575: step 1457, loss 0.220958, acc 0.9375\n",
      "2017-11-06T13:49:58.396431: step 1458, loss 0.174154, acc 0.90625\n",
      "2017-11-06T13:50:02.722505: step 1459, loss 0.122428, acc 0.9375\n",
      "2017-11-06T13:50:06.764377: step 1460, loss 0.195596, acc 0.90625\n",
      "2017-11-06T13:50:10.699172: step 1461, loss 0.211777, acc 0.9375\n",
      "2017-11-06T13:50:14.742044: step 1462, loss 0.71346, acc 0.78125\n",
      "2017-11-06T13:50:18.727877: step 1463, loss 0.080832, acc 0.96875\n",
      "2017-11-06T13:50:22.695696: step 1464, loss 0.341851, acc 0.84375\n",
      "2017-11-06T13:50:26.667519: step 1465, loss 0.13537, acc 0.9375\n",
      "2017-11-06T13:50:30.596310: step 1466, loss 0.019412, acc 1\n",
      "2017-11-06T13:50:35.020453: step 1467, loss 0.092922, acc 0.96875\n",
      "2017-11-06T13:50:39.271474: step 1468, loss 0.0860945, acc 0.9375\n",
      "2017-11-06T13:50:43.226284: step 1469, loss 0.0572967, acc 0.96875\n",
      "2017-11-06T13:50:47.170087: step 1470, loss 0.0982614, acc 0.9375\n",
      "2017-11-06T13:50:51.077863: step 1471, loss 0.132555, acc 0.9375\n",
      "2017-11-06T13:50:55.070701: step 1472, loss 0.200355, acc 0.84375\n",
      "2017-11-06T13:50:59.018506: step 1473, loss 0.376175, acc 0.84375\n",
      "2017-11-06T13:51:02.941293: step 1474, loss 0.182901, acc 0.875\n",
      "2017-11-06T13:51:07.065445: step 1475, loss 0.0307656, acc 0.96875\n",
      "2017-11-06T13:51:09.548209: step 1476, loss 0.091906, acc 0.95\n",
      "2017-11-06T13:51:13.521031: step 1477, loss 0.185575, acc 0.9375\n",
      "2017-11-06T13:51:17.541890: step 1478, loss 0.136872, acc 0.9375\n",
      "2017-11-06T13:51:21.494697: step 1479, loss 0.061814, acc 0.96875\n",
      "2017-11-06T13:51:25.495539: step 1480, loss 0.175845, acc 0.9375\n",
      "2017-11-06T13:51:29.439343: step 1481, loss 0.0844239, acc 0.96875\n",
      "2017-11-06T13:51:33.447190: step 1482, loss 0.282275, acc 0.90625\n",
      "2017-11-06T13:51:37.405002: step 1483, loss 0.227744, acc 0.96875\n",
      "2017-11-06T13:51:41.666029: step 1484, loss 0.0517879, acc 0.96875\n",
      "2017-11-06T13:51:45.863011: step 1485, loss 0.289037, acc 0.8125\n",
      "2017-11-06T13:51:49.879866: step 1486, loss 0.364397, acc 0.875\n",
      "2017-11-06T13:51:53.800652: step 1487, loss 0.106531, acc 0.9375\n",
      "2017-11-06T13:51:57.807500: step 1488, loss 0.165429, acc 0.90625\n",
      "2017-11-06T13:52:01.784324: step 1489, loss 0.160276, acc 0.90625\n",
      "2017-11-06T13:52:05.754147: step 1490, loss 0.204726, acc 0.875\n",
      "2017-11-06T13:52:09.744980: step 1491, loss 0.149372, acc 0.90625\n",
      "2017-11-06T13:52:13.718805: step 1492, loss 0.192335, acc 0.90625\n",
      "2017-11-06T13:52:17.732657: step 1493, loss 0.193183, acc 0.9375\n",
      "2017-11-06T13:52:21.696473: step 1494, loss 0.20078, acc 0.90625\n",
      "2017-11-06T13:52:25.697316: step 1495, loss 0.161746, acc 0.9375\n",
      "2017-11-06T13:52:29.684151: step 1496, loss 0.205477, acc 0.96875\n",
      "2017-11-06T13:52:33.914919: step 1497, loss 0.247476, acc 0.875\n",
      "2017-11-06T13:52:37.996819: step 1498, loss 0.193564, acc 0.875\n",
      "2017-11-06T13:52:42.012673: step 1499, loss 0.496983, acc 0.8125\n",
      "2017-11-06T13:52:46.263693: step 1500, loss 0.252448, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:52:49.234805: step 1500, loss 1.11166, acc 0.666667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-06T13:52:54.521602: step 1501, loss 0.161982, acc 0.9375\n",
      "2017-11-06T13:52:58.483417: step 1502, loss 0.170792, acc 0.90625\n",
      "2017-11-06T13:53:02.505276: step 1503, loss 0.0904095, acc 0.96875\n",
      "2017-11-06T13:53:06.514125: step 1504, loss 0.347332, acc 0.8125\n",
      "2017-11-06T13:53:10.599480: step 1505, loss 0.222974, acc 0.875\n",
      "2017-11-06T13:53:14.611332: step 1506, loss 0.090644, acc 0.96875\n",
      "2017-11-06T13:53:18.659207: step 1507, loss 0.282527, acc 0.90625\n",
      "2017-11-06T13:53:22.784138: step 1508, loss 0.191532, acc 0.9375\n",
      "2017-11-06T13:53:27.127224: step 1509, loss 0.0913687, acc 0.9375\n",
      "2017-11-06T13:53:31.358230: step 1510, loss 0.0918417, acc 0.96875\n",
      "2017-11-06T13:53:35.355070: step 1511, loss 0.294405, acc 0.90625\n",
      "2017-11-06T13:53:37.977934: step 1512, loss 0.141788, acc 0.9\n",
      "2017-11-06T13:53:41.968770: step 1513, loss 0.120899, acc 0.96875\n",
      "2017-11-06T13:53:45.980620: step 1514, loss 0.259182, acc 0.84375\n",
      "2017-11-06T13:53:49.982464: step 1515, loss 0.288697, acc 0.875\n",
      "2017-11-06T13:53:54.373586: step 1516, loss 0.275055, acc 0.90625\n",
      "2017-11-06T13:53:58.385436: step 1517, loss 0.381532, acc 0.875\n",
      "2017-11-06T13:54:02.390281: step 1518, loss 0.357731, acc 0.84375\n",
      "2017-11-06T13:54:06.389123: step 1519, loss 0.192779, acc 0.9375\n",
      "2017-11-06T13:54:10.409978: step 1520, loss 0.031641, acc 1\n",
      "2017-11-06T13:54:14.401815: step 1521, loss 0.0262616, acc 1\n",
      "2017-11-06T13:54:18.417668: step 1522, loss 0.306527, acc 0.875\n",
      "2017-11-06T13:54:22.393495: step 1523, loss 0.102821, acc 0.96875\n",
      "2017-11-06T13:54:26.448374: step 1524, loss 0.17038, acc 0.90625\n",
      "2017-11-06T13:54:30.440211: step 1525, loss 0.172215, acc 0.96875\n",
      "2017-11-06T13:54:34.661210: step 1526, loss 0.298314, acc 0.84375\n",
      "2017-11-06T13:54:38.691074: step 1527, loss 0.320447, acc 0.875\n",
      "2017-11-06T13:54:42.709928: step 1528, loss 0.246072, acc 0.90625\n",
      "2017-11-06T13:54:46.710772: step 1529, loss 0.175517, acc 0.9375\n",
      "2017-11-06T13:54:50.705611: step 1530, loss 0.127947, acc 0.96875\n",
      "2017-11-06T13:54:54.657418: step 1531, loss 0.277395, acc 0.875\n",
      "2017-11-06T13:54:59.166622: step 1532, loss 0.101396, acc 0.9375\n",
      "2017-11-06T13:55:03.286549: step 1533, loss 0.0839935, acc 0.96875\n",
      "2017-11-06T13:55:07.283390: step 1534, loss 0.175294, acc 0.9375\n",
      "2017-11-06T13:55:11.304247: step 1535, loss 0.326733, acc 0.875\n",
      "2017-11-06T13:55:15.318098: step 1536, loss 0.394456, acc 0.90625\n",
      "2017-11-06T13:55:19.326947: step 1537, loss 0.197129, acc 0.9375\n",
      "2017-11-06T13:55:23.288762: step 1538, loss 0.0488899, acc 1\n",
      "2017-11-06T13:55:27.279598: step 1539, loss 0.295996, acc 0.875\n",
      "2017-11-06T13:55:31.268176: step 1540, loss 0.170621, acc 0.90625\n",
      "2017-11-06T13:55:35.279027: step 1541, loss 0.27577, acc 0.90625\n",
      "2017-11-06T13:55:39.279869: step 1542, loss 0.308296, acc 0.84375\n",
      "2017-11-06T13:55:43.245687: step 1543, loss 0.31351, acc 0.84375\n",
      "2017-11-06T13:55:47.329589: step 1544, loss 0.165393, acc 0.9375\n",
      "2017-11-06T13:55:51.407486: step 1545, loss 0.2608, acc 0.875\n",
      "2017-11-06T13:55:55.418336: step 1546, loss 0.240048, acc 0.9375\n",
      "2017-11-06T13:55:59.473218: step 1547, loss 0.155534, acc 0.90625\n",
      "2017-11-06T13:56:02.195153: step 1548, loss 0.203489, acc 0.9\n",
      "2017-11-06T13:56:06.637308: step 1549, loss 0.276924, acc 0.90625\n",
      "2017-11-06T13:56:10.630144: step 1550, loss 0.314401, acc 0.84375\n",
      "2017-11-06T13:56:14.760079: step 1551, loss 0.292829, acc 0.90625\n",
      "2017-11-06T13:56:18.804954: step 1552, loss 0.148521, acc 0.9375\n",
      "2017-11-06T13:56:22.878848: step 1553, loss 0.369554, acc 0.875\n",
      "2017-11-06T13:56:27.013786: step 1554, loss 0.557698, acc 0.8125\n",
      "2017-11-06T13:56:31.098689: step 1555, loss 0.0304853, acc 1\n",
      "2017-11-06T13:56:35.285663: step 1556, loss 0.13368, acc 0.9375\n",
      "2017-11-06T13:56:39.362560: step 1557, loss 0.0806004, acc 0.96875\n",
      "2017-11-06T13:56:43.332381: step 1558, loss 0.25773, acc 0.875\n",
      "2017-11-06T13:56:47.369249: step 1559, loss 0.202715, acc 0.9375\n",
      "2017-11-06T13:56:51.373094: step 1560, loss 0.167557, acc 0.90625\n",
      "2017-11-06T13:56:55.348921: step 1561, loss 0.274375, acc 0.90625\n",
      "2017-11-06T13:56:59.364773: step 1562, loss 0.29827, acc 0.90625\n",
      "2017-11-06T13:57:03.329591: step 1563, loss 0.29235, acc 0.90625\n",
      "2017-11-06T13:57:07.369460: step 1564, loss 0.165975, acc 0.9375\n",
      "2017-11-06T13:57:11.807614: step 1565, loss 0.080024, acc 0.9375\n",
      "2017-11-06T13:57:15.953559: step 1566, loss 0.159642, acc 0.96875\n",
      "2017-11-06T13:57:19.953401: step 1567, loss 0.473658, acc 0.875\n",
      "2017-11-06T13:57:23.989269: step 1568, loss 0.188721, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T13:57:28.107196: step 1569, loss 0.413161, acc 0.875\n",
      "2017-11-06T13:57:32.124049: step 1570, loss 0.00483811, acc 1\n",
      "2017-11-06T13:57:36.114885: step 1571, loss 0.0539693, acc 0.96875\n",
      "2017-11-06T13:57:40.146750: step 1572, loss 0.0439584, acc 0.96875\n",
      "2017-11-06T13:57:44.133583: step 1573, loss 0.372234, acc 0.90625\n",
      "2017-11-06T13:57:48.189465: step 1574, loss 0.228186, acc 0.90625\n",
      "2017-11-06T13:57:52.165291: step 1575, loss 0.32253, acc 0.90625\n",
      "2017-11-06T13:57:56.177141: step 1576, loss 0.282032, acc 0.90625\n",
      "2017-11-06T13:58:00.209005: step 1577, loss 0.102941, acc 0.90625\n",
      "2017-11-06T13:58:04.191835: step 1578, loss 0.153951, acc 0.9375\n",
      "2017-11-06T13:58:08.182671: step 1579, loss 0.205362, acc 0.90625\n",
      "2017-11-06T13:58:12.200526: step 1580, loss 0.35918, acc 0.8125\n",
      "2017-11-06T13:58:16.676706: step 1581, loss 0.353417, acc 0.9375\n",
      "2017-11-06T13:58:20.698563: step 1582, loss 0.0648767, acc 1\n",
      "2017-11-06T13:58:24.734433: step 1583, loss 0.405653, acc 0.84375\n",
      "2017-11-06T13:58:27.303256: step 1584, loss 0.0290595, acc 1\n",
      "2017-11-06T13:58:31.318526: step 1585, loss 0.391995, acc 0.84375\n",
      "2017-11-06T13:58:35.555539: step 1586, loss 0.10543, acc 0.96875\n",
      "2017-11-06T13:58:39.515351: step 1587, loss 0.31214, acc 0.90625\n",
      "2017-11-06T13:58:43.532205: step 1588, loss 0.234755, acc 0.90625\n",
      "2017-11-06T13:58:47.604099: step 1589, loss 0.154141, acc 0.9375\n",
      "2017-11-06T13:58:51.599938: step 1590, loss 0.0857724, acc 0.96875\n",
      "2017-11-06T13:58:55.649815: step 1591, loss 0.434609, acc 0.84375\n",
      "2017-11-06T13:58:59.659666: step 1592, loss 0.192362, acc 0.9375\n",
      "2017-11-06T13:59:03.712544: step 1593, loss 0.128628, acc 0.9375\n",
      "2017-11-06T13:59:07.683365: step 1594, loss 0.0729195, acc 0.96875\n",
      "2017-11-06T13:59:11.648183: step 1595, loss 0.0734384, acc 0.96875\n",
      "2017-11-06T13:59:15.732084: step 1596, loss 0.1249, acc 0.9375\n",
      "2017-11-06T13:59:20.004120: step 1597, loss 0.221324, acc 0.90625\n",
      "2017-11-06T13:59:24.525332: step 1598, loss 0.15143, acc 0.90625\n",
      "2017-11-06T13:59:28.737325: step 1599, loss 0.231728, acc 0.875\n",
      "2017-11-06T13:59:32.760183: step 1600, loss 0.0818687, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T13:59:35.316000: step 1600, loss 1.13434, acc 0.7\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-06T13:59:40.720234: step 1601, loss 0.0434046, acc 1\n",
      "2017-11-06T13:59:44.785122: step 1602, loss 0.0528043, acc 1\n",
      "2017-11-06T13:59:48.816987: step 1603, loss 0.0859131, acc 0.9375\n",
      "2017-11-06T13:59:52.819833: step 1604, loss 0.662134, acc 0.78125\n",
      "2017-11-06T13:59:56.886721: step 1605, loss 0.21082, acc 0.90625\n",
      "2017-11-06T14:00:01.144746: step 1606, loss 0.299913, acc 0.8125\n",
      "2017-11-06T14:00:05.231651: step 1607, loss 0.155916, acc 0.9375\n",
      "2017-11-06T14:00:09.245502: step 1608, loss 0.0388258, acc 1\n",
      "2017-11-06T14:00:13.319397: step 1609, loss 0.145319, acc 0.96875\n",
      "2017-11-06T14:00:17.400504: step 1610, loss 0.170985, acc 0.9375\n",
      "2017-11-06T14:00:21.499209: step 1611, loss 0.214045, acc 0.875\n",
      "2017-11-06T14:00:25.781251: step 1612, loss 0.252106, acc 0.84375\n",
      "2017-11-06T14:00:30.056291: step 1613, loss 0.273173, acc 0.875\n",
      "2017-11-06T14:00:34.355344: step 1614, loss 0.173345, acc 0.9375\n",
      "2017-11-06T14:00:38.398217: step 1615, loss 0.395827, acc 0.75\n",
      "2017-11-06T14:00:42.385050: step 1616, loss 0.116813, acc 0.90625\n",
      "2017-11-06T14:00:46.441932: step 1617, loss 0.0574106, acc 0.96875\n",
      "2017-11-06T14:00:50.448780: step 1618, loss 0.130699, acc 0.9375\n",
      "2017-11-06T14:00:54.473640: step 1619, loss 0.33764, acc 0.875\n",
      "2017-11-06T14:00:57.084494: step 1620, loss 0.0697963, acc 0.95\n",
      "2017-11-06T14:01:01.092342: step 1621, loss 0.242048, acc 0.9375\n",
      "2017-11-06T14:01:05.126208: step 1622, loss 0.145804, acc 0.9375\n",
      "2017-11-06T14:01:09.178087: step 1623, loss 0.145694, acc 0.90625\n",
      "2017-11-06T14:01:13.227965: step 1624, loss 0.216973, acc 0.84375\n",
      "2017-11-06T14:01:17.328879: step 1625, loss 0.135196, acc 0.9375\n",
      "2017-11-06T14:01:21.342731: step 1626, loss 0.0605653, acc 0.96875\n",
      "2017-11-06T14:01:25.427633: step 1627, loss 0.165194, acc 0.875\n",
      "2017-11-06T14:01:29.465502: step 1628, loss 0.18063, acc 0.90625\n",
      "2017-11-06T14:01:33.918439: step 1629, loss 0.108717, acc 0.9375\n",
      "2017-11-06T14:01:38.220496: step 1630, loss 0.0431418, acc 1\n",
      "2017-11-06T14:01:42.324412: step 1631, loss 0.154049, acc 0.9375\n",
      "2017-11-06T14:01:46.443338: step 1632, loss 0.278854, acc 0.875\n",
      "2017-11-06T14:01:50.411159: step 1633, loss 0.259159, acc 0.84375\n",
      "2017-11-06T14:01:54.445024: step 1634, loss 0.126595, acc 0.90625\n",
      "2017-11-06T14:01:58.532929: step 1635, loss 0.211846, acc 0.90625\n",
      "2017-11-06T14:02:02.597817: step 1636, loss 0.125652, acc 0.9375\n",
      "2017-11-06T14:02:06.647695: step 1637, loss 0.0601833, acc 0.96875\n",
      "2017-11-06T14:02:10.749610: step 1638, loss 0.19603, acc 0.875\n",
      "2017-11-06T14:02:14.836513: step 1639, loss 0.200909, acc 0.9375\n",
      "2017-11-06T14:02:18.926420: step 1640, loss 0.0214008, acc 1\n",
      "2017-11-06T14:02:22.945275: step 1641, loss 0.400724, acc 0.84375\n",
      "2017-11-06T14:02:27.085217: step 1642, loss 0.0789715, acc 0.96875\n",
      "2017-11-06T14:02:31.106092: step 1643, loss 0.123052, acc 0.90625\n",
      "2017-11-06T14:02:35.415135: step 1644, loss 0.0845964, acc 0.9375\n",
      "2017-11-06T14:02:39.915333: step 1645, loss 0.0870124, acc 0.96875\n",
      "2017-11-06T14:02:43.980221: step 1646, loss 0.250698, acc 0.875\n",
      "2017-11-06T14:02:48.065124: step 1647, loss 0.217674, acc 0.90625\n",
      "2017-11-06T14:02:52.088984: step 1648, loss 0.325131, acc 0.8125\n",
      "2017-11-06T14:02:56.126852: step 1649, loss 0.152501, acc 0.90625\n",
      "2017-11-06T14:03:00.196744: step 1650, loss 0.305198, acc 0.875\n",
      "2017-11-06T14:03:04.275642: step 1651, loss 0.052551, acc 0.96875\n",
      "2017-11-06T14:03:08.296499: step 1652, loss 0.236397, acc 0.90625\n",
      "2017-11-06T14:03:12.357384: step 1653, loss 0.240978, acc 0.90625\n",
      "2017-11-06T14:03:16.439285: step 1654, loss 0.127398, acc 0.9375\n",
      "2017-11-06T14:03:20.453136: step 1655, loss 0.243815, acc 0.84375\n",
      "2017-11-06T14:03:22.962920: step 1656, loss 0.0740191, acc 0.95\n",
      "2017-11-06T14:03:26.961761: step 1657, loss 0.171816, acc 0.9375\n",
      "2017-11-06T14:03:31.007637: step 1658, loss 0.0961622, acc 0.96875\n",
      "2017-11-06T14:03:34.981460: step 1659, loss 0.210618, acc 0.9375\n",
      "2017-11-06T14:03:39.006320: step 1660, loss 0.138348, acc 0.9375\n",
      "2017-11-06T14:03:43.528533: step 1661, loss 0.102567, acc 0.96875\n",
      "2017-11-06T14:03:47.823584: step 1662, loss 0.0920115, acc 0.96875\n",
      "2017-11-06T14:03:52.044584: step 1663, loss 0.154377, acc 0.90625\n",
      "2017-11-06T14:03:56.102466: step 1664, loss 0.175367, acc 0.9375\n",
      "2017-11-06T14:04:00.160350: step 1665, loss 0.273649, acc 0.90625\n",
      "2017-11-06T14:04:04.344323: step 1666, loss 0.304386, acc 0.90625\n",
      "2017-11-06T14:04:08.425223: step 1667, loss 0.280558, acc 0.84375\n",
      "2017-11-06T14:04:12.651227: step 1668, loss 0.23795, acc 0.9375\n",
      "2017-11-06T14:04:16.731124: step 1669, loss 0.249447, acc 0.875\n",
      "2017-11-06T14:04:20.879072: step 1670, loss 0.0499756, acc 0.96875\n",
      "2017-11-06T14:04:25.247176: step 1671, loss 0.119828, acc 0.90625\n",
      "2017-11-06T14:04:29.695335: step 1672, loss 0.212415, acc 0.90625\n",
      "2017-11-06T14:04:34.078279: step 1673, loss 0.137086, acc 0.9375\n",
      "2017-11-06T14:04:38.267255: step 1674, loss 0.286932, acc 0.90625\n",
      "2017-11-06T14:04:42.454229: step 1675, loss 0.184191, acc 0.90625\n",
      "2017-11-06T14:04:46.663220: step 1676, loss 0.193295, acc 0.9375\n",
      "2017-11-06T14:04:51.117385: step 1677, loss 0.389714, acc 0.875\n",
      "2017-11-06T14:04:55.129236: step 1678, loss 0.161213, acc 0.9375\n",
      "2017-11-06T14:04:59.217140: step 1679, loss 0.160792, acc 0.9375\n",
      "2017-11-06T14:05:03.177956: step 1680, loss 0.284551, acc 0.875\n",
      "2017-11-06T14:05:07.184801: step 1681, loss 0.070138, acc 0.9375\n",
      "2017-11-06T14:05:11.161627: step 1682, loss 0.259213, acc 0.875\n",
      "2017-11-06T14:05:15.270546: step 1683, loss 0.0802669, acc 0.96875\n",
      "2017-11-06T14:05:19.277394: step 1684, loss 0.141411, acc 0.96875\n",
      "2017-11-06T14:05:23.285242: step 1685, loss 0.262083, acc 0.875\n",
      "2017-11-06T14:05:27.280080: step 1686, loss 0.272421, acc 0.90625\n",
      "2017-11-06T14:05:31.302939: step 1687, loss 0.38787, acc 0.84375\n",
      "2017-11-06T14:05:35.272759: step 1688, loss 0.307356, acc 0.8125\n",
      "2017-11-06T14:05:39.274602: step 1689, loss 0.0513365, acc 0.96875\n",
      "2017-11-06T14:05:43.276446: step 1690, loss 0.0324508, acc 1\n",
      "2017-11-06T14:05:47.297303: step 1691, loss 0.272328, acc 0.8125\n",
      "2017-11-06T14:05:49.854120: step 1692, loss 0.125127, acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T14:05:54.088128: step 1693, loss 0.172544, acc 0.90625\n",
      "2017-11-06T14:05:58.439220: step 1694, loss 0.0706394, acc 1\n",
      "2017-11-06T14:06:02.468082: step 1695, loss 0.0659967, acc 0.96875\n",
      "2017-11-06T14:06:06.501949: step 1696, loss 0.261078, acc 0.90625\n",
      "2017-11-06T14:06:10.550825: step 1697, loss 0.0523212, acc 0.96875\n",
      "2017-11-06T14:06:14.660746: step 1698, loss 0.203321, acc 0.90625\n",
      "2017-11-06T14:06:18.665591: step 1699, loss 0.138136, acc 0.9375\n",
      "2017-11-06T14:06:22.683446: step 1700, loss 0.472914, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:06:25.389369: step 1700, loss 1.13321, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-06T14:06:31.037357: step 1701, loss 0.243459, acc 0.84375\n",
      "2017-11-06T14:06:35.269364: step 1702, loss 0.220004, acc 0.9375\n",
      "2017-11-06T14:06:39.355268: step 1703, loss 0.149236, acc 0.90625\n",
      "2017-11-06T14:06:43.452199: step 1704, loss 0.113126, acc 0.96875\n",
      "2017-11-06T14:06:47.438011: step 1705, loss 0.0609294, acc 1\n",
      "2017-11-06T14:06:51.419842: step 1706, loss 0.0776227, acc 0.96875\n",
      "2017-11-06T14:06:55.419682: step 1707, loss 0.109901, acc 0.9375\n",
      "2017-11-06T14:06:59.693719: step 1708, loss 0.279239, acc 0.875\n",
      "2017-11-06T14:07:03.953746: step 1709, loss 0.197544, acc 0.9375\n",
      "2017-11-06T14:07:07.914561: step 1710, loss 0.235745, acc 0.9375\n",
      "2017-11-06T14:07:11.929415: step 1711, loss 0.170543, acc 0.96875\n",
      "2017-11-06T14:07:15.989298: step 1712, loss 0.151406, acc 0.9375\n",
      "2017-11-06T14:07:20.032269: step 1713, loss 0.0984369, acc 0.90625\n",
      "2017-11-06T14:07:24.095155: step 1714, loss 0.0915378, acc 0.96875\n",
      "2017-11-06T14:07:28.092995: step 1715, loss 0.323103, acc 0.90625\n",
      "2017-11-06T14:07:32.121858: step 1716, loss 0.242234, acc 0.90625\n",
      "2017-11-06T14:07:36.130464: step 1717, loss 0.296985, acc 0.90625\n",
      "2017-11-06T14:07:40.168333: step 1718, loss 0.0518812, acc 1\n",
      "2017-11-06T14:07:44.174179: step 1719, loss 0.414209, acc 0.90625\n",
      "2017-11-06T14:07:48.276094: step 1720, loss 0.126442, acc 0.9375\n",
      "2017-11-06T14:07:52.244916: step 1721, loss 0.157487, acc 0.9375\n",
      "2017-11-06T14:07:56.249759: step 1722, loss 0.226369, acc 0.9375\n",
      "2017-11-06T14:08:00.322654: step 1723, loss 0.440425, acc 0.84375\n",
      "2017-11-06T14:08:04.525640: step 1724, loss 0.169549, acc 0.9375\n",
      "2017-11-06T14:08:08.831700: step 1725, loss 0.125014, acc 0.9375\n",
      "2017-11-06T14:08:12.827539: step 1726, loss 0.0552781, acc 1\n",
      "2017-11-06T14:08:16.860404: step 1727, loss 0.30641, acc 0.875\n",
      "2017-11-06T14:08:19.347173: step 1728, loss 0.256917, acc 0.9\n",
      "2017-11-06T14:08:23.538149: step 1729, loss 0.247679, acc 0.90625\n",
      "2017-11-06T14:08:27.629056: step 1730, loss 0.290351, acc 0.875\n",
      "2017-11-06T14:08:31.668926: step 1731, loss 0.359127, acc 0.875\n",
      "2017-11-06T14:08:35.875916: step 1732, loss 0.499378, acc 0.875\n",
      "2017-11-06T14:08:39.863751: step 1733, loss 0.295721, acc 0.90625\n",
      "2017-11-06T14:08:43.870596: step 1734, loss 0.0728878, acc 0.96875\n",
      "2017-11-06T14:08:47.877445: step 1735, loss 0.219824, acc 0.90625\n",
      "2017-11-06T14:08:51.864276: step 1736, loss 0.0396743, acc 0.96875\n",
      "2017-11-06T14:08:55.945176: step 1737, loss 0.278044, acc 0.875\n",
      "2017-11-06T14:08:59.938014: step 1738, loss 0.427418, acc 0.84375\n",
      "2017-11-06T14:09:03.944860: step 1739, loss 0.18199, acc 0.9375\n",
      "2017-11-06T14:09:07.921687: step 1740, loss 0.268029, acc 0.90625\n",
      "2017-11-06T14:09:12.288789: step 1741, loss 0.470712, acc 0.875\n",
      "2017-11-06T14:09:16.462754: step 1742, loss 0.238816, acc 0.875\n",
      "2017-11-06T14:09:20.453589: step 1743, loss 0.269535, acc 0.8125\n",
      "2017-11-06T14:09:24.455433: step 1744, loss 0.169077, acc 0.90625\n",
      "2017-11-06T14:09:28.479294: step 1745, loss 0.327008, acc 0.875\n",
      "2017-11-06T14:09:32.518162: step 1746, loss 0.348319, acc 0.84375\n",
      "2017-11-06T14:09:36.523008: step 1747, loss 0.183962, acc 0.90625\n",
      "2017-11-06T14:09:40.511843: step 1748, loss 0.248177, acc 0.90625\n",
      "2017-11-06T14:09:44.525694: step 1749, loss 0.07799, acc 0.96875\n",
      "2017-11-06T14:09:48.494515: step 1750, loss 0.0763977, acc 0.96875\n",
      "2017-11-06T14:09:52.515371: step 1751, loss 0.0929702, acc 0.9375\n",
      "2017-11-06T14:09:56.525220: step 1752, loss 0.338371, acc 0.84375\n",
      "2017-11-06T14:10:00.705191: step 1753, loss 0.450372, acc 0.875\n",
      "2017-11-06T14:10:04.915182: step 1754, loss 0.380219, acc 0.875\n",
      "2017-11-06T14:10:08.894008: step 1755, loss 0.271198, acc 0.84375\n",
      "2017-11-06T14:10:12.892850: step 1756, loss 0.0642073, acc 0.96875\n",
      "2017-11-06T14:10:17.281969: step 1757, loss 0.104598, acc 0.96875\n",
      "2017-11-06T14:10:21.427915: step 1758, loss 0.135115, acc 0.90625\n",
      "2017-11-06T14:10:25.465783: step 1759, loss 0.299335, acc 0.875\n",
      "2017-11-06T14:10:29.450616: step 1760, loss 0.372539, acc 0.84375\n",
      "2017-11-06T14:10:33.729465: step 1761, loss 0.0905462, acc 0.96875\n",
      "2017-11-06T14:10:37.903431: step 1762, loss 0.184157, acc 0.9375\n",
      "2017-11-06T14:10:41.954310: step 1763, loss 0.136602, acc 0.96875\n",
      "2017-11-06T14:10:44.540146: step 1764, loss 0.00877888, acc 1\n",
      "2017-11-06T14:10:48.531982: step 1765, loss 0.138623, acc 0.90625\n",
      "2017-11-06T14:10:52.551839: step 1766, loss 0.233086, acc 0.90625\n",
      "2017-11-06T14:10:56.561688: step 1767, loss 0.00641345, acc 1\n",
      "2017-11-06T14:11:00.601558: step 1768, loss 0.078726, acc 0.96875\n",
      "2017-11-06T14:11:04.612409: step 1769, loss 0.0342415, acc 0.96875\n",
      "2017-11-06T14:11:08.621257: step 1770, loss 0.264305, acc 0.90625\n",
      "2017-11-06T14:11:12.663128: step 1771, loss 0.231365, acc 0.875\n",
      "2017-11-06T14:11:16.674979: step 1772, loss 0.268743, acc 0.9375\n",
      "2017-11-06T14:11:20.858952: step 1773, loss 0.065097, acc 0.96875\n",
      "2017-11-06T14:11:25.257077: step 1774, loss 0.220554, acc 0.90625\n",
      "2017-11-06T14:11:29.259921: step 1775, loss 0.124549, acc 0.9375\n",
      "2017-11-06T14:11:33.336819: step 1776, loss 0.166314, acc 0.9375\n",
      "2017-11-06T14:11:37.383694: step 1777, loss 0.159498, acc 0.9375\n",
      "2017-11-06T14:11:41.341506: step 1778, loss 0.162526, acc 0.9375\n",
      "2017-11-06T14:11:45.350354: step 1779, loss 0.134955, acc 0.90625\n",
      "2017-11-06T14:11:49.361203: step 1780, loss 0.173389, acc 0.84375\n",
      "2017-11-06T14:11:53.395070: step 1781, loss 0.164789, acc 0.90625\n",
      "2017-11-06T14:11:57.386906: step 1782, loss 0.238404, acc 0.8125\n",
      "2017-11-06T14:12:01.402760: step 1783, loss 0.425122, acc 0.78125\n",
      "2017-11-06T14:12:05.409607: step 1784, loss 0.047361, acc 0.96875\n",
      "2017-11-06T14:12:09.446475: step 1785, loss 0.168462, acc 0.9375\n",
      "2017-11-06T14:12:13.678482: step 1786, loss 0.265779, acc 0.8125\n",
      "2017-11-06T14:12:17.662313: step 1787, loss 0.249991, acc 0.875\n",
      "2017-11-06T14:12:21.742212: step 1788, loss 0.442415, acc 0.84375\n",
      "2017-11-06T14:12:25.781081: step 1789, loss 0.20367, acc 0.90625\n",
      "2017-11-06T14:12:30.205226: step 1790, loss 0.57527, acc 0.78125\n",
      "2017-11-06T14:12:34.512286: step 1791, loss 0.0208677, acc 1\n",
      "2017-11-06T14:12:38.522134: step 1792, loss 0.0986339, acc 0.96875\n",
      "2017-11-06T14:12:42.534986: step 1793, loss 0.0620261, acc 0.96875\n",
      "2017-11-06T14:12:46.527823: step 1794, loss 0.133281, acc 0.90625\n",
      "2017-11-06T14:12:50.551684: step 1795, loss 0.291434, acc 0.875\n",
      "2017-11-06T14:12:54.644591: step 1796, loss 0.0790311, acc 0.96875\n",
      "2017-11-06T14:12:58.669450: step 1797, loss 0.171439, acc 0.96875\n",
      "2017-11-06T14:13:02.700315: step 1798, loss 0.427472, acc 0.84375\n",
      "2017-11-06T14:13:06.719170: step 1799, loss 0.319777, acc 0.9375\n",
      "2017-11-06T14:13:09.282994: step 1800, loss 0.0546989, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:13:11.891845: step 1800, loss 1.20934, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-06T14:13:17.203004: step 1801, loss 0.174527, acc 0.9375\n",
      "2017-11-06T14:13:21.215853: step 1802, loss 0.128432, acc 0.9375\n",
      "2017-11-06T14:13:25.404831: step 1803, loss 0.00758105, acc 1\n",
      "2017-11-06T14:13:29.594826: step 1804, loss 0.163547, acc 0.9375\n",
      "2017-11-06T14:13:33.797589: step 1805, loss 0.296917, acc 0.875\n",
      "2017-11-06T14:13:38.002577: step 1806, loss 0.265554, acc 0.90625\n",
      "2017-11-06T14:13:42.008423: step 1807, loss 0.180167, acc 0.90625\n",
      "2017-11-06T14:13:46.001261: step 1808, loss 0.0914235, acc 0.9375\n",
      "2017-11-06T14:13:50.005107: step 1809, loss 0.289655, acc 0.875\n",
      "2017-11-06T14:13:53.999964: step 1810, loss 0.159631, acc 0.9375\n",
      "2017-11-06T14:13:58.003788: step 1811, loss 0.059876, acc 0.96875\n",
      "2017-11-06T14:14:02.037656: step 1812, loss 0.158937, acc 0.9375\n",
      "2017-11-06T14:14:06.039498: step 1813, loss 0.317425, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T14:14:10.059355: step 1814, loss 0.09435, acc 0.96875\n",
      "2017-11-06T14:14:14.011163: step 1815, loss 0.169787, acc 0.90625\n",
      "2017-11-06T14:14:18.006001: step 1816, loss 0.0665014, acc 0.96875\n",
      "2017-11-06T14:14:22.006845: step 1817, loss 0.0938852, acc 0.9375\n",
      "2017-11-06T14:14:26.001683: step 1818, loss 0.125874, acc 0.96875\n",
      "2017-11-06T14:14:29.970503: step 1819, loss 0.150915, acc 0.9375\n",
      "2017-11-06T14:14:34.197506: step 1820, loss 0.228298, acc 0.96875\n",
      "2017-11-06T14:14:38.504566: step 1821, loss 0.136717, acc 0.96875\n",
      "2017-11-06T14:14:42.782605: step 1822, loss 0.29363, acc 0.84375\n",
      "2017-11-06T14:14:46.727410: step 1823, loss 0.307389, acc 0.8125\n",
      "2017-11-06T14:14:50.734257: step 1824, loss 0.0385715, acc 1\n",
      "2017-11-06T14:14:54.811153: step 1825, loss 0.409409, acc 0.78125\n",
      "2017-11-06T14:14:58.788979: step 1826, loss 0.167555, acc 0.90625\n",
      "2017-11-06T14:15:02.793825: step 1827, loss 0.0845216, acc 0.96875\n",
      "2017-11-06T14:15:06.759643: step 1828, loss 0.0228883, acc 1\n",
      "2017-11-06T14:15:10.739472: step 1829, loss 0.182713, acc 0.9375\n",
      "2017-11-06T14:15:14.763330: step 1830, loss 0.244014, acc 0.84375\n",
      "2017-11-06T14:15:18.744158: step 1831, loss 0.214529, acc 0.84375\n",
      "2017-11-06T14:15:22.726988: step 1832, loss 0.224355, acc 0.875\n",
      "2017-11-06T14:15:26.728832: step 1833, loss 0.291057, acc 0.90625\n",
      "2017-11-06T14:15:30.724670: step 1834, loss 0.0673271, acc 1\n",
      "2017-11-06T14:15:34.730517: step 1835, loss 0.218585, acc 0.90625\n",
      "2017-11-06T14:15:37.279328: step 1836, loss 0.119048, acc 0.95\n",
      "2017-11-06T14:15:41.325203: step 1837, loss 0.17326, acc 0.96875\n",
      "2017-11-06T14:15:45.693307: step 1838, loss 0.18017, acc 0.90625\n",
      "2017-11-06T14:15:49.911303: step 1839, loss 0.306052, acc 0.875\n",
      "2017-11-06T14:15:53.927157: step 1840, loss 0.375295, acc 0.84375\n",
      "2017-11-06T14:15:57.919994: step 1841, loss 0.296126, acc 0.84375\n",
      "2017-11-06T14:16:01.864798: step 1842, loss 0.0837685, acc 0.96875\n",
      "2017-11-06T14:16:05.899664: step 1843, loss 0.144471, acc 0.90625\n",
      "2017-11-06T14:16:09.951543: step 1844, loss 0.0920702, acc 0.96875\n",
      "2017-11-06T14:16:13.930370: step 1845, loss 0.131526, acc 0.9375\n",
      "2017-11-06T14:16:18.002263: step 1846, loss 0.0790446, acc 0.9375\n",
      "2017-11-06T14:16:21.988096: step 1847, loss 0.266892, acc 0.84375\n",
      "2017-11-06T14:16:26.014957: step 1848, loss 0.0907359, acc 0.9375\n",
      "2017-11-06T14:16:30.172911: step 1849, loss 0.124205, acc 0.9375\n",
      "2017-11-06T14:16:34.460740: step 1850, loss 0.143502, acc 0.9375\n",
      "2017-11-06T14:16:38.556649: step 1851, loss 0.255141, acc 0.875\n",
      "2017-11-06T14:16:42.574504: step 1852, loss 0.135352, acc 0.90625\n",
      "2017-11-06T14:16:46.584354: step 1853, loss 0.316196, acc 0.8125\n",
      "2017-11-06T14:16:50.962464: step 1854, loss 0.105253, acc 0.9375\n",
      "2017-11-06T14:16:55.161448: step 1855, loss 0.228278, acc 0.90625\n",
      "2017-11-06T14:16:59.146280: step 1856, loss 0.311475, acc 0.84375\n",
      "2017-11-06T14:17:03.155128: step 1857, loss 0.102405, acc 0.96875\n",
      "2017-11-06T14:17:07.145963: step 1858, loss 0.171929, acc 0.875\n",
      "2017-11-06T14:17:11.180831: step 1859, loss 0.216591, acc 0.875\n",
      "2017-11-06T14:17:15.200687: step 1860, loss 0.167855, acc 0.9375\n",
      "2017-11-06T14:17:19.197527: step 1861, loss 0.16283, acc 0.90625\n",
      "2017-11-06T14:17:23.251408: step 1862, loss 0.372841, acc 0.875\n",
      "2017-11-06T14:17:27.401356: step 1863, loss 0.15856, acc 0.9375\n",
      "2017-11-06T14:17:31.448233: step 1864, loss 0.178195, acc 0.9375\n",
      "2017-11-06T14:17:35.433064: step 1865, loss 0.224928, acc 0.9375\n",
      "2017-11-06T14:17:39.478956: step 1866, loss 0.303873, acc 0.90625\n",
      "2017-11-06T14:17:43.502796: step 1867, loss 0.291873, acc 0.875\n",
      "2017-11-06T14:17:47.492632: step 1868, loss 0.143094, acc 0.9375\n",
      "2017-11-06T14:17:51.590543: step 1869, loss 0.059149, acc 0.96875\n",
      "2017-11-06T14:17:55.880592: step 1870, loss 0.269569, acc 0.84375\n",
      "2017-11-06T14:18:00.126608: step 1871, loss 0.480142, acc 0.84375\n",
      "2017-11-06T14:18:02.632389: step 1872, loss 0.249634, acc 0.85\n",
      "2017-11-06T14:18:06.648243: step 1873, loss 0.173704, acc 0.9375\n",
      "2017-11-06T14:18:10.598049: step 1874, loss 0.172293, acc 0.90625\n",
      "2017-11-06T14:18:14.659935: step 1875, loss 0.228104, acc 0.90625\n",
      "2017-11-06T14:18:18.612744: step 1876, loss 0.16734, acc 0.90625\n",
      "2017-11-06T14:18:22.564551: step 1877, loss 0.212245, acc 0.9375\n",
      "2017-11-06T14:18:26.643451: step 1878, loss 0.0653887, acc 0.96875\n",
      "2017-11-06T14:18:30.663306: step 1879, loss 0.14211, acc 0.90625\n",
      "2017-11-06T14:18:34.896315: step 1880, loss 0.162822, acc 0.90625\n",
      "2017-11-06T14:18:38.888150: step 1881, loss 0.0743081, acc 0.96875\n",
      "2017-11-06T14:18:42.979057: step 1882, loss 0.254139, acc 0.875\n",
      "2017-11-06T14:18:47.009921: step 1883, loss 0.202488, acc 0.90625\n",
      "2017-11-06T14:18:50.967733: step 1884, loss 0.256003, acc 0.875\n",
      "2017-11-06T14:18:54.934553: step 1885, loss 0.190315, acc 0.9375\n",
      "2017-11-06T14:18:58.955409: step 1886, loss 0.171173, acc 0.90625\n",
      "2017-11-06T14:19:03.387558: step 1887, loss 0.0167182, acc 1\n",
      "2017-11-06T14:19:07.457450: step 1888, loss 0.133764, acc 0.9375\n",
      "2017-11-06T14:19:11.502324: step 1889, loss 0.0509403, acc 1\n",
      "2017-11-06T14:19:15.509171: step 1890, loss 0.280968, acc 0.875\n",
      "2017-11-06T14:19:19.527026: step 1891, loss 0.230494, acc 0.90625\n",
      "2017-11-06T14:19:23.672972: step 1892, loss 0.0999929, acc 0.96875\n",
      "2017-11-06T14:19:27.817917: step 1893, loss 0.14347, acc 0.90625\n",
      "2017-11-06T14:19:31.870796: step 1894, loss 0.15001, acc 0.9375\n",
      "2017-11-06T14:19:35.809418: step 1895, loss 0.245554, acc 0.90625\n",
      "2017-11-06T14:19:39.835277: step 1896, loss 0.0781341, acc 0.96875\n",
      "2017-11-06T14:19:43.907171: step 1897, loss 0.250673, acc 0.875\n",
      "2017-11-06T14:19:47.956048: step 1898, loss 0.126485, acc 0.96875\n",
      "2017-11-06T14:19:52.056962: step 1899, loss 0.229926, acc 0.84375\n",
      "2017-11-06T14:19:56.029785: step 1900, loss 0.223061, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:19:58.626631: step 1900, loss 1.15619, acc 0.666667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-06T14:20:04.252360: step 1901, loss 0.139817, acc 0.90625\n",
      "2017-11-06T14:20:08.625468: step 1902, loss 0.315367, acc 0.84375\n",
      "2017-11-06T14:20:12.667340: step 1903, loss 0.127253, acc 0.9375\n",
      "2017-11-06T14:20:16.610140: step 1904, loss 0.237652, acc 0.875\n",
      "2017-11-06T14:20:20.578962: step 1905, loss 0.355671, acc 0.84375\n",
      "2017-11-06T14:20:24.562792: step 1906, loss 0.110056, acc 0.96875\n",
      "2017-11-06T14:20:28.577647: step 1907, loss 0.191162, acc 0.9375\n",
      "2017-11-06T14:20:31.146470: step 1908, loss 0.0443033, acc 0.95\n",
      "2017-11-06T14:20:35.357462: step 1909, loss 0.407833, acc 0.875\n",
      "2017-11-06T14:20:39.350299: step 1910, loss 0.297704, acc 0.90625\n",
      "2017-11-06T14:20:43.284094: step 1911, loss 0.172513, acc 0.875\n",
      "2017-11-06T14:20:47.278934: step 1912, loss 0.336541, acc 0.875\n",
      "2017-11-06T14:20:51.222735: step 1913, loss 0.502614, acc 0.8125\n",
      "2017-11-06T14:20:55.198560: step 1914, loss 0.06641, acc 0.96875\n",
      "2017-11-06T14:20:59.095329: step 1915, loss 0.167753, acc 0.90625\n",
      "2017-11-06T14:21:03.052140: step 1916, loss 0.408366, acc 0.875\n",
      "2017-11-06T14:21:07.006950: step 1917, loss 0.199895, acc 0.875\n",
      "2017-11-06T14:21:11.047821: step 1918, loss 0.277235, acc 0.84375\n",
      "2017-11-06T14:21:15.428934: step 1919, loss 0.180743, acc 0.9375\n",
      "2017-11-06T14:21:19.354723: step 1920, loss 0.248481, acc 0.875\n",
      "2017-11-06T14:21:23.315538: step 1921, loss 0.107365, acc 0.9375\n",
      "2017-11-06T14:21:27.425997: step 1922, loss 0.101684, acc 0.96875\n",
      "2017-11-06T14:21:31.418835: step 1923, loss 0.148197, acc 0.9375\n",
      "2017-11-06T14:21:35.403666: step 1924, loss 0.170484, acc 0.875\n",
      "2017-11-06T14:21:39.361479: step 1925, loss 0.214675, acc 0.9375\n",
      "2017-11-06T14:21:43.335301: step 1926, loss 0.180886, acc 0.9375\n",
      "2017-11-06T14:21:47.373170: step 1927, loss 0.168147, acc 0.90625\n",
      "2017-11-06T14:21:51.319976: step 1928, loss 0.206809, acc 0.90625\n",
      "2017-11-06T14:21:55.227752: step 1929, loss 0.406497, acc 0.90625\n",
      "2017-11-06T14:21:59.192569: step 1930, loss 0.182485, acc 0.90625\n",
      "2017-11-06T14:22:03.223433: step 1931, loss 0.283665, acc 0.90625\n",
      "2017-11-06T14:22:07.172239: step 1932, loss 0.200619, acc 0.875\n",
      "2017-11-06T14:22:11.107034: step 1933, loss 0.195511, acc 0.90625\n",
      "2017-11-06T14:22:15.061845: step 1934, loss 0.235238, acc 0.90625\n",
      "2017-11-06T14:22:19.416939: step 1935, loss 0.15183, acc 0.9375\n",
      "2017-11-06T14:22:23.508846: step 1936, loss 0.141885, acc 0.875\n",
      "2017-11-06T14:22:27.462656: step 1937, loss 0.0188429, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T14:22:31.425471: step 1938, loss 0.0625781, acc 0.96875\n",
      "2017-11-06T14:22:35.741302: step 1939, loss 0.163271, acc 0.90625\n",
      "2017-11-06T14:22:39.740144: step 1940, loss 0.027, acc 1\n",
      "2017-11-06T14:22:43.762001: step 1941, loss 0.0734942, acc 0.96875\n",
      "2017-11-06T14:22:47.709806: step 1942, loss 0.182407, acc 0.90625\n",
      "2017-11-06T14:22:51.646603: step 1943, loss 0.283354, acc 0.90625\n",
      "2017-11-06T14:22:54.155386: step 1944, loss 0.0899049, acc 0.95\n",
      "2017-11-06T14:22:58.105193: step 1945, loss 0.135329, acc 0.9375\n",
      "2017-11-06T14:23:02.053998: step 1946, loss 0.11248, acc 0.9375\n",
      "2017-11-06T14:23:06.082861: step 1947, loss 0.0339699, acc 0.96875\n",
      "2017-11-06T14:23:10.011653: step 1948, loss 0.183223, acc 0.90625\n",
      "2017-11-06T14:23:13.962460: step 1949, loss 0.223758, acc 0.84375\n",
      "2017-11-06T14:23:17.900258: step 1950, loss 0.0914164, acc 0.9375\n",
      "2017-11-06T14:23:21.972151: step 1951, loss 0.161615, acc 0.875\n",
      "2017-11-06T14:23:26.682497: step 1952, loss 0.0364929, acc 1\n",
      "2017-11-06T14:23:30.758416: step 1953, loss 0.183644, acc 0.90625\n",
      "2017-11-06T14:23:34.785277: step 1954, loss 0.0855365, acc 0.96875\n",
      "2017-11-06T14:23:38.780115: step 1955, loss 0.157947, acc 0.9375\n",
      "2017-11-06T14:23:42.740930: step 1956, loss 0.486302, acc 0.84375\n",
      "2017-11-06T14:23:46.715753: step 1957, loss 0.247634, acc 0.84375\n",
      "2017-11-06T14:23:50.805659: step 1958, loss 0.258461, acc 0.84375\n",
      "2017-11-06T14:23:54.745459: step 1959, loss 0.168646, acc 0.90625\n",
      "2017-11-06T14:23:58.682256: step 1960, loss 0.05693, acc 0.96875\n",
      "2017-11-06T14:24:02.613049: step 1961, loss 0.0433238, acc 0.96875\n",
      "2017-11-06T14:24:06.579868: step 1962, loss 0.280433, acc 0.84375\n",
      "2017-11-06T14:24:10.508660: step 1963, loss 0.229309, acc 0.9375\n",
      "2017-11-06T14:24:14.462469: step 1964, loss 0.126943, acc 0.9375\n",
      "2017-11-06T14:24:18.405270: step 1965, loss 0.155961, acc 0.9375\n",
      "2017-11-06T14:24:22.460151: step 1966, loss 0.302573, acc 0.875\n",
      "2017-11-06T14:24:26.444983: step 1967, loss 0.210972, acc 0.90625\n",
      "2017-11-06T14:24:30.831099: step 1968, loss 0.141093, acc 0.96875\n",
      "2017-11-06T14:24:35.213213: step 1969, loss 0.224478, acc 0.875\n",
      "2017-11-06T14:24:39.179030: step 1970, loss 0.205188, acc 0.90625\n",
      "2017-11-06T14:24:43.150855: step 1971, loss 0.27187, acc 0.875\n",
      "2017-11-06T14:24:47.086651: step 1972, loss 0.100244, acc 0.96875\n",
      "2017-11-06T14:24:51.088493: step 1973, loss 0.161868, acc 0.9375\n",
      "2017-11-06T14:24:55.084332: step 1974, loss 0.0651466, acc 0.96875\n",
      "2017-11-06T14:24:59.049151: step 1975, loss 0.195387, acc 0.90625\n",
      "2017-11-06T14:25:03.025975: step 1976, loss 0.182108, acc 0.90625\n",
      "2017-11-06T14:25:07.001800: step 1977, loss 0.182193, acc 0.90625\n",
      "2017-11-06T14:25:10.915581: step 1978, loss 0.0757886, acc 0.96875\n",
      "2017-11-06T14:25:14.899412: step 1979, loss 0.145554, acc 0.9375\n",
      "2017-11-06T14:25:17.427208: step 1980, loss 0.116492, acc 0.9\n",
      "2017-11-06T14:25:21.371010: step 1981, loss 0.125569, acc 0.90625\n",
      "2017-11-06T14:25:25.331826: step 1982, loss 0.244026, acc 0.90625\n",
      "2017-11-06T14:25:29.307649: step 1983, loss 0.0460638, acc 1\n",
      "2017-11-06T14:25:33.376633: step 1984, loss 0.437423, acc 0.84375\n",
      "2017-11-06T14:25:37.745524: step 1985, loss 0.154052, acc 0.9375\n",
      "2017-11-06T14:25:41.719347: step 1986, loss 0.121907, acc 0.96875\n",
      "2017-11-06T14:25:45.663150: step 1987, loss 0.393562, acc 0.8125\n",
      "2017-11-06T14:25:49.623964: step 1988, loss 0.125409, acc 0.9375\n",
      "2017-11-06T14:25:53.623807: step 1989, loss 0.333926, acc 0.84375\n",
      "2017-11-06T14:25:57.550596: step 1990, loss 0.308911, acc 0.90625\n",
      "2017-11-06T14:26:01.543433: step 1991, loss 0.201106, acc 0.90625\n",
      "2017-11-06T14:26:05.573298: step 1992, loss 0.395132, acc 0.84375\n",
      "2017-11-06T14:26:09.535113: step 1993, loss 0.154195, acc 0.90625\n",
      "2017-11-06T14:26:13.506935: step 1994, loss 0.186572, acc 0.90625\n",
      "2017-11-06T14:26:17.506776: step 1995, loss 0.397006, acc 0.875\n",
      "2017-11-06T14:26:21.444574: step 1996, loss 0.342778, acc 0.90625\n",
      "2017-11-06T14:26:25.387376: step 1997, loss 0.262281, acc 0.96875\n",
      "2017-11-06T14:26:29.488291: step 1998, loss 0.0342136, acc 0.96875\n",
      "2017-11-06T14:26:33.579197: step 1999, loss 0.174965, acc 0.90625\n",
      "2017-11-06T14:26:37.666100: step 2000, loss 0.190494, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:26:40.516125: step 2000, loss 1.1623, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-06T14:26:46.156344: step 2001, loss 0.1808, acc 0.9375\n",
      "2017-11-06T14:26:50.173200: step 2002, loss 0.247615, acc 0.9375\n",
      "2017-11-06T14:26:54.207065: step 2003, loss 0.0329454, acc 1\n",
      "2017-11-06T14:26:58.150867: step 2004, loss 0.187122, acc 0.90625\n",
      "2017-11-06T14:27:02.151709: step 2005, loss 0.262358, acc 0.90625\n",
      "2017-11-06T14:27:06.155555: step 2006, loss 0.0925427, acc 0.96875\n",
      "2017-11-06T14:27:10.095356: step 2007, loss 0.160883, acc 0.96875\n",
      "2017-11-06T14:27:14.103203: step 2008, loss 0.225341, acc 0.875\n",
      "2017-11-06T14:27:18.016983: step 2009, loss 0.248499, acc 0.90625\n",
      "2017-11-06T14:27:21.959784: step 2010, loss 0.0782835, acc 0.96875\n",
      "2017-11-06T14:27:25.905587: step 2011, loss 0.205545, acc 0.90625\n",
      "2017-11-06T14:27:29.837382: step 2012, loss 0.113689, acc 0.9375\n",
      "2017-11-06T14:27:33.845229: step 2013, loss 0.227372, acc 0.9375\n",
      "2017-11-06T14:27:37.794035: step 2014, loss 0.120703, acc 0.9375\n",
      "2017-11-06T14:27:41.733835: step 2015, loss 0.162872, acc 0.875\n",
      "2017-11-06T14:27:44.354696: step 2016, loss 0.165522, acc 0.9\n",
      "2017-11-06T14:27:48.717796: step 2017, loss 0.249917, acc 0.875\n",
      "2017-11-06T14:27:52.702628: step 2018, loss 0.18455, acc 0.90625\n",
      "2017-11-06T14:27:56.743500: step 2019, loss 0.0244714, acc 1\n",
      "2017-11-06T14:28:00.750347: step 2020, loss 0.0898297, acc 0.9375\n",
      "2017-11-06T14:28:04.683141: step 2021, loss 0.116882, acc 0.96875\n",
      "2017-11-06T14:28:08.661968: step 2022, loss 0.113027, acc 0.9375\n",
      "2017-11-06T14:28:12.671819: step 2023, loss 0.203829, acc 0.90625\n",
      "2017-11-06T14:28:16.669661: step 2024, loss 0.264391, acc 0.875\n",
      "2017-11-06T14:28:20.664496: step 2025, loss 0.0378442, acc 0.96875\n",
      "2017-11-06T14:28:24.750400: step 2026, loss 0.357304, acc 0.90625\n",
      "2017-11-06T14:28:28.874330: step 2027, loss 0.17713, acc 0.90625\n",
      "2017-11-06T14:28:32.935215: step 2028, loss 0.107392, acc 0.9375\n",
      "2017-11-06T14:28:37.097944: step 2029, loss 0.0571215, acc 1\n",
      "2017-11-06T14:28:41.076772: step 2030, loss 0.26071, acc 0.875\n",
      "2017-11-06T14:28:45.022575: step 2031, loss 0.167949, acc 0.9375\n",
      "2017-11-06T14:28:48.966377: step 2032, loss 0.179352, acc 0.9375\n",
      "2017-11-06T14:28:53.336482: step 2033, loss 0.157863, acc 0.875\n",
      "2017-11-06T14:28:57.436396: step 2034, loss 0.171854, acc 0.90625\n",
      "2017-11-06T14:29:01.458253: step 2035, loss 0.106354, acc 0.96875\n",
      "2017-11-06T14:29:05.466101: step 2036, loss 0.231403, acc 0.90625\n",
      "2017-11-06T14:29:09.440925: step 2037, loss 0.426149, acc 0.8125\n",
      "2017-11-06T14:29:13.416751: step 2038, loss 0.281662, acc 0.875\n",
      "2017-11-06T14:29:17.393576: step 2039, loss 0.232206, acc 0.90625\n",
      "2017-11-06T14:29:21.377406: step 2040, loss 0.108659, acc 0.9375\n",
      "2017-11-06T14:29:25.411273: step 2041, loss 0.180227, acc 0.90625\n",
      "2017-11-06T14:29:29.358077: step 2042, loss 0.135848, acc 0.9375\n",
      "2017-11-06T14:29:33.321894: step 2043, loss 0.0982288, acc 0.9375\n",
      "2017-11-06T14:29:37.492117: step 2044, loss 0.263965, acc 0.90625\n",
      "2017-11-06T14:29:41.474945: step 2045, loss 0.217184, acc 0.90625\n",
      "2017-11-06T14:29:45.448770: step 2046, loss 0.0802499, acc 0.96875\n",
      "2017-11-06T14:29:49.447610: step 2047, loss 0.11578, acc 0.9375\n",
      "2017-11-06T14:29:53.400418: step 2048, loss 0.133646, acc 0.9375\n",
      "2017-11-06T14:29:57.745506: step 2049, loss 0.221467, acc 0.875\n",
      "2017-11-06T14:30:02.185661: step 2050, loss 0.244834, acc 0.90625\n",
      "2017-11-06T14:30:06.118455: step 2051, loss 0.135324, acc 0.875\n",
      "2017-11-06T14:30:08.690283: step 2052, loss 0.234899, acc 0.85\n",
      "2017-11-06T14:30:12.638090: step 2053, loss 0.158315, acc 0.90625\n",
      "2017-11-06T14:30:16.658944: step 2054, loss 0.0870112, acc 0.96875\n",
      "2017-11-06T14:30:20.621761: step 2055, loss 0.0385725, acc 0.96875\n",
      "2017-11-06T14:30:24.626610: step 2056, loss 0.207011, acc 0.9375\n",
      "2017-11-06T14:30:28.604432: step 2057, loss 0.115943, acc 0.9375\n",
      "2017-11-06T14:30:32.618285: step 2058, loss 0.0571879, acc 0.96875\n",
      "2017-11-06T14:30:36.770236: step 2059, loss 0.158798, acc 0.90625\n",
      "2017-11-06T14:30:40.696024: step 2060, loss 0.277194, acc 0.84375\n",
      "2017-11-06T14:30:44.642829: step 2061, loss 0.075091, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T14:30:48.596638: step 2062, loss 0.016375, acc 1\n",
      "2017-11-06T14:30:52.570462: step 2063, loss 0.177844, acc 0.90625\n",
      "2017-11-06T14:30:56.534279: step 2064, loss 0.146088, acc 0.9375\n",
      "2017-11-06T14:31:00.568144: step 2065, loss 0.129708, acc 0.90625\n",
      "2017-11-06T14:31:05.009299: step 2066, loss 0.462696, acc 0.75\n",
      "2017-11-06T14:31:09.087197: step 2067, loss 0.131974, acc 0.90625\n",
      "2017-11-06T14:31:13.055017: step 2068, loss 0.292013, acc 0.875\n",
      "2017-11-06T14:31:17.075874: step 2069, loss 0.182379, acc 0.9375\n",
      "2017-11-06T14:31:21.057704: step 2070, loss 0.273164, acc 0.875\n",
      "2017-11-06T14:31:25.032527: step 2071, loss 0.202477, acc 0.875\n",
      "2017-11-06T14:31:29.004349: step 2072, loss 0.0480942, acc 0.96875\n",
      "2017-11-06T14:31:32.952155: step 2073, loss 0.0564235, acc 0.96875\n",
      "2017-11-06T14:31:37.127900: step 2074, loss 0.304852, acc 0.84375\n",
      "2017-11-06T14:31:41.204797: step 2075, loss 0.0995361, acc 0.96875\n",
      "2017-11-06T14:31:45.167613: step 2076, loss 0.158923, acc 0.96875\n",
      "2017-11-06T14:31:49.117421: step 2077, loss 0.0797943, acc 0.9375\n",
      "2017-11-06T14:31:53.082237: step 2078, loss 0.274783, acc 0.875\n",
      "2017-11-06T14:31:57.074073: step 2079, loss 0.0872462, acc 0.96875\n",
      "2017-11-06T14:32:01.123951: step 2080, loss 0.129266, acc 0.9375\n",
      "2017-11-06T14:32:05.077760: step 2081, loss 0.170263, acc 0.875\n",
      "2017-11-06T14:32:09.469881: step 2082, loss 0.274288, acc 0.90625\n",
      "2017-11-06T14:32:13.624833: step 2083, loss 0.258582, acc 0.90625\n",
      "2017-11-06T14:32:17.670709: step 2084, loss 0.172168, acc 0.90625\n",
      "2017-11-06T14:32:21.602501: step 2085, loss 0.123374, acc 0.9375\n",
      "2017-11-06T14:32:25.605346: step 2086, loss 0.242896, acc 0.90625\n",
      "2017-11-06T14:32:29.632207: step 2087, loss 0.0314991, acc 1\n",
      "2017-11-06T14:32:32.150997: step 2088, loss 0.387344, acc 0.8\n",
      "2017-11-06T14:32:36.373998: step 2089, loss 0.148979, acc 0.90625\n",
      "2017-11-06T14:32:40.329808: step 2090, loss 0.275779, acc 0.90625\n",
      "2017-11-06T14:32:44.369678: step 2091, loss 0.144689, acc 0.875\n",
      "2017-11-06T14:32:48.345505: step 2092, loss 0.287344, acc 0.84375\n",
      "2017-11-06T14:32:52.333337: step 2093, loss 0.0325173, acc 1\n",
      "2017-11-06T14:32:56.282143: step 2094, loss 0.110652, acc 0.9375\n",
      "2017-11-06T14:33:00.242957: step 2095, loss 0.22182, acc 0.90625\n",
      "2017-11-06T14:33:04.243800: step 2096, loss 0.0803611, acc 0.9375\n",
      "2017-11-06T14:33:08.265658: step 2097, loss 0.0637488, acc 0.96875\n",
      "2017-11-06T14:33:12.277508: step 2098, loss 0.274475, acc 0.84375\n",
      "2017-11-06T14:33:16.810729: step 2099, loss 0.0271607, acc 1\n",
      "2017-11-06T14:33:20.790557: step 2100, loss 0.328403, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:33:23.316352: step 2100, loss 1.14365, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-06T14:33:28.574768: step 2101, loss 0.0271367, acc 1\n",
      "2017-11-06T14:33:32.493553: step 2102, loss 0.131632, acc 0.9375\n",
      "2017-11-06T14:33:36.462372: step 2103, loss 0.286324, acc 0.90625\n",
      "2017-11-06T14:33:40.724401: step 2104, loss 0.07927, acc 0.96875\n",
      "2017-11-06T14:33:44.828317: step 2105, loss 0.133187, acc 0.96875\n",
      "2017-11-06T14:33:48.937236: step 2106, loss 0.195853, acc 0.875\n",
      "2017-11-06T14:33:52.986113: step 2107, loss 0.0686067, acc 0.9375\n",
      "2017-11-06T14:33:57.075018: step 2108, loss 0.224385, acc 0.875\n",
      "2017-11-06T14:34:01.136905: step 2109, loss 0.428063, acc 0.8125\n",
      "2017-11-06T14:34:05.234817: step 2110, loss 0.0586254, acc 0.96875\n",
      "2017-11-06T14:34:09.290698: step 2111, loss 0.275664, acc 0.9375\n",
      "2017-11-06T14:34:13.303549: step 2112, loss 0.242724, acc 0.9375\n",
      "2017-11-06T14:34:17.323406: step 2113, loss 0.255917, acc 0.84375\n",
      "2017-11-06T14:34:21.988721: step 2114, loss 0.136651, acc 0.90625\n",
      "2017-11-06T14:34:26.522942: step 2115, loss 0.254315, acc 0.875\n",
      "2017-11-06T14:34:30.715922: step 2116, loss 0.0477939, acc 1\n",
      "2017-11-06T14:34:35.033990: step 2117, loss 0.056613, acc 0.96875\n",
      "2017-11-06T14:34:39.152669: step 2118, loss 0.095232, acc 0.96875\n",
      "2017-11-06T14:34:43.320631: step 2119, loss 0.26681, acc 0.90625\n",
      "2017-11-06T14:34:47.494596: step 2120, loss 0.249691, acc 0.90625\n",
      "2017-11-06T14:34:51.711593: step 2121, loss 0.184798, acc 0.90625\n",
      "2017-11-06T14:34:55.747461: step 2122, loss 0.177258, acc 0.90625\n",
      "2017-11-06T14:34:59.814351: step 2123, loss 0.338244, acc 0.875\n",
      "2017-11-06T14:35:02.359159: step 2124, loss 0.344276, acc 0.85\n",
      "2017-11-06T14:35:06.273940: step 2125, loss 0.0536594, acc 0.96875\n",
      "2017-11-06T14:35:10.200731: step 2126, loss 0.185423, acc 0.875\n",
      "2017-11-06T14:35:14.142531: step 2127, loss 0.25021, acc 0.875\n",
      "2017-11-06T14:35:18.095339: step 2128, loss 0.210501, acc 0.875\n",
      "2017-11-06T14:35:22.026132: step 2129, loss 0.0466921, acc 0.96875\n",
      "2017-11-06T14:35:26.306174: step 2130, loss 0.0868988, acc 0.96875\n",
      "2017-11-06T14:35:30.395080: step 2131, loss 0.0988676, acc 0.96875\n",
      "2017-11-06T14:35:34.367902: step 2132, loss 0.403534, acc 0.8125\n",
      "2017-11-06T14:35:38.304701: step 2133, loss 0.0439545, acc 1\n",
      "2017-11-06T14:35:42.253505: step 2134, loss 0.0764986, acc 0.96875\n",
      "2017-11-06T14:35:46.184333: step 2135, loss 0.265, acc 0.875\n",
      "2017-11-06T14:35:50.126133: step 2136, loss 0.120994, acc 0.90625\n",
      "2017-11-06T14:35:54.022902: step 2137, loss 0.240365, acc 0.90625\n",
      "2017-11-06T14:35:57.943688: step 2138, loss 0.0136563, acc 1\n",
      "2017-11-06T14:36:01.866475: step 2139, loss 0.176969, acc 0.9375\n",
      "2017-11-06T14:36:05.820284: step 2140, loss 0.0906584, acc 0.9375\n",
      "2017-11-06T14:36:09.775095: step 2141, loss 0.179681, acc 0.90625\n",
      "2017-11-06T14:36:13.683873: step 2142, loss 0.335915, acc 0.90625\n",
      "2017-11-06T14:36:17.609662: step 2143, loss 0.207213, acc 0.84375\n",
      "2017-11-06T14:36:21.543456: step 2144, loss 0.130191, acc 0.96875\n",
      "2017-11-06T14:36:25.503270: step 2145, loss 0.201304, acc 0.90625\n",
      "2017-11-06T14:36:29.560153: step 2146, loss 0.178328, acc 0.90625\n",
      "2017-11-06T14:36:34.121393: step 2147, loss 0.200893, acc 0.875\n",
      "2017-11-06T14:36:38.138249: step 2148, loss 0.18838, acc 0.90625\n",
      "2017-11-06T14:36:42.041021: step 2149, loss 0.246636, acc 0.875\n",
      "2017-11-06T14:36:45.959805: step 2150, loss 0.202901, acc 0.875\n",
      "2017-11-06T14:36:49.893600: step 2151, loss 0.0737583, acc 0.96875\n",
      "2017-11-06T14:36:53.825394: step 2152, loss 0.27257, acc 0.875\n",
      "2017-11-06T14:36:57.754186: step 2153, loss 0.114049, acc 0.9375\n",
      "2017-11-06T14:37:01.688983: step 2154, loss 0.253149, acc 0.90625\n",
      "2017-11-06T14:37:05.661805: step 2155, loss 0.175286, acc 0.9375\n",
      "2017-11-06T14:37:09.653641: step 2156, loss 0.0883012, acc 0.96875\n",
      "2017-11-06T14:37:13.668494: step 2157, loss 0.118519, acc 0.9375\n",
      "2017-11-06T14:37:17.637314: step 2158, loss 0.24929, acc 0.875\n",
      "2017-11-06T14:37:21.572109: step 2159, loss 0.11746, acc 0.9375\n",
      "2017-11-06T14:37:24.141935: step 2160, loss 0.350849, acc 0.8\n",
      "2017-11-06T14:37:28.127768: step 2161, loss 0.0914496, acc 0.9375\n",
      "2017-11-06T14:37:32.111598: step 2162, loss 0.21895, acc 0.96875\n",
      "2017-11-06T14:37:36.285306: step 2163, loss 0.275475, acc 0.8125\n",
      "2017-11-06T14:37:40.577355: step 2164, loss 0.11729, acc 0.96875\n",
      "2017-11-06T14:37:44.513152: step 2165, loss 0.0860136, acc 0.9375\n",
      "2017-11-06T14:37:48.687630: step 2166, loss 0.137157, acc 0.9375\n",
      "2017-11-06T14:37:52.646444: step 2167, loss 0.208773, acc 0.875\n",
      "2017-11-06T14:37:56.574233: step 2168, loss 0.119879, acc 0.90625\n",
      "2017-11-06T14:38:00.506029: step 2169, loss 0.170993, acc 0.90625\n",
      "2017-11-06T14:38:04.410804: step 2170, loss 0.212489, acc 0.84375\n",
      "2017-11-06T14:38:08.370615: step 2171, loss 0.104577, acc 0.9375\n",
      "2017-11-06T14:38:12.293403: step 2172, loss 0.14516, acc 0.875\n",
      "2017-11-06T14:38:16.230201: step 2173, loss 0.209884, acc 0.875\n",
      "2017-11-06T14:38:20.155989: step 2174, loss 0.144477, acc 0.90625\n",
      "2017-11-06T14:38:24.283922: step 2175, loss 0.104318, acc 0.9375\n",
      "2017-11-06T14:38:28.241735: step 2176, loss 0.10401, acc 0.9375\n",
      "2017-11-06T14:38:32.153516: step 2177, loss 0.257214, acc 0.8125\n",
      "2017-11-06T14:38:36.348496: step 2178, loss 0.0942724, acc 0.9375\n",
      "2017-11-06T14:38:40.339330: step 2179, loss 0.201784, acc 0.90625\n",
      "2017-11-06T14:38:44.724447: step 2180, loss 0.132216, acc 0.9375\n",
      "2017-11-06T14:38:48.709278: step 2181, loss 0.129517, acc 0.9375\n",
      "2017-11-06T14:38:52.674095: step 2182, loss 0.186814, acc 0.9375\n",
      "2017-11-06T14:38:56.625903: step 2183, loss 0.166817, acc 0.9375\n",
      "2017-11-06T14:39:00.525674: step 2184, loss 0.106384, acc 0.96875\n",
      "2017-11-06T14:39:04.479483: step 2185, loss 0.143727, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T14:39:08.468318: step 2186, loss 0.381, acc 0.84375\n",
      "2017-11-06T14:39:12.385100: step 2187, loss 0.227936, acc 0.875\n",
      "2017-11-06T14:39:16.340912: step 2188, loss 0.120715, acc 0.96875\n",
      "2017-11-06T14:39:20.245686: step 2189, loss 0.101121, acc 0.96875\n",
      "2017-11-06T14:39:24.184484: step 2190, loss 0.161953, acc 0.9375\n",
      "2017-11-06T14:39:28.086257: step 2191, loss 0.135138, acc 0.90625\n",
      "2017-11-06T14:39:31.994033: step 2192, loss 0.046003, acc 0.96875\n",
      "2017-11-06T14:39:35.912820: step 2193, loss 0.320889, acc 0.875\n",
      "2017-11-06T14:39:39.861624: step 2194, loss 0.153248, acc 0.90625\n",
      "2017-11-06T14:39:43.792416: step 2195, loss 0.148019, acc 0.9375\n",
      "2017-11-06T14:39:46.382257: step 2196, loss 0.193521, acc 0.85\n",
      "2017-11-06T14:39:50.775486: step 2197, loss 0.196877, acc 0.90625\n",
      "2017-11-06T14:39:54.707280: step 2198, loss 0.111328, acc 0.96875\n",
      "2017-11-06T14:39:58.660089: step 2199, loss 0.0475554, acc 1\n",
      "2017-11-06T14:40:02.895098: step 2200, loss 0.18374, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:40:05.467927: step 2200, loss 1.14046, acc 0.666667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-06T14:40:11.010011: step 2201, loss 0.0606919, acc 0.96875\n",
      "2017-11-06T14:40:14.908781: step 2202, loss 0.316059, acc 0.875\n",
      "2017-11-06T14:40:18.941648: step 2203, loss 0.249902, acc 0.9375\n",
      "2017-11-06T14:40:22.888452: step 2204, loss 0.30073, acc 0.84375\n",
      "2017-11-06T14:40:26.862275: step 2205, loss 0.0805412, acc 0.9375\n",
      "2017-11-06T14:40:30.770051: step 2206, loss 0.219526, acc 0.875\n",
      "2017-11-06T14:40:34.887977: step 2207, loss 0.20145, acc 0.84375\n",
      "2017-11-06T14:40:38.892630: step 2208, loss 0.130335, acc 0.90625\n",
      "2017-11-06T14:40:42.884466: step 2209, loss 0.244412, acc 0.90625\n",
      "2017-11-06T14:40:46.800249: step 2210, loss 0.090002, acc 0.96875\n",
      "2017-11-06T14:40:50.722037: step 2211, loss 0.0036912, acc 1\n",
      "2017-11-06T14:40:55.061118: step 2212, loss 0.0651451, acc 0.96875\n",
      "2017-11-06T14:40:59.102990: step 2213, loss 0.315879, acc 0.84375\n",
      "2017-11-06T14:41:03.024776: step 2214, loss 0.166659, acc 0.90625\n",
      "2017-11-06T14:41:06.930552: step 2215, loss 0.151022, acc 0.90625\n",
      "2017-11-06T14:41:11.406733: step 2216, loss 0.213815, acc 0.875\n",
      "2017-11-06T14:41:16.277193: step 2217, loss 0.15863, acc 0.9375\n",
      "2017-11-06T14:41:20.169960: step 2218, loss 0.137772, acc 0.9375\n",
      "2017-11-06T14:41:24.304898: step 2219, loss 0.32307, acc 0.875\n",
      "2017-11-06T14:41:28.274718: step 2220, loss 0.25064, acc 0.875\n",
      "2017-11-06T14:41:32.307584: step 2221, loss 0.184377, acc 0.9375\n",
      "2017-11-06T14:41:36.316433: step 2222, loss 0.149181, acc 0.90625\n",
      "2017-11-06T14:41:40.286253: step 2223, loss 0.270373, acc 0.875\n",
      "2017-11-06T14:41:44.259076: step 2224, loss 0.056188, acc 0.9375\n",
      "2017-11-06T14:41:48.199876: step 2225, loss 0.212323, acc 0.90625\n",
      "2017-11-06T14:41:52.170802: step 2226, loss 0.180264, acc 0.90625\n",
      "2017-11-06T14:41:56.114603: step 2227, loss 0.120284, acc 0.96875\n",
      "2017-11-06T14:42:00.383638: step 2228, loss 0.3355, acc 0.8125\n",
      "2017-11-06T14:42:04.533585: step 2229, loss 0.2353, acc 0.90625\n",
      "2017-11-06T14:42:08.497403: step 2230, loss 0.233555, acc 0.84375\n",
      "2017-11-06T14:42:12.447208: step 2231, loss 0.275819, acc 0.90625\n",
      "2017-11-06T14:42:14.958993: step 2232, loss 0.290122, acc 0.85\n",
      "2017-11-06T14:42:18.878780: step 2233, loss 0.0989985, acc 0.96875\n",
      "2017-11-06T14:42:22.810574: step 2234, loss 0.0721215, acc 0.96875\n",
      "2017-11-06T14:42:26.832429: step 2235, loss 0.131465, acc 0.96875\n",
      "2017-11-06T14:42:30.810256: step 2236, loss 0.0464491, acc 1\n",
      "2017-11-06T14:42:34.961207: step 2237, loss 0.025359, acc 1\n",
      "2017-11-06T14:42:39.088137: step 2238, loss 0.143936, acc 0.9375\n",
      "2017-11-06T14:42:43.080975: step 2239, loss 0.189381, acc 0.90625\n",
      "2017-11-06T14:42:47.032783: step 2240, loss 0.193041, acc 0.9375\n",
      "2017-11-06T14:42:50.933554: step 2241, loss 0.0711152, acc 0.96875\n",
      "2017-11-06T14:42:54.940402: step 2242, loss 0.361091, acc 0.84375\n",
      "2017-11-06T14:42:58.873196: step 2243, loss 0.257128, acc 0.90625\n",
      "2017-11-06T14:43:02.836013: step 2244, loss 0.26857, acc 0.875\n",
      "2017-11-06T14:43:07.316194: step 2245, loss 0.30187, acc 0.84375\n",
      "2017-11-06T14:43:11.275008: step 2246, loss 0.0674971, acc 0.9375\n",
      "2017-11-06T14:43:15.243830: step 2247, loss 0.111308, acc 0.9375\n",
      "2017-11-06T14:43:19.198640: step 2248, loss 0.105965, acc 1\n",
      "2017-11-06T14:43:23.170461: step 2249, loss 0.277772, acc 0.90625\n",
      "2017-11-06T14:43:27.119265: step 2250, loss 0.126953, acc 0.96875\n",
      "2017-11-06T14:43:31.041052: step 2251, loss 0.432883, acc 0.84375\n",
      "2017-11-06T14:43:34.980851: step 2252, loss 0.279831, acc 0.84375\n",
      "2017-11-06T14:43:38.957509: step 2253, loss 0.150156, acc 0.9375\n",
      "2017-11-06T14:43:42.928330: step 2254, loss 0.170828, acc 0.90625\n",
      "2017-11-06T14:43:46.895148: step 2255, loss 0.366801, acc 0.84375\n",
      "2017-11-06T14:43:50.884983: step 2256, loss 0.277193, acc 0.9375\n",
      "2017-11-06T14:43:54.855805: step 2257, loss 0.220935, acc 0.875\n",
      "2017-11-06T14:43:58.845640: step 2258, loss 0.188703, acc 0.90625\n",
      "2017-11-06T14:44:02.738405: step 2259, loss 0.361651, acc 0.90625\n",
      "2017-11-06T14:44:06.725238: step 2260, loss 0.522591, acc 0.84375\n",
      "2017-11-06T14:44:10.961249: step 2261, loss 0.132565, acc 0.90625\n",
      "2017-11-06T14:44:15.179246: step 2262, loss 0.213651, acc 0.875\n",
      "2017-11-06T14:44:19.165078: step 2263, loss 0.185046, acc 0.9375\n",
      "2017-11-06T14:44:23.269995: step 2264, loss 0.108391, acc 0.96875\n",
      "2017-11-06T14:44:27.403932: step 2265, loss 0.256684, acc 0.90625\n",
      "2017-11-06T14:44:31.305704: step 2266, loss 0.395733, acc 0.84375\n",
      "2017-11-06T14:44:35.548719: step 2267, loss 0.347499, acc 0.84375\n",
      "2017-11-06T14:44:38.064508: step 2268, loss 0.179604, acc 0.95\n",
      "2017-11-06T14:44:42.009310: step 2269, loss 0.108426, acc 0.9375\n",
      "2017-11-06T14:44:45.950109: step 2270, loss 0.115627, acc 0.9375\n",
      "2017-11-06T14:44:49.864891: step 2271, loss 0.2685, acc 0.90625\n",
      "2017-11-06T14:44:53.795684: step 2272, loss 0.191161, acc 0.90625\n",
      "2017-11-06T14:44:57.720474: step 2273, loss 0.10933, acc 0.9375\n",
      "2017-11-06T14:45:01.712309: step 2274, loss 0.131789, acc 0.96875\n",
      "2017-11-06T14:45:05.653111: step 2275, loss 0.367024, acc 0.8125\n",
      "2017-11-06T14:45:09.638943: step 2276, loss 0.181599, acc 0.90625\n",
      "2017-11-06T14:45:13.542715: step 2277, loss 0.0361869, acc 0.96875\n",
      "2017-11-06T14:45:18.054921: step 2278, loss 0.18855, acc 0.875\n",
      "2017-11-06T14:45:22.097794: step 2279, loss 0.207799, acc 0.9375\n",
      "2017-11-06T14:45:26.082625: step 2280, loss 0.120459, acc 0.9375\n",
      "2017-11-06T14:45:30.047442: step 2281, loss 0.0695415, acc 0.96875\n",
      "2017-11-06T14:45:33.997250: step 2282, loss 0.142267, acc 0.9375\n",
      "2017-11-06T14:45:37.947056: step 2283, loss 0.381875, acc 0.84375\n",
      "2017-11-06T14:45:41.929886: step 2284, loss 0.216737, acc 0.90625\n",
      "2017-11-06T14:45:45.900707: step 2285, loss 0.156819, acc 0.9375\n",
      "2017-11-06T14:45:49.853515: step 2286, loss 0.126955, acc 0.90625\n",
      "2017-11-06T14:45:53.787311: step 2287, loss 0.207011, acc 0.875\n",
      "2017-11-06T14:45:57.836187: step 2288, loss 0.120066, acc 0.96875\n",
      "2017-11-06T14:46:01.812012: step 2289, loss 0.476235, acc 0.875\n",
      "2017-11-06T14:46:05.817861: step 2290, loss 0.246411, acc 0.90625\n",
      "2017-11-06T14:46:09.791683: step 2291, loss 0.168591, acc 0.96875\n",
      "2017-11-06T14:46:13.792526: step 2292, loss 0.240595, acc 0.90625\n",
      "2017-11-06T14:46:17.791367: step 2293, loss 0.169306, acc 0.90625\n",
      "2017-11-06T14:46:22.014367: step 2294, loss 0.22529, acc 0.90625\n",
      "2017-11-06T14:46:26.175327: step 2295, loss 0.344825, acc 0.8125\n",
      "2017-11-06T14:46:30.116123: step 2296, loss 0.0592863, acc 1\n",
      "2017-11-06T14:46:34.393164: step 2297, loss 0.35039, acc 0.84375\n",
      "2017-11-06T14:46:38.460825: step 2298, loss 0.133292, acc 0.9375\n",
      "2017-11-06T14:46:42.370603: step 2299, loss 0.156965, acc 0.9375\n",
      "2017-11-06T14:46:46.321409: step 2300, loss 0.161897, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:46:48.873223: step 2300, loss 1.24513, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-06T14:46:54.486742: step 2301, loss 0.107695, acc 0.9375\n",
      "2017-11-06T14:46:58.444553: step 2302, loss 0.186923, acc 0.90625\n",
      "2017-11-06T14:47:02.377349: step 2303, loss 0.192506, acc 0.90625\n",
      "2017-11-06T14:47:04.909147: step 2304, loss 0.958812, acc 0.8\n",
      "2017-11-06T14:47:08.887974: step 2305, loss 0.13387, acc 0.96875\n",
      "2017-11-06T14:47:12.804776: step 2306, loss 0.145261, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T14:47:16.798595: step 2307, loss 0.0484744, acc 1\n",
      "2017-11-06T14:47:20.756407: step 2308, loss 0.375446, acc 0.875\n",
      "2017-11-06T14:47:24.732232: step 2309, loss 0.176133, acc 0.9375\n",
      "2017-11-06T14:47:29.109342: step 2310, loss 0.139485, acc 0.9375\n",
      "2017-11-06T14:47:33.219263: step 2311, loss 0.181888, acc 0.875\n",
      "2017-11-06T14:47:37.204094: step 2312, loss 0.0925944, acc 0.9375\n",
      "2017-11-06T14:47:41.325022: step 2313, loss 0.150809, acc 0.9375\n",
      "2017-11-06T14:47:45.384907: step 2314, loss 0.0603033, acc 0.96875\n",
      "2017-11-06T14:47:49.400760: step 2315, loss 0.29602, acc 0.90625\n",
      "2017-11-06T14:47:53.377586: step 2316, loss 0.111662, acc 0.96875\n",
      "2017-11-06T14:47:57.305380: step 2317, loss 0.0406231, acc 1\n",
      "2017-11-06T14:48:01.342429: step 2318, loss 0.182207, acc 0.875\n",
      "2017-11-06T14:48:05.293236: step 2319, loss 0.0635949, acc 0.96875\n",
      "2017-11-06T14:48:09.280069: step 2320, loss 0.036756, acc 1\n",
      "2017-11-06T14:48:13.255894: step 2321, loss 0.0407488, acc 1\n",
      "2017-11-06T14:48:17.268747: step 2322, loss 0.394103, acc 0.90625\n",
      "2017-11-06T14:48:21.258582: step 2323, loss 0.127276, acc 0.96875\n",
      "2017-11-06T14:48:25.353490: step 2324, loss 0.63053, acc 0.8125\n",
      "2017-11-06T14:48:29.447399: step 2325, loss 0.196377, acc 0.90625\n",
      "2017-11-06T14:48:33.930584: step 2326, loss 0.176397, acc 0.9375\n",
      "2017-11-06T14:48:38.158588: step 2327, loss 0.160646, acc 0.90625\n",
      "2017-11-06T14:48:42.140417: step 2328, loss 0.0182394, acc 1\n",
      "2017-11-06T14:48:46.092225: step 2329, loss 0.20907, acc 0.90625\n",
      "2017-11-06T14:48:50.027021: step 2330, loss 0.382888, acc 0.84375\n",
      "2017-11-06T14:48:53.999844: step 2331, loss 0.259123, acc 0.90625\n",
      "2017-11-06T14:48:58.011694: step 2332, loss 0.224541, acc 0.8125\n",
      "2017-11-06T14:49:01.983518: step 2333, loss 0.391861, acc 0.875\n",
      "2017-11-06T14:49:05.887291: step 2334, loss 0.0562097, acc 1\n",
      "2017-11-06T14:49:09.846104: step 2335, loss 0.210655, acc 0.875\n",
      "2017-11-06T14:49:13.798912: step 2336, loss 0.467776, acc 0.78125\n",
      "2017-11-06T14:49:17.740713: step 2337, loss 0.261312, acc 0.84375\n",
      "2017-11-06T14:49:21.736553: step 2338, loss 0.0820982, acc 0.96875\n",
      "2017-11-06T14:49:25.701369: step 2339, loss 0.267366, acc 0.8125\n",
      "2017-11-06T14:49:28.300216: step 2340, loss 0.00739156, acc 1\n",
      "2017-11-06T14:49:32.287049: step 2341, loss 0.0633515, acc 1\n",
      "2017-11-06T14:49:36.319916: step 2342, loss 0.19149, acc 0.90625\n",
      "2017-11-06T14:49:40.690789: step 2343, loss 0.0344615, acc 1\n",
      "2017-11-06T14:49:44.709645: step 2344, loss 0.262008, acc 0.875\n",
      "2017-11-06T14:49:48.702482: step 2345, loss 0.366052, acc 0.875\n",
      "2017-11-06T14:49:52.733347: step 2346, loss 0.174624, acc 0.9375\n",
      "2017-11-06T14:49:56.660136: step 2347, loss 0.208691, acc 0.90625\n",
      "2017-11-06T14:50:00.836104: step 2348, loss 0.209552, acc 0.90625\n",
      "2017-11-06T14:50:04.889984: step 2349, loss 0.245168, acc 0.875\n",
      "2017-11-06T14:50:08.936860: step 2350, loss 0.184917, acc 0.90625\n",
      "2017-11-06T14:50:12.893671: step 2351, loss 0.166394, acc 0.90625\n",
      "2017-11-06T14:50:16.824464: step 2352, loss 0.0688949, acc 0.96875\n",
      "2017-11-06T14:50:20.794285: step 2353, loss 0.0256675, acc 1\n",
      "2017-11-06T14:50:24.827151: step 2354, loss 0.244368, acc 0.90625\n",
      "2017-11-06T14:50:28.858015: step 2355, loss 0.273793, acc 0.90625\n",
      "2017-11-06T14:50:32.961931: step 2356, loss 0.173724, acc 0.9375\n",
      "2017-11-06T14:50:37.085861: step 2357, loss 0.222188, acc 0.90625\n",
      "2017-11-06T14:50:41.058684: step 2358, loss 0.208641, acc 0.90625\n",
      "2017-11-06T14:50:45.310707: step 2359, loss 0.0541861, acc 1\n",
      "2017-11-06T14:50:49.416623: step 2360, loss 0.0857853, acc 0.96875\n",
      "2017-11-06T14:50:53.369430: step 2361, loss 0.257294, acc 0.875\n",
      "2017-11-06T14:50:57.346257: step 2362, loss 0.216592, acc 0.9375\n",
      "2017-11-06T14:51:01.269043: step 2363, loss 0.212103, acc 0.90625\n",
      "2017-11-06T14:51:05.272889: step 2364, loss 0.194747, acc 0.875\n",
      "2017-11-06T14:51:09.209686: step 2365, loss 0.154619, acc 0.9375\n",
      "2017-11-06T14:51:13.220536: step 2366, loss 0.0630403, acc 0.96875\n",
      "2017-11-06T14:51:17.206368: step 2367, loss 0.111633, acc 0.9375\n",
      "2017-11-06T14:51:21.174187: step 2368, loss 0.322356, acc 0.8125\n",
      "2017-11-06T14:51:25.156017: step 2369, loss 0.236042, acc 0.90625\n",
      "2017-11-06T14:51:29.136845: step 2370, loss 0.14575, acc 0.90625\n",
      "2017-11-06T14:51:33.108668: step 2371, loss 0.0784088, acc 0.9375\n",
      "2017-11-06T14:51:37.077487: step 2372, loss 0.133804, acc 0.96875\n",
      "2017-11-06T14:51:41.044305: step 2373, loss 0.117388, acc 0.9375\n",
      "2017-11-06T14:51:45.036142: step 2374, loss 0.153466, acc 0.9375\n",
      "2017-11-06T14:51:49.226119: step 2375, loss 0.121087, acc 0.90625\n",
      "2017-11-06T14:51:52.122177: step 2376, loss 0.621585, acc 0.8\n",
      "2017-11-06T14:51:56.166051: step 2377, loss 0.19431, acc 0.875\n",
      "2017-11-06T14:52:00.203920: step 2378, loss 0.206724, acc 0.90625\n",
      "2017-11-06T14:52:04.142718: step 2379, loss 0.200131, acc 0.90625\n",
      "2017-11-06T14:52:08.179588: step 2380, loss 0.0199216, acc 1\n",
      "2017-11-06T14:52:12.150408: step 2381, loss 0.0519103, acc 0.96875\n",
      "2017-11-06T14:52:16.148250: step 2382, loss 0.140908, acc 0.90625\n",
      "2017-11-06T14:52:20.096054: step 2383, loss 0.120916, acc 0.96875\n",
      "2017-11-06T14:52:24.117911: step 2384, loss 0.152277, acc 0.96875\n",
      "2017-11-06T14:52:28.136767: step 2385, loss 0.170262, acc 0.90625\n",
      "2017-11-06T14:52:32.130604: step 2386, loss 0.0926079, acc 0.96875\n",
      "2017-11-06T14:52:36.351606: step 2387, loss 0.272373, acc 0.90625\n",
      "2017-11-06T14:52:40.325198: step 2388, loss 0.0338444, acc 1\n",
      "2017-11-06T14:52:44.295019: step 2389, loss 0.288773, acc 0.9375\n",
      "2017-11-06T14:52:48.253833: step 2390, loss 0.291928, acc 0.84375\n",
      "2017-11-06T14:52:52.221671: step 2391, loss 0.3059, acc 0.78125\n",
      "2017-11-06T14:52:56.536717: step 2392, loss 0.114105, acc 0.9375\n",
      "2017-11-06T14:53:00.633628: step 2393, loss 0.203255, acc 0.9375\n",
      "2017-11-06T14:53:04.959703: step 2394, loss 0.201039, acc 0.9375\n",
      "2017-11-06T14:53:09.038600: step 2395, loss 0.082416, acc 0.96875\n",
      "2017-11-06T14:53:12.995413: step 2396, loss 0.241322, acc 0.9375\n",
      "2017-11-06T14:53:16.983246: step 2397, loss 0.306085, acc 0.90625\n",
      "2017-11-06T14:53:20.923045: step 2398, loss 0.516411, acc 0.78125\n",
      "2017-11-06T14:53:25.111021: step 2399, loss 0.194012, acc 0.9375\n",
      "2017-11-06T14:53:29.182914: step 2400, loss 0.0503584, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T14:53:31.775757: step 2400, loss 1.19249, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-06T14:53:37.251981: step 2401, loss 0.109839, acc 0.96875\n",
      "2017-11-06T14:53:41.208793: step 2402, loss 0.183731, acc 0.90625\n",
      "2017-11-06T14:53:45.233652: step 2403, loss 0.203294, acc 0.96875\n",
      "2017-11-06T14:53:49.290535: step 2404, loss 0.0545068, acc 1\n",
      "2017-11-06T14:53:53.295380: step 2405, loss 0.0645603, acc 1\n",
      "2017-11-06T14:53:57.371277: step 2406, loss 0.197513, acc 0.90625\n",
      "2017-11-06T14:54:01.790416: step 2407, loss 0.207637, acc 0.90625\n",
      "2017-11-06T14:54:05.992402: step 2408, loss 0.340649, acc 0.875\n",
      "2017-11-06T14:54:10.025358: step 2409, loss 0.175557, acc 0.90625\n",
      "2017-11-06T14:54:14.043212: step 2410, loss 0.229014, acc 0.875\n",
      "2017-11-06T14:54:18.135120: step 2411, loss 0.264165, acc 0.84375\n",
      "2017-11-06T14:54:20.767991: step 2412, loss 0.260865, acc 0.9\n",
      "2017-11-06T14:54:24.791850: step 2413, loss 0.325461, acc 0.875\n",
      "2017-11-06T14:54:28.889762: step 2414, loss 0.101911, acc 0.96875\n",
      "2017-11-06T14:54:33.108759: step 2415, loss 0.0865334, acc 0.96875\n",
      "2017-11-06T14:54:37.215678: step 2416, loss 0.0895898, acc 0.9375\n",
      "2017-11-06T14:54:41.241539: step 2417, loss 0.13318, acc 0.9375\n",
      "2017-11-06T14:54:45.253388: step 2418, loss 0.138523, acc 0.90625\n",
      "2017-11-06T14:54:49.291258: step 2419, loss 0.245832, acc 0.90625\n",
      "2017-11-06T14:54:53.337132: step 2420, loss 0.0381, acc 0.96875\n",
      "2017-11-06T14:54:57.362993: step 2421, loss 0.334572, acc 0.84375\n",
      "2017-11-06T14:55:01.413871: step 2422, loss 0.104897, acc 0.90625\n",
      "2017-11-06T14:55:05.560818: step 2423, loss 0.149085, acc 0.90625\n",
      "2017-11-06T14:55:09.970951: step 2424, loss 0.0944659, acc 0.96875\n",
      "2017-11-06T14:55:14.038843: step 2425, loss 0.304615, acc 0.84375\n",
      "2017-11-06T14:55:18.065703: step 2426, loss 0.367076, acc 0.90625\n",
      "2017-11-06T14:55:22.085560: step 2427, loss 0.0538077, acc 0.96875\n",
      "2017-11-06T14:55:26.089404: step 2428, loss 0.157372, acc 0.9375\n",
      "2017-11-06T14:55:30.226343: step 2429, loss 0.154069, acc 0.90625\n",
      "2017-11-06T14:55:34.532403: step 2430, loss 0.314278, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T14:55:38.565008: step 2431, loss 0.220576, acc 0.90625\n",
      "2017-11-06T14:55:42.577859: step 2432, loss 0.239561, acc 0.90625\n",
      "2017-11-06T14:55:46.602720: step 2433, loss 0.202341, acc 0.90625\n",
      "2017-11-06T14:55:50.621577: step 2434, loss 0.0861237, acc 0.96875\n",
      "2017-11-06T14:55:54.664449: step 2435, loss 0.198203, acc 0.90625\n",
      "2017-11-06T14:55:58.682304: step 2436, loss 0.143899, acc 0.9375\n",
      "2017-11-06T14:56:02.713166: step 2437, loss 0.299389, acc 0.875\n",
      "2017-11-06T14:56:06.723016: step 2438, loss 0.148066, acc 0.96875\n",
      "2017-11-06T14:56:10.799912: step 2439, loss 0.0817582, acc 0.96875\n",
      "2017-11-06T14:56:15.211047: step 2440, loss 0.209655, acc 0.90625\n",
      "2017-11-06T14:56:19.251918: step 2441, loss 0.121915, acc 0.96875\n",
      "2017-11-06T14:56:23.310802: step 2442, loss 0.181336, acc 0.90625\n",
      "2017-11-06T14:56:27.415719: step 2443, loss 0.263037, acc 0.875\n",
      "2017-11-06T14:56:31.449585: step 2444, loss 0.0824876, acc 0.96875\n",
      "2017-11-06T14:56:35.835701: step 2445, loss 0.242811, acc 0.90625\n",
      "2017-11-06T14:56:39.819532: step 2446, loss 0.255727, acc 0.84375\n",
      "2017-11-06T14:56:43.848395: step 2447, loss 0.29233, acc 0.90625\n",
      "2017-11-06T14:56:46.466255: step 2448, loss 0.3515, acc 0.85\n",
      "2017-11-06T14:56:50.516135: step 2449, loss 0.173314, acc 0.9375\n",
      "2017-11-06T14:56:54.522980: step 2450, loss 0.143265, acc 0.90625\n",
      "2017-11-06T14:56:58.506810: step 2451, loss 0.0634792, acc 0.9375\n",
      "2017-11-06T14:57:02.494644: step 2452, loss 0.183232, acc 0.90625\n",
      "2017-11-06T14:57:06.492484: step 2453, loss 0.31461, acc 0.90625\n",
      "2017-11-06T14:57:10.533355: step 2454, loss 0.170971, acc 0.9375\n",
      "2017-11-06T14:57:14.587237: step 2455, loss 0.304404, acc 0.90625\n",
      "2017-11-06T14:57:18.989364: step 2456, loss 0.0371133, acc 0.96875\n",
      "2017-11-06T14:57:23.236381: step 2457, loss 0.196833, acc 0.9375\n",
      "2017-11-06T14:57:27.217210: step 2458, loss 0.483064, acc 0.875\n",
      "2017-11-06T14:57:31.242070: step 2459, loss 0.212286, acc 0.9375\n",
      "2017-11-06T14:57:35.308960: step 2460, loss 0.101494, acc 0.96875\n",
      "2017-11-06T14:57:39.346829: step 2461, loss 0.19327, acc 0.90625\n",
      "2017-11-06T14:57:43.511789: step 2462, loss 0.163485, acc 0.875\n",
      "2017-11-06T14:57:47.575676: step 2463, loss 0.146886, acc 0.9375\n",
      "2017-11-06T14:57:51.613545: step 2464, loss 0.157407, acc 0.9375\n",
      "2017-11-06T14:57:55.662422: step 2465, loss 0.103789, acc 0.96875\n",
      "2017-11-06T14:57:59.714301: step 2466, loss 0.117364, acc 0.9375\n",
      "2017-11-06T14:58:03.798203: step 2467, loss 0.110097, acc 0.9375\n",
      "2017-11-06T14:58:07.812055: step 2468, loss 0.237723, acc 0.96875\n",
      "2017-11-06T14:58:11.910968: step 2469, loss 0.326111, acc 0.875\n",
      "2017-11-06T14:58:16.037900: step 2470, loss 0.0800033, acc 0.96875\n",
      "2017-11-06T14:58:20.024732: step 2471, loss 0.148359, acc 0.90625\n",
      "2017-11-06T14:58:24.541943: step 2472, loss 0.238624, acc 0.90625\n",
      "2017-11-06T14:58:28.961082: step 2473, loss 0.0215738, acc 1\n",
      "2017-11-06T14:58:33.073003: step 2474, loss 0.169584, acc 0.9375\n",
      "2017-11-06T14:58:37.268985: step 2475, loss 0.176973, acc 0.90625\n",
      "2017-11-06T14:58:41.288601: step 2476, loss 0.13088, acc 0.96875\n",
      "2017-11-06T14:58:45.229402: step 2477, loss 0.0279549, acc 1\n",
      "2017-11-06T14:58:49.181209: step 2478, loss 0.277474, acc 0.75\n",
      "2017-11-06T14:58:53.237091: step 2479, loss 0.250117, acc 0.90625\n",
      "2017-11-06T14:58:57.263952: step 2480, loss 0.1764, acc 0.90625\n",
      "2017-11-06T14:59:01.310828: step 2481, loss 0.274209, acc 0.78125\n",
      "2017-11-06T14:59:05.367710: step 2482, loss 0.229398, acc 0.90625\n",
      "2017-11-06T14:59:09.427595: step 2483, loss 0.192844, acc 0.9375\n",
      "2017-11-06T14:59:12.043454: step 2484, loss 0.278446, acc 0.85\n",
      "2017-11-06T14:59:16.061308: step 2485, loss 0.141308, acc 0.96875\n",
      "2017-11-06T14:59:20.028127: step 2486, loss 0.116923, acc 0.9375\n",
      "2017-11-06T14:59:24.034975: step 2487, loss 0.031329, acc 1\n",
      "2017-11-06T14:59:28.256974: step 2488, loss 0.384986, acc 0.90625\n",
      "2017-11-06T14:59:32.612068: step 2489, loss 0.252624, acc 0.875\n",
      "2017-11-06T14:59:36.692970: step 2490, loss 0.131513, acc 0.96875\n",
      "2017-11-06T14:59:40.781874: step 2491, loss 0.247393, acc 0.90625\n",
      "2017-11-06T14:59:44.826749: step 2492, loss 0.12916, acc 0.9375\n",
      "2017-11-06T14:59:48.868620: step 2493, loss 0.11116, acc 0.9375\n",
      "2017-11-06T14:59:52.892479: step 2494, loss 0.0642518, acc 1\n",
      "2017-11-06T14:59:56.923343: step 2495, loss 0.118319, acc 0.90625\n",
      "2017-11-06T15:00:01.186372: step 2496, loss 0.100992, acc 0.96875\n",
      "2017-11-06T15:00:05.283284: step 2497, loss 0.108752, acc 0.9375\n",
      "2017-11-06T15:00:09.313147: step 2498, loss 0.36631, acc 0.78125\n",
      "2017-11-06T15:00:13.396047: step 2499, loss 0.218818, acc 0.90625\n",
      "2017-11-06T15:00:17.528984: step 2500, loss 0.191296, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:00:20.046773: step 2500, loss 1.17928, acc 0.683333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-06T15:00:25.326909: step 2501, loss 0.00697028, acc 1\n",
      "2017-11-06T15:00:29.329753: step 2502, loss 0.352462, acc 0.84375\n",
      "2017-11-06T15:00:33.703861: step 2503, loss 0.302422, acc 0.90625\n",
      "2017-11-06T15:00:38.056954: step 2504, loss 0.0392183, acc 1\n",
      "2017-11-06T15:00:42.079812: step 2505, loss 0.153794, acc 0.90625\n",
      "2017-11-06T15:00:46.077653: step 2506, loss 0.268797, acc 0.875\n",
      "2017-11-06T15:00:50.108517: step 2507, loss 0.017373, acc 1\n",
      "2017-11-06T15:00:54.196422: step 2508, loss 0.238145, acc 0.875\n",
      "2017-11-06T15:00:58.461003: step 2509, loss 0.250844, acc 0.90625\n",
      "2017-11-06T15:01:02.852130: step 2510, loss 0.17722, acc 0.875\n",
      "2017-11-06T15:01:07.058619: step 2511, loss 0.194419, acc 0.9375\n",
      "2017-11-06T15:01:11.186050: step 2512, loss 0.129137, acc 0.9375\n",
      "2017-11-06T15:01:15.220429: step 2513, loss 0.184594, acc 0.875\n",
      "2017-11-06T15:01:19.314338: step 2514, loss 0.157757, acc 0.90625\n",
      "2017-11-06T15:01:23.383229: step 2515, loss 0.172404, acc 0.9375\n",
      "2017-11-06T15:01:27.351048: step 2516, loss 0.211019, acc 0.9375\n",
      "2017-11-06T15:01:31.313865: step 2517, loss 0.206404, acc 0.90625\n",
      "2017-11-06T15:01:35.398767: step 2518, loss 0.231961, acc 0.9375\n",
      "2017-11-06T15:01:39.667568: step 2519, loss 0.223978, acc 0.84375\n",
      "2017-11-06T15:01:42.548616: step 2520, loss 0.491351, acc 0.8\n",
      "2017-11-06T15:01:46.563469: step 2521, loss 0.115854, acc 0.96875\n",
      "2017-11-06T15:01:50.565311: step 2522, loss 0.122601, acc 0.9375\n",
      "2017-11-06T15:01:54.539135: step 2523, loss 0.206463, acc 0.90625\n",
      "2017-11-06T15:01:58.466926: step 2524, loss 0.292376, acc 0.875\n",
      "2017-11-06T15:02:02.563837: step 2525, loss 0.118072, acc 0.9375\n",
      "2017-11-06T15:02:06.552671: step 2526, loss 0.178658, acc 0.90625\n",
      "2017-11-06T15:02:10.510484: step 2527, loss 0.213016, acc 0.90625\n",
      "2017-11-06T15:02:14.489311: step 2528, loss 0.0507269, acc 1\n",
      "2017-11-06T15:02:18.502162: step 2529, loss 0.0488835, acc 1\n",
      "2017-11-06T15:02:22.421948: step 2530, loss 0.230008, acc 0.90625\n",
      "2017-11-06T15:02:26.413783: step 2531, loss 0.14905, acc 0.9375\n",
      "2017-11-06T15:02:30.342577: step 2532, loss 0.170719, acc 0.90625\n",
      "2017-11-06T15:02:34.587591: step 2533, loss 0.11495, acc 0.96875\n",
      "2017-11-06T15:02:38.598441: step 2534, loss 0.0658562, acc 0.96875\n",
      "2017-11-06T15:02:42.576269: step 2535, loss 0.314959, acc 0.90625\n",
      "2017-11-06T15:02:46.889332: step 2536, loss 0.32988, acc 0.90625\n",
      "2017-11-06T15:02:50.933205: step 2537, loss 0.420946, acc 0.875\n",
      "2017-11-06T15:02:54.900024: step 2538, loss 0.0601218, acc 0.96875\n",
      "2017-11-06T15:02:58.805799: step 2539, loss 0.325427, acc 0.84375\n",
      "2017-11-06T15:03:02.821653: step 2540, loss 0.113648, acc 0.90625\n",
      "2017-11-06T15:03:06.835505: step 2541, loss 0.0750423, acc 0.96875\n",
      "2017-11-06T15:03:10.798320: step 2542, loss 0.120057, acc 0.96875\n",
      "2017-11-06T15:03:14.848199: step 2543, loss 0.209966, acc 0.84375\n",
      "2017-11-06T15:03:18.950113: step 2544, loss 0.155492, acc 0.9375\n",
      "2017-11-06T15:03:23.084051: step 2545, loss 0.140701, acc 0.9375\n",
      "2017-11-06T15:03:27.176958: step 2546, loss 0.215639, acc 0.875\n",
      "2017-11-06T15:03:31.139775: step 2547, loss 0.155655, acc 0.9375\n",
      "2017-11-06T15:03:35.116599: step 2548, loss 0.0465133, acc 1\n",
      "2017-11-06T15:03:39.068407: step 2549, loss 0.391616, acc 0.78125\n",
      "2017-11-06T15:03:43.267392: step 2550, loss 0.256356, acc 0.90625\n",
      "2017-11-06T15:03:47.267233: step 2551, loss 0.337095, acc 0.875\n",
      "2017-11-06T15:03:51.809460: step 2552, loss 0.079476, acc 0.96875\n",
      "2017-11-06T15:03:56.019452: step 2553, loss 0.167241, acc 0.96875\n",
      "2017-11-06T15:04:00.256462: step 2554, loss 0.260635, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T15:04:04.298335: step 2555, loss 0.0468391, acc 0.96875\n",
      "2017-11-06T15:04:07.024271: step 2556, loss 0.108773, acc 0.95\n",
      "2017-11-06T15:04:11.063141: step 2557, loss 0.176863, acc 0.875\n",
      "2017-11-06T15:04:15.250116: step 2558, loss 0.106133, acc 0.96875\n",
      "2017-11-06T15:04:19.294990: step 2559, loss 0.289332, acc 0.875\n",
      "2017-11-06T15:04:23.374889: step 2560, loss 0.0994591, acc 0.9375\n",
      "2017-11-06T15:04:27.501821: step 2561, loss 0.131576, acc 0.90625\n",
      "2017-11-06T15:04:31.613743: step 2562, loss 0.0486401, acc 1\n",
      "2017-11-06T15:04:35.936815: step 2563, loss 0.34874, acc 0.875\n",
      "2017-11-06T15:04:40.033551: step 2564, loss 0.0290568, acc 1\n",
      "2017-11-06T15:04:44.116454: step 2565, loss 0.133538, acc 0.90625\n",
      "2017-11-06T15:04:48.175335: step 2566, loss 0.391574, acc 0.8125\n",
      "2017-11-06T15:04:52.200195: step 2567, loss 0.0694786, acc 0.96875\n",
      "2017-11-06T15:04:56.356148: step 2568, loss 0.164067, acc 0.9375\n",
      "2017-11-06T15:05:00.715246: step 2569, loss 0.147364, acc 0.9375\n",
      "2017-11-06T15:05:04.702078: step 2570, loss 0.0509779, acc 1\n",
      "2017-11-06T15:05:08.646882: step 2571, loss 0.195154, acc 0.9375\n",
      "2017-11-06T15:05:12.659732: step 2572, loss 0.103218, acc 0.96875\n",
      "2017-11-06T15:05:16.620547: step 2573, loss 0.18877, acc 0.90625\n",
      "2017-11-06T15:05:20.609382: step 2574, loss 0.17757, acc 0.90625\n",
      "2017-11-06T15:05:24.585206: step 2575, loss 0.408711, acc 0.84375\n",
      "2017-11-06T15:05:28.597057: step 2576, loss 0.189478, acc 0.90625\n",
      "2017-11-06T15:05:32.626920: step 2577, loss 0.11373, acc 0.9375\n",
      "2017-11-06T15:05:36.637771: step 2578, loss 0.114891, acc 0.96875\n",
      "2017-11-06T15:05:40.623602: step 2579, loss 0.0708742, acc 0.96875\n",
      "2017-11-06T15:05:44.624445: step 2580, loss 0.131595, acc 0.90625\n",
      "2017-11-06T15:05:48.613279: step 2581, loss 0.0491646, acc 0.96875\n",
      "2017-11-06T15:05:52.558082: step 2582, loss 0.207401, acc 0.9375\n",
      "2017-11-06T15:05:56.487875: step 2583, loss 0.0248443, acc 1\n",
      "2017-11-06T15:06:00.512734: step 2584, loss 0.384863, acc 0.90625\n",
      "2017-11-06T15:06:04.932875: step 2585, loss 0.157034, acc 0.9375\n",
      "2017-11-06T15:06:08.923710: step 2586, loss 0.36765, acc 0.875\n",
      "2017-11-06T15:06:12.906540: step 2587, loss 0.184815, acc 0.9375\n",
      "2017-11-06T15:06:16.978455: step 2588, loss 0.0219605, acc 1\n",
      "2017-11-06T15:06:20.974293: step 2589, loss 0.0874177, acc 0.96875\n",
      "2017-11-06T15:06:24.960125: step 2590, loss 0.145361, acc 0.90625\n",
      "2017-11-06T15:06:28.910933: step 2591, loss 0.0711109, acc 0.9375\n",
      "2017-11-06T15:06:31.448735: step 2592, loss 0.145054, acc 0.9\n",
      "2017-11-06T15:06:35.808834: step 2593, loss 0.0893623, acc 0.9375\n",
      "2017-11-06T15:06:39.769648: step 2594, loss 0.195025, acc 0.90625\n",
      "2017-11-06T15:06:43.730462: step 2595, loss 0.0487161, acc 1\n",
      "2017-11-06T15:06:47.710290: step 2596, loss 0.104696, acc 0.9375\n",
      "2017-11-06T15:06:51.658097: step 2597, loss 0.171258, acc 0.90625\n",
      "2017-11-06T15:06:55.632919: step 2598, loss 0.224551, acc 0.875\n",
      "2017-11-06T15:06:59.586730: step 2599, loss 0.221041, acc 0.90625\n",
      "2017-11-06T15:07:03.562556: step 2600, loss 0.0964837, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:07:06.172408: step 2600, loss 1.19585, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-06T15:07:11.921369: step 2601, loss 0.320206, acc 0.84375\n",
      "2017-11-06T15:07:15.940225: step 2602, loss 0.145759, acc 0.9375\n",
      "2017-11-06T15:07:19.918050: step 2603, loss 0.0761852, acc 0.96875\n",
      "2017-11-06T15:07:23.870860: step 2604, loss 0.168942, acc 0.90625\n",
      "2017-11-06T15:07:27.838678: step 2605, loss 0.0700964, acc 0.96875\n",
      "2017-11-06T15:07:31.838522: step 2606, loss 0.0856057, acc 0.9375\n",
      "2017-11-06T15:07:35.811343: step 2607, loss 0.194348, acc 0.90625\n",
      "2017-11-06T15:07:39.868977: step 2608, loss 0.193465, acc 0.875\n",
      "2017-11-06T15:07:43.842801: step 2609, loss 0.18728, acc 0.875\n",
      "2017-11-06T15:07:47.859655: step 2610, loss 0.0886111, acc 1\n",
      "2017-11-06T15:07:51.894523: step 2611, loss 0.158109, acc 0.90625\n",
      "2017-11-06T15:07:55.872349: step 2612, loss 0.0862101, acc 0.96875\n",
      "2017-11-06T15:07:59.950245: step 2613, loss 0.0654842, acc 0.96875\n",
      "2017-11-06T15:08:03.983111: step 2614, loss 0.174536, acc 0.90625\n",
      "2017-11-06T15:08:07.936920: step 2615, loss 0.188616, acc 0.9375\n",
      "2017-11-06T15:08:12.099878: step 2616, loss 0.246793, acc 0.84375\n",
      "2017-11-06T15:08:16.541034: step 2617, loss 0.555245, acc 0.8125\n",
      "2017-11-06T15:08:20.562892: step 2618, loss 0.296714, acc 0.875\n",
      "2017-11-06T15:08:24.752869: step 2619, loss 0.279988, acc 0.84375\n",
      "2017-11-06T15:08:29.024905: step 2620, loss 0.232692, acc 0.9375\n",
      "2017-11-06T15:08:33.252909: step 2621, loss 0.0912386, acc 0.96875\n",
      "2017-11-06T15:08:37.495924: step 2622, loss 0.266215, acc 0.90625\n",
      "2017-11-06T15:08:41.496769: step 2623, loss 0.221103, acc 0.875\n",
      "2017-11-06T15:08:45.544643: step 2624, loss 0.0643325, acc 0.96875\n",
      "2017-11-06T15:08:49.576507: step 2625, loss 0.321969, acc 0.84375\n",
      "2017-11-06T15:08:53.595364: step 2626, loss 0.245895, acc 0.84375\n",
      "2017-11-06T15:08:57.622226: step 2627, loss 0.0929105, acc 0.96875\n",
      "2017-11-06T15:09:00.191050: step 2628, loss 0.0629988, acc 1\n",
      "2017-11-06T15:09:04.217911: step 2629, loss 0.163297, acc 0.875\n",
      "2017-11-06T15:09:08.211749: step 2630, loss 0.237928, acc 0.84375\n",
      "2017-11-06T15:09:12.168560: step 2631, loss 0.0838457, acc 0.96875\n",
      "2017-11-06T15:09:16.208430: step 2632, loss 0.205593, acc 0.90625\n",
      "2017-11-06T15:09:20.639579: step 2633, loss 0.0733628, acc 0.96875\n",
      "2017-11-06T15:09:24.788527: step 2634, loss 0.144396, acc 0.90625\n",
      "2017-11-06T15:09:28.754348: step 2635, loss 0.0890644, acc 0.9375\n",
      "2017-11-06T15:09:32.770200: step 2636, loss 0.113696, acc 0.875\n",
      "2017-11-06T15:09:36.776044: step 2637, loss 0.133512, acc 0.90625\n",
      "2017-11-06T15:09:40.795902: step 2638, loss 0.103821, acc 0.96875\n",
      "2017-11-06T15:09:44.801747: step 2639, loss 0.254373, acc 0.875\n",
      "2017-11-06T15:09:48.810595: step 2640, loss 0.0902149, acc 0.96875\n",
      "2017-11-06T15:09:52.865477: step 2641, loss 0.103974, acc 0.9375\n",
      "2017-11-06T15:09:56.860315: step 2642, loss 0.0577285, acc 0.96875\n",
      "2017-11-06T15:10:01.129352: step 2643, loss 0.203011, acc 0.875\n",
      "2017-11-06T15:10:05.167217: step 2644, loss 0.0849784, acc 0.96875\n",
      "2017-11-06T15:10:09.134036: step 2645, loss 0.304406, acc 0.875\n",
      "2017-11-06T15:10:13.145886: step 2646, loss 0.182817, acc 0.90625\n",
      "2017-11-06T15:10:17.111705: step 2647, loss 0.175741, acc 0.9375\n",
      "2017-11-06T15:10:21.284059: step 2648, loss 0.298134, acc 0.8125\n",
      "2017-11-06T15:10:25.644156: step 2649, loss 0.357338, acc 0.8125\n",
      "2017-11-06T15:10:29.813118: step 2650, loss 0.345151, acc 0.84375\n",
      "2017-11-06T15:10:33.987084: step 2651, loss 0.176731, acc 0.90625\n",
      "2017-11-06T15:10:38.118019: step 2652, loss 0.225402, acc 0.875\n",
      "2017-11-06T15:10:42.115633: step 2653, loss 0.0404529, acc 0.96875\n",
      "2017-11-06T15:10:46.133487: step 2654, loss 0.322791, acc 0.875\n",
      "2017-11-06T15:10:50.068284: step 2655, loss 0.190076, acc 0.90625\n",
      "2017-11-06T15:10:54.059119: step 2656, loss 0.0324977, acc 1\n",
      "2017-11-06T15:10:58.107998: step 2657, loss 0.228089, acc 0.84375\n",
      "2017-11-06T15:11:02.081820: step 2658, loss 0.19115, acc 0.875\n",
      "2017-11-06T15:11:06.158716: step 2659, loss 0.081029, acc 0.96875\n",
      "2017-11-06T15:11:10.141547: step 2660, loss 0.159875, acc 0.9375\n",
      "2017-11-06T15:11:14.096356: step 2661, loss 0.173829, acc 0.9375\n",
      "2017-11-06T15:11:18.105205: step 2662, loss 0.271192, acc 0.90625\n",
      "2017-11-06T15:11:22.121059: step 2663, loss 0.19907, acc 0.875\n",
      "2017-11-06T15:11:24.708899: step 2664, loss 0.207052, acc 0.95\n",
      "2017-11-06T15:11:28.762777: step 2665, loss 0.0868617, acc 0.9375\n",
      "2017-11-06T15:11:33.169909: step 2666, loss 0.0891123, acc 0.96875\n",
      "2017-11-06T15:11:37.196771: step 2667, loss 0.214745, acc 0.90625\n",
      "2017-11-06T15:11:41.142574: step 2668, loss 0.082705, acc 0.96875\n",
      "2017-11-06T15:11:45.210483: step 2669, loss 0.102169, acc 0.96875\n",
      "2017-11-06T15:11:49.147263: step 2670, loss 0.212086, acc 0.875\n",
      "2017-11-06T15:11:53.288203: step 2671, loss 0.229835, acc 0.875\n",
      "2017-11-06T15:11:57.315065: step 2672, loss 0.180984, acc 0.9375\n",
      "2017-11-06T15:12:01.391963: step 2673, loss 0.127672, acc 0.9375\n",
      "2017-11-06T15:12:05.371792: step 2674, loss 0.418226, acc 0.8125\n",
      "2017-11-06T15:12:09.369632: step 2675, loss 0.10201, acc 0.9375\n",
      "2017-11-06T15:12:13.425512: step 2676, loss 0.00697257, acc 1\n",
      "2017-11-06T15:12:17.559450: step 2677, loss 0.315623, acc 0.8125\n",
      "2017-11-06T15:12:21.605325: step 2678, loss 0.102769, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T15:12:25.612172: step 2679, loss 0.218331, acc 0.875\n",
      "2017-11-06T15:12:29.567982: step 2680, loss 0.263294, acc 0.875\n",
      "2017-11-06T15:12:33.757959: step 2681, loss 0.115627, acc 0.96875\n",
      "2017-11-06T15:12:38.310194: step 2682, loss 0.162802, acc 0.90625\n",
      "2017-11-06T15:12:42.395097: step 2683, loss 0.17537, acc 0.96875\n",
      "2017-11-06T15:12:46.351909: step 2684, loss 0.0652487, acc 0.96875\n",
      "2017-11-06T15:12:50.378769: step 2685, loss 0.06757, acc 0.96875\n",
      "2017-11-06T15:12:54.427646: step 2686, loss 0.278788, acc 0.875\n",
      "2017-11-06T15:12:58.451505: step 2687, loss 0.146433, acc 0.90625\n",
      "2017-11-06T15:13:02.427331: step 2688, loss 0.128732, acc 0.9375\n",
      "2017-11-06T15:13:06.485213: step 2689, loss 0.1228, acc 0.96875\n",
      "2017-11-06T15:13:10.518081: step 2690, loss 0.258278, acc 0.90625\n",
      "2017-11-06T15:13:14.498908: step 2691, loss 0.264418, acc 0.90625\n",
      "2017-11-06T15:13:18.565797: step 2692, loss 0.333229, acc 0.8125\n",
      "2017-11-06T15:13:22.714745: step 2693, loss 0.213635, acc 0.875\n",
      "2017-11-06T15:13:26.779634: step 2694, loss 0.0557971, acc 0.96875\n",
      "2017-11-06T15:13:30.839518: step 2695, loss 0.150323, acc 0.90625\n",
      "2017-11-06T15:13:34.807338: step 2696, loss 0.136885, acc 0.96875\n",
      "2017-11-06T15:13:38.836201: step 2697, loss 0.322293, acc 0.75\n",
      "2017-11-06T15:13:43.259152: step 2698, loss 0.0539941, acc 0.96875\n",
      "2017-11-06T15:13:47.372074: step 2699, loss 0.337891, acc 0.8125\n",
      "2017-11-06T15:13:50.018955: step 2700, loss 0.02635, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:13:52.586781: step 2700, loss 1.18199, acc 0.7\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-06T15:13:58.008892: step 2701, loss 0.209605, acc 0.875\n",
      "2017-11-06T15:14:01.987719: step 2702, loss 0.0307502, acc 1\n",
      "2017-11-06T15:14:05.965546: step 2703, loss 0.0892408, acc 0.96875\n",
      "2017-11-06T15:14:09.985401: step 2704, loss 0.187719, acc 0.90625\n",
      "2017-11-06T15:14:14.006259: step 2705, loss 0.010743, acc 1\n",
      "2017-11-06T15:14:18.059139: step 2706, loss 0.0887464, acc 0.9375\n",
      "2017-11-06T15:14:22.089002: step 2707, loss 0.0405757, acc 0.96875\n",
      "2017-11-06T15:14:26.153890: step 2708, loss 0.119652, acc 0.9375\n",
      "2017-11-06T15:14:30.137721: step 2709, loss 0.0533886, acc 1\n",
      "2017-11-06T15:14:34.337706: step 2710, loss 0.163937, acc 0.9375\n",
      "2017-11-06T15:14:38.487653: step 2711, loss 0.212476, acc 0.90625\n",
      "2017-11-06T15:14:42.479490: step 2712, loss 0.27303, acc 0.875\n",
      "2017-11-06T15:14:46.725507: step 2713, loss 0.497057, acc 0.75\n",
      "2017-11-06T15:14:51.097614: step 2714, loss 0.0948886, acc 0.96875\n",
      "2017-11-06T15:14:55.156498: step 2715, loss 0.100541, acc 0.9375\n",
      "2017-11-06T15:14:59.244403: step 2716, loss 0.188359, acc 0.9375\n",
      "2017-11-06T15:15:03.283272: step 2717, loss 0.170884, acc 0.9375\n",
      "2017-11-06T15:15:07.268104: step 2718, loss 0.152726, acc 0.90625\n",
      "2017-11-06T15:15:11.308975: step 2719, loss 0.11426, acc 0.96875\n",
      "2017-11-06T15:15:15.343841: step 2720, loss 0.177786, acc 0.875\n",
      "2017-11-06T15:15:19.343684: step 2721, loss 0.183435, acc 0.90625\n",
      "2017-11-06T15:15:23.378551: step 2722, loss 0.36838, acc 0.8125\n",
      "2017-11-06T15:15:27.439436: step 2723, loss 0.0585384, acc 0.96875\n",
      "2017-11-06T15:15:31.465296: step 2724, loss 0.077059, acc 0.96875\n",
      "2017-11-06T15:15:35.453130: step 2725, loss 0.332041, acc 0.875\n",
      "2017-11-06T15:15:39.483996: step 2726, loss 0.198264, acc 0.90625\n",
      "2017-11-06T15:15:43.465823: step 2727, loss 0.305487, acc 0.90625\n",
      "2017-11-06T15:15:47.479675: step 2728, loss 0.180932, acc 0.90625\n",
      "2017-11-06T15:15:51.652641: step 2729, loss 0.238939, acc 0.875\n",
      "2017-11-06T15:15:56.080787: step 2730, loss 0.212496, acc 0.90625\n",
      "2017-11-06T15:16:00.139671: step 2731, loss 0.522514, acc 0.78125\n",
      "2017-11-06T15:16:04.152524: step 2732, loss 0.143855, acc 0.9375\n",
      "2017-11-06T15:16:08.191392: step 2733, loss 0.104275, acc 0.96875\n",
      "2017-11-06T15:16:12.275294: step 2734, loss 0.192464, acc 0.9375\n",
      "2017-11-06T15:16:16.275136: step 2735, loss 0.432886, acc 0.84375\n",
      "2017-11-06T15:16:18.806934: step 2736, loss 0.419063, acc 0.8\n",
      "2017-11-06T15:16:22.831795: step 2737, loss 0.184049, acc 0.90625\n",
      "2017-11-06T15:16:26.901687: step 2738, loss 0.166635, acc 0.90625\n",
      "2017-11-06T15:16:30.819470: step 2739, loss 0.037674, acc 1\n",
      "2017-11-06T15:16:35.223599: step 2740, loss 0.220762, acc 0.875\n",
      "2017-11-06T15:16:39.272478: step 2741, loss 0.163975, acc 0.9375\n",
      "2017-11-06T15:16:43.270466: step 2742, loss 0.216604, acc 0.875\n",
      "2017-11-06T15:16:47.314338: step 2743, loss 0.104318, acc 0.96875\n",
      "2017-11-06T15:16:51.307175: step 2744, loss 0.143071, acc 0.90625\n",
      "2017-11-06T15:16:55.386073: step 2745, loss 0.162979, acc 0.9375\n",
      "2017-11-06T15:16:59.704141: step 2746, loss 0.0762032, acc 0.96875\n",
      "2017-11-06T15:17:03.896120: step 2747, loss 0.0919168, acc 0.96875\n",
      "2017-11-06T15:17:07.900965: step 2748, loss 0.147173, acc 0.96875\n",
      "2017-11-06T15:17:11.939836: step 2749, loss 0.0461881, acc 1\n",
      "2017-11-06T15:17:15.910658: step 2750, loss 0.0246947, acc 1\n",
      "2017-11-06T15:17:19.918505: step 2751, loss 0.0333441, acc 0.96875\n",
      "2017-11-06T15:17:23.883321: step 2752, loss 0.124984, acc 0.90625\n",
      "2017-11-06T15:17:27.960218: step 2753, loss 0.198277, acc 0.90625\n",
      "2017-11-06T15:17:31.943048: step 2754, loss 0.108921, acc 0.96875\n",
      "2017-11-06T15:17:35.903863: step 2755, loss 0.142048, acc 0.9375\n",
      "2017-11-06T15:17:39.917715: step 2756, loss 0.0343943, acc 1\n",
      "2017-11-06T15:17:43.829494: step 2757, loss 0.316088, acc 0.84375\n",
      "2017-11-06T15:17:47.786306: step 2758, loss 0.194226, acc 0.90625\n",
      "2017-11-06T15:17:51.777142: step 2759, loss 0.338244, acc 0.875\n",
      "2017-11-06T15:17:55.754968: step 2760, loss 0.0191204, acc 1\n",
      "2017-11-06T15:17:59.817855: step 2761, loss 0.404381, acc 0.875\n",
      "2017-11-06T15:18:04.069876: step 2762, loss 0.193878, acc 0.9375\n",
      "2017-11-06T15:18:08.450989: step 2763, loss 0.183798, acc 0.90625\n",
      "2017-11-06T15:18:12.516878: step 2764, loss 0.540774, acc 0.84375\n",
      "2017-11-06T15:18:16.596777: step 2765, loss 0.0600508, acc 0.96875\n",
      "2017-11-06T15:18:20.655661: step 2766, loss 0.14055, acc 0.9375\n",
      "2017-11-06T15:18:24.772586: step 2767, loss 0.374176, acc 0.84375\n",
      "2017-11-06T15:18:28.817460: step 2768, loss 0.28517, acc 0.875\n",
      "2017-11-06T15:18:32.949396: step 2769, loss 0.168756, acc 0.96875\n",
      "2017-11-06T15:18:37.089337: step 2770, loss 0.347927, acc 0.84375\n",
      "2017-11-06T15:18:41.139215: step 2771, loss 0.547546, acc 0.84375\n",
      "2017-11-06T15:18:43.663008: step 2772, loss 0.0707058, acc 1\n",
      "2017-11-06T15:18:47.675862: step 2773, loss 0.252854, acc 0.9375\n",
      "2017-11-06T15:18:51.704723: step 2774, loss 0.0850166, acc 0.96875\n",
      "2017-11-06T15:18:55.800634: step 2775, loss 0.0796586, acc 0.96875\n",
      "2017-11-06T15:18:59.804477: step 2776, loss 0.141726, acc 0.90625\n",
      "2017-11-06T15:19:03.854357: step 2777, loss 0.254739, acc 0.84375\n",
      "2017-11-06T15:19:07.906234: step 2778, loss 0.204521, acc 0.875\n",
      "2017-11-06T15:19:12.452464: step 2779, loss 0.0467206, acc 1\n",
      "2017-11-06T15:19:16.526360: step 2780, loss 0.18067, acc 0.9375\n",
      "2017-11-06T15:19:20.556223: step 2781, loss 0.0209552, acc 1\n",
      "2017-11-06T15:19:24.767215: step 2782, loss 0.215247, acc 0.875\n",
      "2017-11-06T15:19:28.880138: step 2783, loss 0.21889, acc 0.875\n",
      "2017-11-06T15:19:32.896991: step 2784, loss 0.161932, acc 0.90625\n",
      "2017-11-06T15:19:36.902838: step 2785, loss 0.0807275, acc 0.96875\n",
      "2017-11-06T15:19:40.940707: step 2786, loss 0.0493326, acc 1\n",
      "2017-11-06T15:19:44.910350: step 2787, loss 0.195791, acc 0.9375\n",
      "2017-11-06T15:19:48.904188: step 2788, loss 0.141165, acc 0.90625\n",
      "2017-11-06T15:19:52.938054: step 2789, loss 0.149263, acc 0.90625\n",
      "2017-11-06T15:19:56.911879: step 2790, loss 0.0615964, acc 0.96875\n",
      "2017-11-06T15:20:01.203927: step 2791, loss 0.296857, acc 0.875\n",
      "2017-11-06T15:20:05.274820: step 2792, loss 0.23208, acc 0.875\n",
      "2017-11-06T15:20:09.308686: step 2793, loss 0.108225, acc 0.9375\n",
      "2017-11-06T15:20:13.274504: step 2794, loss 0.270734, acc 0.875\n",
      "2017-11-06T15:20:17.698647: step 2795, loss 0.155775, acc 0.9375\n",
      "2017-11-06T15:20:21.728512: step 2796, loss 0.121601, acc 0.96875\n",
      "2017-11-06T15:20:25.794399: step 2797, loss 0.162784, acc 0.9375\n",
      "2017-11-06T15:20:29.787237: step 2798, loss 0.163203, acc 0.90625\n",
      "2017-11-06T15:20:34.031252: step 2799, loss 0.257178, acc 0.9375\n",
      "2017-11-06T15:20:38.090136: step 2800, loss 0.0450852, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:20:40.702993: step 2800, loss 1.23045, acc 0.666667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2800\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T15:20:45.935979: step 2801, loss 0.232331, acc 0.90625\n",
      "2017-11-06T15:20:49.958838: step 2802, loss 0.208525, acc 0.875\n",
      "2017-11-06T15:20:54.020725: step 2803, loss 0.231357, acc 0.875\n",
      "2017-11-06T15:20:57.997551: step 2804, loss 0.119103, acc 0.9375\n",
      "2017-11-06T15:21:02.036419: step 2805, loss 0.231445, acc 0.875\n",
      "2017-11-06T15:21:06.095303: step 2806, loss 0.178676, acc 0.90625\n",
      "2017-11-06T15:21:10.053118: step 2807, loss 0.261025, acc 0.875\n",
      "2017-11-06T15:21:12.669975: step 2808, loss 0.319503, acc 0.8\n",
      "2017-11-06T15:21:16.682827: step 2809, loss 0.164241, acc 0.90625\n",
      "2017-11-06T15:21:20.910830: step 2810, loss 0.1671, acc 0.9375\n",
      "2017-11-06T15:21:25.189871: step 2811, loss 0.156096, acc 0.90625\n",
      "2017-11-06T15:21:29.191714: step 2812, loss 0.425864, acc 0.84375\n",
      "2017-11-06T15:21:33.245595: step 2813, loss 0.163903, acc 0.9375\n",
      "2017-11-06T15:21:37.240434: step 2814, loss 0.111381, acc 0.90625\n",
      "2017-11-06T15:21:41.267295: step 2815, loss 0.439968, acc 0.8125\n",
      "2017-11-06T15:21:45.288152: step 2816, loss 0.277882, acc 0.875\n",
      "2017-11-06T15:21:49.248966: step 2817, loss 0.197852, acc 0.9375\n",
      "2017-11-06T15:21:53.294841: step 2818, loss 0.0294241, acc 1\n",
      "2017-11-06T15:21:57.308693: step 2819, loss 0.127792, acc 0.96875\n",
      "2017-11-06T15:22:01.306534: step 2820, loss 0.207697, acc 0.90625\n",
      "2017-11-06T15:22:05.307376: step 2821, loss 0.0768892, acc 0.9375\n",
      "2017-11-06T15:22:09.371264: step 2822, loss 0.0880171, acc 0.96875\n",
      "2017-11-06T15:22:13.319069: step 2823, loss 0.126913, acc 0.90625\n",
      "2017-11-06T15:22:17.389961: step 2824, loss 0.219875, acc 0.90625\n",
      "2017-11-06T15:22:21.333763: step 2825, loss 0.0246694, acc 1\n",
      "2017-11-06T15:22:25.478709: step 2826, loss 0.219812, acc 0.875\n",
      "2017-11-06T15:22:29.934875: step 2827, loss 0.140859, acc 0.9375\n",
      "2017-11-06T15:22:34.271957: step 2828, loss 0.284441, acc 0.84375\n",
      "2017-11-06T15:22:38.344851: step 2829, loss 0.242398, acc 0.84375\n",
      "2017-11-06T15:22:42.351443: step 2830, loss 0.300617, acc 0.84375\n",
      "2017-11-06T15:22:46.286240: step 2831, loss 0.132569, acc 0.9375\n",
      "2017-11-06T15:22:50.200020: step 2832, loss 0.252873, acc 0.90625\n",
      "2017-11-06T15:22:54.147826: step 2833, loss 0.122618, acc 0.96875\n",
      "2017-11-06T15:22:58.053600: step 2834, loss 0.0621797, acc 0.96875\n",
      "2017-11-06T15:23:01.970383: step 2835, loss 0.216552, acc 0.875\n",
      "2017-11-06T15:23:05.933199: step 2836, loss 0.0852995, acc 0.96875\n",
      "2017-11-06T15:23:09.968066: step 2837, loss 0.330361, acc 0.8125\n",
      "2017-11-06T15:23:13.915871: step 2838, loss 0.120195, acc 0.9375\n",
      "2017-11-06T15:23:17.840659: step 2839, loss 0.270824, acc 0.90625\n",
      "2017-11-06T15:23:21.909551: step 2840, loss 0.470815, acc 0.8125\n",
      "2017-11-06T15:23:25.913395: step 2841, loss 0.498502, acc 0.875\n",
      "2017-11-06T15:23:29.828177: step 2842, loss 0.100257, acc 0.9375\n",
      "2017-11-06T15:23:34.243314: step 2843, loss 0.0777606, acc 0.96875\n",
      "2017-11-06T15:23:36.859173: step 2844, loss 0.222316, acc 0.9\n",
      "2017-11-06T15:23:40.826993: step 2845, loss 0.164084, acc 0.875\n",
      "2017-11-06T15:23:44.739773: step 2846, loss 0.138149, acc 0.9375\n",
      "2017-11-06T15:23:48.682574: step 2847, loss 0.0790969, acc 0.96875\n",
      "2017-11-06T15:23:52.631381: step 2848, loss 0.111639, acc 0.96875\n",
      "2017-11-06T15:23:56.585189: step 2849, loss 0.144181, acc 0.9375\n",
      "2017-11-06T15:24:00.535997: step 2850, loss 0.101377, acc 0.9375\n",
      "2017-11-06T15:24:04.565860: step 2851, loss 0.0733859, acc 0.96875\n",
      "2017-11-06T15:24:08.559697: step 2852, loss 0.200524, acc 0.90625\n",
      "2017-11-06T15:24:12.510504: step 2853, loss 0.340081, acc 0.84375\n",
      "2017-11-06T15:24:16.438296: step 2854, loss 0.203326, acc 0.90625\n",
      "2017-11-06T15:24:20.402114: step 2855, loss 0.144744, acc 0.96875\n",
      "2017-11-06T15:24:24.347917: step 2856, loss 0.302832, acc 0.875\n",
      "2017-11-06T15:24:28.273707: step 2857, loss 0.341209, acc 0.875\n",
      "2017-11-06T15:24:32.260690: step 2858, loss 0.266244, acc 0.84375\n",
      "2017-11-06T15:24:36.509709: step 2859, loss 0.186902, acc 0.90625\n",
      "2017-11-06T15:24:40.844790: step 2860, loss 0.129131, acc 0.9375\n",
      "2017-11-06T15:24:44.787591: step 2861, loss 0.238411, acc 0.875\n",
      "2017-11-06T15:24:48.704374: step 2862, loss 0.263762, acc 0.9375\n",
      "2017-11-06T15:24:52.654182: step 2863, loss 0.112428, acc 0.9375\n",
      "2017-11-06T15:24:56.558956: step 2864, loss 0.278049, acc 0.90625\n",
      "2017-11-06T15:25:00.484745: step 2865, loss 0.135486, acc 0.9375\n",
      "2017-11-06T15:25:04.462571: step 2866, loss 0.180891, acc 0.84375\n",
      "2017-11-06T15:25:08.437397: step 2867, loss 0.11234, acc 0.9375\n",
      "2017-11-06T15:25:12.358181: step 2868, loss 0.345338, acc 0.84375\n",
      "2017-11-06T15:25:16.311991: step 2869, loss 0.1536, acc 0.875\n",
      "2017-11-06T15:25:20.228773: step 2870, loss 0.0620922, acc 0.96875\n",
      "2017-11-06T15:25:24.150562: step 2871, loss 0.366523, acc 0.875\n",
      "2017-11-06T15:25:28.082354: step 2872, loss 0.075059, acc 0.96875\n",
      "2017-11-06T15:25:32.026157: step 2873, loss 0.240683, acc 0.9375\n",
      "2017-11-06T15:25:35.945941: step 2874, loss 0.0802292, acc 0.96875\n",
      "2017-11-06T15:25:39.931773: step 2875, loss 0.0819916, acc 0.96875\n",
      "2017-11-06T15:25:44.158547: step 2876, loss 0.319716, acc 0.875\n",
      "2017-11-06T15:25:48.289481: step 2877, loss 0.0816097, acc 0.96875\n",
      "2017-11-06T15:25:52.281318: step 2878, loss 0.246331, acc 0.90625\n",
      "2017-11-06T15:25:56.182089: step 2879, loss 0.304604, acc 0.875\n",
      "2017-11-06T15:25:58.693874: step 2880, loss 0.178124, acc 0.85\n",
      "2017-11-06T15:26:02.670699: step 2881, loss 0.0584933, acc 0.96875\n",
      "2017-11-06T15:26:06.569472: step 2882, loss 0.168085, acc 0.90625\n",
      "2017-11-06T15:26:10.591327: step 2883, loss 0.188009, acc 0.875\n",
      "2017-11-06T15:26:14.538134: step 2884, loss 0.241669, acc 0.875\n",
      "2017-11-06T15:26:18.468924: step 2885, loss 0.0781006, acc 0.9375\n",
      "2017-11-06T15:26:22.394714: step 2886, loss 0.0710132, acc 0.96875\n",
      "2017-11-06T15:26:26.362535: step 2887, loss 0.104249, acc 0.96875\n",
      "2017-11-06T15:26:30.331354: step 2888, loss 0.109709, acc 0.9375\n",
      "2017-11-06T15:26:34.830770: step 2889, loss 0.124012, acc 0.9375\n",
      "2017-11-06T15:26:39.219393: step 2890, loss 0.119269, acc 0.9375\n",
      "2017-11-06T15:26:43.412889: step 2891, loss 0.231909, acc 0.90625\n",
      "2017-11-06T15:26:47.425769: step 2892, loss 0.20814, acc 0.90625\n",
      "2017-11-06T15:26:51.751843: step 2893, loss 0.124003, acc 0.90625\n",
      "2017-11-06T15:26:55.695645: step 2894, loss 0.341055, acc 0.84375\n",
      "2017-11-06T15:26:59.653457: step 2895, loss 0.0737867, acc 0.96875\n",
      "2017-11-06T15:27:03.600262: step 2896, loss 0.1202, acc 0.96875\n",
      "2017-11-06T15:27:07.538060: step 2897, loss 0.0301802, acc 0.96875\n",
      "2017-11-06T15:27:11.491871: step 2898, loss 0.0707723, acc 0.9375\n",
      "2017-11-06T15:27:15.415657: step 2899, loss 0.341993, acc 0.84375\n",
      "2017-11-06T15:27:19.371468: step 2900, loss 0.102645, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:27:21.892259: step 2900, loss 1.23427, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-06T15:27:27.115184: step 2901, loss 0.263475, acc 0.875\n",
      "2017-11-06T15:27:31.040974: step 2902, loss 0.44661, acc 0.84375\n",
      "2017-11-06T15:27:34.982774: step 2903, loss 0.352948, acc 0.8125\n",
      "2017-11-06T15:27:38.878542: step 2904, loss 0.115984, acc 0.96875\n",
      "2017-11-06T15:27:42.773312: step 2905, loss 0.0720814, acc 0.96875\n",
      "2017-11-06T15:27:46.716111: step 2906, loss 0.175319, acc 0.9375\n",
      "2017-11-06T15:27:50.649905: step 2907, loss 0.285989, acc 0.875\n",
      "2017-11-06T15:27:54.878911: step 2908, loss 0.276578, acc 0.84375\n",
      "2017-11-06T15:27:58.990833: step 2909, loss 0.114568, acc 0.9375\n",
      "2017-11-06T15:28:02.946647: step 2910, loss 0.207934, acc 0.90625\n",
      "2017-11-06T15:28:06.849418: step 2911, loss 0.257166, acc 0.90625\n",
      "2017-11-06T15:28:10.804227: step 2912, loss 0.186464, acc 0.90625\n",
      "2017-11-06T15:28:14.744025: step 2913, loss 0.109803, acc 0.96875\n",
      "2017-11-06T15:28:18.718850: step 2914, loss 0.356801, acc 0.8125\n",
      "2017-11-06T15:28:22.735704: step 2915, loss 0.128239, acc 0.9375\n",
      "2017-11-06T15:28:25.574722: step 2916, loss 0.11297, acc 0.95\n",
      "2017-11-06T15:28:29.604585: step 2917, loss 0.101413, acc 0.96875\n",
      "2017-11-06T15:28:33.821581: step 2918, loss 0.0500346, acc 1\n",
      "2017-11-06T15:28:37.846441: step 2919, loss 0.2047, acc 0.96875\n",
      "2017-11-06T15:28:41.843281: step 2920, loss 0.210809, acc 0.90625\n",
      "2017-11-06T15:28:45.858952: step 2921, loss 0.0695633, acc 0.96875\n",
      "2017-11-06T15:28:49.834776: step 2922, loss 0.126715, acc 0.9375\n",
      "2017-11-06T15:28:53.827613: step 2923, loss 0.107159, acc 0.9375\n",
      "2017-11-06T15:28:57.752402: step 2924, loss 0.202007, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T15:29:02.164537: step 2925, loss 0.0771607, acc 0.96875\n",
      "2017-11-06T15:29:06.219418: step 2926, loss 0.0720586, acc 0.96875\n",
      "2017-11-06T15:29:10.204249: step 2927, loss 0.120881, acc 0.9375\n",
      "2017-11-06T15:29:14.146051: step 2928, loss 0.143106, acc 0.90625\n",
      "2017-11-06T15:29:18.135887: step 2929, loss 0.227447, acc 0.90625\n",
      "2017-11-06T15:29:22.101703: step 2930, loss 0.202558, acc 0.90625\n",
      "2017-11-06T15:29:26.043504: step 2931, loss 0.103804, acc 0.9375\n",
      "2017-11-06T15:29:29.942274: step 2932, loss 0.202511, acc 0.84375\n",
      "2017-11-06T15:29:33.974139: step 2933, loss 0.283143, acc 0.875\n",
      "2017-11-06T15:29:37.937956: step 2934, loss 0.350617, acc 0.84375\n",
      "2017-11-06T15:29:41.906776: step 2935, loss 0.204904, acc 0.875\n",
      "2017-11-06T15:29:45.863588: step 2936, loss 0.18111, acc 0.90625\n",
      "2017-11-06T15:29:49.839412: step 2937, loss 0.0650643, acc 0.9375\n",
      "2017-11-06T15:29:53.761199: step 2938, loss 0.0453685, acc 0.96875\n",
      "2017-11-06T15:29:57.714007: step 2939, loss 0.319824, acc 0.84375\n",
      "2017-11-06T15:30:01.910990: step 2940, loss 0.285352, acc 0.875\n",
      "2017-11-06T15:30:06.097964: step 2941, loss 0.272625, acc 0.90625\n",
      "2017-11-06T15:30:10.289943: step 2942, loss 0.17899, acc 0.875\n",
      "2017-11-06T15:30:14.266769: step 2943, loss 0.363533, acc 0.90625\n",
      "2017-11-06T15:30:18.228584: step 2944, loss 0.214969, acc 0.84375\n",
      "2017-11-06T15:30:22.164382: step 2945, loss 0.149959, acc 0.90625\n",
      "2017-11-06T15:30:26.155216: step 2946, loss 0.236416, acc 0.84375\n",
      "2017-11-06T15:30:30.121033: step 2947, loss 0.315867, acc 0.8125\n",
      "2017-11-06T15:30:34.275986: step 2948, loss 0.0754343, acc 0.96875\n",
      "2017-11-06T15:30:38.248809: step 2949, loss 0.132278, acc 0.9375\n",
      "2017-11-06T15:30:42.172597: step 2950, loss 0.475457, acc 0.78125\n",
      "2017-11-06T15:30:46.084377: step 2951, loss 0.190877, acc 0.90625\n",
      "2017-11-06T15:30:48.657205: step 2952, loss 0.374987, acc 0.85\n",
      "2017-11-06T15:30:52.608012: step 2953, loss 0.299934, acc 0.875\n",
      "2017-11-06T15:30:56.542807: step 2954, loss 0.159786, acc 0.90625\n",
      "2017-11-06T15:31:00.459590: step 2955, loss 0.104998, acc 0.96875\n",
      "2017-11-06T15:31:04.370369: step 2956, loss 0.125127, acc 0.96875\n",
      "2017-11-06T15:31:08.428253: step 2957, loss 0.326541, acc 0.84375\n",
      "2017-11-06T15:31:12.844390: step 2958, loss 0.201425, acc 0.9375\n",
      "2017-11-06T15:31:16.967320: step 2959, loss 0.203394, acc 0.9375\n",
      "2017-11-06T15:31:20.946147: step 2960, loss 0.335053, acc 0.84375\n",
      "2017-11-06T15:31:24.882945: step 2961, loss 0.0877374, acc 0.9375\n",
      "2017-11-06T15:31:28.828748: step 2962, loss 0.147727, acc 0.90625\n",
      "2017-11-06T15:31:32.789562: step 2963, loss 0.277127, acc 0.875\n",
      "2017-11-06T15:31:36.964528: step 2964, loss 0.350574, acc 0.8125\n",
      "2017-11-06T15:31:41.553830: step 2965, loss 0.341754, acc 0.84375\n",
      "2017-11-06T15:31:45.718792: step 2966, loss 0.0721296, acc 0.96875\n",
      "2017-11-06T15:31:50.004335: step 2967, loss 0.0294724, acc 1\n",
      "2017-11-06T15:31:54.146296: step 2968, loss 0.141518, acc 0.9375\n",
      "2017-11-06T15:31:58.158175: step 2969, loss 0.115348, acc 0.96875\n",
      "2017-11-06T15:32:02.088950: step 2970, loss 0.26443, acc 0.90625\n",
      "2017-11-06T15:32:06.136828: step 2971, loss 0.50661, acc 0.84375\n",
      "2017-11-06T15:32:10.313794: step 2972, loss 0.245817, acc 0.90625\n",
      "2017-11-06T15:32:14.301630: step 2973, loss 0.164007, acc 0.90625\n",
      "2017-11-06T15:32:18.726772: step 2974, loss 0.140512, acc 0.9375\n",
      "2017-11-06T15:32:22.791660: step 2975, loss 0.0304802, acc 1\n",
      "2017-11-06T15:32:26.771489: step 2976, loss 0.106608, acc 0.9375\n",
      "2017-11-06T15:32:30.732302: step 2977, loss 0.103828, acc 0.96875\n",
      "2017-11-06T15:32:34.919278: step 2978, loss 0.135284, acc 0.9375\n",
      "2017-11-06T15:32:38.961233: step 2979, loss 0.127947, acc 0.90625\n",
      "2017-11-06T15:32:42.990095: step 2980, loss 0.15851, acc 0.90625\n",
      "2017-11-06T15:32:46.965921: step 2981, loss 0.220929, acc 0.90625\n",
      "2017-11-06T15:32:50.964762: step 2982, loss 0.131902, acc 0.9375\n",
      "2017-11-06T15:32:54.925576: step 2983, loss 0.259366, acc 0.875\n",
      "2017-11-06T15:32:58.924420: step 2984, loss 0.167905, acc 0.96875\n",
      "2017-11-06T15:33:02.860214: step 2985, loss 0.389299, acc 0.78125\n",
      "2017-11-06T15:33:06.856053: step 2986, loss 0.248518, acc 0.875\n",
      "2017-11-06T15:33:10.811864: step 2987, loss 0.161897, acc 0.9375\n",
      "2017-11-06T15:33:13.298631: step 2988, loss 0.208227, acc 0.9\n",
      "2017-11-06T15:33:17.257443: step 2989, loss 0.0574093, acc 0.96875\n",
      "2017-11-06T15:33:21.429409: step 2990, loss 0.30615, acc 0.875\n",
      "2017-11-06T15:33:26.007661: step 2991, loss 0.362889, acc 0.875\n",
      "2017-11-06T15:33:29.989490: step 2992, loss 0.215871, acc 0.96875\n",
      "2017-11-06T15:33:33.968317: step 2993, loss 0.043435, acc 1\n",
      "2017-11-06T15:33:37.958152: step 2994, loss 0.193436, acc 0.90625\n",
      "2017-11-06T15:33:42.125113: step 2995, loss 0.172569, acc 0.9375\n",
      "2017-11-06T15:33:46.151974: step 2996, loss 0.285643, acc 0.875\n",
      "2017-11-06T15:33:50.297920: step 2997, loss 0.0302762, acc 1\n",
      "2017-11-06T15:33:54.293760: step 2998, loss 0.0827289, acc 0.9375\n",
      "2017-11-06T15:33:58.476731: step 2999, loss 0.107112, acc 0.96875\n",
      "2017-11-06T15:34:02.571641: step 3000, loss 0.131898, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:34:05.200512: step 3000, loss 1.18771, acc 0.683333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-06T15:34:10.699416: step 3001, loss 0.0630178, acc 0.96875\n",
      "2017-11-06T15:34:14.797328: step 3002, loss 0.157296, acc 0.96875\n",
      "2017-11-06T15:34:18.972295: step 3003, loss 0.138917, acc 0.875\n",
      "2017-11-06T15:34:23.125245: step 3004, loss 0.204989, acc 0.875\n",
      "2017-11-06T15:34:27.383271: step 3005, loss 0.179237, acc 0.90625\n",
      "2017-11-06T15:34:31.852447: step 3006, loss 0.338571, acc 0.875\n",
      "2017-11-06T15:34:36.299606: step 3007, loss 0.106593, acc 0.96875\n",
      "2017-11-06T15:34:40.545624: step 3008, loss 0.294115, acc 0.84375\n",
      "2017-11-06T15:34:44.552235: step 3009, loss 0.169695, acc 0.90625\n",
      "2017-11-06T15:34:48.481027: step 3010, loss 0.0765424, acc 0.96875\n",
      "2017-11-06T15:34:52.471864: step 3011, loss 0.213103, acc 0.875\n",
      "2017-11-06T15:34:56.536750: step 3012, loss 0.204631, acc 0.875\n",
      "2017-11-06T15:35:00.531590: step 3013, loss 0.135653, acc 0.96875\n",
      "2017-11-06T15:35:04.513419: step 3014, loss 0.176397, acc 0.875\n",
      "2017-11-06T15:35:08.497251: step 3015, loss 0.339199, acc 0.8125\n",
      "2017-11-06T15:35:12.533117: step 3016, loss 0.0977619, acc 0.90625\n",
      "2017-11-06T15:35:16.530957: step 3017, loss 0.213999, acc 0.875\n",
      "2017-11-06T15:35:20.541807: step 3018, loss 0.068209, acc 0.96875\n",
      "2017-11-06T15:35:24.559662: step 3019, loss 0.23495, acc 0.90625\n",
      "2017-11-06T15:35:28.570512: step 3020, loss 0.0910795, acc 1\n",
      "2017-11-06T15:35:32.829538: step 3021, loss 0.240185, acc 0.875\n",
      "2017-11-06T15:35:37.267691: step 3022, loss 0.0287886, acc 1\n",
      "2017-11-06T15:35:41.345590: step 3023, loss 0.246177, acc 0.875\n",
      "2017-11-06T15:35:43.884393: step 3024, loss 0.112017, acc 0.95\n",
      "2017-11-06T15:35:47.871226: step 3025, loss 0.135345, acc 0.96875\n",
      "2017-11-06T15:35:51.881075: step 3026, loss 0.14439, acc 0.96875\n",
      "2017-11-06T15:35:55.958972: step 3027, loss 0.140094, acc 0.96875\n",
      "2017-11-06T15:35:59.981831: step 3028, loss 0.0657607, acc 0.96875\n",
      "2017-11-06T15:36:03.980672: step 3029, loss 0.39114, acc 0.8125\n",
      "2017-11-06T15:36:08.022544: step 3030, loss 0.203811, acc 0.875\n",
      "2017-11-06T15:36:12.026389: step 3031, loss 0.102644, acc 0.9375\n",
      "2017-11-06T15:36:16.031235: step 3032, loss 0.29777, acc 0.84375\n",
      "2017-11-06T15:36:20.022070: step 3033, loss 0.158294, acc 0.90625\n",
      "2017-11-06T15:36:24.049933: step 3034, loss 0.166179, acc 0.875\n",
      "2017-11-06T15:36:28.033763: step 3035, loss 0.18773, acc 0.96875\n",
      "2017-11-06T15:36:32.014591: step 3036, loss 0.130187, acc 0.9375\n",
      "2017-11-06T15:36:36.242595: step 3037, loss 0.194782, acc 0.90625\n",
      "2017-11-06T15:36:40.739791: step 3038, loss 0.110559, acc 0.9375\n",
      "2017-11-06T15:36:44.877731: step 3039, loss 0.124733, acc 0.90625\n",
      "2017-11-06T15:36:48.931612: step 3040, loss 0.116038, acc 0.9375\n",
      "2017-11-06T15:36:52.880417: step 3041, loss 0.251851, acc 0.875\n",
      "2017-11-06T15:36:56.899272: step 3042, loss 0.140815, acc 0.90625\n",
      "2017-11-06T15:37:00.852083: step 3043, loss 0.170684, acc 0.9375\n",
      "2017-11-06T15:37:04.799887: step 3044, loss 0.171942, acc 0.9375\n",
      "2017-11-06T15:37:08.811737: step 3045, loss 0.0858297, acc 0.96875\n",
      "2017-11-06T15:37:12.790564: step 3046, loss 0.105353, acc 0.96875\n",
      "2017-11-06T15:37:16.750378: step 3047, loss 0.157606, acc 0.90625\n",
      "2017-11-06T15:37:20.777241: step 3048, loss 0.110981, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T15:37:24.763071: step 3049, loss 0.198632, acc 0.875\n",
      "2017-11-06T15:37:28.753907: step 3050, loss 0.180131, acc 0.90625\n",
      "2017-11-06T15:37:32.948888: step 3051, loss 0.211152, acc 0.875\n",
      "2017-11-06T15:37:36.963740: step 3052, loss 0.154367, acc 0.9375\n",
      "2017-11-06T15:37:40.962582: step 3053, loss 0.228079, acc 0.90625\n",
      "2017-11-06T15:37:45.173333: step 3054, loss 0.139886, acc 0.9375\n",
      "2017-11-06T15:37:49.508433: step 3055, loss 0.109706, acc 0.96875\n",
      "2017-11-06T15:37:53.479234: step 3056, loss 0.178785, acc 0.96875\n",
      "2017-11-06T15:37:57.482079: step 3057, loss 0.308054, acc 0.84375\n",
      "2017-11-06T15:38:01.487925: step 3058, loss 0.171935, acc 0.9375\n",
      "2017-11-06T15:38:05.511786: step 3059, loss 0.348445, acc 0.90625\n",
      "2017-11-06T15:38:08.079609: step 3060, loss 0.133574, acc 0.95\n",
      "2017-11-06T15:38:12.069444: step 3061, loss 0.198055, acc 0.90625\n",
      "2017-11-06T15:38:16.104311: step 3062, loss 0.141327, acc 0.90625\n",
      "2017-11-06T15:38:20.103152: step 3063, loss 0.0639577, acc 1\n",
      "2017-11-06T15:38:24.241093: step 3064, loss 0.0601743, acc 0.96875\n",
      "2017-11-06T15:38:28.367024: step 3065, loss 0.201956, acc 0.9375\n",
      "2017-11-06T15:38:32.384879: step 3066, loss 0.378898, acc 0.84375\n",
      "2017-11-06T15:38:36.562848: step 3067, loss 0.0973922, acc 0.96875\n",
      "2017-11-06T15:38:40.572697: step 3068, loss 0.237913, acc 0.875\n",
      "2017-11-06T15:38:44.556527: step 3069, loss 0.144668, acc 0.96875\n",
      "2017-11-06T15:38:48.629421: step 3070, loss 0.104877, acc 0.96875\n",
      "2017-11-06T15:38:53.098597: step 3071, loss 0.194923, acc 0.9375\n",
      "2017-11-06T15:38:57.147475: step 3072, loss 0.245693, acc 0.90625\n",
      "2017-11-06T15:39:01.170332: step 3073, loss 0.131005, acc 0.90625\n",
      "2017-11-06T15:39:05.179181: step 3074, loss 0.233534, acc 0.9375\n",
      "2017-11-06T15:39:09.235062: step 3075, loss 0.233143, acc 0.90625\n",
      "2017-11-06T15:39:13.233903: step 3076, loss 0.144561, acc 0.9375\n",
      "2017-11-06T15:39:17.239751: step 3077, loss 0.121442, acc 0.96875\n",
      "2017-11-06T15:39:21.185554: step 3078, loss 0.0416235, acc 0.96875\n",
      "2017-11-06T15:39:25.195403: step 3079, loss 0.13549, acc 0.9375\n",
      "2017-11-06T15:39:29.197247: step 3080, loss 0.197149, acc 0.9375\n",
      "2017-11-06T15:39:33.170069: step 3081, loss 0.326136, acc 0.875\n",
      "2017-11-06T15:39:37.166909: step 3082, loss 0.173101, acc 0.9375\n",
      "2017-11-06T15:39:41.146738: step 3083, loss 0.141296, acc 0.9375\n",
      "2017-11-06T15:39:45.065522: step 3084, loss 0.0373988, acc 1\n",
      "2017-11-06T15:39:49.126407: step 3085, loss 0.0657679, acc 0.96875\n",
      "2017-11-06T15:39:53.103233: step 3086, loss 0.193391, acc 0.90625\n",
      "2017-11-06T15:39:57.463330: step 3087, loss 0.352241, acc 0.8125\n",
      "2017-11-06T15:40:01.980540: step 3088, loss 0.257586, acc 0.875\n",
      "2017-11-06T15:40:06.044427: step 3089, loss 0.331323, acc 0.84375\n",
      "2017-11-06T15:40:10.053276: step 3090, loss 0.204599, acc 0.9375\n",
      "2017-11-06T15:40:14.005084: step 3091, loss 0.305142, acc 0.8125\n",
      "2017-11-06T15:40:18.025941: step 3092, loss 0.22146, acc 0.9375\n",
      "2017-11-06T15:40:22.030787: step 3093, loss 0.170369, acc 0.875\n",
      "2017-11-06T15:40:26.022623: step 3094, loss 0.275194, acc 0.8125\n",
      "2017-11-06T15:40:30.009456: step 3095, loss 0.21499, acc 0.90625\n",
      "2017-11-06T15:40:32.690362: step 3096, loss 0.138386, acc 0.95\n",
      "2017-11-06T15:40:36.909359: step 3097, loss 0.0716073, acc 0.96875\n",
      "2017-11-06T15:40:40.948229: step 3098, loss 0.177502, acc 0.90625\n",
      "2017-11-06T15:40:44.944880: step 3099, loss 0.12186, acc 0.9375\n",
      "2017-11-06T15:40:48.920827: step 3100, loss 0.0431865, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:40:51.475642: step 3100, loss 1.21323, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-06T15:40:56.788321: step 3101, loss 0.170356, acc 0.90625\n",
      "2017-11-06T15:41:00.934266: step 3102, loss 0.319073, acc 0.84375\n",
      "2017-11-06T15:41:05.270347: step 3103, loss 0.28914, acc 0.875\n",
      "2017-11-06T15:41:09.262184: step 3104, loss 0.148197, acc 0.96875\n",
      "2017-11-06T15:41:14.413844: step 3105, loss 0.088865, acc 0.96875\n",
      "2017-11-06T15:41:18.567795: step 3106, loss 0.223163, acc 0.90625\n",
      "2017-11-06T15:41:22.651716: step 3107, loss 0.132644, acc 0.875\n",
      "2017-11-06T15:41:26.573485: step 3108, loss 0.0316312, acc 1\n",
      "2017-11-06T15:41:30.579331: step 3109, loss 0.344689, acc 0.84375\n",
      "2017-11-06T15:41:34.527135: step 3110, loss 0.116983, acc 0.9375\n",
      "2017-11-06T15:41:38.486950: step 3111, loss 0.165019, acc 0.9375\n",
      "2017-11-06T15:41:42.499802: step 3112, loss 0.162152, acc 0.90625\n",
      "2017-11-06T15:41:46.495639: step 3113, loss 0.114878, acc 0.9375\n",
      "2017-11-06T15:41:50.498483: step 3114, loss 0.475755, acc 0.84375\n",
      "2017-11-06T15:41:54.632421: step 3115, loss 0.157163, acc 0.90625\n",
      "2017-11-06T15:41:58.617252: step 3116, loss 0.195814, acc 0.9375\n",
      "2017-11-06T15:42:02.627101: step 3117, loss 0.0530222, acc 0.96875\n",
      "2017-11-06T15:42:06.752032: step 3118, loss 0.0194738, acc 1\n",
      "2017-11-06T15:42:11.030071: step 3119, loss 0.123876, acc 0.90625\n",
      "2017-11-06T15:42:15.061936: step 3120, loss 0.358356, acc 0.84375\n",
      "2017-11-06T15:42:19.070785: step 3121, loss 0.143895, acc 0.90625\n",
      "2017-11-06T15:42:23.047610: step 3122, loss 0.112149, acc 0.9375\n",
      "2017-11-06T15:42:27.102493: step 3123, loss 0.10613, acc 0.9375\n",
      "2017-11-06T15:42:31.118345: step 3124, loss 0.151539, acc 0.9375\n",
      "2017-11-06T15:42:35.372369: step 3125, loss 0.237205, acc 0.90625\n",
      "2017-11-06T15:42:39.371209: step 3126, loss 0.277028, acc 0.875\n",
      "2017-11-06T15:42:43.345032: step 3127, loss 0.39494, acc 0.84375\n",
      "2017-11-06T15:42:47.352880: step 3128, loss 0.170753, acc 0.875\n",
      "2017-11-06T15:42:51.378956: step 3129, loss 0.188791, acc 0.875\n",
      "2017-11-06T15:42:55.307750: step 3130, loss 0.235333, acc 0.90625\n",
      "2017-11-06T15:42:59.293580: step 3131, loss 0.234731, acc 0.875\n",
      "2017-11-06T15:43:01.785350: step 3132, loss 0.146583, acc 0.95\n",
      "2017-11-06T15:43:05.752169: step 3133, loss 0.140861, acc 0.90625\n",
      "2017-11-06T15:43:09.813055: step 3134, loss 0.233485, acc 0.90625\n",
      "2017-11-06T15:43:14.118115: step 3135, loss 0.0901559, acc 0.9375\n",
      "2017-11-06T15:43:18.335111: step 3136, loss 0.257067, acc 0.875\n",
      "2017-11-06T15:43:22.320942: step 3137, loss 0.084032, acc 0.96875\n",
      "2017-11-06T15:43:26.347803: step 3138, loss 0.189828, acc 0.9375\n",
      "2017-11-06T15:43:30.305615: step 3139, loss 0.107865, acc 0.90625\n",
      "2017-11-06T15:43:34.247416: step 3140, loss 0.276942, acc 0.875\n",
      "2017-11-06T15:43:38.240253: step 3141, loss 0.13164, acc 0.90625\n",
      "2017-11-06T15:43:42.202068: step 3142, loss 0.171747, acc 0.90625\n",
      "2017-11-06T15:43:46.147678: step 3143, loss 0.217751, acc 0.90625\n",
      "2017-11-06T15:43:50.162531: step 3144, loss 0.124999, acc 0.96875\n",
      "2017-11-06T15:43:54.285461: step 3145, loss 0.349702, acc 0.875\n",
      "2017-11-06T15:43:58.260284: step 3146, loss 0.142801, acc 0.96875\n",
      "2017-11-06T15:44:02.276138: step 3147, loss 0.235115, acc 0.84375\n",
      "2017-11-06T15:44:06.238954: step 3148, loss 0.196031, acc 0.90625\n",
      "2017-11-06T15:44:10.197767: step 3149, loss 0.173348, acc 0.9375\n",
      "2017-11-06T15:44:14.199611: step 3150, loss 0.147032, acc 0.90625\n",
      "2017-11-06T15:44:18.404598: step 3151, loss 0.194028, acc 0.875\n",
      "2017-11-06T15:44:22.814732: step 3152, loss 0.139607, acc 0.9375\n",
      "2017-11-06T15:44:27.179835: step 3153, loss 0.113961, acc 0.9375\n",
      "2017-11-06T15:44:31.129641: step 3154, loss 0.257575, acc 0.8125\n",
      "2017-11-06T15:44:35.373655: step 3155, loss 0.202173, acc 0.875\n",
      "2017-11-06T15:44:39.356487: step 3156, loss 0.164001, acc 0.96875\n",
      "2017-11-06T15:44:43.346321: step 3157, loss 0.0517233, acc 1\n",
      "2017-11-06T15:44:47.399201: step 3158, loss 0.211544, acc 0.875\n",
      "2017-11-06T15:44:51.356198: step 3159, loss 0.0748676, acc 0.96875\n",
      "2017-11-06T15:44:55.346033: step 3160, loss 0.180556, acc 0.90625\n",
      "2017-11-06T15:44:59.310850: step 3161, loss 0.243236, acc 0.84375\n",
      "2017-11-06T15:45:03.396754: step 3162, loss 0.239453, acc 0.875\n",
      "2017-11-06T15:45:07.401599: step 3163, loss 0.0463513, acc 1\n",
      "2017-11-06T15:45:11.377425: step 3164, loss 0.345646, acc 0.8125\n",
      "2017-11-06T15:45:15.459324: step 3165, loss 0.185526, acc 0.90625\n",
      "2017-11-06T15:45:19.380112: step 3166, loss 0.172596, acc 0.90625\n",
      "2017-11-06T15:45:23.539065: step 3167, loss 0.122754, acc 0.9375\n",
      "2017-11-06T15:45:26.432121: step 3168, loss 0.176749, acc 0.95\n",
      "2017-11-06T15:45:30.545043: step 3169, loss 0.0440534, acc 0.96875\n",
      "2017-11-06T15:45:34.572905: step 3170, loss 0.0906438, acc 0.9375\n",
      "2017-11-06T15:45:38.596765: step 3171, loss 0.0915802, acc 0.96875\n",
      "2017-11-06T15:45:42.605614: step 3172, loss 0.0975146, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T15:45:46.661495: step 3173, loss 0.26462, acc 0.90625\n",
      "2017-11-06T15:45:50.727384: step 3174, loss 0.0704092, acc 0.9375\n",
      "2017-11-06T15:45:54.813287: step 3175, loss 0.214501, acc 0.875\n",
      "2017-11-06T15:45:58.799121: step 3176, loss 0.253005, acc 0.90625\n",
      "2017-11-06T15:46:02.821977: step 3177, loss 0.274044, acc 0.875\n",
      "2017-11-06T15:46:06.813815: step 3178, loss 0.246116, acc 0.90625\n",
      "2017-11-06T15:46:10.832670: step 3179, loss 0.142821, acc 0.90625\n",
      "2017-11-06T15:46:14.842518: step 3180, loss 0.0959444, acc 0.96875\n",
      "2017-11-06T15:46:18.927422: step 3181, loss 0.200811, acc 0.9375\n",
      "2017-11-06T15:46:22.855211: step 3182, loss 0.170911, acc 0.96875\n",
      "2017-11-06T15:46:26.845048: step 3183, loss 0.250817, acc 0.84375\n",
      "2017-11-06T15:46:31.175123: step 3184, loss 0.256538, acc 0.90625\n",
      "2017-11-06T15:46:35.624284: step 3185, loss 0.122432, acc 0.96875\n",
      "2017-11-06T15:46:39.722196: step 3186, loss 0.117852, acc 0.90625\n",
      "2017-11-06T15:46:43.725040: step 3187, loss 0.185613, acc 0.9375\n",
      "2017-11-06T15:46:47.710635: step 3188, loss 0.0416774, acc 1\n",
      "2017-11-06T15:46:51.752525: step 3189, loss 0.104268, acc 0.96875\n",
      "2017-11-06T15:46:55.744342: step 3190, loss 0.0623285, acc 1\n",
      "2017-11-06T15:46:59.718165: step 3191, loss 0.196577, acc 0.875\n",
      "2017-11-06T15:47:03.729015: step 3192, loss 0.125217, acc 0.90625\n",
      "2017-11-06T15:47:07.756877: step 3193, loss 0.236791, acc 0.9375\n",
      "2017-11-06T15:47:11.762724: step 3194, loss 0.0874988, acc 0.9375\n",
      "2017-11-06T15:47:15.789585: step 3195, loss 0.226277, acc 0.875\n",
      "2017-11-06T15:47:19.742394: step 3196, loss 0.211049, acc 0.90625\n",
      "2017-11-06T15:47:23.801278: step 3197, loss 0.114914, acc 0.9375\n",
      "2017-11-06T15:47:27.764094: step 3198, loss 0.1596, acc 0.9375\n",
      "2017-11-06T15:47:31.751927: step 3199, loss 0.10774, acc 0.9375\n",
      "2017-11-06T15:47:35.882862: step 3200, loss 0.140935, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:47:38.791929: step 3200, loss 1.18759, acc 0.666667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-06T15:47:44.161240: step 3201, loss 0.18341, acc 0.90625\n",
      "2017-11-06T15:47:48.215120: step 3202, loss 0.0572564, acc 1\n",
      "2017-11-06T15:47:52.179938: step 3203, loss 0.118654, acc 0.96875\n",
      "2017-11-06T15:47:54.812809: step 3204, loss 0.196056, acc 0.9\n",
      "2017-11-06T15:47:58.807647: step 3205, loss 0.22856, acc 0.90625\n",
      "2017-11-06T15:48:02.811492: step 3206, loss 0.224234, acc 0.90625\n",
      "2017-11-06T15:48:06.842356: step 3207, loss 0.154715, acc 0.9375\n",
      "2017-11-06T15:48:10.848203: step 3208, loss 0.146858, acc 0.90625\n",
      "2017-11-06T15:48:14.825029: step 3209, loss 0.190014, acc 0.96875\n",
      "2017-11-06T15:48:18.865899: step 3210, loss 0.160896, acc 0.9375\n",
      "2017-11-06T15:48:23.022853: step 3211, loss 0.297872, acc 0.90625\n",
      "2017-11-06T15:48:27.012688: step 3212, loss 0.384741, acc 0.875\n",
      "2017-11-06T15:48:30.952489: step 3213, loss 0.206266, acc 0.875\n",
      "2017-11-06T15:48:35.114445: step 3214, loss 0.236949, acc 0.9375\n",
      "2017-11-06T15:48:39.120290: step 3215, loss 0.0619711, acc 0.96875\n",
      "2017-11-06T15:48:43.490397: step 3216, loss 0.204381, acc 0.875\n",
      "2017-11-06T15:48:47.472225: step 3217, loss 0.0207492, acc 1\n",
      "2017-11-06T15:48:51.399016: step 3218, loss 0.165027, acc 0.9375\n",
      "2017-11-06T15:48:55.513106: step 3219, loss 0.138097, acc 0.90625\n",
      "2017-11-06T15:48:59.507945: step 3220, loss 0.170255, acc 0.90625\n",
      "2017-11-06T15:49:03.485771: step 3221, loss 0.119962, acc 0.90625\n",
      "2017-11-06T15:49:07.404556: step 3222, loss 0.0605186, acc 0.96875\n",
      "2017-11-06T15:49:11.360367: step 3223, loss 0.173276, acc 0.90625\n",
      "2017-11-06T15:49:15.333189: step 3224, loss 0.419978, acc 0.90625\n",
      "2017-11-06T15:49:19.318021: step 3225, loss 0.142459, acc 0.9375\n",
      "2017-11-06T15:49:23.289843: step 3226, loss 0.347169, acc 0.84375\n",
      "2017-11-06T15:49:27.334717: step 3227, loss 0.310648, acc 0.84375\n",
      "2017-11-06T15:49:31.295531: step 3228, loss 0.111618, acc 0.9375\n",
      "2017-11-06T15:49:35.224323: step 3229, loss 0.272257, acc 0.84375\n",
      "2017-11-06T15:49:39.151113: step 3230, loss 0.141762, acc 0.9375\n",
      "2017-11-06T15:49:43.085910: step 3231, loss 0.170805, acc 0.90625\n",
      "2017-11-06T15:49:47.327743: step 3232, loss 0.0663758, acc 1\n",
      "2017-11-06T15:49:51.445669: step 3233, loss 0.177642, acc 0.90625\n",
      "2017-11-06T15:49:55.433503: step 3234, loss 0.230043, acc 0.875\n",
      "2017-11-06T15:49:59.331272: step 3235, loss 0.0756783, acc 0.96875\n",
      "2017-11-06T15:50:03.577289: step 3236, loss 0.219513, acc 0.84375\n",
      "2017-11-06T15:50:07.529098: step 3237, loss 0.528361, acc 0.8125\n",
      "2017-11-06T15:50:11.463893: step 3238, loss 0.18508, acc 0.9375\n",
      "2017-11-06T15:50:15.361662: step 3239, loss 0.288851, acc 0.84375\n",
      "2017-11-06T15:50:17.934490: step 3240, loss 0.406999, acc 0.9\n",
      "2017-11-06T15:50:21.894306: step 3241, loss 0.0971104, acc 0.9375\n",
      "2017-11-06T15:50:25.851117: step 3242, loss 0.136604, acc 0.90625\n",
      "2017-11-06T15:50:29.783910: step 3243, loss 0.0457182, acc 1\n",
      "2017-11-06T15:50:33.947868: step 3244, loss 0.131187, acc 0.96875\n",
      "2017-11-06T15:50:37.926695: step 3245, loss 0.322101, acc 0.90625\n",
      "2017-11-06T15:50:41.873500: step 3246, loss 0.224074, acc 0.90625\n",
      "2017-11-06T15:50:45.791283: step 3247, loss 0.063981, acc 0.96875\n",
      "2017-11-06T15:50:49.695059: step 3248, loss 0.0353013, acc 0.96875\n",
      "2017-11-06T15:50:54.038143: step 3249, loss 0.253219, acc 0.90625\n",
      "2017-11-06T15:50:58.124051: step 3250, loss 0.172196, acc 0.90625\n",
      "2017-11-06T15:51:02.052840: step 3251, loss 0.211221, acc 0.90625\n",
      "2017-11-06T15:51:05.920586: step 3252, loss 0.238768, acc 0.90625\n",
      "2017-11-06T15:51:09.860387: step 3253, loss 0.148914, acc 0.9375\n",
      "2017-11-06T15:51:13.775169: step 3254, loss 0.0881785, acc 0.96875\n",
      "2017-11-06T15:51:17.725976: step 3255, loss 0.161698, acc 0.90625\n",
      "2017-11-06T15:51:21.662773: step 3256, loss 0.111115, acc 0.90625\n",
      "2017-11-06T15:51:25.617582: step 3257, loss 0.0853089, acc 0.9375\n",
      "2017-11-06T15:51:29.571391: step 3258, loss 0.11462, acc 0.9375\n",
      "2017-11-06T15:51:33.475165: step 3259, loss 0.29446, acc 0.90625\n",
      "2017-11-06T15:51:37.406959: step 3260, loss 0.254704, acc 0.875\n",
      "2017-11-06T15:51:41.335752: step 3261, loss 0.0796015, acc 0.96875\n",
      "2017-11-06T15:51:45.236523: step 3262, loss 0.48134, acc 0.8125\n",
      "2017-11-06T15:51:49.213348: step 3263, loss 0.214522, acc 0.9375\n",
      "2017-11-06T15:51:53.110116: step 3264, loss 0.165547, acc 0.875\n",
      "2017-11-06T15:51:57.324110: step 3265, loss 0.300058, acc 0.875\n",
      "2017-11-06T15:52:01.777275: step 3266, loss 0.257882, acc 0.9375\n",
      "2017-11-06T15:52:05.814144: step 3267, loss 0.160103, acc 0.96875\n",
      "2017-11-06T15:52:09.772958: step 3268, loss 0.100955, acc 0.9375\n",
      "2017-11-06T15:52:13.721762: step 3269, loss 0.172284, acc 0.90625\n",
      "2017-11-06T15:52:17.674571: step 3270, loss 0.164405, acc 0.875\n",
      "2017-11-06T15:52:21.620376: step 3271, loss 0.233361, acc 0.875\n",
      "2017-11-06T15:52:25.594198: step 3272, loss 0.0379036, acc 1\n",
      "2017-11-06T15:52:29.567021: step 3273, loss 0.247246, acc 0.90625\n",
      "2017-11-06T15:52:33.656926: step 3274, loss 0.11732, acc 0.9375\n",
      "2017-11-06T15:52:37.674781: step 3275, loss 0.327033, acc 0.90625\n",
      "2017-11-06T15:52:40.205581: step 3276, loss 0.16906, acc 0.95\n",
      "2017-11-06T15:52:44.133371: step 3277, loss 0.174654, acc 0.875\n",
      "2017-11-06T15:52:48.042058: step 3278, loss 0.0715115, acc 0.96875\n",
      "2017-11-06T15:52:51.944830: step 3279, loss 0.117232, acc 0.9375\n",
      "2017-11-06T15:52:55.908647: step 3280, loss 0.493418, acc 0.84375\n",
      "2017-11-06T15:52:59.995660: step 3281, loss 0.287605, acc 0.90625\n",
      "2017-11-06T15:53:04.201649: step 3282, loss 0.136033, acc 0.9375\n",
      "2017-11-06T15:53:08.510711: step 3283, loss 0.14475, acc 0.875\n",
      "2017-11-06T15:53:12.494541: step 3284, loss 0.209792, acc 0.90625\n",
      "2017-11-06T15:53:16.458360: step 3285, loss 0.100965, acc 0.9375\n",
      "2017-11-06T15:53:20.396157: step 3286, loss 0.0572465, acc 0.96875\n",
      "2017-11-06T15:53:24.337957: step 3287, loss 0.166954, acc 0.90625\n",
      "2017-11-06T15:53:28.302773: step 3288, loss 0.0724553, acc 0.96875\n",
      "2017-11-06T15:53:32.243574: step 3289, loss 0.220141, acc 0.90625\n",
      "2017-11-06T15:53:36.192381: step 3290, loss 0.0861421, acc 0.96875\n",
      "2017-11-06T15:53:40.100157: step 3291, loss 0.218655, acc 0.875\n",
      "2017-11-06T15:53:44.092993: step 3292, loss 0.0943718, acc 0.9375\n",
      "2017-11-06T15:53:47.990763: step 3293, loss 0.0791951, acc 0.96875\n",
      "2017-11-06T15:53:51.924558: step 3294, loss 0.389964, acc 0.71875\n",
      "2017-11-06T15:53:55.892379: step 3295, loss 0.328755, acc 0.8125\n",
      "2017-11-06T15:53:59.781140: step 3296, loss 0.202348, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T15:54:03.815006: step 3297, loss 0.279053, acc 0.90625\n",
      "2017-11-06T15:54:07.844872: step 3298, loss 0.223071, acc 0.875\n",
      "2017-11-06T15:54:12.158936: step 3299, loss 0.140037, acc 0.90625\n",
      "2017-11-06T15:54:16.106742: step 3300, loss 0.0852252, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T15:54:18.633535: step 3300, loss 1.19177, acc 0.666667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-06T15:54:24.020658: step 3301, loss 0.116343, acc 0.9375\n",
      "2017-11-06T15:54:28.131582: step 3302, loss 0.209288, acc 0.9375\n",
      "2017-11-06T15:54:32.104403: step 3303, loss 0.256992, acc 0.875\n",
      "2017-11-06T15:54:36.253350: step 3304, loss 0.285673, acc 0.875\n",
      "2017-11-06T15:54:40.162128: step 3305, loss 0.24778, acc 0.90625\n",
      "2017-11-06T15:54:44.098925: step 3306, loss 0.0984704, acc 0.9375\n",
      "2017-11-06T15:54:48.011706: step 3307, loss 0.210495, acc 0.9375\n",
      "2017-11-06T15:54:51.952506: step 3308, loss 0.164212, acc 0.9375\n",
      "2017-11-06T15:54:55.920324: step 3309, loss 0.149042, acc 0.90625\n",
      "2017-11-06T15:54:59.875275: step 3310, loss 0.258913, acc 0.84375\n",
      "2017-11-06T15:55:03.817076: step 3311, loss 0.0433557, acc 0.96875\n",
      "2017-11-06T15:55:06.332864: step 3312, loss 0.234002, acc 0.85\n",
      "2017-11-06T15:55:10.239641: step 3313, loss 0.0803651, acc 0.9375\n",
      "2017-11-06T15:55:14.377579: step 3314, loss 0.13871, acc 0.9375\n",
      "2017-11-06T15:55:18.578564: step 3315, loss 0.177886, acc 0.90625\n",
      "2017-11-06T15:55:22.487342: step 3316, loss 0.148482, acc 0.9375\n",
      "2017-11-06T15:55:26.461165: step 3317, loss 0.0976453, acc 0.9375\n",
      "2017-11-06T15:55:30.407971: step 3318, loss 0.207785, acc 0.90625\n",
      "2017-11-06T15:55:34.394802: step 3319, loss 0.153787, acc 0.90625\n",
      "2017-11-06T15:55:38.342609: step 3320, loss 0.0946382, acc 0.96875\n",
      "2017-11-06T15:55:42.261392: step 3321, loss 0.0406204, acc 1\n",
      "2017-11-06T15:55:46.190971: step 3322, loss 0.224055, acc 0.90625\n",
      "2017-11-06T15:55:50.144781: step 3323, loss 0.0571773, acc 1\n",
      "2017-11-06T15:55:54.112601: step 3324, loss 0.252649, acc 0.9375\n",
      "2017-11-06T15:55:58.082421: step 3325, loss 0.204606, acc 0.9375\n",
      "2017-11-06T15:56:02.015217: step 3326, loss 0.24685, acc 0.875\n",
      "2017-11-06T15:56:05.994043: step 3327, loss 0.313348, acc 0.90625\n",
      "2017-11-06T15:56:09.894813: step 3328, loss 0.191462, acc 0.875\n",
      "2017-11-06T15:56:13.831611: step 3329, loss 0.190477, acc 0.90625\n",
      "2017-11-06T15:56:17.813441: step 3330, loss 0.219112, acc 0.9375\n",
      "2017-11-06T15:56:22.169536: step 3331, loss 0.104964, acc 0.96875\n",
      "2017-11-06T15:56:26.219413: step 3332, loss 0.248444, acc 0.84375\n",
      "2017-11-06T15:56:30.171222: step 3333, loss 0.0905208, acc 0.96875\n",
      "2017-11-06T15:56:34.361199: step 3334, loss 0.163618, acc 0.9375\n",
      "2017-11-06T15:56:38.302999: step 3335, loss 0.302495, acc 0.84375\n",
      "2017-11-06T15:56:42.398911: step 3336, loss 0.130191, acc 0.9375\n",
      "2017-11-06T15:56:46.296679: step 3337, loss 0.254066, acc 0.90625\n",
      "2017-11-06T15:56:50.227474: step 3338, loss 0.190991, acc 0.90625\n",
      "2017-11-06T15:56:54.179280: step 3339, loss 0.114709, acc 0.90625\n",
      "2017-11-06T15:56:58.130087: step 3340, loss 0.16763, acc 0.90625\n",
      "2017-11-06T15:57:02.112917: step 3341, loss 0.187641, acc 0.9375\n",
      "2017-11-06T15:57:06.037706: step 3342, loss 0.201163, acc 0.90625\n",
      "2017-11-06T15:57:09.982510: step 3343, loss 0.235195, acc 0.9375\n",
      "2017-11-06T15:57:13.951329: step 3344, loss 0.222533, acc 0.9375\n",
      "2017-11-06T15:57:17.893132: step 3345, loss 0.213028, acc 0.90625\n",
      "2017-11-06T15:57:21.843937: step 3346, loss 0.188866, acc 0.90625\n",
      "2017-11-06T15:57:26.039918: step 3347, loss 0.0670155, acc 0.96875\n",
      "2017-11-06T15:57:28.798879: step 3348, loss 0.286883, acc 0.85\n",
      "2017-11-06T15:57:32.795719: step 3349, loss 0.193766, acc 0.9375\n",
      "2017-11-06T15:57:36.726513: step 3350, loss 0.159102, acc 0.90625\n",
      "2017-11-06T15:57:40.695332: step 3351, loss 0.0865442, acc 0.96875\n",
      "2017-11-06T15:57:44.596123: step 3352, loss 0.0768406, acc 0.96875\n",
      "2017-11-06T15:57:48.490871: step 3353, loss 0.209515, acc 0.90625\n",
      "2017-11-06T15:57:52.467696: step 3354, loss 0.162497, acc 0.9375\n",
      "2017-11-06T15:57:56.429512: step 3355, loss 0.149982, acc 0.9375\n",
      "2017-11-06T15:58:00.380338: step 3356, loss 0.208172, acc 0.90625\n",
      "2017-11-06T15:58:04.312113: step 3357, loss 0.0614189, acc 0.96875\n",
      "2017-11-06T15:58:08.234900: step 3358, loss 0.171238, acc 0.9375\n",
      "2017-11-06T15:58:12.199718: step 3359, loss 0.0824743, acc 0.96875\n",
      "2017-11-06T15:58:16.142518: step 3360, loss 0.134295, acc 0.90625\n",
      "2017-11-06T15:58:20.112339: step 3361, loss 0.075553, acc 0.96875\n",
      "2017-11-06T15:58:24.250279: step 3362, loss 0.575654, acc 0.78125\n",
      "2017-11-06T15:58:28.364203: step 3363, loss 0.216146, acc 0.9375\n",
      "2017-11-06T15:58:32.612221: step 3364, loss 0.0331092, acc 1\n",
      "2017-11-06T15:58:37.009346: step 3365, loss 0.0867445, acc 0.96875\n",
      "2017-11-06T15:58:40.950145: step 3366, loss 0.187116, acc 0.9375\n",
      "2017-11-06T15:58:44.858924: step 3367, loss 0.263024, acc 0.84375\n",
      "2017-11-06T15:58:48.813493: step 3368, loss 0.232871, acc 0.90625\n",
      "2017-11-06T15:58:52.809332: step 3369, loss 0.181181, acc 0.875\n",
      "2017-11-06T15:58:56.747131: step 3370, loss 0.129309, acc 0.9375\n",
      "2017-11-06T15:59:00.679925: step 3371, loss 0.0230229, acc 1\n",
      "2017-11-06T15:59:04.667825: step 3372, loss 0.382472, acc 0.8125\n",
      "2017-11-06T15:59:08.641651: step 3373, loss 0.215751, acc 0.90625\n",
      "2017-11-06T15:59:12.635487: step 3374, loss 0.213415, acc 0.90625\n",
      "2017-11-06T15:59:16.674356: step 3375, loss 0.257128, acc 0.90625\n",
      "2017-11-06T15:59:20.645177: step 3376, loss 0.119655, acc 0.9375\n",
      "2017-11-06T15:59:24.651025: step 3377, loss 0.153292, acc 0.9375\n",
      "2017-11-06T15:59:28.592824: step 3378, loss 0.358417, acc 0.8125\n",
      "2017-11-06T15:59:32.618686: step 3379, loss 0.191029, acc 0.9375\n",
      "2017-11-06T15:59:36.808663: step 3380, loss 0.120677, acc 0.96875\n",
      "2017-11-06T15:59:41.124729: step 3381, loss 0.205136, acc 0.84375\n",
      "2017-11-06T15:59:45.128574: step 3382, loss 0.0813164, acc 0.9375\n",
      "2017-11-06T15:59:49.123412: step 3383, loss 0.261223, acc 0.875\n",
      "2017-11-06T15:59:51.625190: step 3384, loss 0.25041, acc 0.9\n",
      "2017-11-06T15:59:55.531966: step 3385, loss 0.101549, acc 0.9375\n",
      "2017-11-06T15:59:59.495782: step 3386, loss 0.0750806, acc 0.96875\n",
      "2017-11-06T16:00:03.753808: step 3387, loss 0.196226, acc 0.84375\n",
      "2017-11-06T16:00:07.665587: step 3388, loss 0.117053, acc 0.9375\n",
      "2017-11-06T16:00:11.642413: step 3389, loss 0.191472, acc 0.875\n",
      "2017-11-06T16:00:15.619240: step 3390, loss 0.183112, acc 0.9375\n",
      "2017-11-06T16:00:19.569046: step 3391, loss 0.144455, acc 0.9375\n",
      "2017-11-06T16:00:23.545871: step 3392, loss 0.116009, acc 0.96875\n",
      "2017-11-06T16:00:27.473662: step 3393, loss 0.105299, acc 0.96875\n",
      "2017-11-06T16:00:31.413463: step 3394, loss 0.126935, acc 0.96875\n",
      "2017-11-06T16:00:35.674489: step 3395, loss 0.328888, acc 0.90625\n",
      "2017-11-06T16:00:39.667326: step 3396, loss 0.0998695, acc 0.96875\n",
      "2017-11-06T16:00:43.882321: step 3397, loss 0.15245, acc 0.90625\n",
      "2017-11-06T16:00:48.053284: step 3398, loss 0.175487, acc 0.9375\n",
      "2017-11-06T16:00:51.993084: step 3399, loss 0.353977, acc 0.90625\n",
      "2017-11-06T16:00:55.929882: step 3400, loss 0.261124, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:00:58.472690: step 3400, loss 1.17004, acc 0.683333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-06T16:01:03.707715: step 3401, loss 0.177326, acc 0.9375\n",
      "2017-11-06T16:01:07.724568: step 3402, loss 0.0908723, acc 0.96875\n",
      "2017-11-06T16:01:11.714404: step 3403, loss 0.367633, acc 0.875\n",
      "2017-11-06T16:01:15.701237: step 3404, loss 0.203861, acc 0.90625\n",
      "2017-11-06T16:01:19.646040: step 3405, loss 0.261049, acc 0.875\n",
      "2017-11-06T16:01:23.580837: step 3406, loss 0.19732, acc 0.84375\n",
      "2017-11-06T16:01:27.542650: step 3407, loss 0.204615, acc 0.90625\n",
      "2017-11-06T16:01:31.561506: step 3408, loss 0.13492, acc 0.90625\n",
      "2017-11-06T16:01:35.710453: step 3409, loss 0.11468, acc 0.9375\n",
      "2017-11-06T16:01:39.842389: step 3410, loss 0.345124, acc 0.8125\n",
      "2017-11-06T16:01:43.823218: step 3411, loss 0.0267121, acc 1\n",
      "2017-11-06T16:01:47.946928: step 3412, loss 0.0699444, acc 0.9375\n",
      "2017-11-06T16:01:52.250986: step 3413, loss 0.0441708, acc 1\n",
      "2017-11-06T16:01:56.199792: step 3414, loss 0.108115, acc 0.9375\n",
      "2017-11-06T16:02:00.185624: step 3415, loss 0.355656, acc 0.90625\n",
      "2017-11-06T16:02:04.153443: step 3416, loss 0.504382, acc 0.875\n",
      "2017-11-06T16:02:08.121263: step 3417, loss 0.0559246, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:02:12.074071: step 3418, loss 0.0764982, acc 0.9375\n",
      "2017-11-06T16:02:16.030884: step 3419, loss 0.034066, acc 1\n",
      "2017-11-06T16:02:18.503640: step 3420, loss 0.33877, acc 0.85\n",
      "2017-11-06T16:02:22.479466: step 3421, loss 0.143604, acc 0.9375\n",
      "2017-11-06T16:02:26.518336: step 3422, loss 0.0572605, acc 0.96875\n",
      "2017-11-06T16:02:30.470142: step 3423, loss 0.155153, acc 0.9375\n",
      "2017-11-06T16:02:34.661120: step 3424, loss 0.0618058, acc 0.96875\n",
      "2017-11-06T16:02:38.673972: step 3425, loss 0.322388, acc 0.875\n",
      "2017-11-06T16:02:42.694829: step 3426, loss 0.037636, acc 1\n",
      "2017-11-06T16:02:46.720689: step 3427, loss 0.0933953, acc 0.96875\n",
      "2017-11-06T16:02:50.712525: step 3428, loss 0.175762, acc 0.90625\n",
      "2017-11-06T16:02:55.096641: step 3429, loss 0.253435, acc 0.8125\n",
      "2017-11-06T16:02:59.211565: step 3430, loss 0.209176, acc 0.875\n",
      "2017-11-06T16:03:03.178385: step 3431, loss 0.0946404, acc 0.9375\n",
      "2017-11-06T16:03:07.132192: step 3432, loss 0.0996813, acc 0.96875\n",
      "2017-11-06T16:03:11.201083: step 3433, loss 0.111619, acc 0.9375\n",
      "2017-11-06T16:03:15.163899: step 3434, loss 0.0302434, acc 0.96875\n",
      "2017-11-06T16:03:19.108702: step 3435, loss 0.0744495, acc 0.96875\n",
      "2017-11-06T16:03:23.182597: step 3436, loss 0.0814958, acc 0.9375\n",
      "2017-11-06T16:03:27.461637: step 3437, loss 0.0930887, acc 0.9375\n",
      "2017-11-06T16:03:31.426454: step 3438, loss 0.179886, acc 0.90625\n",
      "2017-11-06T16:03:35.409284: step 3439, loss 0.25381, acc 0.90625\n",
      "2017-11-06T16:03:39.538218: step 3440, loss 0.363392, acc 0.84375\n",
      "2017-11-06T16:03:43.613114: step 3441, loss 0.127651, acc 0.9375\n",
      "2017-11-06T16:03:47.677001: step 3442, loss 0.0605454, acc 0.96875\n",
      "2017-11-06T16:03:51.816943: step 3443, loss 0.394416, acc 0.84375\n",
      "2017-11-06T16:03:55.858815: step 3444, loss 0.107064, acc 0.96875\n",
      "2017-11-06T16:04:00.317984: step 3445, loss 0.0649495, acc 0.96875\n",
      "2017-11-06T16:04:04.636051: step 3446, loss 0.345458, acc 0.84375\n",
      "2017-11-06T16:04:08.860053: step 3447, loss 0.358481, acc 0.84375\n",
      "2017-11-06T16:04:12.875906: step 3448, loss 0.251346, acc 0.84375\n",
      "2017-11-06T16:04:17.082895: step 3449, loss 0.154577, acc 0.9375\n",
      "2017-11-06T16:04:21.136776: step 3450, loss 0.236131, acc 0.84375\n",
      "2017-11-06T16:04:25.291728: step 3451, loss 0.110844, acc 0.9375\n",
      "2017-11-06T16:04:29.484707: step 3452, loss 0.238067, acc 0.875\n",
      "2017-11-06T16:04:33.763748: step 3453, loss 0.183037, acc 0.875\n",
      "2017-11-06T16:04:37.906691: step 3454, loss 0.177487, acc 0.9375\n",
      "2017-11-06T16:04:42.105675: step 3455, loss 0.218131, acc 0.90625\n",
      "2017-11-06T16:04:44.758560: step 3456, loss 0.105571, acc 0.9\n",
      "2017-11-06T16:04:48.945431: step 3457, loss 0.011375, acc 1\n",
      "2017-11-06T16:04:53.034336: step 3458, loss 0.299714, acc 0.90625\n",
      "2017-11-06T16:04:57.089217: step 3459, loss 0.120844, acc 0.9375\n",
      "2017-11-06T16:05:01.101068: step 3460, loss 0.0752556, acc 0.96875\n",
      "2017-11-06T16:05:05.294047: step 3461, loss 0.183072, acc 0.90625\n",
      "2017-11-06T16:05:09.640135: step 3462, loss 0.159086, acc 0.96875\n",
      "2017-11-06T16:05:13.683008: step 3463, loss 0.175112, acc 0.9375\n",
      "2017-11-06T16:05:17.672843: step 3464, loss 0.0813415, acc 0.96875\n",
      "2017-11-06T16:05:21.625671: step 3465, loss 0.359677, acc 0.875\n",
      "2017-11-06T16:05:25.651512: step 3466, loss 0.0481847, acc 0.96875\n",
      "2017-11-06T16:05:29.605322: step 3467, loss 0.143489, acc 0.9375\n",
      "2017-11-06T16:05:33.620174: step 3468, loss 0.395446, acc 0.875\n",
      "2017-11-06T16:05:37.615012: step 3469, loss 0.13137, acc 0.90625\n",
      "2017-11-06T16:05:41.603847: step 3470, loss 0.183893, acc 0.90625\n",
      "2017-11-06T16:05:45.609693: step 3471, loss 0.165902, acc 0.9375\n",
      "2017-11-06T16:05:49.588522: step 3472, loss 0.357209, acc 0.8125\n",
      "2017-11-06T16:05:53.601371: step 3473, loss 0.0681098, acc 0.96875\n",
      "2017-11-06T16:05:57.531166: step 3474, loss 0.0907608, acc 0.9375\n",
      "2017-11-06T16:06:01.478969: step 3475, loss 0.130506, acc 0.96875\n",
      "2017-11-06T16:06:05.480813: step 3476, loss 0.0093481, acc 1\n",
      "2017-11-06T16:06:09.520683: step 3477, loss 0.10133, acc 0.90625\n",
      "2017-11-06T16:06:13.927815: step 3478, loss 0.348669, acc 0.875\n",
      "2017-11-06T16:06:17.928657: step 3479, loss 0.0783476, acc 0.96875\n",
      "2017-11-06T16:06:21.923495: step 3480, loss 0.247005, acc 0.90625\n",
      "2017-11-06T16:06:25.973374: step 3481, loss 0.216983, acc 0.9375\n",
      "2017-11-06T16:06:29.941193: step 3482, loss 0.156565, acc 0.9375\n",
      "2017-11-06T16:06:34.114158: step 3483, loss 0.339195, acc 0.875\n",
      "2017-11-06T16:06:38.140018: step 3484, loss 0.414318, acc 0.84375\n",
      "2017-11-06T16:06:42.209911: step 3485, loss 0.2201, acc 0.875\n",
      "2017-11-06T16:06:46.257787: step 3486, loss 0.0575261, acc 0.96875\n",
      "2017-11-06T16:06:50.277642: step 3487, loss 0.0991672, acc 0.96875\n",
      "2017-11-06T16:06:54.256469: step 3488, loss 0.226586, acc 0.90625\n",
      "2017-11-06T16:06:58.211279: step 3489, loss 0.17448, acc 0.9375\n",
      "2017-11-06T16:07:02.206118: step 3490, loss 0.218288, acc 0.90625\n",
      "2017-11-06T16:07:06.220971: step 3491, loss 0.138426, acc 0.90625\n",
      "2017-11-06T16:07:08.738760: step 3492, loss 0.106384, acc 0.95\n",
      "2017-11-06T16:07:12.701576: step 3493, loss 0.119042, acc 0.9375\n",
      "2017-11-06T16:07:17.002674: step 3494, loss 0.129434, acc 0.875\n",
      "2017-11-06T16:07:21.150621: step 3495, loss 0.12351, acc 0.9375\n",
      "2017-11-06T16:07:25.161471: step 3496, loss 0.111927, acc 0.9375\n",
      "2017-11-06T16:07:29.151306: step 3497, loss 0.273029, acc 0.90625\n",
      "2017-11-06T16:07:33.164158: step 3498, loss 0.162581, acc 0.90625\n",
      "2017-11-06T16:07:37.117968: step 3499, loss 0.18924, acc 0.875\n",
      "2017-11-06T16:07:41.137823: step 3500, loss 0.0717445, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:07:43.654611: step 3500, loss 1.2456, acc 0.683333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-06T16:07:49.028462: step 3501, loss 0.194518, acc 0.90625\n",
      "2017-11-06T16:07:52.981271: step 3502, loss 0.117726, acc 0.9375\n",
      "2017-11-06T16:07:56.999125: step 3503, loss 0.0766234, acc 0.96875\n",
      "2017-11-06T16:08:00.954936: step 3504, loss 0.135736, acc 0.9375\n",
      "2017-11-06T16:08:04.922755: step 3505, loss 0.0308815, acc 1\n",
      "2017-11-06T16:08:08.875564: step 3506, loss 0.102192, acc 0.9375\n",
      "2017-11-06T16:08:12.813362: step 3507, loss 0.0733659, acc 0.9375\n",
      "2017-11-06T16:08:16.836220: step 3508, loss 0.165727, acc 0.9375\n",
      "2017-11-06T16:08:20.861080: step 3509, loss 0.174634, acc 0.9375\n",
      "2017-11-06T16:08:25.255202: step 3510, loss 0.279436, acc 0.90625\n",
      "2017-11-06T16:08:29.225023: step 3511, loss 0.151034, acc 0.90625\n",
      "2017-11-06T16:08:33.378975: step 3512, loss 0.163102, acc 0.9375\n",
      "2017-11-06T16:08:37.441861: step 3513, loss 0.112669, acc 0.9375\n",
      "2017-11-06T16:08:41.429695: step 3514, loss 0.0903032, acc 0.96875\n",
      "2017-11-06T16:08:45.433540: step 3515, loss 0.34084, acc 0.875\n",
      "2017-11-06T16:08:49.505433: step 3516, loss 0.108531, acc 0.96875\n",
      "2017-11-06T16:08:53.499271: step 3517, loss 0.186693, acc 0.90625\n",
      "2017-11-06T16:08:57.497113: step 3518, loss 0.0467309, acc 0.96875\n",
      "2017-11-06T16:09:01.476939: step 3519, loss 0.232953, acc 0.90625\n",
      "2017-11-06T16:09:05.489791: step 3520, loss 0.290357, acc 0.90625\n",
      "2017-11-06T16:09:09.497638: step 3521, loss 0.131252, acc 0.9375\n",
      "2017-11-06T16:09:13.523499: step 3522, loss 0.148876, acc 0.9375\n",
      "2017-11-06T16:09:17.642331: step 3523, loss 0.139507, acc 0.9375\n",
      "2017-11-06T16:09:21.665189: step 3524, loss 0.153789, acc 0.9375\n",
      "2017-11-06T16:09:25.935223: step 3525, loss 0.335074, acc 0.875\n",
      "2017-11-06T16:09:30.532490: step 3526, loss 0.241484, acc 0.90625\n",
      "2017-11-06T16:09:34.590373: step 3527, loss 0.230016, acc 0.875\n",
      "2017-11-06T16:09:37.127176: step 3528, loss 0.399547, acc 0.9\n",
      "2017-11-06T16:09:41.179055: step 3529, loss 0.190819, acc 0.9375\n",
      "2017-11-06T16:09:45.235937: step 3530, loss 0.114942, acc 0.9375\n",
      "2017-11-06T16:09:49.403899: step 3531, loss 0.161104, acc 0.90625\n",
      "2017-11-06T16:09:53.464784: step 3532, loss 0.0625414, acc 0.96875\n",
      "2017-11-06T16:09:57.504654: step 3533, loss 0.219476, acc 0.875\n",
      "2017-11-06T16:10:01.808732: step 3534, loss 0.342121, acc 0.84375\n",
      "2017-11-06T16:10:05.850585: step 3535, loss 0.0549781, acc 0.96875\n",
      "2017-11-06T16:10:09.849426: step 3536, loss 0.0584252, acc 0.96875\n",
      "2017-11-06T16:10:13.890297: step 3537, loss 0.277899, acc 0.875\n",
      "2017-11-06T16:10:17.887137: step 3538, loss 0.128844, acc 0.9375\n",
      "2017-11-06T16:10:21.875972: step 3539, loss 0.258211, acc 0.84375\n",
      "2017-11-06T16:10:25.935856: step 3540, loss 0.12201, acc 0.875\n",
      "2017-11-06T16:10:29.894669: step 3541, loss 0.210541, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:10:34.458912: step 3542, loss 0.319237, acc 0.8125\n",
      "2017-11-06T16:10:38.588846: step 3543, loss 0.115954, acc 0.96875\n",
      "2017-11-06T16:10:42.617709: step 3544, loss 0.14263, acc 0.90625\n",
      "2017-11-06T16:10:46.566515: step 3545, loss 0.0730819, acc 0.96875\n",
      "2017-11-06T16:10:50.591134: step 3546, loss 0.212804, acc 0.90625\n",
      "2017-11-06T16:10:54.587972: step 3547, loss 0.070542, acc 0.96875\n",
      "2017-11-06T16:10:58.572803: step 3548, loss 0.176219, acc 0.875\n",
      "2017-11-06T16:11:02.619679: step 3549, loss 0.131069, acc 0.90625\n",
      "2017-11-06T16:11:06.610515: step 3550, loss 0.163691, acc 0.96875\n",
      "2017-11-06T16:11:10.606353: step 3551, loss 0.228181, acc 0.875\n",
      "2017-11-06T16:11:14.581178: step 3552, loss 0.28053, acc 0.84375\n",
      "2017-11-06T16:11:18.602084: step 3553, loss 0.128001, acc 0.96875\n",
      "2017-11-06T16:11:22.581910: step 3554, loss 0.0799096, acc 0.9375\n",
      "2017-11-06T16:11:26.572746: step 3555, loss 0.0366966, acc 1\n",
      "2017-11-06T16:11:30.565583: step 3556, loss 0.194004, acc 0.9375\n",
      "2017-11-06T16:11:34.544411: step 3557, loss 0.328419, acc 0.84375\n",
      "2017-11-06T16:11:38.744397: step 3558, loss 0.17845, acc 0.90625\n",
      "2017-11-06T16:11:43.009425: step 3559, loss 0.154635, acc 0.9375\n",
      "2017-11-06T16:11:46.975243: step 3560, loss 0.112003, acc 0.9375\n",
      "2017-11-06T16:11:50.995099: step 3561, loss 0.167336, acc 0.9375\n",
      "2017-11-06T16:11:55.185095: step 3562, loss 0.249706, acc 0.875\n",
      "2017-11-06T16:11:59.148893: step 3563, loss 0.264776, acc 0.875\n",
      "2017-11-06T16:12:01.683694: step 3564, loss 0.0978329, acc 0.9\n",
      "2017-11-06T16:12:05.695544: step 3565, loss 0.130274, acc 0.9375\n",
      "2017-11-06T16:12:09.653358: step 3566, loss 0.112724, acc 0.96875\n",
      "2017-11-06T16:12:13.644192: step 3567, loss 0.0766098, acc 0.9375\n",
      "2017-11-06T16:12:17.606007: step 3568, loss 0.132347, acc 0.96875\n",
      "2017-11-06T16:12:21.575828: step 3569, loss 0.249795, acc 0.875\n",
      "2017-11-06T16:12:25.627708: step 3570, loss 0.0382177, acc 0.96875\n",
      "2017-11-06T16:12:29.642561: step 3571, loss 0.0939012, acc 0.9375\n",
      "2017-11-06T16:12:33.895581: step 3572, loss 0.0657292, acc 0.9375\n",
      "2017-11-06T16:12:37.949462: step 3573, loss 0.1527, acc 0.9375\n",
      "2017-11-06T16:12:41.960312: step 3574, loss 0.145249, acc 0.90625\n",
      "2017-11-06T16:12:46.316407: step 3575, loss 0.287939, acc 0.90625\n",
      "2017-11-06T16:12:50.384299: step 3576, loss 0.40708, acc 0.75\n",
      "2017-11-06T16:12:54.389143: step 3577, loss 0.110825, acc 0.96875\n",
      "2017-11-06T16:12:58.412002: step 3578, loss 0.184325, acc 0.90625\n",
      "2017-11-06T16:13:02.446869: step 3579, loss 0.0864081, acc 0.96875\n",
      "2017-11-06T16:13:06.463722: step 3580, loss 0.179108, acc 0.90625\n",
      "2017-11-06T16:13:10.516605: step 3581, loss 0.227728, acc 0.90625\n",
      "2017-11-06T16:13:14.463408: step 3582, loss 0.151008, acc 0.9375\n",
      "2017-11-06T16:13:18.508282: step 3583, loss 0.227238, acc 0.9375\n",
      "2017-11-06T16:13:22.707265: step 3584, loss 0.190735, acc 0.90625\n",
      "2017-11-06T16:13:26.709108: step 3585, loss 0.206944, acc 0.90625\n",
      "2017-11-06T16:13:30.722960: step 3586, loss 0.116977, acc 0.9375\n",
      "2017-11-06T16:13:34.705790: step 3587, loss 0.0923567, acc 0.9375\n",
      "2017-11-06T16:13:38.674610: step 3588, loss 0.0233373, acc 1\n",
      "2017-11-06T16:13:42.662443: step 3589, loss 0.245112, acc 0.875\n",
      "2017-11-06T16:13:46.664287: step 3590, loss 0.08778, acc 0.96875\n",
      "2017-11-06T16:13:50.967109: step 3591, loss 0.0261995, acc 1\n",
      "2017-11-06T16:13:55.219130: step 3592, loss 0.329993, acc 0.84375\n",
      "2017-11-06T16:13:59.192955: step 3593, loss 0.202654, acc 0.90625\n",
      "2017-11-06T16:14:03.164776: step 3594, loss 0.263112, acc 0.875\n",
      "2017-11-06T16:14:07.104577: step 3595, loss 0.0790524, acc 0.96875\n",
      "2017-11-06T16:14:11.097413: step 3596, loss 0.205328, acc 0.90625\n",
      "2017-11-06T16:14:15.105260: step 3597, loss 0.107371, acc 0.9375\n",
      "2017-11-06T16:14:19.088090: step 3598, loss 0.253374, acc 0.8125\n",
      "2017-11-06T16:14:23.065918: step 3599, loss 0.467407, acc 0.8125\n",
      "2017-11-06T16:14:25.598718: step 3600, loss 0.0123802, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:14:28.168542: step 3600, loss 1.25816, acc 0.7\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\7\\1509945086\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C9928E10>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\n",
      "\n",
      "2017-11-06T16:14:38.453933: step 1, loss 3.38856, acc 0.40625\n",
      "2017-11-06T16:14:42.358707: step 2, loss 2.41946, acc 0.59375\n",
      "2017-11-06T16:14:46.335534: step 3, loss 2.31278, acc 0.6875\n",
      "2017-11-06T16:14:50.254317: step 4, loss 1.67919, acc 0.75\n",
      "2017-11-06T16:14:54.295190: step 5, loss 0.0664478, acc 0.96875\n",
      "2017-11-06T16:14:58.745351: step 6, loss 0.337425, acc 0.9375\n",
      "2017-11-06T16:15:02.671140: step 7, loss 0.70957, acc 0.9375\n",
      "2017-11-06T16:15:06.551897: step 8, loss 1.54057, acc 0.875\n",
      "2017-11-06T16:15:10.456673: step 9, loss 1.17664, acc 0.90625\n",
      "2017-11-06T16:15:14.430495: step 10, loss 4.01856, acc 0.8125\n",
      "2017-11-06T16:15:18.343276: step 11, loss 0.547148, acc 0.96875\n",
      "2017-11-06T16:15:22.281075: step 12, loss 2.309, acc 0.8125\n",
      "2017-11-06T16:15:26.214869: step 13, loss 0.705395, acc 0.9375\n",
      "2017-11-06T16:15:30.184690: step 14, loss 2.23573, acc 0.8125\n",
      "2017-11-06T16:15:34.089464: step 15, loss 1.81367, acc 0.90625\n",
      "2017-11-06T16:15:38.023259: step 16, loss 2.52047, acc 0.84375\n",
      "2017-11-06T16:15:41.939041: step 17, loss 1.33795, acc 0.90625\n",
      "2017-11-06T16:15:45.860827: step 18, loss 3.32422, acc 0.75\n",
      "2017-11-06T16:15:49.794623: step 19, loss 0.794497, acc 0.96875\n",
      "2017-11-06T16:15:53.750434: step 20, loss 2.60421, acc 0.84375\n",
      "2017-11-06T16:15:57.705244: step 21, loss 0.955778, acc 0.90625\n",
      "2017-11-06T16:16:01.919238: step 22, loss 1.63697, acc 0.84375\n",
      "2017-11-06T16:16:06.075191: step 23, loss 0.505028, acc 0.90625\n",
      "2017-11-06T16:16:09.970959: step 24, loss 1.14444, acc 0.84375\n",
      "2017-11-06T16:16:13.940781: step 25, loss 0.871732, acc 0.71875\n",
      "2017-11-06T16:16:17.884582: step 26, loss 1.81086, acc 0.78125\n",
      "2017-11-06T16:16:21.814374: step 27, loss 0.467398, acc 0.90625\n",
      "2017-11-06T16:16:25.748169: step 28, loss 1.00313, acc 0.78125\n",
      "2017-11-06T16:16:29.683966: step 29, loss 0.933537, acc 0.84375\n",
      "2017-11-06T16:16:33.789883: step 30, loss 1.68578, acc 0.75\n",
      "2017-11-06T16:16:37.877788: step 31, loss 2.60005, acc 0.625\n",
      "2017-11-06T16:16:41.864621: step 32, loss 1.03424, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:16:45.907494: step 33, loss 0.629359, acc 0.84375\n",
      "2017-11-06T16:16:49.934130: step 34, loss 1.93245, acc 0.78125\n",
      "2017-11-06T16:16:53.888939: step 35, loss 1.78011, acc 0.78125\n",
      "2017-11-06T16:16:56.401725: step 36, loss 1.42882, acc 0.75\n",
      "2017-11-06T16:17:00.420580: step 37, loss 1.09693, acc 0.75\n",
      "2017-11-06T16:17:04.396407: step 38, loss 0.574688, acc 0.84375\n",
      "2017-11-06T16:17:08.739491: step 39, loss 0.981553, acc 0.875\n",
      "2017-11-06T16:17:12.776360: step 40, loss 1.27798, acc 0.8125\n",
      "2017-11-06T16:17:16.713158: step 41, loss 2.25528, acc 0.75\n",
      "2017-11-06T16:17:20.694987: step 42, loss 1.16697, acc 0.8125\n",
      "2017-11-06T16:17:24.698831: step 43, loss 0.16458, acc 0.9375\n",
      "2017-11-06T16:17:28.667651: step 44, loss 0.821421, acc 0.875\n",
      "2017-11-06T16:17:32.659487: step 45, loss 0.794451, acc 0.875\n",
      "2017-11-06T16:17:36.615299: step 46, loss 0.172326, acc 0.90625\n",
      "2017-11-06T16:17:40.629151: step 47, loss 1.07517, acc 0.84375\n",
      "2017-11-06T16:17:44.601973: step 48, loss 0.881367, acc 0.875\n",
      "2017-11-06T16:17:48.593809: step 49, loss 1.70875, acc 0.90625\n",
      "2017-11-06T16:17:52.535610: step 50, loss 2.42045, acc 0.84375\n",
      "2017-11-06T16:17:56.552464: step 51, loss 0.411349, acc 0.9375\n",
      "2017-11-06T16:18:00.585330: step 52, loss 0.592447, acc 0.875\n",
      "2017-11-06T16:18:04.543142: step 53, loss 1.36801, acc 0.9375\n",
      "2017-11-06T16:18:08.459925: step 54, loss 0.996696, acc 0.875\n",
      "2017-11-06T16:18:12.631889: step 55, loss 0.49784, acc 0.8125\n",
      "2017-11-06T16:18:16.933946: step 56, loss 0.0791506, acc 0.96875\n",
      "2017-11-06T16:18:20.889757: step 57, loss 1.32474, acc 0.84375\n",
      "2017-11-06T16:18:24.975662: step 58, loss 0.115151, acc 0.875\n",
      "2017-11-06T16:18:29.060562: step 59, loss 0.0748328, acc 0.96875\n",
      "2017-11-06T16:18:33.107438: step 60, loss 0.754392, acc 0.8125\n",
      "2017-11-06T16:18:37.199345: step 61, loss 0.905785, acc 0.8125\n",
      "2017-11-06T16:18:41.201189: step 62, loss 1.21712, acc 0.78125\n",
      "2017-11-06T16:18:45.143990: step 63, loss 0.841091, acc 0.875\n",
      "2017-11-06T16:18:49.083790: step 64, loss 1.3736, acc 0.78125\n",
      "2017-11-06T16:18:52.989565: step 65, loss 0.629731, acc 0.75\n",
      "2017-11-06T16:18:56.966391: step 66, loss 0.577767, acc 0.875\n",
      "2017-11-06T16:19:00.940215: step 67, loss 0.971078, acc 0.875\n",
      "2017-11-06T16:19:04.879015: step 68, loss 0.568506, acc 0.78125\n",
      "2017-11-06T16:19:08.923888: step 69, loss 1.08925, acc 0.84375\n",
      "2017-11-06T16:19:12.832664: step 70, loss 0.492357, acc 0.84375\n",
      "2017-11-06T16:19:16.850519: step 71, loss 0.788189, acc 0.8125\n",
      "2017-11-06T16:19:19.718557: step 72, loss 1.78907, acc 0.7\n",
      "2017-11-06T16:19:23.872508: step 73, loss 1.17025, acc 0.78125\n",
      "2017-11-06T16:19:27.795296: step 74, loss 1.59411, acc 0.78125\n",
      "2017-11-06T16:19:31.739099: step 75, loss 1.1672, acc 0.8125\n",
      "2017-11-06T16:19:35.695910: step 76, loss 0.726551, acc 0.875\n",
      "2017-11-06T16:19:39.652721: step 77, loss 0.428625, acc 0.90625\n",
      "2017-11-06T16:19:43.572506: step 78, loss 1.00636, acc 0.84375\n",
      "2017-11-06T16:19:47.638395: step 79, loss 0.783496, acc 0.875\n",
      "2017-11-06T16:19:51.602021: step 80, loss 0.151969, acc 0.9375\n",
      "2017-11-06T16:19:55.631885: step 81, loss 0.594196, acc 0.84375\n",
      "2017-11-06T16:19:59.647737: step 82, loss 0.641197, acc 0.875\n",
      "2017-11-06T16:20:03.951796: step 83, loss 0.434038, acc 0.78125\n",
      "2017-11-06T16:20:07.989665: step 84, loss 1.03552, acc 0.84375\n",
      "2017-11-06T16:20:11.907449: step 85, loss 0.841484, acc 0.84375\n",
      "2017-11-06T16:20:15.844245: step 86, loss 0.525253, acc 0.875\n",
      "2017-11-06T16:20:19.831080: step 87, loss 1.74424, acc 0.78125\n",
      "2017-11-06T16:20:24.068089: step 88, loss 0.355593, acc 0.90625\n",
      "2017-11-06T16:20:28.313105: step 89, loss 1.28278, acc 0.84375\n",
      "2017-11-06T16:20:32.224886: step 90, loss 0.664074, acc 0.90625\n",
      "2017-11-06T16:20:36.428872: step 91, loss 0.293148, acc 0.875\n",
      "2017-11-06T16:20:40.437722: step 92, loss 0.783664, acc 0.9375\n",
      "2017-11-06T16:20:44.385527: step 93, loss 1.31399, acc 0.75\n",
      "2017-11-06T16:20:48.345339: step 94, loss 0.814691, acc 0.875\n",
      "2017-11-06T16:20:52.302151: step 95, loss 0.292045, acc 0.875\n",
      "2017-11-06T16:20:56.268969: step 96, loss 0.656675, acc 0.84375\n",
      "2017-11-06T16:21:00.235788: step 97, loss 0.195006, acc 0.9375\n",
      "2017-11-06T16:21:04.171584: step 98, loss 1.09766, acc 0.84375\n",
      "2017-11-06T16:21:08.124393: step 99, loss 0.254347, acc 0.9375\n",
      "2017-11-06T16:21:12.069197: step 100, loss 0.708703, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:21:14.676048: step 100, loss 1.38728, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-100\n",
      "\n",
      "2017-11-06T16:21:20.051533: step 101, loss 0.208521, acc 0.90625\n",
      "2017-11-06T16:21:24.013347: step 102, loss 0.392342, acc 0.90625\n",
      "2017-11-06T16:21:28.014190: step 103, loss 1.09749, acc 0.8125\n",
      "2017-11-06T16:21:32.385298: step 104, loss 0.810609, acc 0.84375\n",
      "2017-11-06T16:21:36.368127: step 105, loss 0.807803, acc 0.875\n",
      "2017-11-06T16:21:40.326938: step 106, loss 0.601969, acc 0.875\n",
      "2017-11-06T16:21:44.277747: step 107, loss 0.393591, acc 0.90625\n",
      "2017-11-06T16:21:46.772518: step 108, loss 1.9283, acc 0.65\n",
      "2017-11-06T16:21:50.775363: step 109, loss 0.186423, acc 0.9375\n",
      "2017-11-06T16:21:54.705154: step 110, loss 1.07101, acc 0.8125\n",
      "2017-11-06T16:21:58.625940: step 111, loss 0.685988, acc 0.84375\n",
      "2017-11-06T16:22:02.597762: step 112, loss 0.993576, acc 0.875\n",
      "2017-11-06T16:22:06.596605: step 113, loss 0.177785, acc 0.9375\n",
      "2017-11-06T16:22:10.534402: step 114, loss 1.17466, acc 0.8125\n",
      "2017-11-06T16:22:14.515230: step 115, loss 1.13155, acc 0.8125\n",
      "2017-11-06T16:22:18.463055: step 116, loss 1.14804, acc 0.71875\n",
      "2017-11-06T16:22:22.455875: step 117, loss 0.883899, acc 0.84375\n",
      "2017-11-06T16:22:26.388667: step 118, loss 1.47083, acc 0.71875\n",
      "2017-11-06T16:22:30.318460: step 119, loss 0.910613, acc 0.78125\n",
      "2017-11-06T16:22:34.756613: step 120, loss 0.459293, acc 0.875\n",
      "2017-11-06T16:22:39.021643: step 121, loss 0.514671, acc 0.875\n",
      "2017-11-06T16:22:42.952437: step 122, loss 0.815857, acc 0.75\n",
      "2017-11-06T16:22:46.901242: step 123, loss 0.380114, acc 0.90625\n",
      "2017-11-06T16:22:50.817797: step 124, loss 0.792319, acc 0.875\n",
      "2017-11-06T16:22:54.776609: step 125, loss 0.959169, acc 0.84375\n",
      "2017-11-06T16:22:58.737424: step 126, loss 0.482648, acc 0.9375\n",
      "2017-11-06T16:23:02.699239: step 127, loss 0.650026, acc 0.90625\n",
      "2017-11-06T16:23:06.698080: step 128, loss 1.15784, acc 0.78125\n",
      "2017-11-06T16:23:10.726943: step 129, loss 0.413615, acc 0.9375\n",
      "2017-11-06T16:23:14.639723: step 130, loss 0.413434, acc 0.96875\n",
      "2017-11-06T16:23:18.622553: step 131, loss 0.681103, acc 0.875\n",
      "2017-11-06T16:23:22.561353: step 132, loss 0.668131, acc 0.9375\n",
      "2017-11-06T16:23:26.503152: step 133, loss 0.0617631, acc 0.96875\n",
      "2017-11-06T16:23:30.496990: step 134, loss 1.56724, acc 0.875\n",
      "2017-11-06T16:23:34.458806: step 135, loss 0.401474, acc 0.9375\n",
      "2017-11-06T16:23:38.437633: step 136, loss 0.631936, acc 0.84375\n",
      "2017-11-06T16:23:42.752698: step 137, loss 0.14597, acc 0.9375\n",
      "2017-11-06T16:23:46.823591: step 138, loss 0.321282, acc 0.875\n",
      "2017-11-06T16:23:50.803419: step 139, loss 0.630258, acc 0.90625\n",
      "2017-11-06T16:23:54.736215: step 140, loss 0.794191, acc 0.84375\n",
      "2017-11-06T16:23:58.697030: step 141, loss 0.946065, acc 0.875\n",
      "2017-11-06T16:24:02.679857: step 142, loss 0.305747, acc 0.84375\n",
      "2017-11-06T16:24:06.672697: step 143, loss 0.530676, acc 0.8125\n",
      "2017-11-06T16:24:09.266537: step 144, loss 1.38347, acc 0.85\n",
      "2017-11-06T16:24:13.280389: step 145, loss 0.595149, acc 0.9375\n",
      "2017-11-06T16:24:17.323263: step 146, loss 0.617334, acc 0.8125\n",
      "2017-11-06T16:24:21.319101: step 147, loss 0.216178, acc 0.875\n",
      "2017-11-06T16:24:25.501073: step 148, loss 0.439119, acc 0.90625\n",
      "2017-11-06T16:24:29.487905: step 149, loss 0.635747, acc 0.78125\n",
      "2017-11-06T16:24:33.635853: step 150, loss 1.40689, acc 0.75\n",
      "2017-11-06T16:24:37.814823: step 151, loss 0.29393, acc 0.9375\n",
      "2017-11-06T16:24:41.788647: step 152, loss 1.25737, acc 0.84375\n",
      "2017-11-06T16:24:45.949602: step 153, loss 0.522652, acc 0.8125\n",
      "2017-11-06T16:24:50.293689: step 154, loss 0.515529, acc 0.875\n",
      "2017-11-06T16:24:54.286526: step 155, loss 0.225923, acc 0.875\n",
      "2017-11-06T16:24:58.225325: step 156, loss 0.77658, acc 0.84375\n",
      "2017-11-06T16:25:02.241178: step 157, loss 0.207646, acc 0.9375\n",
      "2017-11-06T16:25:06.200992: step 158, loss 0.795747, acc 0.8125\n",
      "2017-11-06T16:25:10.249870: step 159, loss 0.683026, acc 0.8125\n",
      "2017-11-06T16:25:14.207681: step 160, loss 1.05933, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:25:18.183506: step 161, loss 0.908442, acc 0.75\n",
      "2017-11-06T16:25:22.137315: step 162, loss 0.451383, acc 0.90625\n",
      "2017-11-06T16:25:26.163176: step 163, loss 0.112399, acc 0.96875\n",
      "2017-11-06T16:25:30.121989: step 164, loss 0.422026, acc 0.875\n",
      "2017-11-06T16:25:34.046777: step 165, loss 0.206509, acc 0.9375\n",
      "2017-11-06T16:25:38.017599: step 166, loss 0.310486, acc 0.90625\n",
      "2017-11-06T16:25:42.053466: step 167, loss 0.551264, acc 0.84375\n",
      "2017-11-06T16:25:46.052308: step 168, loss 0.839252, acc 0.875\n",
      "2017-11-06T16:25:50.141999: step 169, loss 0.806208, acc 0.84375\n",
      "2017-11-06T16:25:54.486087: step 170, loss 0.391773, acc 0.84375\n",
      "2017-11-06T16:25:58.619021: step 171, loss 0.40227, acc 0.90625\n",
      "2017-11-06T16:26:02.684911: step 172, loss 0.777204, acc 0.8125\n",
      "2017-11-06T16:26:06.692759: step 173, loss 0.47261, acc 0.84375\n",
      "2017-11-06T16:26:10.620549: step 174, loss 0.52935, acc 0.875\n",
      "2017-11-06T16:26:14.557346: step 175, loss 0.312254, acc 0.9375\n",
      "2017-11-06T16:26:18.652256: step 176, loss 1.13376, acc 0.78125\n",
      "2017-11-06T16:26:22.633085: step 177, loss 0.337341, acc 0.84375\n",
      "2017-11-06T16:26:26.670954: step 178, loss 0.5576, acc 0.9375\n",
      "2017-11-06T16:26:30.640774: step 179, loss 0.461075, acc 0.875\n",
      "2017-11-06T16:26:33.321680: step 180, loss 0.472604, acc 0.85\n",
      "2017-11-06T16:26:37.484639: step 181, loss 0.497621, acc 0.875\n",
      "2017-11-06T16:26:41.452458: step 182, loss 0.483501, acc 0.9375\n",
      "2017-11-06T16:26:45.509339: step 183, loss 0.460778, acc 0.90625\n",
      "2017-11-06T16:26:49.481161: step 184, loss 0.282099, acc 0.9375\n",
      "2017-11-06T16:26:53.651897: step 185, loss 0.10903, acc 0.96875\n",
      "2017-11-06T16:26:57.840873: step 186, loss 0.323203, acc 0.96875\n",
      "2017-11-06T16:27:02.003831: step 187, loss 0.812782, acc 0.84375\n",
      "2017-11-06T16:27:06.019684: step 188, loss 0.477855, acc 0.84375\n",
      "2017-11-06T16:27:09.963486: step 189, loss 0.589891, acc 0.84375\n",
      "2017-11-06T16:27:13.956324: step 190, loss 0.830671, acc 0.875\n",
      "2017-11-06T16:27:17.975179: step 191, loss 0.168912, acc 0.9375\n",
      "2017-11-06T16:27:21.958010: step 192, loss 0.852023, acc 0.78125\n",
      "2017-11-06T16:27:25.936838: step 193, loss 1.07208, acc 0.71875\n",
      "2017-11-06T16:27:29.888644: step 194, loss 0.637125, acc 0.90625\n",
      "2017-11-06T16:27:33.823440: step 195, loss 0.196491, acc 0.90625\n",
      "2017-11-06T16:27:37.833289: step 196, loss 0.267861, acc 0.96875\n",
      "2017-11-06T16:27:41.836133: step 197, loss 0.750897, acc 0.8125\n",
      "2017-11-06T16:27:45.762924: step 198, loss 0.602326, acc 0.875\n",
      "2017-11-06T16:27:49.776775: step 199, loss 0.327381, acc 0.9375\n",
      "2017-11-06T16:27:53.723579: step 200, loss 0.590649, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:27:56.304414: step 200, loss 1.68617, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-200\n",
      "\n",
      "2017-11-06T16:28:02.309757: step 201, loss 0.023253, acc 1\n",
      "2017-11-06T16:28:06.674860: step 202, loss 0.462487, acc 0.90625\n",
      "2017-11-06T16:28:10.704723: step 203, loss 0.421533, acc 0.875\n",
      "2017-11-06T16:28:14.761605: step 204, loss 0.840223, acc 0.84375\n",
      "2017-11-06T16:28:18.837501: step 205, loss 1.31074, acc 0.84375\n",
      "2017-11-06T16:28:22.821332: step 206, loss 0.634534, acc 0.875\n",
      "2017-11-06T16:28:27.027320: step 207, loss 1.09339, acc 0.8125\n",
      "2017-11-06T16:28:31.008150: step 208, loss 0.604697, acc 0.8125\n",
      "2017-11-06T16:28:35.194123: step 209, loss 0.706464, acc 0.875\n",
      "2017-11-06T16:28:39.226989: step 210, loss 0.487603, acc 0.90625\n",
      "2017-11-06T16:28:43.182800: step 211, loss 0.526994, acc 0.875\n",
      "2017-11-06T16:28:47.095579: step 212, loss 0.80745, acc 0.8125\n",
      "2017-11-06T16:28:51.102209: step 213, loss 0.281011, acc 0.90625\n",
      "2017-11-06T16:28:55.422253: step 214, loss 0.173289, acc 0.96875\n",
      "2017-11-06T16:28:59.385067: step 215, loss 0.477324, acc 0.9375\n",
      "2017-11-06T16:29:01.981912: step 216, loss 0.456871, acc 0.9\n",
      "2017-11-06T16:29:06.013778: step 217, loss 0.542754, acc 0.84375\n",
      "2017-11-06T16:29:10.353860: step 218, loss 0.963405, acc 0.84375\n",
      "2017-11-06T16:29:14.401737: step 219, loss 0.165136, acc 0.96875\n",
      "2017-11-06T16:29:18.403581: step 220, loss 0.502407, acc 0.90625\n",
      "2017-11-06T16:29:22.397418: step 221, loss 0.856159, acc 0.84375\n",
      "2017-11-06T16:29:26.389254: step 222, loss 0.0769091, acc 0.96875\n",
      "2017-11-06T16:29:30.397101: step 223, loss 0.499539, acc 0.90625\n",
      "2017-11-06T16:29:34.314886: step 224, loss 0.588705, acc 0.84375\n",
      "2017-11-06T16:29:38.369769: step 225, loss 1.11767, acc 0.75\n",
      "2017-11-06T16:29:42.453669: step 226, loss 0.133072, acc 0.96875\n",
      "2017-11-06T16:29:46.548578: step 227, loss 0.958235, acc 0.8125\n",
      "2017-11-06T16:29:50.525405: step 228, loss 0.264676, acc 0.875\n",
      "2017-11-06T16:29:54.532251: step 229, loss 1.03208, acc 0.75\n",
      "2017-11-06T16:29:58.538097: step 230, loss 0.382714, acc 0.875\n",
      "2017-11-06T16:30:02.842155: step 231, loss 0.273646, acc 0.96875\n",
      "2017-11-06T16:30:06.915049: step 232, loss 0.55913, acc 0.8125\n",
      "2017-11-06T16:30:10.923898: step 233, loss 1.01379, acc 0.75\n",
      "2017-11-06T16:30:15.246970: step 234, loss 0.57884, acc 0.875\n",
      "2017-11-06T16:30:19.540020: step 235, loss 0.79885, acc 0.84375\n",
      "2017-11-06T16:30:23.671955: step 236, loss 0.594034, acc 0.90625\n",
      "2017-11-06T16:30:27.741847: step 237, loss 0.504449, acc 0.90625\n",
      "2017-11-06T16:30:31.742691: step 238, loss 0.207204, acc 0.90625\n",
      "2017-11-06T16:30:36.020730: step 239, loss 0.953255, acc 0.875\n",
      "2017-11-06T16:30:40.093644: step 240, loss 0.409735, acc 0.9375\n",
      "2017-11-06T16:30:44.022415: step 241, loss 0.238846, acc 0.90625\n",
      "2017-11-06T16:30:48.031265: step 242, loss 0.32241, acc 0.90625\n",
      "2017-11-06T16:30:51.962057: step 243, loss 0.558907, acc 0.90625\n",
      "2017-11-06T16:30:55.953895: step 244, loss 0.433914, acc 0.90625\n",
      "2017-11-06T16:30:59.945730: step 245, loss 0.732783, acc 0.84375\n",
      "2017-11-06T16:31:03.985600: step 246, loss 0.253441, acc 0.9375\n",
      "2017-11-06T16:31:08.001454: step 247, loss 0.797315, acc 0.875\n",
      "2017-11-06T16:31:12.222453: step 248, loss 0.387783, acc 0.875\n",
      "2017-11-06T16:31:16.530369: step 249, loss 0.303079, acc 0.875\n",
      "2017-11-06T16:31:21.246719: step 250, loss 0.191526, acc 0.96875\n",
      "2017-11-06T16:31:25.588304: step 251, loss 0.771522, acc 0.875\n",
      "2017-11-06T16:31:28.161641: step 252, loss 0.00549373, acc 1\n",
      "2017-11-06T16:31:32.208516: step 253, loss 0.666308, acc 0.90625\n",
      "2017-11-06T16:31:36.464042: step 254, loss 0.769469, acc 0.90625\n",
      "2017-11-06T16:31:40.565957: step 255, loss 0.900293, acc 0.8125\n",
      "2017-11-06T16:31:44.637850: step 256, loss 0.605485, acc 0.84375\n",
      "2017-11-06T16:31:48.656706: step 257, loss 0.25783, acc 0.96875\n",
      "2017-11-06T16:31:52.671356: step 258, loss 0.533975, acc 0.90625\n",
      "2017-11-06T16:31:56.630170: step 259, loss 0.527463, acc 0.84375\n",
      "2017-11-06T16:32:00.613000: step 260, loss 0.406349, acc 0.90625\n",
      "2017-11-06T16:32:04.672884: step 261, loss 0.287704, acc 0.875\n",
      "2017-11-06T16:32:08.649709: step 262, loss 0.412159, acc 0.90625\n",
      "2017-11-06T16:32:12.730610: step 263, loss 0.973348, acc 0.84375\n",
      "2017-11-06T16:32:16.736459: step 264, loss 0.287304, acc 0.84375\n",
      "2017-11-06T16:32:20.673253: step 265, loss 0.488188, acc 0.90625\n",
      "2017-11-06T16:32:24.897273: step 266, loss 0.43073, acc 0.9375\n",
      "2017-11-06T16:32:29.156280: step 267, loss 0.0812671, acc 0.96875\n",
      "2017-11-06T16:32:33.243184: step 268, loss 0.47202, acc 0.90625\n",
      "2017-11-06T16:32:37.343097: step 269, loss 0.096676, acc 0.96875\n",
      "2017-11-06T16:32:41.336936: step 270, loss 0.788746, acc 0.90625\n",
      "2017-11-06T16:32:45.348806: step 271, loss 0.834236, acc 0.9375\n",
      "2017-11-06T16:32:49.290587: step 272, loss 0.353099, acc 0.96875\n",
      "2017-11-06T16:32:53.322452: step 273, loss 0.361367, acc 0.84375\n",
      "2017-11-06T16:32:57.333301: step 274, loss 0.2913, acc 0.96875\n",
      "2017-11-06T16:33:01.296117: step 275, loss 0.357078, acc 0.90625\n",
      "2017-11-06T16:33:05.339990: step 276, loss 0.288239, acc 0.9375\n",
      "2017-11-06T16:33:09.350840: step 277, loss 0.239802, acc 0.9375\n",
      "2017-11-06T16:33:13.332669: step 278, loss 0.723644, acc 0.875\n",
      "2017-11-06T16:33:17.281475: step 279, loss 1.38576, acc 0.8125\n",
      "2017-11-06T16:33:21.289323: step 280, loss 1.2731, acc 0.78125\n",
      "2017-11-06T16:33:25.521330: step 281, loss 0.758427, acc 0.84375\n",
      "2017-11-06T16:33:29.765345: step 282, loss 0.151765, acc 0.9375\n",
      "2017-11-06T16:33:34.115437: step 283, loss 0.437871, acc 0.875\n",
      "2017-11-06T16:33:38.123285: step 284, loss 0.0732661, acc 0.96875\n",
      "2017-11-06T16:33:42.324270: step 285, loss 0.989147, acc 0.84375\n",
      "2017-11-06T16:33:46.564282: step 286, loss 0.082046, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:33:50.583137: step 287, loss 0.16115, acc 0.96875\n",
      "2017-11-06T16:33:53.304072: step 288, loss 1.6564, acc 0.8\n",
      "2017-11-06T16:33:57.493048: step 289, loss 0.371765, acc 0.9375\n",
      "2017-11-06T16:34:01.547928: step 290, loss 0.265374, acc 0.90625\n",
      "2017-11-06T16:34:05.793946: step 291, loss 0.519498, acc 0.90625\n",
      "2017-11-06T16:34:09.884852: step 292, loss 0.764251, acc 0.84375\n",
      "2017-11-06T16:34:14.122863: step 293, loss 0.259889, acc 0.96875\n",
      "2017-11-06T16:34:18.142719: step 294, loss 0.824657, acc 0.875\n",
      "2017-11-06T16:34:22.279659: step 295, loss 0.425863, acc 0.875\n",
      "2017-11-06T16:34:26.601730: step 296, loss 0.139577, acc 0.9375\n",
      "2017-11-06T16:34:30.729663: step 297, loss 1.10074, acc 0.78125\n",
      "2017-11-06T16:34:35.120783: step 298, loss 0.172848, acc 0.90625\n",
      "2017-11-06T16:34:39.777092: step 299, loss 1.07793, acc 0.8125\n",
      "2017-11-06T16:34:43.969070: step 300, loss 0.488641, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:34:46.554908: step 300, loss 1.75446, acc 0.8\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-300\n",
      "\n",
      "2017-11-06T16:34:51.899575: step 301, loss 0.939662, acc 0.8125\n",
      "2017-11-06T16:34:55.997487: step 302, loss 0.100523, acc 0.9375\n",
      "2017-11-06T16:35:00.129422: step 303, loss 0.554239, acc 0.9375\n",
      "2017-11-06T16:35:04.129265: step 304, loss 0.920569, acc 0.875\n",
      "2017-11-06T16:35:08.205162: step 305, loss 0.500676, acc 0.90625\n",
      "2017-11-06T16:35:12.245032: step 306, loss 0.0845446, acc 0.96875\n",
      "2017-11-06T16:35:16.260885: step 307, loss 0.382112, acc 0.90625\n",
      "2017-11-06T16:35:20.266731: step 308, loss 0.512237, acc 0.9375\n",
      "2017-11-06T16:35:24.287588: step 309, loss 0.578973, acc 0.9375\n",
      "2017-11-06T16:35:28.438538: step 310, loss 0.998017, acc 0.875\n",
      "2017-11-06T16:35:32.456392: step 311, loss 0.954843, acc 0.84375\n",
      "2017-11-06T16:35:36.465241: step 312, loss 0.973871, acc 0.875\n",
      "2017-11-06T16:35:40.536133: step 313, loss 0.324506, acc 0.875\n",
      "2017-11-06T16:35:45.008311: step 314, loss 0.767179, acc 0.875\n",
      "2017-11-06T16:35:49.062191: step 315, loss 0.0712719, acc 0.96875\n",
      "2017-11-06T16:35:53.038035: step 316, loss 0.913073, acc 0.71875\n",
      "2017-11-06T16:35:57.060874: step 317, loss 0.624308, acc 0.9375\n",
      "2017-11-06T16:36:01.048709: step 318, loss 0.208403, acc 0.90625\n",
      "2017-11-06T16:36:05.058557: step 319, loss 0.19013, acc 0.96875\n",
      "2017-11-06T16:36:09.029398: step 320, loss 0.482275, acc 0.8125\n",
      "2017-11-06T16:36:13.018213: step 321, loss 0.811745, acc 0.75\n",
      "2017-11-06T16:36:17.010050: step 322, loss 0.172094, acc 0.90625\n",
      "2017-11-06T16:36:21.064930: step 323, loss 1.02436, acc 0.8125\n",
      "2017-11-06T16:36:23.538688: step 324, loss 0.642876, acc 0.9\n",
      "2017-11-06T16:36:27.654613: step 325, loss 0.0128527, acc 1\n",
      "2017-11-06T16:36:31.619430: step 326, loss 0.0504576, acc 0.96875\n",
      "2017-11-06T16:36:35.910479: step 327, loss 0.589242, acc 0.84375\n",
      "2017-11-06T16:36:39.947347: step 328, loss 0.443598, acc 0.875\n",
      "2017-11-06T16:36:43.953193: step 329, loss 0.403518, acc 0.90625\n",
      "2017-11-06T16:36:48.265258: step 330, loss 0.425184, acc 0.90625\n",
      "2017-11-06T16:36:52.535292: step 331, loss 0.014235, acc 1\n",
      "2017-11-06T16:36:56.576163: step 332, loss 0.606181, acc 0.90625\n",
      "2017-11-06T16:37:00.674525: step 333, loss 0.800897, acc 0.8125\n",
      "2017-11-06T16:37:04.607319: step 334, loss 0.405975, acc 0.9375\n",
      "2017-11-06T16:37:08.615165: step 335, loss 0.254117, acc 0.875\n",
      "2017-11-06T16:37:12.640025: step 336, loss 0.317419, acc 0.9375\n",
      "2017-11-06T16:37:16.599839: step 337, loss 0.301997, acc 0.96875\n",
      "2017-11-06T16:37:20.627701: step 338, loss 0.0660479, acc 0.96875\n",
      "2017-11-06T16:37:24.615534: step 339, loss 0.380559, acc 0.90625\n",
      "2017-11-06T16:37:28.609372: step 340, loss 0.680811, acc 0.875\n",
      "2017-11-06T16:37:32.649243: step 341, loss 0.350026, acc 0.84375\n",
      "2017-11-06T16:37:36.668098: step 342, loss 0.312637, acc 0.90625\n",
      "2017-11-06T16:37:40.759005: step 343, loss 0.759781, acc 0.84375\n",
      "2017-11-06T16:37:44.779863: step 344, loss 0.0189766, acc 1\n",
      "2017-11-06T16:37:48.893785: step 345, loss 0.449452, acc 0.90625\n",
      "2017-11-06T16:37:53.148784: step 346, loss 0.518454, acc 0.90625\n",
      "2017-11-06T16:37:57.525894: step 347, loss 0.387549, acc 0.875\n",
      "2017-11-06T16:38:01.578774: step 348, loss 0.439307, acc 0.9375\n",
      "2017-11-06T16:38:05.597630: step 349, loss 0.277246, acc 0.90625\n",
      "2017-11-06T16:38:09.592468: step 350, loss 0.722545, acc 0.8125\n",
      "2017-11-06T16:38:13.663362: step 351, loss 0.234974, acc 0.9375\n",
      "2017-11-06T16:38:17.778284: step 352, loss 0.657746, acc 0.8125\n",
      "2017-11-06T16:38:21.733096: step 353, loss 1.28185, acc 0.6875\n",
      "2017-11-06T16:38:25.841013: step 354, loss 0.170359, acc 0.90625\n",
      "2017-11-06T16:38:30.134067: step 355, loss 0.761444, acc 0.875\n",
      "2017-11-06T16:38:34.304027: step 356, loss 0.268849, acc 0.90625\n",
      "2017-11-06T16:38:38.355906: step 357, loss 0.533312, acc 0.84375\n",
      "2017-11-06T16:38:42.347743: step 358, loss 0.342656, acc 0.90625\n",
      "2017-11-06T16:38:46.338577: step 359, loss 0.898175, acc 0.90625\n",
      "2017-11-06T16:38:48.870376: step 360, loss 0.686007, acc 0.8\n",
      "2017-11-06T16:38:52.858210: step 361, loss 0.293017, acc 0.90625\n",
      "2017-11-06T16:38:56.851047: step 362, loss 0.448708, acc 0.90625\n",
      "2017-11-06T16:39:01.342656: step 363, loss 0.469557, acc 0.875\n",
      "2017-11-06T16:39:05.512619: step 364, loss 0.317327, acc 0.9375\n",
      "2017-11-06T16:39:09.525471: step 365, loss 0.377047, acc 0.90625\n",
      "2017-11-06T16:39:13.536321: step 366, loss 0.246031, acc 0.875\n",
      "2017-11-06T16:39:17.593204: step 367, loss 0.588397, acc 0.90625\n",
      "2017-11-06T16:39:21.648084: step 368, loss 0.407317, acc 0.9375\n",
      "2017-11-06T16:39:25.662938: step 369, loss 0.667655, acc 0.8125\n",
      "2017-11-06T16:39:29.649770: step 370, loss 0.63572, acc 0.90625\n",
      "2017-11-06T16:39:33.633601: step 371, loss 0.592239, acc 0.9375\n",
      "2017-11-06T16:39:37.615430: step 372, loss 0.649125, acc 0.90625\n",
      "2017-11-06T16:39:41.626280: step 373, loss 0.368261, acc 0.84375\n",
      "2017-11-06T16:39:45.631125: step 374, loss 0.138724, acc 0.96875\n",
      "2017-11-06T16:39:49.609952: step 375, loss 0.430017, acc 0.875\n",
      "2017-11-06T16:39:53.640817: step 376, loss 0.870551, acc 0.84375\n",
      "2017-11-06T16:39:57.579616: step 377, loss 0.32323, acc 0.9375\n",
      "2017-11-06T16:40:01.883673: step 378, loss 0.671636, acc 0.8125\n",
      "2017-11-06T16:40:06.276795: step 379, loss 0.0476784, acc 0.96875\n",
      "2017-11-06T16:40:10.443756: step 380, loss 0.569464, acc 0.84375\n",
      "2017-11-06T16:40:14.432591: step 381, loss 0.390804, acc 0.84375\n",
      "2017-11-06T16:40:18.516492: step 382, loss 0.236238, acc 0.84375\n",
      "2017-11-06T16:40:22.489314: step 383, loss 0.239025, acc 0.875\n",
      "2017-11-06T16:40:26.514174: step 384, loss 0.665955, acc 0.875\n",
      "2017-11-06T16:40:30.481994: step 385, loss 0.190311, acc 0.96875\n",
      "2017-11-06T16:40:34.729011: step 386, loss 0.67234, acc 0.8125\n",
      "2017-11-06T16:40:38.755873: step 387, loss 0.477698, acc 0.875\n",
      "2017-11-06T16:40:42.794742: step 388, loss 0.805848, acc 0.8125\n",
      "2017-11-06T16:40:46.756557: step 389, loss 0.598901, acc 0.90625\n",
      "2017-11-06T16:40:50.667336: step 390, loss 0.349699, acc 0.90625\n",
      "2017-11-06T16:40:54.598897: step 391, loss 0.393665, acc 0.90625\n",
      "2017-11-06T16:40:58.620753: step 392, loss 0.700431, acc 0.875\n",
      "2017-11-06T16:41:02.842575: step 393, loss 0.000780434, acc 1\n",
      "2017-11-06T16:41:06.865433: step 394, loss 1.07938, acc 0.78125\n",
      "2017-11-06T16:41:11.480713: step 395, loss 0.00038812, acc 1\n",
      "2017-11-06T16:41:14.900142: step 396, loss 0.658091, acc 0.9\n",
      "2017-11-06T16:41:19.159168: step 397, loss 0.481983, acc 0.90625\n",
      "2017-11-06T16:41:23.255079: step 398, loss 0.119633, acc 0.96875\n",
      "2017-11-06T16:41:27.249917: step 399, loss 0.700544, acc 0.875\n",
      "2017-11-06T16:41:31.299795: step 400, loss 0.704088, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:41:33.878627: step 400, loss 1.84006, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-400\n",
      "\n",
      "2017-11-06T16:41:39.950870: step 401, loss 0.93149, acc 0.84375\n",
      "2017-11-06T16:41:43.971727: step 402, loss 0.408742, acc 0.84375\n",
      "2017-11-06T16:41:48.016602: step 403, loss 0.692755, acc 0.875\n",
      "2017-11-06T16:41:51.970411: step 404, loss 0.320444, acc 0.84375\n",
      "2017-11-06T16:41:55.913212: step 405, loss 0.1665, acc 0.90625\n",
      "2017-11-06T16:41:59.894040: step 406, loss 0.165973, acc 0.96875\n",
      "2017-11-06T16:42:04.033982: step 407, loss 0.872751, acc 0.75\n",
      "2017-11-06T16:42:08.043832: step 408, loss 0.220934, acc 0.9375\n",
      "2017-11-06T16:42:11.978628: step 409, loss 0.399948, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:42:16.191621: step 410, loss 0.249443, acc 0.96875\n",
      "2017-11-06T16:42:20.478667: step 411, loss 0.490288, acc 0.84375\n",
      "2017-11-06T16:42:24.485516: step 412, loss 0.440602, acc 0.9375\n",
      "2017-11-06T16:42:28.489359: step 413, loss 0.376734, acc 0.90625\n",
      "2017-11-06T16:42:32.476192: step 414, loss 0.365526, acc 0.90625\n",
      "2017-11-06T16:42:36.725211: step 415, loss 0.384102, acc 0.875\n",
      "2017-11-06T16:42:40.755075: step 416, loss 0.428821, acc 0.875\n",
      "2017-11-06T16:42:44.776932: step 417, loss 0.556928, acc 0.90625\n",
      "2017-11-06T16:42:48.814802: step 418, loss 0.382447, acc 0.90625\n",
      "2017-11-06T16:42:52.827652: step 419, loss 0.939691, acc 0.78125\n",
      "2017-11-06T16:42:56.879531: step 420, loss 0.695526, acc 0.90625\n",
      "2017-11-06T16:43:00.894386: step 421, loss 0.517196, acc 0.90625\n",
      "2017-11-06T16:43:04.981288: step 422, loss 0.677009, acc 0.875\n",
      "2017-11-06T16:43:08.969121: step 423, loss 0.176143, acc 0.9375\n",
      "2017-11-06T16:43:12.974968: step 424, loss 1.19986, acc 0.78125\n",
      "2017-11-06T16:43:17.009836: step 425, loss 0.597346, acc 0.875\n",
      "2017-11-06T16:43:21.268400: step 426, loss 0.505102, acc 0.875\n",
      "2017-11-06T16:43:25.774603: step 427, loss 0.803663, acc 0.875\n",
      "2017-11-06T16:43:29.813472: step 428, loss 0.40139, acc 0.90625\n",
      "2017-11-06T16:43:33.887367: step 429, loss 0.767992, acc 0.90625\n",
      "2017-11-06T16:43:37.845179: step 430, loss 0.25, acc 0.9375\n",
      "2017-11-06T16:43:41.938087: step 431, loss 0.531698, acc 0.875\n",
      "2017-11-06T16:43:44.457878: step 432, loss 0.223775, acc 0.85\n",
      "2017-11-06T16:43:48.465726: step 433, loss 0.318178, acc 0.875\n",
      "2017-11-06T16:43:52.499592: step 434, loss 0.452493, acc 0.875\n",
      "2017-11-06T16:43:56.559098: step 435, loss 0.300473, acc 0.875\n",
      "2017-11-06T16:44:00.591964: step 436, loss 0.37725, acc 0.875\n",
      "2017-11-06T16:44:04.515752: step 437, loss 0.608029, acc 0.8125\n",
      "2017-11-06T16:44:08.559625: step 438, loss 0.459082, acc 0.875\n",
      "2017-11-06T16:44:12.528445: step 439, loss 0.59521, acc 0.84375\n",
      "2017-11-06T16:44:16.494263: step 440, loss 0.424341, acc 0.8125\n",
      "2017-11-06T16:44:20.524127: step 441, loss 0.353147, acc 0.875\n",
      "2017-11-06T16:44:24.523968: step 442, loss 0.493752, acc 0.90625\n",
      "2017-11-06T16:44:28.881065: step 443, loss 0.167647, acc 0.96875\n",
      "2017-11-06T16:44:33.160105: step 444, loss 0.172827, acc 0.9375\n",
      "2017-11-06T16:44:37.326065: step 445, loss 0.0142019, acc 1\n",
      "2017-11-06T16:44:41.449995: step 446, loss 0.515166, acc 0.90625\n",
      "2017-11-06T16:44:45.427822: step 447, loss 0.646088, acc 0.90625\n",
      "2017-11-06T16:44:49.478702: step 448, loss 0.736099, acc 0.84375\n",
      "2017-11-06T16:44:53.568606: step 449, loss 0.673867, acc 0.90625\n",
      "2017-11-06T16:44:57.603473: step 450, loss 0.417985, acc 0.84375\n",
      "2017-11-06T16:45:01.571292: step 451, loss 0.0377045, acc 0.96875\n",
      "2017-11-06T16:45:05.598154: step 452, loss 0.0897721, acc 0.96875\n",
      "2017-11-06T16:45:09.645029: step 453, loss 0.365247, acc 0.90625\n",
      "2017-11-06T16:45:13.781969: step 454, loss 0.0443457, acc 1\n",
      "2017-11-06T16:45:17.768801: step 455, loss 0.13498, acc 0.96875\n",
      "2017-11-06T16:45:21.813675: step 456, loss 0.132966, acc 0.9375\n",
      "2017-11-06T16:45:25.780494: step 457, loss 0.377057, acc 0.9375\n",
      "2017-11-06T16:45:29.815361: step 458, loss 0.0787366, acc 0.96875\n",
      "2017-11-06T16:45:34.145438: step 459, loss 0.492099, acc 0.84375\n",
      "2017-11-06T16:45:38.440489: step 460, loss 0.642742, acc 0.84375\n",
      "2017-11-06T16:45:42.407308: step 461, loss 0.655123, acc 0.90625\n",
      "2017-11-06T16:45:46.401145: step 462, loss 0.63264, acc 0.90625\n",
      "2017-11-06T16:45:50.454027: step 463, loss 0.454093, acc 0.875\n",
      "2017-11-06T16:45:54.495897: step 464, loss 0.303012, acc 0.9375\n",
      "2017-11-06T16:45:58.532765: step 465, loss 0.739343, acc 0.78125\n",
      "2017-11-06T16:46:02.509591: step 466, loss 0.629604, acc 0.75\n",
      "2017-11-06T16:46:06.592492: step 467, loss 0.233255, acc 0.9375\n",
      "2017-11-06T16:46:09.342447: step 468, loss 0.378453, acc 0.85\n",
      "2017-11-06T16:46:13.499400: step 469, loss 0.302069, acc 0.875\n",
      "2017-11-06T16:46:17.674367: step 470, loss 0.398698, acc 0.90625\n",
      "2017-11-06T16:46:21.903373: step 471, loss 0.0860528, acc 0.96875\n",
      "2017-11-06T16:46:26.036308: step 472, loss 0.958375, acc 0.84375\n",
      "2017-11-06T16:46:30.155235: step 473, loss 0.232584, acc 0.9375\n",
      "2017-11-06T16:46:34.421266: step 474, loss 0.627463, acc 0.78125\n",
      "2017-11-06T16:46:38.749342: step 475, loss 0.615572, acc 0.875\n",
      "2017-11-06T16:46:43.026380: step 476, loss 0.36221, acc 0.9375\n",
      "2017-11-06T16:46:47.130296: step 477, loss 0.277044, acc 0.9375\n",
      "2017-11-06T16:46:51.417343: step 478, loss 0.949683, acc 0.875\n",
      "2017-11-06T16:46:55.461000: step 479, loss 0.143323, acc 0.9375\n",
      "2017-11-06T16:46:59.564915: step 480, loss 0.138151, acc 0.90625\n",
      "2017-11-06T16:47:03.660826: step 481, loss 0.272077, acc 0.875\n",
      "2017-11-06T16:47:07.716059: step 482, loss 0.429438, acc 0.875\n",
      "2017-11-06T16:47:11.789953: step 483, loss 0.945801, acc 0.84375\n",
      "2017-11-06T16:47:15.806807: step 484, loss 0.330845, acc 0.90625\n",
      "2017-11-06T16:47:19.765620: step 485, loss 0.388211, acc 0.84375\n",
      "2017-11-06T16:47:23.768464: step 486, loss 0.776758, acc 0.875\n",
      "2017-11-06T16:47:27.732281: step 487, loss 0.0334249, acc 1\n",
      "2017-11-06T16:47:31.707106: step 488, loss 0.0846156, acc 0.96875\n",
      "2017-11-06T16:47:35.735967: step 489, loss 0.279309, acc 0.9375\n",
      "2017-11-06T16:47:39.698784: step 490, loss 0.239789, acc 0.9375\n",
      "2017-11-06T16:47:43.869747: step 491, loss 0.509397, acc 0.875\n",
      "2017-11-06T16:47:48.217836: step 492, loss 0.106051, acc 0.96875\n",
      "2017-11-06T16:47:52.293734: step 493, loss 0.651125, acc 0.78125\n",
      "2017-11-06T16:47:56.256548: step 494, loss 0.33039, acc 0.875\n",
      "2017-11-06T16:48:00.198349: step 495, loss 0.30262, acc 0.90625\n",
      "2017-11-06T16:48:04.154160: step 496, loss 0.515351, acc 0.90625\n",
      "2017-11-06T16:48:08.100964: step 497, loss 0.352372, acc 0.90625\n",
      "2017-11-06T16:48:12.133831: step 498, loss 0.591716, acc 0.875\n",
      "2017-11-06T16:48:16.185708: step 499, loss 0.439338, acc 0.90625\n",
      "2017-11-06T16:48:20.142520: step 500, loss 0.0939999, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:48:22.764385: step 500, loss 1.88698, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-500\n",
      "\n",
      "2017-11-06T16:48:29.617665: step 501, loss 0.466492, acc 0.90625\n",
      "2017-11-06T16:48:33.847671: step 502, loss 1.12452, acc 0.875\n",
      "2017-11-06T16:48:37.888542: step 503, loss 0.506233, acc 0.875\n",
      "2017-11-06T16:48:40.492394: step 504, loss 0.227831, acc 0.9\n",
      "2017-11-06T16:48:44.416180: step 505, loss 0.446069, acc 0.90625\n",
      "2017-11-06T16:48:48.503086: step 506, loss 0.222439, acc 0.9375\n",
      "2017-11-06T16:48:52.919222: step 507, loss 0.181049, acc 0.9375\n",
      "2017-11-06T16:48:56.988113: step 508, loss 0.764455, acc 0.8125\n",
      "2017-11-06T16:49:00.955933: step 509, loss 0.101542, acc 0.96875\n",
      "2017-11-06T16:49:04.964781: step 510, loss 0.316239, acc 0.90625\n",
      "2017-11-06T16:49:09.044681: step 511, loss 0.150154, acc 0.9375\n",
      "2017-11-06T16:49:12.974472: step 512, loss 0.666093, acc 0.84375\n",
      "2017-11-06T16:49:16.983320: step 513, loss 0.129818, acc 0.9375\n",
      "2017-11-06T16:49:20.966152: step 514, loss 0.364972, acc 0.9375\n",
      "2017-11-06T16:49:24.873927: step 515, loss 0.130407, acc 0.96875\n",
      "2017-11-06T16:49:28.831740: step 516, loss 0.410537, acc 0.875\n",
      "2017-11-06T16:49:32.810567: step 517, loss 0.0936128, acc 0.9375\n",
      "2017-11-06T16:49:36.728350: step 518, loss 0.930809, acc 0.75\n",
      "2017-11-06T16:49:40.703174: step 519, loss 0.565298, acc 0.875\n",
      "2017-11-06T16:49:44.743045: step 520, loss 0.792123, acc 0.8125\n",
      "2017-11-06T16:49:48.713867: step 521, loss 0.227543, acc 0.9375\n",
      "2017-11-06T16:49:52.697697: step 522, loss 0.382839, acc 0.9375\n",
      "2017-11-06T16:49:57.137734: step 523, loss 0.0655011, acc 0.96875\n",
      "2017-11-06T16:50:01.663949: step 524, loss 0.136483, acc 0.9375\n",
      "2017-11-06T16:50:05.695814: step 525, loss 0.808145, acc 0.84375\n",
      "2017-11-06T16:50:09.937829: step 526, loss 0.10527, acc 0.9375\n",
      "2017-11-06T16:50:14.238885: step 527, loss 0.0167123, acc 1\n",
      "2017-11-06T16:50:19.009274: step 528, loss 0.695599, acc 0.875\n",
      "2017-11-06T16:50:23.215262: step 529, loss 0.326595, acc 0.9375\n",
      "2017-11-06T16:50:27.354204: step 530, loss 0.497021, acc 0.96875\n",
      "2017-11-06T16:50:31.415089: step 531, loss 0.525826, acc 0.875\n",
      "2017-11-06T16:50:35.689126: step 532, loss 0.467509, acc 0.875\n",
      "2017-11-06T16:50:39.804049: step 533, loss 0.145026, acc 0.9375\n",
      "2017-11-06T16:50:43.957000: step 534, loss 0.499737, acc 0.84375\n",
      "2017-11-06T16:50:48.025891: step 535, loss 1.00333, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:50:51.989709: step 536, loss 0.317189, acc 0.9375\n",
      "2017-11-06T16:50:56.047591: step 537, loss 0.797756, acc 0.875\n",
      "2017-11-06T16:51:00.247575: step 538, loss 0.44613, acc 0.90625\n",
      "2017-11-06T16:51:04.596668: step 539, loss 1.0748, acc 0.875\n",
      "2017-11-06T16:51:07.209522: step 540, loss 0.305093, acc 0.9\n",
      "2017-11-06T16:51:11.243388: step 541, loss 0.519328, acc 0.84375\n",
      "2017-11-06T16:51:15.355310: step 542, loss 0.425444, acc 0.875\n",
      "2017-11-06T16:51:19.270091: step 543, loss 0.169382, acc 0.9375\n",
      "2017-11-06T16:51:23.299955: step 544, loss 0.173932, acc 0.90625\n",
      "2017-11-06T16:51:27.346832: step 545, loss 0.487595, acc 0.8125\n",
      "2017-11-06T16:51:31.352677: step 546, loss 0.492273, acc 0.84375\n",
      "2017-11-06T16:51:35.601696: step 547, loss 0.540103, acc 0.9375\n",
      "2017-11-06T16:51:41.611966: step 548, loss 0.00656911, acc 1\n",
      "2017-11-06T16:51:48.835099: step 549, loss 0.371685, acc 0.90625\n",
      "2017-11-06T16:51:56.267380: step 550, loss 0.144165, acc 0.9375\n",
      "2017-11-06T16:52:03.782720: step 551, loss 0.710219, acc 0.90625\n",
      "2017-11-06T16:52:11.523219: step 552, loss 0.350506, acc 0.9375\n",
      "2017-11-06T16:52:18.830412: step 553, loss 0.566961, acc 0.875\n",
      "2017-11-06T16:52:26.184637: step 554, loss 0.130267, acc 0.9375\n",
      "2017-11-06T16:52:33.230643: step 555, loss 0.114717, acc 0.9375\n",
      "2017-11-06T16:52:40.324685: step 556, loss 0.15408, acc 0.875\n",
      "2017-11-06T16:52:46.826304: step 557, loss 0.217299, acc 0.96875\n",
      "2017-11-06T16:52:51.607701: step 558, loss 0.432557, acc 0.84375\n",
      "2017-11-06T16:52:55.976579: step 559, loss 0.238335, acc 0.90625\n",
      "2017-11-06T16:53:00.681922: step 560, loss 0.278547, acc 0.9375\n",
      "2017-11-06T16:53:05.290197: step 561, loss 0.358992, acc 0.9375\n",
      "2017-11-06T16:53:09.333068: step 562, loss 0.0455896, acc 1\n",
      "2017-11-06T16:53:14.070437: step 563, loss 0.00876853, acc 1\n",
      "2017-11-06T16:53:18.623670: step 564, loss 0.584877, acc 0.71875\n",
      "2017-11-06T16:53:23.640235: step 565, loss 0.591902, acc 0.8125\n",
      "2017-11-06T16:53:27.994329: step 566, loss 0.0717443, acc 0.96875\n",
      "2017-11-06T16:53:32.234341: step 567, loss 0.188028, acc 0.9375\n",
      "2017-11-06T16:53:36.320244: step 568, loss 0.494493, acc 0.8125\n",
      "2017-11-06T16:53:40.380129: step 569, loss 0.31631, acc 0.90625\n",
      "2017-11-06T16:53:44.392980: step 570, loss 0.138229, acc 0.9375\n",
      "2017-11-06T16:53:48.359799: step 571, loss 0.601435, acc 0.84375\n",
      "2017-11-06T16:53:52.381657: step 572, loss 0.532894, acc 0.90625\n",
      "2017-11-06T16:53:56.419525: step 573, loss 0.797391, acc 0.8125\n",
      "2017-11-06T16:54:00.405359: step 574, loss 0.235117, acc 0.875\n",
      "2017-11-06T16:54:04.346157: step 575, loss 0.420128, acc 0.875\n",
      "2017-11-06T16:54:06.862946: step 576, loss 0.509384, acc 0.9\n",
      "2017-11-06T16:54:10.865790: step 577, loss 0.472258, acc 0.90625\n",
      "2017-11-06T16:54:14.804589: step 578, loss 0.205634, acc 0.90625\n",
      "2017-11-06T16:54:19.174695: step 579, loss 0.0203283, acc 1\n",
      "2017-11-06T16:54:23.427716: step 580, loss 0.288963, acc 0.90625\n",
      "2017-11-06T16:54:27.374521: step 581, loss 0.389838, acc 0.84375\n",
      "2017-11-06T16:54:31.353347: step 582, loss 0.569867, acc 0.8125\n",
      "2017-11-06T16:54:35.588356: step 583, loss 0.360372, acc 0.9375\n",
      "2017-11-06T16:54:39.612216: step 584, loss 0.2649, acc 0.90625\n",
      "2017-11-06T16:54:43.542008: step 585, loss 0.19788, acc 0.9375\n",
      "2017-11-06T16:54:47.622908: step 586, loss 0.284702, acc 0.90625\n",
      "2017-11-06T16:54:51.601736: step 587, loss 0.372228, acc 0.875\n",
      "2017-11-06T16:54:55.673628: step 588, loss 0.0508057, acc 0.96875\n",
      "2017-11-06T16:54:59.699489: step 589, loss 0.336489, acc 0.875\n",
      "2017-11-06T16:55:03.711339: step 590, loss 0.983433, acc 0.84375\n",
      "2017-11-06T16:55:07.678157: step 591, loss 0.0262234, acc 1\n",
      "2017-11-06T16:55:11.680001: step 592, loss 0.0632729, acc 0.96875\n",
      "2017-11-06T16:55:15.961075: step 593, loss 0.38716, acc 0.9375\n",
      "2017-11-06T16:55:19.925892: step 594, loss 0.224232, acc 0.9375\n",
      "2017-11-06T16:55:24.105862: step 595, loss 0.306635, acc 0.90625\n",
      "2017-11-06T16:55:28.396911: step 596, loss 0.0734257, acc 0.96875\n",
      "2017-11-06T16:55:32.400756: step 597, loss 0.0109027, acc 1\n",
      "2017-11-06T16:55:36.439626: step 598, loss 0.459727, acc 0.875\n",
      "2017-11-06T16:55:40.445472: step 599, loss 0.448479, acc 0.90625\n",
      "2017-11-06T16:55:44.424299: step 600, loss 0.487005, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T16:55:47.092195: step 600, loss 1.53267, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-600\n",
      "\n",
      "2017-11-06T16:55:52.677820: step 601, loss 0.680515, acc 0.90625\n",
      "2017-11-06T16:55:56.638443: step 602, loss 0.426919, acc 0.875\n",
      "2017-11-06T16:56:00.659300: step 603, loss 0.274893, acc 0.90625\n",
      "2017-11-06T16:56:04.688163: step 604, loss 0.490415, acc 0.84375\n",
      "2017-11-06T16:56:08.647977: step 605, loss 0.249774, acc 0.90625\n",
      "2017-11-06T16:56:12.598784: step 606, loss 0.567001, acc 0.875\n",
      "2017-11-06T16:56:16.632649: step 607, loss 0.66086, acc 0.875\n",
      "2017-11-06T16:56:20.537424: step 608, loss 0.369269, acc 0.875\n",
      "2017-11-06T16:56:24.968088: step 609, loss 0.448454, acc 0.90625\n",
      "2017-11-06T16:56:29.353705: step 610, loss 0.527113, acc 0.78125\n",
      "2017-11-06T16:56:34.129097: step 611, loss 0.500622, acc 0.84375\n",
      "2017-11-06T16:56:36.777479: step 612, loss 0.283884, acc 0.85\n",
      "2017-11-06T16:56:40.859381: step 613, loss 0.783109, acc 0.78125\n",
      "2017-11-06T16:56:44.792178: step 614, loss 0.396079, acc 0.8125\n",
      "2017-11-06T16:56:48.866070: step 615, loss 0.383587, acc 0.90625\n",
      "2017-11-06T16:56:52.920952: step 616, loss 0.246787, acc 0.9375\n",
      "2017-11-06T16:56:56.889772: step 617, loss 0.0522258, acc 0.96875\n",
      "2017-11-06T16:57:00.850586: step 618, loss 0.69111, acc 0.8125\n",
      "2017-11-06T16:57:04.864438: step 619, loss 0.48007, acc 0.875\n",
      "2017-11-06T16:57:08.826254: step 620, loss 0.388631, acc 0.9375\n",
      "2017-11-06T16:57:12.831100: step 621, loss 0.315386, acc 0.90625\n",
      "2017-11-06T16:57:16.940019: step 622, loss 0.347978, acc 0.9375\n",
      "2017-11-06T16:57:20.923849: step 623, loss 0.255445, acc 0.90625\n",
      "2017-11-06T16:57:24.919689: step 624, loss 0.402163, acc 0.875\n",
      "2017-11-06T16:57:28.927536: step 625, loss 0.0592746, acc 0.96875\n",
      "2017-11-06T16:57:32.964405: step 626, loss 0.129282, acc 0.96875\n",
      "2017-11-06T16:57:37.463601: step 627, loss 0.901409, acc 0.8125\n",
      "2017-11-06T16:57:41.592535: step 628, loss 0.12466, acc 0.96875\n",
      "2017-11-06T16:57:45.632406: step 629, loss 0.525072, acc 0.84375\n",
      "2017-11-06T16:57:49.637251: step 630, loss 0.469491, acc 0.90625\n",
      "2017-11-06T16:57:53.641096: step 631, loss 0.0235121, acc 1\n",
      "2017-11-06T16:57:57.590902: step 632, loss 0.461841, acc 0.8125\n",
      "2017-11-06T16:58:01.564727: step 633, loss 0.61546, acc 0.8125\n",
      "2017-11-06T16:58:05.575576: step 634, loss 0.216937, acc 0.875\n",
      "2017-11-06T16:58:09.553402: step 635, loss 0.329717, acc 0.90625\n",
      "2017-11-06T16:58:13.495204: step 636, loss 0.401192, acc 0.90625\n",
      "2017-11-06T16:58:17.596118: step 637, loss 0.361113, acc 0.90625\n",
      "2017-11-06T16:58:21.622978: step 638, loss 0.47011, acc 0.84375\n",
      "2017-11-06T16:58:25.882004: step 639, loss 0.47285, acc 0.8125\n",
      "2017-11-06T16:58:29.867837: step 640, loss 0.495566, acc 0.875\n",
      "2017-11-06T16:58:34.052810: step 641, loss 0.38374, acc 0.9375\n",
      "2017-11-06T16:58:38.166734: step 642, loss 0.21937, acc 0.90625\n",
      "2017-11-06T16:58:42.512821: step 643, loss 0.289337, acc 0.90625\n",
      "2017-11-06T16:58:46.598724: step 644, loss 0.236367, acc 0.9375\n",
      "2017-11-06T16:58:50.575550: step 645, loss 0.486748, acc 0.84375\n",
      "2017-11-06T16:58:54.516350: step 646, loss 0.793205, acc 0.84375\n",
      "2017-11-06T16:58:58.497976: step 647, loss 0.356165, acc 0.9375\n",
      "2017-11-06T16:59:01.069804: step 648, loss 0.0848911, acc 0.95\n",
      "2017-11-06T16:59:05.140696: step 649, loss 0.15795, acc 0.90625\n",
      "2017-11-06T16:59:09.098508: step 650, loss 0.00103067, acc 1\n",
      "2017-11-06T16:59:13.028300: step 651, loss 0.483945, acc 0.8125\n",
      "2017-11-06T16:59:17.034147: step 652, loss 0.435799, acc 0.875\n",
      "2017-11-06T16:59:20.975948: step 653, loss 0.343612, acc 0.90625\n",
      "2017-11-06T16:59:25.006812: step 654, loss 0.412973, acc 0.875\n",
      "2017-11-06T16:59:29.000651: step 655, loss 0.184583, acc 0.9375\n",
      "2017-11-06T16:59:32.938448: step 656, loss 0.790671, acc 0.84375\n",
      "2017-11-06T16:59:36.928283: step 657, loss 0.325504, acc 0.90625\n",
      "2017-11-06T16:59:40.966152: step 658, loss 0.401814, acc 0.78125\n",
      "2017-11-06T16:59:45.013027: step 659, loss 0.466171, acc 0.875\n",
      "2017-11-06T16:59:49.399143: step 660, loss 0.271648, acc 0.9375\n",
      "2017-11-06T16:59:53.501058: step 661, loss 0.655568, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T16:59:57.434854: step 662, loss 0.485986, acc 0.84375\n",
      "2017-11-06T17:00:01.698883: step 663, loss 0.364095, acc 0.9375\n",
      "2017-11-06T17:00:05.690719: step 664, loss 0.242405, acc 0.90625\n",
      "2017-11-06T17:00:09.797637: step 665, loss 0.262495, acc 0.9375\n",
      "2017-11-06T17:00:13.789474: step 666, loss 0.455533, acc 0.875\n",
      "2017-11-06T17:00:17.710260: step 667, loss 0.419807, acc 0.875\n",
      "2017-11-06T17:00:21.645056: step 668, loss 0.546174, acc 0.875\n",
      "2017-11-06T17:00:25.629888: step 669, loss 0.14844, acc 0.9375\n",
      "2017-11-06T17:00:29.578694: step 670, loss 0.545521, acc 0.84375\n",
      "2017-11-06T17:00:33.680607: step 671, loss 0.4646, acc 0.90625\n",
      "2017-11-06T17:00:37.897604: step 672, loss 0.447644, acc 0.8125\n",
      "2017-11-06T17:00:41.931470: step 673, loss 0.270102, acc 0.9375\n",
      "2017-11-06T17:00:45.946322: step 674, loss 0.298985, acc 0.9375\n",
      "2017-11-06T17:00:50.019216: step 675, loss 0.43467, acc 0.90625\n",
      "2017-11-06T17:00:54.497398: step 676, loss 0.593455, acc 0.875\n",
      "2017-11-06T17:00:58.511251: step 677, loss 0.534949, acc 0.875\n",
      "2017-11-06T17:01:02.555124: step 678, loss 0.162511, acc 0.90625\n",
      "2017-11-06T17:01:06.632020: step 679, loss 1.04267, acc 0.75\n",
      "2017-11-06T17:01:10.727931: step 680, loss 0.00055767, acc 1\n",
      "2017-11-06T17:01:14.858867: step 681, loss 0.0160765, acc 1\n",
      "2017-11-06T17:01:19.057070: step 682, loss 0.065123, acc 0.96875\n",
      "2017-11-06T17:01:23.068921: step 683, loss 0.300088, acc 0.9375\n",
      "2017-11-06T17:01:25.606724: step 684, loss 0.299771, acc 0.9\n",
      "2017-11-06T17:01:29.570540: step 685, loss 0.288861, acc 0.90625\n",
      "2017-11-06T17:01:33.516345: step 686, loss 0.37756, acc 0.90625\n",
      "2017-11-06T17:01:37.698315: step 687, loss 0.187795, acc 0.9375\n",
      "2017-11-06T17:01:41.786220: step 688, loss 0.752442, acc 0.8125\n",
      "2017-11-06T17:01:45.848106: step 689, loss 0.546737, acc 0.875\n",
      "2017-11-06T17:01:49.765890: step 690, loss 0.613759, acc 0.875\n",
      "2017-11-06T17:01:53.715696: step 691, loss 0.0690689, acc 0.96875\n",
      "2017-11-06T17:01:57.915556: step 692, loss 0.180149, acc 0.9375\n",
      "2017-11-06T17:02:02.380730: step 693, loss 0.516835, acc 0.875\n",
      "2017-11-06T17:02:06.416597: step 694, loss 0.454318, acc 0.875\n",
      "2017-11-06T17:02:10.491492: step 695, loss 0.247257, acc 0.9375\n",
      "2017-11-06T17:02:14.444301: step 696, loss 0.454844, acc 0.84375\n",
      "2017-11-06T17:02:18.500183: step 697, loss 0.167539, acc 0.9375\n",
      "2017-11-06T17:02:22.440983: step 698, loss 0.743269, acc 0.8125\n",
      "2017-11-06T17:02:26.425815: step 699, loss 0.25623, acc 0.90625\n",
      "2017-11-06T17:02:30.401640: step 700, loss 0.307878, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:02:33.108563: step 700, loss 1.26488, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-700\n",
      "\n",
      "2017-11-06T17:02:39.005849: step 701, loss 0.27106, acc 0.9375\n",
      "2017-11-06T17:02:42.974670: step 702, loss 0.158119, acc 0.9375\n",
      "2017-11-06T17:02:46.985519: step 703, loss 0.611797, acc 0.78125\n",
      "2017-11-06T17:02:50.959344: step 704, loss 0.0872535, acc 0.9375\n",
      "2017-11-06T17:02:54.989206: step 705, loss 0.213972, acc 0.90625\n",
      "2017-11-06T17:02:59.017068: step 706, loss 0.424809, acc 0.84375\n",
      "2017-11-06T17:03:03.187031: step 707, loss 0.359069, acc 0.9375\n",
      "2017-11-06T17:03:07.471076: step 708, loss 0.260085, acc 0.90625\n",
      "2017-11-06T17:03:11.594005: step 709, loss 0.0165538, acc 1\n",
      "2017-11-06T17:03:15.517792: step 710, loss 0.229911, acc 0.875\n",
      "2017-11-06T17:03:19.629715: step 711, loss 0.0447145, acc 0.96875\n",
      "2017-11-06T17:03:23.704610: step 712, loss 0.436039, acc 0.90625\n",
      "2017-11-06T17:03:27.970642: step 713, loss 0.476312, acc 0.90625\n",
      "2017-11-06T17:03:31.965481: step 714, loss 0.334456, acc 0.90625\n",
      "2017-11-06T17:03:35.977330: step 715, loss 0.797719, acc 0.84375\n",
      "2017-11-06T17:03:39.998187: step 716, loss 0.317184, acc 0.9375\n",
      "2017-11-06T17:03:44.179157: step 717, loss 0.17997, acc 0.96875\n",
      "2017-11-06T17:03:48.249050: step 718, loss 0.487486, acc 0.84375\n",
      "2017-11-06T17:03:52.511078: step 719, loss 0.172526, acc 0.96875\n",
      "2017-11-06T17:03:55.184979: step 720, loss 0.273869, acc 0.95\n",
      "2017-11-06T17:03:59.389966: step 721, loss 0.0708941, acc 0.9375\n",
      "2017-11-06T17:04:03.534910: step 722, loss 0.609966, acc 0.84375\n",
      "2017-11-06T17:04:07.692867: step 723, loss 0.375507, acc 0.84375\n",
      "2017-11-06T17:04:12.160039: step 724, loss 0.456987, acc 0.84375\n",
      "2017-11-06T17:04:16.351017: step 725, loss 0.225335, acc 0.9375\n",
      "2017-11-06T17:04:20.367871: step 726, loss 0.261781, acc 0.90625\n",
      "2017-11-06T17:04:24.458778: step 727, loss 0.667635, acc 0.90625\n",
      "2017-11-06T17:04:28.643752: step 728, loss 0.187574, acc 0.90625\n",
      "2017-11-06T17:04:32.774687: step 729, loss 0.0822421, acc 0.96875\n",
      "2017-11-06T17:04:37.094757: step 730, loss 0.545843, acc 0.875\n",
      "2017-11-06T17:04:41.174656: step 731, loss 0.359179, acc 0.90625\n",
      "2017-11-06T17:04:45.293582: step 732, loss 0.432888, acc 0.875\n",
      "2017-11-06T17:04:49.402501: step 733, loss 0.939718, acc 0.8125\n",
      "2017-11-06T17:04:53.550449: step 734, loss 0.425427, acc 0.9375\n",
      "2017-11-06T17:04:57.771209: step 735, loss 0.384125, acc 0.90625\n",
      "2017-11-06T17:05:01.872123: step 736, loss 0.561102, acc 0.84375\n",
      "2017-11-06T17:05:06.090120: step 737, loss 0.0405594, acc 0.96875\n",
      "2017-11-06T17:05:10.099969: step 738, loss 0.132118, acc 0.90625\n",
      "2017-11-06T17:05:14.327973: step 739, loss 0.41422, acc 0.90625\n",
      "2017-11-06T17:05:18.667057: step 740, loss 0.438529, acc 0.84375\n",
      "2017-11-06T17:05:22.775908: step 741, loss 0.114122, acc 0.9375\n",
      "2017-11-06T17:05:26.758739: step 742, loss 0.524507, acc 0.84375\n",
      "2017-11-06T17:05:30.782598: step 743, loss 0.597218, acc 0.84375\n",
      "2017-11-06T17:05:34.777435: step 744, loss 0.717039, acc 0.875\n",
      "2017-11-06T17:05:38.914375: step 745, loss 0.433587, acc 0.84375\n",
      "2017-11-06T17:05:42.913216: step 746, loss 0.280376, acc 0.9375\n",
      "2017-11-06T17:05:46.978106: step 747, loss 0.42694, acc 0.875\n",
      "2017-11-06T17:05:51.008969: step 748, loss 0.214576, acc 0.90625\n",
      "2017-11-06T17:05:55.022821: step 749, loss 0.374659, acc 0.875\n",
      "2017-11-06T17:05:59.075700: step 750, loss 0.248493, acc 0.875\n",
      "2017-11-06T17:06:03.131583: step 751, loss 0.234661, acc 0.9375\n",
      "2017-11-06T17:06:07.044362: step 752, loss 0.255404, acc 0.9375\n",
      "2017-11-06T17:06:11.163289: step 753, loss 0.356462, acc 0.84375\n",
      "2017-11-06T17:06:15.187148: step 754, loss 0.215496, acc 0.9375\n",
      "2017-11-06T17:06:19.467189: step 755, loss 0.368108, acc 0.875\n",
      "2017-11-06T17:06:22.378258: step 756, loss 0.819053, acc 0.85\n",
      "2017-11-06T17:06:26.464161: step 757, loss 0.370896, acc 0.875\n",
      "2017-11-06T17:06:30.409965: step 758, loss 0.147608, acc 0.90625\n",
      "2017-11-06T17:06:34.725031: step 759, loss 0.275804, acc 0.9375\n",
      "2017-11-06T17:06:38.762900: step 760, loss 0.21701, acc 0.90625\n",
      "2017-11-06T17:06:42.790762: step 761, loss 0.291917, acc 0.9375\n",
      "2017-11-06T17:06:46.855650: step 762, loss 0.259248, acc 0.9375\n",
      "2017-11-06T17:06:50.923541: step 763, loss 0.0698979, acc 0.96875\n",
      "2017-11-06T17:06:54.950402: step 764, loss 0.528602, acc 0.875\n",
      "2017-11-06T17:06:58.910216: step 765, loss 0.248349, acc 0.875\n",
      "2017-11-06T17:07:02.873032: step 766, loss 0.258849, acc 0.90625\n",
      "2017-11-06T17:07:06.873874: step 767, loss 0.110414, acc 0.96875\n",
      "2017-11-06T17:07:10.911743: step 768, loss 0.358468, acc 0.875\n",
      "2017-11-06T17:07:14.944608: step 769, loss 0.09823, acc 0.96875\n",
      "2017-11-06T17:07:19.014501: step 770, loss 0.251101, acc 0.9375\n",
      "2017-11-06T17:07:23.150529: step 771, loss 0.202411, acc 0.9375\n",
      "2017-11-06T17:07:27.545652: step 772, loss 0.18469, acc 0.90625\n",
      "2017-11-06T17:07:31.729625: step 773, loss 0.403461, acc 0.84375\n",
      "2017-11-06T17:07:35.773498: step 774, loss 0.0286405, acc 1\n",
      "2017-11-06T17:07:39.817371: step 775, loss 0.191981, acc 0.9375\n",
      "2017-11-06T17:07:43.870251: step 776, loss 0.0464436, acc 1\n",
      "2017-11-06T17:07:47.861087: step 777, loss 0.530327, acc 0.84375\n",
      "2017-11-06T17:07:51.900958: step 778, loss 0.372142, acc 0.875\n",
      "2017-11-06T17:07:55.905803: step 779, loss 0.00948299, acc 1\n",
      "2017-11-06T17:07:59.876383: step 780, loss 0.382298, acc 0.90625\n",
      "2017-11-06T17:08:03.840200: step 781, loss 0.469531, acc 0.90625\n",
      "2017-11-06T17:08:07.845046: step 782, loss 0.285463, acc 0.875\n",
      "2017-11-06T17:08:11.773837: step 783, loss 0.249621, acc 0.90625\n",
      "2017-11-06T17:08:15.749662: step 784, loss 0.0314563, acc 1\n",
      "2017-11-06T17:08:19.744502: step 785, loss 0.154256, acc 0.9375\n",
      "2017-11-06T17:08:23.845415: step 786, loss 0.085294, acc 0.96875\n",
      "2017-11-06T17:08:28.001367: step 787, loss 0.481466, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T17:08:32.634659: step 788, loss 0.321145, acc 0.875\n",
      "2017-11-06T17:08:36.950726: step 789, loss 0.113813, acc 0.96875\n",
      "2017-11-06T17:08:40.913542: step 790, loss 0.246427, acc 0.875\n",
      "2017-11-06T17:08:44.850339: step 791, loss 0.265902, acc 0.90625\n",
      "2017-11-06T17:08:47.367127: step 792, loss 0.849531, acc 0.75\n",
      "2017-11-06T17:08:51.299922: step 793, loss 0.184871, acc 0.9375\n",
      "2017-11-06T17:08:55.259737: step 794, loss 0.150685, acc 0.9375\n",
      "2017-11-06T17:08:59.357647: step 795, loss 0.216251, acc 0.90625\n",
      "2017-11-06T17:09:03.731875: step 796, loss 0.444698, acc 0.875\n",
      "2017-11-06T17:09:08.031431: step 797, loss 0.452159, acc 0.875\n",
      "2017-11-06T17:09:12.160865: step 798, loss 0.496854, acc 0.90625\n",
      "2017-11-06T17:09:16.180243: step 799, loss 0.18934, acc 0.90625\n",
      "2017-11-06T17:09:20.173582: step 800, loss 0.0747864, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:09:22.830472: step 800, loss 1.2728, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-800\n",
      "\n",
      "2017-11-06T17:09:28.330983: step 801, loss 0.526487, acc 0.8125\n",
      "2017-11-06T17:09:32.251750: step 802, loss 0.125988, acc 0.9375\n",
      "2017-11-06T17:09:36.401698: step 803, loss 0.265954, acc 0.90625\n",
      "2017-11-06T17:09:40.680739: step 804, loss 0.0307691, acc 1\n",
      "2017-11-06T17:09:44.614533: step 805, loss 0.683888, acc 0.78125\n",
      "2017-11-06T17:09:48.573347: step 806, loss 0.0210792, acc 1\n",
      "2017-11-06T17:09:52.585197: step 807, loss 0.273946, acc 0.90625\n",
      "2017-11-06T17:09:56.583039: step 808, loss 0.216672, acc 0.875\n",
      "2017-11-06T17:10:00.628913: step 809, loss 0.28423, acc 0.875\n",
      "2017-11-06T17:10:04.701807: step 810, loss 0.175795, acc 0.9375\n",
      "2017-11-06T17:10:08.655616: step 811, loss 0.125206, acc 0.90625\n",
      "2017-11-06T17:10:12.641448: step 812, loss 0.295698, acc 0.9375\n",
      "2017-11-06T17:10:16.614271: step 813, loss 0.157161, acc 0.90625\n",
      "2017-11-06T17:10:20.568080: step 814, loss 0.371492, acc 0.84375\n",
      "2017-11-06T17:10:24.577931: step 815, loss 0.310113, acc 0.90625\n",
      "2017-11-06T17:10:28.552754: step 816, loss 0.146188, acc 0.96875\n",
      "2017-11-06T17:10:32.587622: step 817, loss 0.466696, acc 0.875\n",
      "2017-11-06T17:10:36.717555: step 818, loss 0.0624262, acc 1\n",
      "2017-11-06T17:10:40.734409: step 819, loss 0.527869, acc 0.8125\n",
      "2017-11-06T17:10:45.131535: step 820, loss 0.0688337, acc 0.96875\n",
      "2017-11-06T17:10:49.173406: step 821, loss 0.187606, acc 0.875\n",
      "2017-11-06T17:10:53.163241: step 822, loss 0.186963, acc 0.9375\n",
      "2017-11-06T17:10:57.104041: step 823, loss 0.119689, acc 0.9375\n",
      "2017-11-06T17:11:01.111689: step 824, loss 0.150791, acc 0.96875\n",
      "2017-11-06T17:11:05.065499: step 825, loss 0.41186, acc 0.875\n",
      "2017-11-06T17:11:09.120379: step 826, loss 0.292737, acc 0.90625\n",
      "2017-11-06T17:11:13.081194: step 827, loss 0.403726, acc 0.875\n",
      "2017-11-06T17:11:15.701056: step 828, loss 0.564103, acc 0.9\n",
      "2017-11-06T17:11:19.731921: step 829, loss 0.293271, acc 0.8125\n",
      "2017-11-06T17:11:23.834835: step 830, loss 0.329101, acc 0.90625\n",
      "2017-11-06T17:11:27.881711: step 831, loss 0.356965, acc 0.875\n",
      "2017-11-06T17:11:31.838522: step 832, loss 0.105534, acc 0.9375\n",
      "2017-11-06T17:11:35.859379: step 833, loss 0.218476, acc 0.90625\n",
      "2017-11-06T17:11:39.837205: step 834, loss 0.418372, acc 0.90625\n",
      "2017-11-06T17:11:43.783009: step 835, loss 0.418124, acc 0.84375\n",
      "2017-11-06T17:11:47.975988: step 836, loss 0.112364, acc 0.9375\n",
      "2017-11-06T17:11:52.291055: step 837, loss 0.370561, acc 0.84375\n",
      "2017-11-06T17:11:56.265878: step 838, loss 0.0546994, acc 0.96875\n",
      "2017-11-06T17:12:00.217687: step 839, loss 0.0119761, acc 1\n",
      "2017-11-06T17:12:04.184505: step 840, loss 0.14437, acc 0.96875\n",
      "2017-11-06T17:12:08.307436: step 841, loss 0.381906, acc 0.90625\n",
      "2017-11-06T17:12:12.288263: step 842, loss 0.372699, acc 0.84375\n",
      "2017-11-06T17:12:16.260085: step 843, loss 0.213169, acc 0.90625\n",
      "2017-11-06T17:12:20.243916: step 844, loss 0.193803, acc 0.9375\n",
      "2017-11-06T17:12:24.222743: step 845, loss 0.182881, acc 0.9375\n",
      "2017-11-06T17:12:28.214581: step 846, loss 0.73251, acc 0.8125\n",
      "2017-11-06T17:12:32.145372: step 847, loss 0.531302, acc 0.84375\n",
      "2017-11-06T17:12:36.367372: step 848, loss 0.527032, acc 0.875\n",
      "2017-11-06T17:12:40.357207: step 849, loss 0.193245, acc 0.9375\n",
      "2017-11-06T17:12:44.274991: step 850, loss 0.11326, acc 0.9375\n",
      "2017-11-06T17:12:48.218793: step 851, loss 0.167848, acc 0.96875\n",
      "2017-11-06T17:12:52.276677: step 852, loss 0.301107, acc 0.875\n",
      "2017-11-06T17:12:56.718833: step 853, loss 0.078966, acc 0.96875\n",
      "2017-11-06T17:13:00.735687: step 854, loss 0.0566238, acc 1\n",
      "2017-11-06T17:13:04.676487: step 855, loss 0.263206, acc 0.90625\n",
      "2017-11-06T17:13:08.669324: step 856, loss 0.155737, acc 0.90625\n",
      "2017-11-06T17:13:12.648151: step 857, loss 0.273038, acc 0.875\n",
      "2017-11-06T17:13:16.701031: step 858, loss 0.596449, acc 0.8125\n",
      "2017-11-06T17:13:20.768922: step 859, loss 0.29829, acc 0.90625\n",
      "2017-11-06T17:13:24.910865: step 860, loss 0.0416684, acc 1\n",
      "2017-11-06T17:13:29.192908: step 861, loss 0.317191, acc 0.8125\n",
      "2017-11-06T17:13:33.201757: step 862, loss 0.176354, acc 0.9375\n",
      "2017-11-06T17:13:37.122541: step 863, loss 0.632528, acc 0.78125\n",
      "2017-11-06T17:13:39.667349: step 864, loss 0.933934, acc 0.8\n",
      "2017-11-06T17:13:43.662188: step 865, loss 0.229482, acc 0.90625\n",
      "2017-11-06T17:13:47.663031: step 866, loss 0.0102604, acc 1\n",
      "2017-11-06T17:13:51.685889: step 867, loss 0.0082318, acc 1\n",
      "2017-11-06T17:13:55.667719: step 868, loss 0.870164, acc 0.84375\n",
      "2017-11-06T17:13:59.778393: step 869, loss 0.591191, acc 0.90625\n",
      "2017-11-06T17:14:04.087455: step 870, loss 0.365011, acc 0.90625\n",
      "2017-11-06T17:14:08.068284: step 871, loss 0.222763, acc 0.875\n",
      "2017-11-06T17:14:11.986067: step 872, loss 0.0779561, acc 0.96875\n",
      "2017-11-06T17:14:16.017933: step 873, loss 0.422687, acc 0.875\n",
      "2017-11-06T17:14:19.971742: step 874, loss 0.675261, acc 0.875\n",
      "2017-11-06T17:14:23.929554: step 875, loss 0.297881, acc 0.875\n",
      "2017-11-06T17:14:27.929396: step 876, loss 0.493221, acc 0.90625\n",
      "2017-11-06T17:14:31.907223: step 877, loss 0.141102, acc 0.9375\n",
      "2017-11-06T17:14:36.157242: step 878, loss 0.334687, acc 0.90625\n",
      "2017-11-06T17:14:40.098042: step 879, loss 0.0931127, acc 0.9375\n",
      "2017-11-06T17:14:44.072866: step 880, loss 0.243458, acc 0.9375\n",
      "2017-11-06T17:14:48.023674: step 881, loss 0.219608, acc 0.9375\n",
      "2017-11-06T17:14:52.029520: step 882, loss 0.367291, acc 0.9375\n",
      "2017-11-06T17:14:55.974325: step 883, loss 0.330115, acc 0.84375\n",
      "2017-11-06T17:14:59.925130: step 884, loss 0.0791712, acc 0.9375\n",
      "2017-11-06T17:15:03.993021: step 885, loss 0.282563, acc 0.90625\n",
      "2017-11-06T17:15:08.395148: step 886, loss 0.240394, acc 0.90625\n",
      "2017-11-06T17:15:12.383984: step 887, loss 0.378084, acc 0.875\n",
      "2017-11-06T17:15:16.384825: step 888, loss 0.223688, acc 0.90625\n",
      "2017-11-06T17:15:20.332631: step 889, loss 0.0857336, acc 0.9375\n",
      "2017-11-06T17:15:24.278436: step 890, loss 0.512548, acc 0.875\n",
      "2017-11-06T17:15:28.283280: step 891, loss 0.399481, acc 0.84375\n",
      "2017-11-06T17:15:32.309222: step 892, loss 0.22231, acc 0.9375\n",
      "2017-11-06T17:15:36.297056: step 893, loss 0.515126, acc 0.84375\n",
      "2017-11-06T17:15:40.286891: step 894, loss 0.578422, acc 0.8125\n",
      "2017-11-06T17:15:44.240700: step 895, loss 0.0931216, acc 0.96875\n",
      "2017-11-06T17:15:48.206518: step 896, loss 0.427299, acc 0.8125\n",
      "2017-11-06T17:15:52.140313: step 897, loss 0.143846, acc 0.9375\n",
      "2017-11-06T17:15:56.120141: step 898, loss 0.367253, acc 0.84375\n",
      "2017-11-06T17:16:00.077953: step 899, loss 0.233118, acc 0.875\n",
      "2017-11-06T17:16:02.655784: step 900, loss 0.488239, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:16:05.265639: step 900, loss 1.08032, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-900\n",
      "\n",
      "2017-11-06T17:16:11.003011: step 901, loss 0.232833, acc 0.9375\n",
      "2017-11-06T17:16:15.273046: step 902, loss 0.0674964, acc 0.96875\n",
      "2017-11-06T17:16:19.309914: step 903, loss 0.249947, acc 0.875\n",
      "2017-11-06T17:16:23.243709: step 904, loss 0.148243, acc 0.9375\n",
      "2017-11-06T17:16:27.263565: step 905, loss 0.159665, acc 0.9375\n",
      "2017-11-06T17:16:31.271413: step 906, loss 0.308308, acc 0.90625\n",
      "2017-11-06T17:16:35.444378: step 907, loss 0.135499, acc 0.96875\n",
      "2017-11-06T17:16:39.477244: step 908, loss 0.22714, acc 0.875\n",
      "2017-11-06T17:16:43.449066: step 909, loss 0.974962, acc 0.75\n",
      "2017-11-06T17:16:47.423890: step 910, loss 0.139775, acc 0.9375\n",
      "2017-11-06T17:16:51.419729: step 911, loss 0.530714, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T17:16:55.536655: step 912, loss 0.175083, acc 0.96875\n",
      "2017-11-06T17:16:59.482287: step 913, loss 0.398865, acc 0.90625\n",
      "2017-11-06T17:17:03.524159: step 914, loss 0.296375, acc 0.9375\n",
      "2017-11-06T17:17:07.508990: step 915, loss 0.386377, acc 0.84375\n",
      "2017-11-06T17:17:11.444787: step 916, loss 0.520816, acc 0.8125\n",
      "2017-11-06T17:17:15.581727: step 917, loss 0.383752, acc 0.875\n",
      "2017-11-06T17:17:19.975849: step 918, loss 0.204435, acc 0.90625\n",
      "2017-11-06T17:17:23.918650: step 919, loss 0.187173, acc 0.90625\n",
      "2017-11-06T17:17:27.872459: step 920, loss 0.370466, acc 0.84375\n",
      "2017-11-06T17:17:31.904325: step 921, loss 0.0308677, acc 1\n",
      "2017-11-06T17:17:35.931207: step 922, loss 0.109873, acc 0.90625\n",
      "2017-11-06T17:17:39.851993: step 923, loss 0.63093, acc 0.84375\n",
      "2017-11-06T17:17:43.877854: step 924, loss 0.0372844, acc 1\n",
      "2017-11-06T17:17:47.835667: step 925, loss 0.140229, acc 0.9375\n",
      "2017-11-06T17:17:51.831506: step 926, loss 0.340338, acc 0.90625\n",
      "2017-11-06T17:17:55.821340: step 927, loss 0.0182356, acc 1\n",
      "2017-11-06T17:17:59.739124: step 928, loss 0.0750252, acc 0.9375\n",
      "2017-11-06T17:18:03.687930: step 929, loss 0.366461, acc 0.875\n",
      "2017-11-06T17:18:07.698780: step 930, loss 0.352281, acc 0.875\n",
      "2017-11-06T17:18:11.649587: step 931, loss 0.303213, acc 0.84375\n",
      "2017-11-06T17:18:15.640423: step 932, loss 0.192233, acc 0.96875\n",
      "2017-11-06T17:18:19.574219: step 933, loss 0.292983, acc 0.84375\n",
      "2017-11-06T17:18:24.111441: step 934, loss 0.287971, acc 0.90625\n",
      "2017-11-06T17:18:28.312428: step 935, loss 0.581109, acc 0.875\n",
      "2017-11-06T17:18:30.839223: step 936, loss 0.326528, acc 0.8\n",
      "2017-11-06T17:18:35.029200: step 937, loss 0.186061, acc 0.90625\n",
      "2017-11-06T17:18:38.991015: step 938, loss 0.273189, acc 0.84375\n",
      "2017-11-06T17:18:42.972844: step 939, loss 0.19743, acc 0.9375\n",
      "2017-11-06T17:18:47.002707: step 940, loss 0.516482, acc 0.84375\n",
      "2017-11-06T17:18:50.900477: step 941, loss 0.219965, acc 0.9375\n",
      "2017-11-06T17:18:54.904321: step 942, loss 0.353297, acc 0.8125\n",
      "2017-11-06T17:18:58.834113: step 943, loss 0.261597, acc 0.875\n",
      "2017-11-06T17:19:02.787924: step 944, loss 0.416516, acc 0.84375\n",
      "2017-11-06T17:19:06.709709: step 945, loss 0.312995, acc 0.9375\n",
      "2017-11-06T17:19:10.612483: step 946, loss 0.0494857, acc 1\n",
      "2017-11-06T17:19:14.633340: step 947, loss 0.426263, acc 0.84375\n",
      "2017-11-06T17:19:18.663203: step 948, loss 0.088827, acc 0.96875\n",
      "2017-11-06T17:19:22.671052: step 949, loss 0.105804, acc 0.9375\n",
      "2017-11-06T17:19:26.771965: step 950, loss 0.119269, acc 0.9375\n",
      "2017-11-06T17:19:31.118055: step 951, loss 0.180142, acc 0.875\n",
      "2017-11-06T17:19:35.118896: step 952, loss 0.080123, acc 0.9375\n",
      "2017-11-06T17:19:39.085770: step 953, loss 0.399485, acc 0.8125\n",
      "2017-11-06T17:19:43.019566: step 954, loss 0.153181, acc 0.96875\n",
      "2017-11-06T17:19:47.056434: step 955, loss 0.464847, acc 0.875\n",
      "2017-11-06T17:19:51.015247: step 956, loss 0.209422, acc 0.90625\n",
      "2017-11-06T17:19:55.078133: step 957, loss 0.282624, acc 0.90625\n",
      "2017-11-06T17:19:59.123770: step 958, loss 0.149704, acc 0.9375\n",
      "2017-11-06T17:20:03.402810: step 959, loss 0.192214, acc 0.9375\n",
      "2017-11-06T17:20:07.370630: step 960, loss 0.114022, acc 0.96875\n",
      "2017-11-06T17:20:11.370472: step 961, loss 0.418177, acc 0.875\n",
      "2017-11-06T17:20:15.331287: step 962, loss 0.321507, acc 0.875\n",
      "2017-11-06T17:20:19.330127: step 963, loss 0.0399499, acc 1\n",
      "2017-11-06T17:20:23.294944: step 964, loss 0.169862, acc 0.9375\n",
      "2017-11-06T17:20:27.324808: step 965, loss 0.469815, acc 0.875\n",
      "2017-11-06T17:20:31.308639: step 966, loss 0.182115, acc 0.90625\n",
      "2017-11-06T17:20:35.919915: step 967, loss 0.554187, acc 0.84375\n",
      "2017-11-06T17:20:39.929764: step 968, loss 0.216707, acc 0.9375\n",
      "2017-11-06T17:20:43.829535: step 969, loss 0.458988, acc 0.84375\n",
      "2017-11-06T17:20:47.761329: step 970, loss 0.0318802, acc 1\n",
      "2017-11-06T17:20:51.692122: step 971, loss 0.031395, acc 0.96875\n",
      "2017-11-06T17:20:54.259948: step 972, loss 0.0732447, acc 0.95\n",
      "2017-11-06T17:20:58.205752: step 973, loss 0.11258, acc 0.96875\n",
      "2017-11-06T17:21:02.152554: step 974, loss 0.387375, acc 0.9375\n",
      "2017-11-06T17:21:06.070339: step 975, loss 0.135018, acc 0.9375\n",
      "2017-11-06T17:21:10.000132: step 976, loss 0.0414002, acc 1\n",
      "2017-11-06T17:21:13.931924: step 977, loss 0.211407, acc 0.90625\n",
      "2017-11-06T17:21:17.813683: step 978, loss 0.0648039, acc 1\n",
      "2017-11-06T17:21:21.750480: step 979, loss 0.156501, acc 0.875\n",
      "2017-11-06T17:21:25.692280: step 980, loss 0.286456, acc 0.90625\n",
      "2017-11-06T17:21:29.701129: step 981, loss 0.109017, acc 0.96875\n",
      "2017-11-06T17:21:33.682958: step 982, loss 0.168801, acc 0.90625\n",
      "2017-11-06T17:21:37.768863: step 983, loss 0.0607865, acc 1\n",
      "2017-11-06T17:21:42.244042: step 984, loss 0.158071, acc 0.96875\n",
      "2017-11-06T17:21:46.225871: step 985, loss 0.24316, acc 0.9375\n",
      "2017-11-06T17:21:50.277750: step 986, loss 0.230296, acc 0.9375\n",
      "2017-11-06T17:21:54.239565: step 987, loss 0.216369, acc 0.9375\n",
      "2017-11-06T17:21:58.180364: step 988, loss 0.14113, acc 0.9375\n",
      "2017-11-06T17:22:02.134176: step 989, loss 0.162176, acc 0.9375\n",
      "2017-11-06T17:22:06.066968: step 990, loss 0.258257, acc 0.90625\n",
      "2017-11-06T17:22:09.991758: step 991, loss 0.230747, acc 0.84375\n",
      "2017-11-06T17:22:13.959577: step 992, loss 0.372019, acc 0.9375\n",
      "2017-11-06T17:22:17.852342: step 993, loss 0.287736, acc 0.84375\n",
      "2017-11-06T17:22:21.782134: step 994, loss 0.489764, acc 0.84375\n",
      "2017-11-06T17:22:25.735944: step 995, loss 0.198747, acc 0.90625\n",
      "2017-11-06T17:22:29.648724: step 996, loss 0.230554, acc 0.90625\n",
      "2017-11-06T17:22:33.769652: step 997, loss 0.160795, acc 0.9375\n",
      "2017-11-06T17:22:37.822532: step 998, loss 0.334463, acc 0.84375\n",
      "2017-11-06T17:22:41.751325: step 999, loss 0.127023, acc 0.9375\n",
      "2017-11-06T17:22:46.153451: step 1000, loss 0.214393, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:22:48.839360: step 1000, loss 0.912464, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-06T17:22:54.222936: step 1001, loss 0.284955, acc 0.90625\n",
      "2017-11-06T17:22:58.168740: step 1002, loss 0.279698, acc 0.90625\n",
      "2017-11-06T17:23:02.094296: step 1003, loss 0.0885205, acc 0.96875\n",
      "2017-11-06T17:23:06.006074: step 1004, loss 0.470248, acc 0.875\n",
      "2017-11-06T17:23:09.940870: step 1005, loss 0.306686, acc 0.875\n",
      "2017-11-06T17:23:13.924701: step 1006, loss 0.169641, acc 0.9375\n",
      "2017-11-06T17:23:17.882515: step 1007, loss 0.244962, acc 0.875\n",
      "2017-11-06T17:23:20.461346: step 1008, loss 0.616972, acc 0.75\n",
      "2017-11-06T17:23:24.517228: step 1009, loss 0.404918, acc 0.875\n",
      "2017-11-06T17:23:28.641157: step 1010, loss 0.226679, acc 0.9375\n",
      "2017-11-06T17:23:32.566947: step 1011, loss 0.491796, acc 0.8125\n",
      "2017-11-06T17:23:36.509749: step 1012, loss 0.284174, acc 0.84375\n",
      "2017-11-06T17:23:40.503587: step 1013, loss 0.241218, acc 0.9375\n",
      "2017-11-06T17:23:44.487436: step 1014, loss 0.142588, acc 0.90625\n",
      "2017-11-06T17:23:48.434221: step 1015, loss 0.0722816, acc 1\n",
      "2017-11-06T17:23:52.810331: step 1016, loss 0.095443, acc 0.9375\n",
      "2017-11-06T17:23:56.823182: step 1017, loss 0.222432, acc 0.9375\n",
      "2017-11-06T17:24:00.814018: step 1018, loss 0.284142, acc 0.875\n",
      "2017-11-06T17:24:04.717791: step 1019, loss 0.391712, acc 0.84375\n",
      "2017-11-06T17:24:08.664596: step 1020, loss 0.181302, acc 0.9375\n",
      "2017-11-06T17:24:12.577376: step 1021, loss 0.032989, acc 1\n",
      "2017-11-06T17:24:16.595231: step 1022, loss 0.199373, acc 0.9375\n",
      "2017-11-06T17:24:20.542035: step 1023, loss 0.252236, acc 0.9375\n",
      "2017-11-06T17:24:24.534872: step 1024, loss 0.0385125, acc 0.96875\n",
      "2017-11-06T17:24:28.481677: step 1025, loss 0.363727, acc 0.875\n",
      "2017-11-06T17:24:32.473513: step 1026, loss 0.133508, acc 0.9375\n",
      "2017-11-06T17:24:36.571425: step 1027, loss 0.301808, acc 0.90625\n",
      "2017-11-06T17:24:40.581274: step 1028, loss 0.137171, acc 0.96875\n",
      "2017-11-06T17:24:44.530080: step 1029, loss 0.368087, acc 0.875\n",
      "2017-11-06T17:24:48.437857: step 1030, loss 0.349186, acc 0.84375\n",
      "2017-11-06T17:24:52.373653: step 1031, loss 0.33735, acc 0.8125\n",
      "2017-11-06T17:24:56.574638: step 1032, loss 0.27371, acc 0.90625\n",
      "2017-11-06T17:25:00.712578: step 1033, loss 0.134655, acc 0.96875\n",
      "2017-11-06T17:25:04.666387: step 1034, loss 0.206088, acc 0.9375\n",
      "2017-11-06T17:25:08.573163: step 1035, loss 0.10321, acc 0.96875\n",
      "2017-11-06T17:25:12.492951: step 1036, loss 0.426147, acc 0.84375\n",
      "2017-11-06T17:25:16.477780: step 1037, loss 0.192185, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T17:25:20.365544: step 1038, loss 0.544485, acc 0.75\n",
      "2017-11-06T17:25:24.327358: step 1039, loss 0.0934027, acc 0.9375\n",
      "2017-11-06T17:25:28.333204: step 1040, loss 0.169338, acc 0.90625\n",
      "2017-11-06T17:25:32.314032: step 1041, loss 0.214947, acc 0.875\n",
      "2017-11-06T17:25:36.242824: step 1042, loss 0.376438, acc 0.90625\n",
      "2017-11-06T17:25:40.186626: step 1043, loss 0.461644, acc 0.875\n",
      "2017-11-06T17:25:42.838510: step 1044, loss 0.0924397, acc 0.95\n",
      "2017-11-06T17:25:46.856455: step 1045, loss 0.121226, acc 0.90625\n",
      "2017-11-06T17:25:50.765233: step 1046, loss 0.3019, acc 0.90625\n",
      "2017-11-06T17:25:54.757069: step 1047, loss 0.270302, acc 0.875\n",
      "2017-11-06T17:25:58.653838: step 1048, loss 0.219684, acc 0.875\n",
      "2017-11-06T17:26:03.079725: step 1049, loss 0.209205, acc 0.875\n",
      "2017-11-06T17:26:07.214663: step 1050, loss 0.277073, acc 0.9375\n",
      "2017-11-06T17:26:11.226513: step 1051, loss 0.254953, acc 0.90625\n",
      "2017-11-06T17:26:15.180322: step 1052, loss 0.12438, acc 0.9375\n",
      "2017-11-06T17:26:19.219192: step 1053, loss 0.307799, acc 0.90625\n",
      "2017-11-06T17:26:23.131973: step 1054, loss 0.1898, acc 0.96875\n",
      "2017-11-06T17:26:27.154832: step 1055, loss 0.190268, acc 0.9375\n",
      "2017-11-06T17:26:31.104638: step 1056, loss 0.176536, acc 0.90625\n",
      "2017-11-06T17:26:35.293614: step 1057, loss 0.207161, acc 0.90625\n",
      "2017-11-06T17:26:39.265436: step 1058, loss 0.0578693, acc 0.96875\n",
      "2017-11-06T17:26:43.239260: step 1059, loss 0.358433, acc 0.875\n",
      "2017-11-06T17:26:47.194070: step 1060, loss 0.260108, acc 0.9375\n",
      "2017-11-06T17:26:51.151882: step 1061, loss 0.20826, acc 0.9375\n",
      "2017-11-06T17:26:55.248793: step 1062, loss 0.0704877, acc 0.96875\n",
      "2017-11-06T17:26:59.231623: step 1063, loss 0.146916, acc 0.9375\n",
      "2017-11-06T17:27:03.146405: step 1064, loss 0.160882, acc 0.9375\n",
      "2017-11-06T17:27:07.357397: step 1065, loss 0.142623, acc 0.96875\n",
      "2017-11-06T17:27:11.563387: step 1066, loss 0.14705, acc 0.9375\n",
      "2017-11-06T17:27:15.517194: step 1067, loss 0.267713, acc 0.875\n",
      "2017-11-06T17:27:19.505029: step 1068, loss 0.191291, acc 0.9375\n",
      "2017-11-06T17:27:23.483855: step 1069, loss 0.116884, acc 0.9375\n",
      "2017-11-06T17:27:27.458680: step 1070, loss 0.433898, acc 0.84375\n",
      "2017-11-06T17:27:31.383468: step 1071, loss 0.162169, acc 0.90625\n",
      "2017-11-06T17:27:35.340279: step 1072, loss 0.142121, acc 0.9375\n",
      "2017-11-06T17:27:39.299093: step 1073, loss 0.235591, acc 0.9375\n",
      "2017-11-06T17:27:43.190858: step 1074, loss 0.452058, acc 0.875\n",
      "2017-11-06T17:27:47.226865: step 1075, loss 0.158987, acc 0.90625\n",
      "2017-11-06T17:27:51.177670: step 1076, loss 0.11682, acc 0.90625\n",
      "2017-11-06T17:27:55.151494: step 1077, loss 0.34062, acc 0.875\n",
      "2017-11-06T17:27:59.082287: step 1078, loss 0.0568834, acc 0.96875\n",
      "2017-11-06T17:28:02.984059: step 1079, loss 0.340801, acc 0.8125\n",
      "2017-11-06T17:28:05.537874: step 1080, loss 0.414346, acc 0.9\n",
      "2017-11-06T17:28:09.448653: step 1081, loss 0.0977855, acc 0.9375\n",
      "2017-11-06T17:28:13.657643: step 1082, loss 0.205975, acc 0.84375\n",
      "2017-11-06T17:28:17.832610: step 1083, loss 0.0626959, acc 0.96875\n",
      "2017-11-06T17:28:21.876483: step 1084, loss 0.140771, acc 0.9375\n",
      "2017-11-06T17:28:26.122500: step 1085, loss 0.356278, acc 0.875\n",
      "2017-11-06T17:28:30.339496: step 1086, loss 0.200347, acc 0.90625\n",
      "2017-11-06T17:28:34.487444: step 1087, loss 0.225491, acc 0.90625\n",
      "2017-11-06T17:28:38.515306: step 1088, loss 0.361904, acc 0.84375\n",
      "2017-11-06T17:28:42.424083: step 1089, loss 0.287834, acc 0.90625\n",
      "2017-11-06T17:28:46.410917: step 1090, loss 0.230233, acc 0.9375\n",
      "2017-11-06T17:28:50.355719: step 1091, loss 0.29676, acc 0.875\n",
      "2017-11-06T17:28:54.404597: step 1092, loss 0.183613, acc 0.9375\n",
      "2017-11-06T17:28:58.386425: step 1093, loss 0.230478, acc 0.875\n",
      "2017-11-06T17:29:02.310967: step 1094, loss 0.130791, acc 0.9375\n",
      "2017-11-06T17:29:06.212741: step 1095, loss 0.127226, acc 0.9375\n",
      "2017-11-06T17:29:10.152539: step 1096, loss 0.281591, acc 0.90625\n",
      "2017-11-06T17:29:14.107351: step 1097, loss 0.0388598, acc 1\n",
      "2017-11-06T17:29:18.289320: step 1098, loss 0.282245, acc 0.875\n",
      "2017-11-06T17:29:22.460285: step 1099, loss 0.407412, acc 0.84375\n",
      "2017-11-06T17:29:26.513164: step 1100, loss 0.189772, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:29:29.095000: step 1100, loss 0.991634, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-06T17:29:34.617691: step 1101, loss 0.146886, acc 0.9375\n",
      "2017-11-06T17:29:38.547485: step 1102, loss 0.450689, acc 0.875\n",
      "2017-11-06T17:29:42.511300: step 1103, loss 0.109801, acc 0.96875\n",
      "2017-11-06T17:29:46.389055: step 1104, loss 0.330328, acc 0.875\n",
      "2017-11-06T17:29:50.477962: step 1105, loss 0.196566, acc 0.90625\n",
      "2017-11-06T17:29:54.371727: step 1106, loss 0.28365, acc 0.90625\n",
      "2017-11-06T17:29:58.323535: step 1107, loss 0.132146, acc 0.9375\n",
      "2017-11-06T17:30:02.504506: step 1108, loss 0.20315, acc 0.9375\n",
      "2017-11-06T17:30:06.476328: step 1109, loss 0.38639, acc 0.90625\n",
      "2017-11-06T17:30:10.474168: step 1110, loss 0.00560319, acc 1\n",
      "2017-11-06T17:30:14.487020: step 1111, loss 0.178363, acc 0.9375\n",
      "2017-11-06T17:30:18.410807: step 1112, loss 0.151026, acc 0.96875\n",
      "2017-11-06T17:30:22.425661: step 1113, loss 0.41452, acc 0.75\n",
      "2017-11-06T17:30:26.795766: step 1114, loss 0.0448924, acc 1\n",
      "2017-11-06T17:30:30.717552: step 1115, loss 0.398634, acc 0.875\n",
      "2017-11-06T17:30:33.419472: step 1116, loss 0.398845, acc 0.85\n",
      "2017-11-06T17:30:37.517384: step 1117, loss 0.0474262, acc 1\n",
      "2017-11-06T17:30:41.512224: step 1118, loss 0.0722067, acc 1\n",
      "2017-11-06T17:30:45.490049: step 1119, loss 0.0789324, acc 0.96875\n",
      "2017-11-06T17:30:49.454867: step 1120, loss 0.129332, acc 0.9375\n",
      "2017-11-06T17:30:53.460712: step 1121, loss 0.0446145, acc 0.96875\n",
      "2017-11-06T17:30:57.466559: step 1122, loss 0.23992, acc 0.90625\n",
      "2017-11-06T17:31:01.548460: step 1123, loss 0.0777937, acc 0.9375\n",
      "2017-11-06T17:31:05.924210: step 1124, loss 0.100941, acc 0.96875\n",
      "2017-11-06T17:31:10.252786: step 1125, loss 0.185356, acc 0.90625\n",
      "2017-11-06T17:31:14.410282: step 1126, loss 0.47983, acc 0.84375\n",
      "2017-11-06T17:31:18.428660: step 1127, loss 0.281727, acc 0.84375\n",
      "2017-11-06T17:31:22.370461: step 1128, loss 0.22546, acc 0.84375\n",
      "2017-11-06T17:31:26.392318: step 1129, loss 0.165727, acc 0.9375\n",
      "2017-11-06T17:31:30.736406: step 1130, loss 0.300978, acc 0.90625\n",
      "2017-11-06T17:31:34.948399: step 1131, loss 0.262691, acc 0.875\n",
      "2017-11-06T17:31:39.053315: step 1132, loss 0.236151, acc 0.84375\n",
      "2017-11-06T17:31:43.102193: step 1133, loss 0.191874, acc 0.9375\n",
      "2017-11-06T17:31:47.080018: step 1134, loss 0.235731, acc 0.90625\n",
      "2017-11-06T17:31:51.080861: step 1135, loss 0.264052, acc 0.90625\n",
      "2017-11-06T17:31:55.127736: step 1136, loss 0.272776, acc 0.875\n",
      "2017-11-06T17:31:59.145591: step 1137, loss 0.345848, acc 0.875\n",
      "2017-11-06T17:32:03.112187: step 1138, loss 0.0248509, acc 1\n",
      "2017-11-06T17:32:07.078006: step 1139, loss 0.171341, acc 0.90625\n",
      "2017-11-06T17:32:11.176918: step 1140, loss 0.169474, acc 0.90625\n",
      "2017-11-06T17:32:15.102707: step 1141, loss 0.065775, acc 0.96875\n",
      "2017-11-06T17:32:19.231641: step 1142, loss 0.211021, acc 0.90625\n",
      "2017-11-06T17:32:23.129411: step 1143, loss 0.0404455, acc 0.96875\n",
      "2017-11-06T17:32:27.249338: step 1144, loss 0.124798, acc 0.90625\n",
      "2017-11-06T17:32:31.182135: step 1145, loss 0.35864, acc 0.90625\n",
      "2017-11-06T17:32:35.714353: step 1146, loss 0.191187, acc 0.9375\n",
      "2017-11-06T17:32:40.022413: step 1147, loss 0.339246, acc 0.84375\n",
      "2017-11-06T17:32:43.950205: step 1148, loss 0.466703, acc 0.78125\n",
      "2017-11-06T17:32:47.985072: step 1149, loss 0.174467, acc 0.96875\n",
      "2017-11-06T17:32:52.016936: step 1150, loss 0.321923, acc 0.90625\n",
      "2017-11-06T17:32:55.965742: step 1151, loss 0.279762, acc 0.84375\n",
      "2017-11-06T17:32:58.563590: step 1152, loss 0.353566, acc 0.85\n",
      "2017-11-06T17:33:02.618469: step 1153, loss 0.33699, acc 0.84375\n",
      "2017-11-06T17:33:06.602300: step 1154, loss 0.384602, acc 0.84375\n",
      "2017-11-06T17:33:10.610148: step 1155, loss 0.120794, acc 0.9375\n",
      "2017-11-06T17:33:14.645016: step 1156, loss 0.313417, acc 0.875\n",
      "2017-11-06T17:33:18.635850: step 1157, loss 0.101273, acc 0.9375\n",
      "2017-11-06T17:33:22.630689: step 1158, loss 0.193737, acc 0.90625\n",
      "2017-11-06T17:33:26.753619: step 1159, loss 0.498629, acc 0.84375\n",
      "2017-11-06T17:33:30.797491: step 1160, loss 0.323618, acc 0.84375\n",
      "2017-11-06T17:33:34.714275: step 1161, loss 0.241022, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T17:33:38.794173: step 1162, loss 0.213745, acc 0.90625\n",
      "2017-11-06T17:33:43.352412: step 1163, loss 0.253294, acc 0.90625\n",
      "2017-11-06T17:33:47.445321: step 1164, loss 0.36628, acc 0.875\n",
      "2017-11-06T17:33:51.595270: step 1165, loss 0.305726, acc 0.875\n",
      "2017-11-06T17:33:55.702188: step 1166, loss 0.368089, acc 0.875\n",
      "2017-11-06T17:33:59.910178: step 1167, loss 0.271273, acc 0.90625\n",
      "2017-11-06T17:34:04.010090: step 1168, loss 0.12134, acc 0.9375\n",
      "2017-11-06T17:34:08.005930: step 1169, loss 0.227728, acc 0.90625\n",
      "2017-11-06T17:34:12.082827: step 1170, loss 0.268713, acc 0.875\n",
      "2017-11-06T17:34:16.153719: step 1171, loss 0.118594, acc 0.96875\n",
      "2017-11-06T17:34:20.198593: step 1172, loss 0.149505, acc 0.90625\n",
      "2017-11-06T17:34:24.381567: step 1173, loss 0.189455, acc 0.90625\n",
      "2017-11-06T17:34:28.556532: step 1174, loss 0.0975961, acc 0.96875\n",
      "2017-11-06T17:34:32.710483: step 1175, loss 0.0718058, acc 0.96875\n",
      "2017-11-06T17:34:37.038559: step 1176, loss 0.130772, acc 0.96875\n",
      "2017-11-06T17:34:41.194512: step 1177, loss 0.220923, acc 0.875\n",
      "2017-11-06T17:34:45.451537: step 1178, loss 0.157407, acc 0.90625\n",
      "2017-11-06T17:34:49.888689: step 1179, loss 0.176188, acc 0.9375\n",
      "2017-11-06T17:34:54.040639: step 1180, loss 0.0381801, acc 0.96875\n",
      "2017-11-06T17:34:58.230618: step 1181, loss 0.444398, acc 0.84375\n",
      "2017-11-06T17:35:02.282265: step 1182, loss 0.11771, acc 0.9375\n",
      "2017-11-06T17:35:06.273102: step 1183, loss 0.252612, acc 0.90625\n",
      "2017-11-06T17:35:10.198890: step 1184, loss 0.248796, acc 0.9375\n",
      "2017-11-06T17:35:14.289798: step 1185, loss 0.1993, acc 0.90625\n",
      "2017-11-06T17:35:18.281634: step 1186, loss 0.148378, acc 0.96875\n",
      "2017-11-06T17:35:22.227438: step 1187, loss 0.0545964, acc 1\n",
      "2017-11-06T17:35:24.848299: step 1188, loss 0.0948358, acc 0.95\n",
      "2017-11-06T17:35:28.896176: step 1189, loss 0.216998, acc 0.90625\n",
      "2017-11-06T17:35:32.827969: step 1190, loss 0.197634, acc 0.875\n",
      "2017-11-06T17:35:36.766768: step 1191, loss 0.179812, acc 0.9375\n",
      "2017-11-06T17:35:40.772616: step 1192, loss 0.384894, acc 0.875\n",
      "2017-11-06T17:35:44.801477: step 1193, loss 0.115328, acc 0.9375\n",
      "2017-11-06T17:35:48.735272: step 1194, loss 0.172303, acc 0.9375\n",
      "2017-11-06T17:35:52.988294: step 1195, loss 0.0598888, acc 0.96875\n",
      "2017-11-06T17:35:57.272339: step 1196, loss 0.216148, acc 0.875\n",
      "2017-11-06T17:36:01.336226: step 1197, loss 0.248305, acc 0.875\n",
      "2017-11-06T17:36:05.272024: step 1198, loss 0.0972146, acc 0.9375\n",
      "2017-11-06T17:36:09.241843: step 1199, loss 0.225402, acc 0.90625\n",
      "2017-11-06T17:36:13.234680: step 1200, loss 0.322668, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:36:15.851540: step 1200, loss 0.991337, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-06T17:36:21.677563: step 1201, loss 0.0895305, acc 0.96875\n",
      "2017-11-06T17:36:25.944595: step 1202, loss 0.473404, acc 0.8125\n",
      "2017-11-06T17:36:30.182607: step 1203, loss 0.229922, acc 0.9375\n",
      "2017-11-06T17:36:34.420306: step 1204, loss 0.290874, acc 0.90625\n",
      "2017-11-06T17:36:38.498210: step 1205, loss 0.214844, acc 0.84375\n",
      "2017-11-06T17:36:42.507061: step 1206, loss 0.200708, acc 0.90625\n",
      "2017-11-06T17:36:46.494895: step 1207, loss 0.239539, acc 0.875\n",
      "2017-11-06T17:36:50.568789: step 1208, loss 0.127014, acc 0.96875\n",
      "2017-11-06T17:36:54.535608: step 1209, loss 0.269366, acc 0.84375\n",
      "2017-11-06T17:36:59.043811: step 1210, loss 0.113183, acc 0.96875\n",
      "2017-11-06T17:37:03.225782: step 1211, loss 0.19728, acc 0.90625\n",
      "2017-11-06T17:37:07.225625: step 1212, loss 0.193745, acc 0.875\n",
      "2017-11-06T17:37:11.234473: step 1213, loss 0.0324796, acc 1\n",
      "2017-11-06T17:37:15.235316: step 1214, loss 0.129564, acc 0.9375\n",
      "2017-11-06T17:37:19.260176: step 1215, loss 0.163286, acc 0.875\n",
      "2017-11-06T17:37:23.273027: step 1216, loss 0.151094, acc 0.96875\n",
      "2017-11-06T17:37:27.245849: step 1217, loss 0.221883, acc 0.90625\n",
      "2017-11-06T17:37:31.183649: step 1218, loss 0.184559, acc 0.96875\n",
      "2017-11-06T17:37:35.225520: step 1219, loss 0.124548, acc 0.9375\n",
      "2017-11-06T17:37:39.240372: step 1220, loss 0.14393, acc 0.90625\n",
      "2017-11-06T17:37:43.196183: step 1221, loss 0.233131, acc 0.9375\n",
      "2017-11-06T17:37:47.120972: step 1222, loss 0.614675, acc 0.6875\n",
      "2017-11-06T17:37:51.072780: step 1223, loss 0.464699, acc 0.8125\n",
      "2017-11-06T17:37:53.872059: step 1224, loss 0.0871646, acc 0.95\n",
      "2017-11-06T17:37:57.870901: step 1225, loss 0.12944, acc 0.9375\n",
      "2017-11-06T17:38:01.969579: step 1226, loss 0.0519995, acc 0.96875\n",
      "2017-11-06T17:38:06.237611: step 1227, loss 0.114759, acc 0.96875\n",
      "2017-11-06T17:38:10.175409: step 1228, loss 0.13828, acc 0.96875\n",
      "2017-11-06T17:38:14.192264: step 1229, loss 0.206022, acc 0.9375\n",
      "2017-11-06T17:38:18.196108: step 1230, loss 0.083459, acc 0.96875\n",
      "2017-11-06T17:38:22.231976: step 1231, loss 0.273277, acc 0.9375\n",
      "2017-11-06T17:38:26.426957: step 1232, loss 0.340095, acc 0.84375\n",
      "2017-11-06T17:38:30.474833: step 1233, loss 0.0374392, acc 0.96875\n",
      "2017-11-06T17:38:34.748870: step 1234, loss 0.297821, acc 0.8125\n",
      "2017-11-06T17:38:38.789742: step 1235, loss 0.155552, acc 0.90625\n",
      "2017-11-06T17:38:42.745552: step 1236, loss 0.29374, acc 0.90625\n",
      "2017-11-06T17:38:46.715373: step 1237, loss 0.0624371, acc 1\n",
      "2017-11-06T17:38:50.670183: step 1238, loss 0.191922, acc 0.90625\n",
      "2017-11-06T17:38:54.683034: step 1239, loss 0.0312117, acc 1\n",
      "2017-11-06T17:38:58.611826: step 1240, loss 0.22616, acc 0.84375\n",
      "2017-11-06T17:39:02.588651: step 1241, loss 0.148819, acc 0.9375\n",
      "2017-11-06T17:39:06.569481: step 1242, loss 0.35151, acc 0.84375\n",
      "2017-11-06T17:39:10.966604: step 1243, loss 0.262246, acc 0.90625\n",
      "2017-11-06T17:39:15.089534: step 1244, loss 0.336619, acc 0.875\n",
      "2017-11-06T17:39:19.137410: step 1245, loss 0.292139, acc 0.90625\n",
      "2017-11-06T17:39:23.150261: step 1246, loss 0.26573, acc 0.90625\n",
      "2017-11-06T17:39:27.134092: step 1247, loss 0.110906, acc 0.96875\n",
      "2017-11-06T17:39:31.167958: step 1248, loss 0.243239, acc 0.90625\n",
      "2017-11-06T17:39:35.169801: step 1249, loss 0.366686, acc 0.84375\n",
      "2017-11-06T17:39:39.125612: step 1250, loss 0.237714, acc 0.875\n",
      "2017-11-06T17:39:43.081422: step 1251, loss 0.483747, acc 0.8125\n",
      "2017-11-06T17:39:47.064253: step 1252, loss 0.118362, acc 0.9375\n",
      "2017-11-06T17:39:51.011057: step 1253, loss 0.0185652, acc 1\n",
      "2017-11-06T17:39:54.953859: step 1254, loss 0.228811, acc 0.90625\n",
      "2017-11-06T17:39:58.950699: step 1255, loss 0.140015, acc 0.96875\n",
      "2017-11-06T17:40:03.221733: step 1256, loss 0.228317, acc 0.9375\n",
      "2017-11-06T17:40:07.193555: step 1257, loss 0.27023, acc 0.8125\n",
      "2017-11-06T17:40:11.155371: step 1258, loss 0.341922, acc 0.875\n",
      "2017-11-06T17:40:15.356356: step 1259, loss 0.191036, acc 0.875\n",
      "2017-11-06T17:40:18.181363: step 1260, loss 0.404097, acc 0.85\n",
      "2017-11-06T17:40:22.238245: step 1261, loss 0.165001, acc 0.9375\n",
      "2017-11-06T17:40:26.179046: step 1262, loss 0.0974462, acc 0.96875\n",
      "2017-11-06T17:40:30.187894: step 1263, loss 0.27489, acc 0.875\n",
      "2017-11-06T17:40:34.372867: step 1264, loss 0.0805656, acc 0.96875\n",
      "2017-11-06T17:40:38.405733: step 1265, loss 0.0888435, acc 0.96875\n",
      "2017-11-06T17:40:42.381558: step 1266, loss 0.225715, acc 0.875\n",
      "2017-11-06T17:40:46.338370: step 1267, loss 0.348585, acc 0.84375\n",
      "2017-11-06T17:40:50.341214: step 1268, loss 0.0279105, acc 1\n",
      "2017-11-06T17:40:54.268004: step 1269, loss 0.190397, acc 0.96875\n",
      "2017-11-06T17:40:58.251834: step 1270, loss 0.221958, acc 0.875\n",
      "2017-11-06T17:41:02.251471: step 1271, loss 0.256308, acc 0.84375\n",
      "2017-11-06T17:41:06.198276: step 1272, loss 0.357683, acc 0.84375\n",
      "2017-11-06T17:41:10.478317: step 1273, loss 0.259515, acc 0.875\n",
      "2017-11-06T17:41:15.577942: step 1274, loss 0.303166, acc 0.84375\n",
      "2017-11-06T17:41:19.708876: step 1275, loss 0.227703, acc 0.90625\n",
      "2017-11-06T17:41:24.189059: step 1276, loss 0.351067, acc 0.78125\n",
      "2017-11-06T17:41:28.163883: step 1277, loss 0.0965623, acc 0.96875\n",
      "2017-11-06T17:41:32.087671: step 1278, loss 0.121187, acc 0.96875\n",
      "2017-11-06T17:41:36.037479: step 1279, loss 0.131115, acc 0.96875\n",
      "2017-11-06T17:41:40.004297: step 1280, loss 0.119402, acc 0.9375\n",
      "2017-11-06T17:41:43.993132: step 1281, loss 0.292893, acc 0.84375\n",
      "2017-11-06T17:41:47.942937: step 1282, loss 0.0760564, acc 0.9375\n",
      "2017-11-06T17:41:52.005824: step 1283, loss 0.622955, acc 0.78125\n",
      "2017-11-06T17:41:55.995659: step 1284, loss 0.297349, acc 0.90625\n",
      "2017-11-06T17:42:00.004566: step 1285, loss 0.0940819, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T17:42:03.976388: step 1286, loss 0.0215534, acc 1\n",
      "2017-11-06T17:42:07.913185: step 1287, loss 0.238669, acc 0.90625\n",
      "2017-11-06T17:42:11.901019: step 1288, loss 0.155244, acc 0.90625\n",
      "2017-11-06T17:42:15.863835: step 1289, loss 0.183007, acc 0.9375\n",
      "2017-11-06T17:42:19.996771: step 1290, loss 0.356881, acc 0.90625\n",
      "2017-11-06T17:42:24.044648: step 1291, loss 0.216349, acc 0.90625\n",
      "2017-11-06T17:42:28.538841: step 1292, loss 0.235972, acc 0.9375\n",
      "2017-11-06T17:42:32.658769: step 1293, loss 0.0316038, acc 1\n",
      "2017-11-06T17:42:36.909789: step 1294, loss 0.140106, acc 0.9375\n",
      "2017-11-06T17:42:40.860596: step 1295, loss 0.173142, acc 0.9375\n",
      "2017-11-06T17:42:43.447435: step 1296, loss 0.0437487, acc 1\n",
      "2017-11-06T17:42:47.489306: step 1297, loss 0.0513045, acc 1\n",
      "2017-11-06T17:42:51.433109: step 1298, loss 0.123132, acc 0.9375\n",
      "2017-11-06T17:42:55.455967: step 1299, loss 0.0127652, acc 1\n",
      "2017-11-06T17:42:59.372751: step 1300, loss 0.219952, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:43:02.015630: step 1300, loss 0.9475, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-06T17:43:07.516323: step 1301, loss 0.297881, acc 0.875\n",
      "2017-11-06T17:43:11.520167: step 1302, loss 0.236558, acc 0.90625\n",
      "2017-11-06T17:43:15.594062: step 1303, loss 0.156826, acc 0.90625\n",
      "2017-11-06T17:43:19.631931: step 1304, loss 0.201872, acc 0.90625\n",
      "2017-11-06T17:43:23.795891: step 1305, loss 0.0888429, acc 0.9375\n",
      "2017-11-06T17:43:27.932829: step 1306, loss 0.0223536, acc 1\n",
      "2017-11-06T17:43:32.198860: step 1307, loss 0.216499, acc 0.9375\n",
      "2017-11-06T17:43:36.334801: step 1308, loss 0.128076, acc 0.96875\n",
      "2017-11-06T17:43:40.337644: step 1309, loss 0.136608, acc 0.96875\n",
      "2017-11-06T17:43:44.388521: step 1310, loss 0.25578, acc 0.875\n",
      "2017-11-06T17:43:48.415383: step 1311, loss 0.150252, acc 0.875\n",
      "2017-11-06T17:43:52.477269: step 1312, loss 0.301086, acc 0.875\n",
      "2017-11-06T17:43:56.619212: step 1313, loss 0.174721, acc 0.9375\n",
      "2017-11-06T17:44:00.652078: step 1314, loss 0.369102, acc 0.84375\n",
      "2017-11-06T17:44:04.682705: step 1315, loss 0.213813, acc 0.875\n",
      "2017-11-06T17:44:08.671540: step 1316, loss 0.0963853, acc 0.96875\n",
      "2017-11-06T17:44:12.705405: step 1317, loss 0.151881, acc 0.96875\n",
      "2017-11-06T17:44:16.703247: step 1318, loss 0.261716, acc 0.875\n",
      "2017-11-06T17:44:20.739114: step 1319, loss 0.169054, acc 0.9375\n",
      "2017-11-06T17:44:24.781988: step 1320, loss 0.116927, acc 0.9375\n",
      "2017-11-06T17:44:28.844873: step 1321, loss 0.295682, acc 0.84375\n",
      "2017-11-06T17:44:33.012835: step 1322, loss 0.280841, acc 0.875\n",
      "2017-11-06T17:44:37.474004: step 1323, loss 0.440842, acc 0.8125\n",
      "2017-11-06T17:44:41.712016: step 1324, loss 0.303659, acc 0.84375\n",
      "2017-11-06T17:44:45.817934: step 1325, loss 0.263509, acc 0.90625\n",
      "2017-11-06T17:44:49.797762: step 1326, loss 0.0416629, acc 1\n",
      "2017-11-06T17:44:53.878660: step 1327, loss 0.0618143, acc 0.96875\n",
      "2017-11-06T17:44:57.842477: step 1328, loss 0.248814, acc 0.875\n",
      "2017-11-06T17:45:01.898359: step 1329, loss 0.538519, acc 0.84375\n",
      "2017-11-06T17:45:05.960245: step 1330, loss 0.434038, acc 0.78125\n",
      "2017-11-06T17:45:09.989108: step 1331, loss 0.404963, acc 0.875\n",
      "2017-11-06T17:45:12.606968: step 1332, loss 0.26191, acc 0.9\n",
      "2017-11-06T17:45:16.633830: step 1333, loss 0.140622, acc 0.9375\n",
      "2017-11-06T17:45:20.582634: step 1334, loss 0.125973, acc 0.90625\n",
      "2017-11-06T17:45:24.595486: step 1335, loss 0.192616, acc 0.9375\n",
      "2017-11-06T17:45:28.624349: step 1336, loss 0.210526, acc 0.90625\n",
      "2017-11-06T17:45:32.599173: step 1337, loss 0.315644, acc 0.90625\n",
      "2017-11-06T17:45:36.568994: step 1338, loss 0.24623, acc 0.84375\n",
      "2017-11-06T17:45:40.676913: step 1339, loss 0.0251477, acc 0.96875\n",
      "2017-11-06T17:45:45.054023: step 1340, loss 0.248054, acc 0.84375\n",
      "2017-11-06T17:45:49.195966: step 1341, loss 0.364787, acc 0.875\n",
      "2017-11-06T17:45:53.269861: step 1342, loss 0.284202, acc 0.90625\n",
      "2017-11-06T17:45:57.219667: step 1343, loss 0.162161, acc 0.9375\n",
      "2017-11-06T17:46:01.303569: step 1344, loss 0.189995, acc 0.9375\n",
      "2017-11-06T17:46:05.262382: step 1345, loss 0.197235, acc 0.9375\n",
      "2017-11-06T17:46:09.272233: step 1346, loss 0.120755, acc 0.9375\n",
      "2017-11-06T17:46:13.344124: step 1347, loss 0.146947, acc 0.9375\n",
      "2017-11-06T17:46:17.352973: step 1348, loss 0.320081, acc 0.90625\n",
      "2017-11-06T17:46:21.350813: step 1349, loss 0.355506, acc 0.84375\n",
      "2017-11-06T17:46:25.369669: step 1350, loss 0.244898, acc 0.90625\n",
      "2017-11-06T17:46:29.416544: step 1351, loss 0.255318, acc 0.875\n",
      "2017-11-06T17:46:33.578501: step 1352, loss 0.292913, acc 0.875\n",
      "2017-11-06T17:46:37.718443: step 1353, loss 0.290676, acc 0.875\n",
      "2017-11-06T17:46:41.773325: step 1354, loss 0.210183, acc 0.90625\n",
      "2017-11-06T17:46:45.846218: step 1355, loss 0.162094, acc 0.90625\n",
      "2017-11-06T17:46:50.224330: step 1356, loss 0.210878, acc 0.84375\n",
      "2017-11-06T17:46:54.429316: step 1357, loss 0.202071, acc 0.9375\n",
      "2017-11-06T17:46:58.492204: step 1358, loss 0.0522492, acc 1\n",
      "2017-11-06T17:47:02.513825: step 1359, loss 0.0887982, acc 0.9375\n",
      "2017-11-06T17:47:06.503660: step 1360, loss 0.11449, acc 0.9375\n",
      "2017-11-06T17:47:10.529520: step 1361, loss 0.139835, acc 0.9375\n",
      "2017-11-06T17:47:14.540371: step 1362, loss 0.151269, acc 0.9375\n",
      "2017-11-06T17:47:18.553221: step 1363, loss 0.357669, acc 0.78125\n",
      "2017-11-06T17:47:22.533049: step 1364, loss 0.162143, acc 0.9375\n",
      "2017-11-06T17:47:26.569916: step 1365, loss 0.4361, acc 0.84375\n",
      "2017-11-06T17:47:30.571761: step 1366, loss 0.187449, acc 0.90625\n",
      "2017-11-06T17:47:34.546586: step 1367, loss 0.134219, acc 0.9375\n",
      "2017-11-06T17:47:37.140428: step 1368, loss 0.290123, acc 0.85\n",
      "2017-11-06T17:47:41.209318: step 1369, loss 0.220597, acc 0.90625\n",
      "2017-11-06T17:47:45.166132: step 1370, loss 0.222, acc 0.875\n",
      "2017-11-06T17:47:49.187989: step 1371, loss 0.355405, acc 0.84375\n",
      "2017-11-06T17:47:53.355950: step 1372, loss 0.130534, acc 0.96875\n",
      "2017-11-06T17:47:57.694032: step 1373, loss 0.255761, acc 0.84375\n",
      "2017-11-06T17:48:01.794946: step 1374, loss 0.234656, acc 0.84375\n",
      "2017-11-06T17:48:05.758763: step 1375, loss 0.121668, acc 0.96875\n",
      "2017-11-06T17:48:09.781621: step 1376, loss 0.103166, acc 0.96875\n",
      "2017-11-06T17:48:13.862520: step 1377, loss 0.223133, acc 0.90625\n",
      "2017-11-06T17:48:17.842348: step 1378, loss 0.0930639, acc 0.96875\n",
      "2017-11-06T17:48:22.010310: step 1379, loss 0.214404, acc 0.875\n",
      "2017-11-06T17:48:26.309364: step 1380, loss 0.154508, acc 0.90625\n",
      "2017-11-06T17:48:30.329221: step 1381, loss 0.157005, acc 0.9375\n",
      "2017-11-06T17:48:34.582244: step 1382, loss 0.192703, acc 0.90625\n",
      "2017-11-06T17:48:38.681155: step 1383, loss 0.265622, acc 0.875\n",
      "2017-11-06T17:48:42.660983: step 1384, loss 0.200161, acc 0.84375\n",
      "2017-11-06T17:48:46.656823: step 1385, loss 0.144845, acc 0.90625\n",
      "2017-11-06T17:48:50.613634: step 1386, loss 0.1807, acc 0.90625\n",
      "2017-11-06T17:48:54.664512: step 1387, loss 0.0635854, acc 0.96875\n",
      "2017-11-06T17:48:58.807456: step 1388, loss 0.0856126, acc 0.96875\n",
      "2017-11-06T17:49:03.211585: step 1389, loss 0.218693, acc 0.84375\n",
      "2017-11-06T17:49:07.229439: step 1390, loss 0.427536, acc 0.8125\n",
      "2017-11-06T17:49:11.193256: step 1391, loss 0.244066, acc 0.90625\n",
      "2017-11-06T17:49:15.219116: step 1392, loss 0.145522, acc 0.875\n",
      "2017-11-06T17:49:19.316028: step 1393, loss 0.338696, acc 0.90625\n",
      "2017-11-06T17:49:23.239816: step 1394, loss 0.173018, acc 0.90625\n",
      "2017-11-06T17:49:27.230651: step 1395, loss 0.0692521, acc 0.96875\n",
      "2017-11-06T17:49:31.230494: step 1396, loss 0.133283, acc 0.9375\n",
      "2017-11-06T17:49:35.276368: step 1397, loss 0.00519094, acc 1\n",
      "2017-11-06T17:49:39.270207: step 1398, loss 0.264674, acc 0.90625\n",
      "2017-11-06T17:49:43.317082: step 1399, loss 0.0438205, acc 1\n",
      "2017-11-06T17:49:47.383972: step 1400, loss 0.0564328, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:49:50.079887: step 1400, loss 0.964766, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-06T17:49:55.498357: step 1401, loss 0.362407, acc 0.8125\n",
      "2017-11-06T17:49:59.501199: step 1402, loss 0.125868, acc 0.9375\n",
      "2017-11-06T17:50:03.909453: step 1403, loss 0.151407, acc 0.9375\n",
      "2017-11-06T17:50:06.761480: step 1404, loss 0.390707, acc 0.8\n",
      "2017-11-06T17:50:10.976476: step 1405, loss 0.416173, acc 0.78125\n",
      "2017-11-06T17:50:14.977316: step 1406, loss 0.390743, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T17:50:19.001176: step 1407, loss 0.229369, acc 0.90625\n",
      "2017-11-06T17:50:22.928967: step 1408, loss 0.129909, acc 0.9375\n",
      "2017-11-06T17:50:26.873770: step 1409, loss 0.387854, acc 0.84375\n",
      "2017-11-06T17:50:30.807565: step 1410, loss 0.186004, acc 0.90625\n",
      "2017-11-06T17:50:35.008550: step 1411, loss 0.168984, acc 0.875\n",
      "2017-11-06T17:50:39.018399: step 1412, loss 0.127763, acc 0.9375\n",
      "2017-11-06T17:50:42.970207: step 1413, loss 0.222453, acc 0.90625\n",
      "2017-11-06T17:50:46.946033: step 1414, loss 0.167455, acc 0.9375\n",
      "2017-11-06T17:50:50.956882: step 1415, loss 0.149022, acc 0.90625\n",
      "2017-11-06T17:50:54.919698: step 1416, loss 0.472164, acc 0.75\n",
      "2017-11-06T17:50:58.869505: step 1417, loss 0.452825, acc 0.875\n",
      "2017-11-06T17:51:02.808303: step 1418, loss 0.2613, acc 0.84375\n",
      "2017-11-06T17:51:06.730090: step 1419, loss 0.0790873, acc 0.96875\n",
      "2017-11-06T17:51:10.874035: step 1420, loss 0.243848, acc 0.9375\n",
      "2017-11-06T17:51:15.158078: step 1421, loss 0.0952953, acc 0.96875\n",
      "2017-11-06T17:51:19.197948: step 1422, loss 0.397271, acc 0.84375\n",
      "2017-11-06T17:51:23.116733: step 1423, loss 0.0934753, acc 0.96875\n",
      "2017-11-06T17:51:27.088555: step 1424, loss 0.211469, acc 0.90625\n",
      "2017-11-06T17:51:31.051371: step 1425, loss 0.0964336, acc 0.9375\n",
      "2017-11-06T17:51:35.078232: step 1426, loss 0.129066, acc 0.9375\n",
      "2017-11-06T17:51:38.999019: step 1427, loss 0.191725, acc 0.90625\n",
      "2017-11-06T17:51:42.994857: step 1428, loss 0.146236, acc 0.96875\n",
      "2017-11-06T17:51:46.989696: step 1429, loss 0.232977, acc 0.90625\n",
      "2017-11-06T17:51:50.957515: step 1430, loss 0.130337, acc 0.90625\n",
      "2017-11-06T17:51:54.893311: step 1431, loss 0.116466, acc 0.9375\n",
      "2017-11-06T17:51:58.857127: step 1432, loss 0.111881, acc 0.96875\n",
      "2017-11-06T17:52:02.773910: step 1433, loss 0.248838, acc 0.875\n",
      "2017-11-06T17:52:06.786782: step 1434, loss 0.415181, acc 0.8125\n",
      "2017-11-06T17:52:10.751600: step 1435, loss 0.125236, acc 0.9375\n",
      "2017-11-06T17:52:14.707411: step 1436, loss 0.272154, acc 0.90625\n",
      "2017-11-06T17:52:19.126551: step 1437, loss 0.130436, acc 0.9375\n",
      "2017-11-06T17:52:23.224464: step 1438, loss 0.0895898, acc 1\n",
      "2017-11-06T17:52:27.319371: step 1439, loss 0.200144, acc 0.9375\n",
      "2017-11-06T17:52:29.851171: step 1440, loss 0.211545, acc 0.9\n",
      "2017-11-06T17:52:34.078174: step 1441, loss 0.325022, acc 0.875\n",
      "2017-11-06T17:52:38.183091: step 1442, loss 0.205018, acc 0.90625\n",
      "2017-11-06T17:52:42.229967: step 1443, loss 0.155345, acc 0.90625\n",
      "2017-11-06T17:52:46.242818: step 1444, loss 0.252456, acc 0.84375\n",
      "2017-11-06T17:52:50.310708: step 1445, loss 0.398445, acc 0.8125\n",
      "2017-11-06T17:52:54.386604: step 1446, loss 0.122807, acc 0.96875\n",
      "2017-11-06T17:52:58.384445: step 1447, loss 0.0950672, acc 0.9375\n",
      "2017-11-06T17:53:02.463098: step 1448, loss 0.314727, acc 0.875\n",
      "2017-11-06T17:53:06.530989: step 1449, loss 0.204743, acc 0.9375\n",
      "2017-11-06T17:53:10.678936: step 1450, loss 0.26973, acc 0.84375\n",
      "2017-11-06T17:53:14.716805: step 1451, loss 0.230424, acc 0.875\n",
      "2017-11-06T17:53:18.748670: step 1452, loss 0.34927, acc 0.875\n",
      "2017-11-06T17:53:23.097760: step 1453, loss 0.441391, acc 0.84375\n",
      "2017-11-06T17:53:27.297745: step 1454, loss 0.0604161, acc 1\n",
      "2017-11-06T17:53:31.287579: step 1455, loss 0.13327, acc 0.90625\n",
      "2017-11-06T17:53:35.355470: step 1456, loss 0.307669, acc 0.875\n",
      "2017-11-06T17:53:39.349307: step 1457, loss 0.488949, acc 0.84375\n",
      "2017-11-06T17:53:43.300115: step 1458, loss 0.327442, acc 0.90625\n",
      "2017-11-06T17:53:47.256926: step 1459, loss 0.369876, acc 0.875\n",
      "2017-11-06T17:53:51.214739: step 1460, loss 0.19195, acc 0.9375\n",
      "2017-11-06T17:53:55.244603: step 1461, loss 0.266179, acc 0.875\n",
      "2017-11-06T17:53:59.233436: step 1462, loss 0.185116, acc 0.90625\n",
      "2017-11-06T17:54:03.208261: step 1463, loss 0.0496506, acc 0.96875\n",
      "2017-11-06T17:54:07.232119: step 1464, loss 0.449236, acc 0.875\n",
      "2017-11-06T17:54:11.211948: step 1465, loss 0.102874, acc 0.96875\n",
      "2017-11-06T17:54:15.189775: step 1466, loss 0.029821, acc 1\n",
      "2017-11-06T17:54:19.205627: step 1467, loss 0.0804235, acc 0.96875\n",
      "2017-11-06T17:54:23.393603: step 1468, loss 0.327821, acc 0.90625\n",
      "2017-11-06T17:54:27.825752: step 1469, loss 0.335081, acc 0.90625\n",
      "2017-11-06T17:54:32.225879: step 1470, loss 0.15152, acc 0.90625\n",
      "2017-11-06T17:54:36.444877: step 1471, loss 0.260881, acc 0.90625\n",
      "2017-11-06T17:54:40.447720: step 1472, loss 0.327835, acc 0.90625\n",
      "2017-11-06T17:54:44.386519: step 1473, loss 0.361414, acc 0.875\n",
      "2017-11-06T17:54:48.394367: step 1474, loss 0.349685, acc 0.8125\n",
      "2017-11-06T17:54:52.364188: step 1475, loss 0.0873902, acc 0.9375\n",
      "2017-11-06T17:54:54.888982: step 1476, loss 0.343254, acc 0.85\n",
      "2017-11-06T17:54:58.911840: step 1477, loss 0.163645, acc 0.9375\n",
      "2017-11-06T17:55:02.912683: step 1478, loss 0.219087, acc 0.90625\n",
      "2017-11-06T17:55:06.904519: step 1479, loss 0.226884, acc 0.90625\n",
      "2017-11-06T17:55:10.883346: step 1480, loss 0.303101, acc 0.90625\n",
      "2017-11-06T17:55:14.860172: step 1481, loss 0.138693, acc 0.96875\n",
      "2017-11-06T17:55:18.854010: step 1482, loss 0.305042, acc 0.84375\n",
      "2017-11-06T17:55:22.779800: step 1483, loss 0.17214, acc 0.90625\n",
      "2017-11-06T17:55:26.851692: step 1484, loss 0.10842, acc 0.96875\n",
      "2017-11-06T17:55:30.813507: step 1485, loss 0.192347, acc 0.9375\n",
      "2017-11-06T17:55:35.197623: step 1486, loss 0.271382, acc 0.84375\n",
      "2017-11-06T17:55:39.355577: step 1487, loss 0.185487, acc 0.96875\n",
      "2017-11-06T17:55:43.303383: step 1488, loss 0.152663, acc 0.90625\n",
      "2017-11-06T17:55:47.257192: step 1489, loss 0.0465945, acc 0.96875\n",
      "2017-11-06T17:55:51.289057: step 1490, loss 0.0932081, acc 0.96875\n",
      "2017-11-06T17:55:55.296904: step 1491, loss 0.19201, acc 0.90625\n",
      "2017-11-06T17:55:59.242708: step 1492, loss 0.166862, acc 0.90625\n",
      "2017-11-06T17:56:03.307367: step 1493, loss 0.141097, acc 0.96875\n",
      "2017-11-06T17:56:07.281190: step 1494, loss 0.313168, acc 0.90625\n",
      "2017-11-06T17:56:11.238002: step 1495, loss 0.1872, acc 0.875\n",
      "2017-11-06T17:56:15.242847: step 1496, loss 0.271757, acc 0.84375\n",
      "2017-11-06T17:56:19.314740: step 1497, loss 0.0545527, acc 1\n",
      "2017-11-06T17:56:23.254540: step 1498, loss 0.168884, acc 0.9375\n",
      "2017-11-06T17:56:27.318427: step 1499, loss 0.32972, acc 0.84375\n",
      "2017-11-06T17:56:31.331279: step 1500, loss 0.109507, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T17:56:34.177301: step 1500, loss 0.829176, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-06T17:56:40.249615: step 1501, loss 0.162271, acc 0.9375\n",
      "2017-11-06T17:56:44.436591: step 1502, loss 0.137261, acc 0.96875\n",
      "2017-11-06T17:56:48.440436: step 1503, loss 0.200447, acc 0.84375\n",
      "2017-11-06T17:56:52.458290: step 1504, loss 0.243026, acc 0.84375\n",
      "2017-11-06T17:56:56.520176: step 1505, loss 0.18574, acc 0.9375\n",
      "2017-11-06T17:57:00.662120: step 1506, loss 0.497095, acc 0.84375\n",
      "2017-11-06T17:57:04.654957: step 1507, loss 0.317404, acc 0.8125\n",
      "2017-11-06T17:57:08.756872: step 1508, loss 0.221322, acc 0.9375\n",
      "2017-11-06T17:57:12.835769: step 1509, loss 0.141885, acc 0.90625\n",
      "2017-11-06T17:57:16.901659: step 1510, loss 0.143177, acc 0.96875\n",
      "2017-11-06T17:57:20.911507: step 1511, loss 0.0936287, acc 0.96875\n",
      "2017-11-06T17:57:23.510354: step 1512, loss 0.307094, acc 0.85\n",
      "2017-11-06T17:57:27.654298: step 1513, loss 0.0844715, acc 0.96875\n",
      "2017-11-06T17:57:31.675155: step 1514, loss 0.406536, acc 0.8125\n",
      "2017-11-06T17:57:35.716027: step 1515, loss 0.318828, acc 0.84375\n",
      "2017-11-06T17:57:39.774911: step 1516, loss 0.28899, acc 0.875\n",
      "2017-11-06T17:57:43.940871: step 1517, loss 0.208234, acc 0.9375\n",
      "2017-11-06T17:57:48.327988: step 1518, loss 0.221666, acc 0.875\n",
      "2017-11-06T17:57:52.431904: step 1519, loss 0.24254, acc 0.875\n",
      "2017-11-06T17:57:56.499794: step 1520, loss 0.121995, acc 0.96875\n",
      "2017-11-06T17:58:00.547672: step 1521, loss 0.179002, acc 0.90625\n",
      "2017-11-06T17:58:04.591544: step 1522, loss 0.40566, acc 0.875\n",
      "2017-11-06T17:58:08.660435: step 1523, loss 0.159638, acc 0.96875\n",
      "2017-11-06T17:58:12.747339: step 1524, loss 0.442516, acc 0.84375\n",
      "2017-11-06T17:58:16.817231: step 1525, loss 0.170109, acc 0.90625\n",
      "2017-11-06T17:58:20.815071: step 1526, loss 0.400519, acc 0.8125\n",
      "2017-11-06T17:58:24.964020: step 1527, loss 0.160115, acc 0.9375\n",
      "2017-11-06T17:58:29.167006: step 1528, loss 0.153644, acc 0.90625\n",
      "2017-11-06T17:58:33.319957: step 1529, loss 0.0411117, acc 1\n",
      "2017-11-06T17:58:37.533951: step 1530, loss 0.249599, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T17:58:41.540798: step 1531, loss 0.0734947, acc 0.9375\n",
      "2017-11-06T17:58:45.583671: step 1532, loss 0.168391, acc 0.90625\n",
      "2017-11-06T17:58:49.750631: step 1533, loss 0.132151, acc 0.9375\n",
      "2017-11-06T17:58:54.065698: step 1534, loss 0.0968076, acc 0.9375\n",
      "2017-11-06T17:58:58.161608: step 1535, loss 0.177806, acc 0.84375\n",
      "2017-11-06T17:59:02.149441: step 1536, loss 0.65902, acc 0.84375\n",
      "2017-11-06T17:59:06.115027: step 1537, loss 0.20971, acc 0.9375\n",
      "2017-11-06T17:59:10.105863: step 1538, loss 0.332907, acc 0.875\n",
      "2017-11-06T17:59:14.147735: step 1539, loss 0.380479, acc 0.84375\n",
      "2017-11-06T17:59:18.151580: step 1540, loss 0.186209, acc 0.90625\n",
      "2017-11-06T17:59:22.232479: step 1541, loss 0.0819649, acc 0.96875\n",
      "2017-11-06T17:59:26.226317: step 1542, loss 0.0424102, acc 0.96875\n",
      "2017-11-06T17:59:30.275194: step 1543, loss 0.288474, acc 0.8125\n",
      "2017-11-06T17:59:34.323071: step 1544, loss 0.178379, acc 0.90625\n",
      "2017-11-06T17:59:38.430989: step 1545, loss 0.192739, acc 0.90625\n",
      "2017-11-06T17:59:42.432833: step 1546, loss 0.0738596, acc 0.96875\n",
      "2017-11-06T17:59:46.449689: step 1547, loss 0.0439989, acc 0.96875\n",
      "2017-11-06T17:59:49.018512: step 1548, loss 0.0111157, acc 1\n",
      "2017-11-06T17:59:53.037367: step 1549, loss 0.202418, acc 0.875\n",
      "2017-11-06T17:59:57.415478: step 1550, loss 0.224104, acc 0.84375\n",
      "2017-11-06T18:00:01.883653: step 1551, loss 0.303187, acc 0.90625\n",
      "2017-11-06T18:00:05.906511: step 1552, loss 0.138458, acc 0.9375\n",
      "2017-11-06T18:00:09.942379: step 1553, loss 0.289804, acc 0.875\n",
      "2017-11-06T18:00:13.982249: step 1554, loss 0.262072, acc 0.84375\n",
      "2017-11-06T18:00:17.957074: step 1555, loss 0.179527, acc 0.90625\n",
      "2017-11-06T18:00:21.924893: step 1556, loss 0.20516, acc 0.875\n",
      "2017-11-06T18:00:25.960761: step 1557, loss 0.35325, acc 0.8125\n",
      "2017-11-06T18:00:30.018644: step 1558, loss 0.166742, acc 0.90625\n",
      "2017-11-06T18:00:34.274668: step 1559, loss 0.105994, acc 0.90625\n",
      "2017-11-06T18:00:38.357569: step 1560, loss 0.119206, acc 0.96875\n",
      "2017-11-06T18:00:42.343401: step 1561, loss 0.0915191, acc 0.96875\n",
      "2017-11-06T18:00:46.373264: step 1562, loss 0.349874, acc 0.875\n",
      "2017-11-06T18:00:50.332077: step 1563, loss 0.138619, acc 0.9375\n",
      "2017-11-06T18:00:54.315909: step 1564, loss 0.224317, acc 0.84375\n",
      "2017-11-06T18:00:58.349774: step 1565, loss 0.0697156, acc 0.96875\n",
      "2017-11-06T18:01:02.676849: step 1566, loss 0.207771, acc 0.84375\n",
      "2017-11-06T18:01:06.880837: step 1567, loss 0.220062, acc 0.84375\n",
      "2017-11-06T18:01:10.943723: step 1568, loss 0.168674, acc 0.90625\n",
      "2017-11-06T18:01:14.945569: step 1569, loss 0.143075, acc 0.90625\n",
      "2017-11-06T18:01:18.973428: step 1570, loss 0.0728968, acc 0.96875\n",
      "2017-11-06T18:01:22.998288: step 1571, loss 0.264955, acc 0.875\n",
      "2017-11-06T18:01:27.050168: step 1572, loss 0.0718253, acc 0.96875\n",
      "2017-11-06T18:01:31.097043: step 1573, loss 0.24601, acc 0.90625\n",
      "2017-11-06T18:01:35.170939: step 1574, loss 0.0606941, acc 0.96875\n",
      "2017-11-06T18:01:39.389936: step 1575, loss 0.2207, acc 0.9375\n",
      "2017-11-06T18:01:43.522873: step 1576, loss 0.189222, acc 0.90625\n",
      "2017-11-06T18:01:47.501699: step 1577, loss 0.126394, acc 0.9375\n",
      "2017-11-06T18:01:51.499539: step 1578, loss 0.305837, acc 0.875\n",
      "2017-11-06T18:01:55.518394: step 1579, loss 0.233975, acc 0.875\n",
      "2017-11-06T18:01:59.565270: step 1580, loss 0.0991255, acc 0.96875\n",
      "2017-11-06T18:02:03.521909: step 1581, loss 0.0907007, acc 0.96875\n",
      "2017-11-06T18:02:07.822965: step 1582, loss 0.0960651, acc 1\n",
      "2017-11-06T18:02:12.155043: step 1583, loss 0.0868007, acc 0.96875\n",
      "2017-11-06T18:02:14.742882: step 1584, loss 0.223571, acc 0.85\n",
      "2017-11-06T18:02:18.824782: step 1585, loss 0.392106, acc 0.84375\n",
      "2017-11-06T18:02:22.865653: step 1586, loss 0.260804, acc 0.84375\n",
      "2017-11-06T18:02:26.895517: step 1587, loss 0.0787342, acc 0.96875\n",
      "2017-11-06T18:02:30.989425: step 1588, loss 0.137061, acc 0.875\n",
      "2017-11-06T18:02:35.323505: step 1589, loss 0.148206, acc 0.90625\n",
      "2017-11-06T18:02:39.399400: step 1590, loss 0.369661, acc 0.84375\n",
      "2017-11-06T18:02:43.490307: step 1591, loss 0.336744, acc 0.84375\n",
      "2017-11-06T18:02:47.448120: step 1592, loss 0.311675, acc 0.8125\n",
      "2017-11-06T18:02:51.430950: step 1593, loss 0.177091, acc 0.90625\n",
      "2017-11-06T18:02:55.555880: step 1594, loss 0.158506, acc 0.90625\n",
      "2017-11-06T18:02:59.570734: step 1595, loss 0.148282, acc 0.9375\n",
      "2017-11-06T18:03:03.602598: step 1596, loss 0.137081, acc 0.96875\n",
      "2017-11-06T18:03:07.552405: step 1597, loss 0.218581, acc 0.875\n",
      "2017-11-06T18:03:11.623297: step 1598, loss 0.160091, acc 0.9375\n",
      "2017-11-06T18:03:16.111487: step 1599, loss 0.183773, acc 0.90625\n",
      "2017-11-06T18:03:20.096318: step 1600, loss 0.0954203, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:03:22.763213: step 1600, loss 0.802622, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-06T18:03:28.510821: step 1601, loss 0.20855, acc 0.9375\n",
      "2017-11-06T18:03:32.475639: step 1602, loss 0.227695, acc 0.8125\n",
      "2017-11-06T18:03:36.484487: step 1603, loss 0.0236548, acc 1\n",
      "2017-11-06T18:03:40.581400: step 1604, loss 0.299458, acc 0.84375\n",
      "2017-11-06T18:03:44.643284: step 1605, loss 0.228293, acc 0.90625\n",
      "2017-11-06T18:03:48.640124: step 1606, loss 0.431114, acc 0.84375\n",
      "2017-11-06T18:03:52.740037: step 1607, loss 0.172708, acc 0.9375\n",
      "2017-11-06T18:03:56.723868: step 1608, loss 0.0753781, acc 0.9375\n",
      "2017-11-06T18:04:00.706697: step 1609, loss 0.126838, acc 0.96875\n",
      "2017-11-06T18:04:04.758578: step 1610, loss 0.103106, acc 0.9375\n",
      "2017-11-06T18:04:08.765424: step 1611, loss 0.0953235, acc 0.96875\n",
      "2017-11-06T18:04:12.939398: step 1612, loss 0.218991, acc 0.875\n",
      "2017-11-06T18:04:17.024302: step 1613, loss 0.164223, acc 0.90625\n",
      "2017-11-06T18:04:21.473462: step 1614, loss 0.156623, acc 0.90625\n",
      "2017-11-06T18:04:25.647427: step 1615, loss 0.109054, acc 0.9375\n",
      "2017-11-06T18:04:29.676291: step 1616, loss 0.224892, acc 0.90625\n",
      "2017-11-06T18:04:33.860263: step 1617, loss 0.295547, acc 0.90625\n",
      "2017-11-06T18:04:38.036230: step 1618, loss 0.154157, acc 0.90625\n",
      "2017-11-06T18:04:42.162162: step 1619, loss 0.22444, acc 0.90625\n",
      "2017-11-06T18:04:44.779022: step 1620, loss 0.128281, acc 0.95\n",
      "2017-11-06T18:04:48.972001: step 1621, loss 0.372815, acc 0.84375\n",
      "2017-11-06T18:04:53.039891: step 1622, loss 0.127629, acc 0.9375\n",
      "2017-11-06T18:04:57.220862: step 1623, loss 0.280818, acc 0.875\n",
      "2017-11-06T18:05:01.327780: step 1624, loss 0.304179, acc 0.90625\n",
      "2017-11-06T18:05:05.430450: step 1625, loss 0.286534, acc 0.875\n",
      "2017-11-06T18:05:09.381257: step 1626, loss 0.0967395, acc 0.96875\n",
      "2017-11-06T18:05:13.325061: step 1627, loss 0.246408, acc 0.8125\n",
      "2017-11-06T18:05:17.332907: step 1628, loss 0.0929327, acc 0.96875\n",
      "2017-11-06T18:05:21.339754: step 1629, loss 0.185826, acc 0.90625\n",
      "2017-11-06T18:05:25.585770: step 1630, loss 0.301647, acc 0.875\n",
      "2017-11-06T18:05:29.752731: step 1631, loss 0.301224, acc 0.90625\n",
      "2017-11-06T18:05:33.711545: step 1632, loss 0.0991981, acc 0.96875\n",
      "2017-11-06T18:05:37.696376: step 1633, loss 0.227216, acc 0.90625\n",
      "2017-11-06T18:05:41.718233: step 1634, loss 0.18868, acc 0.90625\n",
      "2017-11-06T18:05:45.723100: step 1635, loss 0.113006, acc 0.9375\n",
      "2017-11-06T18:05:49.714916: step 1636, loss 0.114392, acc 0.9375\n",
      "2017-11-06T18:05:53.685737: step 1637, loss 0.11599, acc 0.9375\n",
      "2017-11-06T18:05:57.791654: step 1638, loss 0.123381, acc 0.9375\n",
      "2017-11-06T18:06:01.823519: step 1639, loss 0.103147, acc 0.9375\n",
      "2017-11-06T18:06:05.798343: step 1640, loss 0.0851249, acc 0.96875\n",
      "2017-11-06T18:06:09.714127: step 1641, loss 0.119068, acc 0.9375\n",
      "2017-11-06T18:06:13.763083: step 1642, loss 0.374149, acc 0.8125\n",
      "2017-11-06T18:06:17.762925: step 1643, loss 0.424238, acc 0.875\n",
      "2017-11-06T18:06:21.722738: step 1644, loss 0.136148, acc 0.90625\n",
      "2017-11-06T18:06:25.734590: step 1645, loss 0.147801, acc 0.90625\n",
      "2017-11-06T18:06:30.027641: step 1646, loss 0.220122, acc 0.90625\n",
      "2017-11-06T18:06:34.537844: step 1647, loss 0.169817, acc 0.9375\n",
      "2017-11-06T18:06:38.580717: step 1648, loss 0.136439, acc 0.9375\n",
      "2017-11-06T18:06:42.607578: step 1649, loss 0.176168, acc 0.90625\n",
      "2017-11-06T18:06:46.577399: step 1650, loss 0.175071, acc 0.875\n",
      "2017-11-06T18:06:50.550222: step 1651, loss 0.189093, acc 0.90625\n",
      "2017-11-06T18:06:54.537056: step 1652, loss 0.0829389, acc 0.9375\n",
      "2017-11-06T18:06:58.543902: step 1653, loss 0.289192, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T18:07:02.683844: step 1654, loss 0.134317, acc 0.9375\n",
      "2017-11-06T18:07:06.634651: step 1655, loss 0.308215, acc 0.84375\n",
      "2017-11-06T18:07:09.200473: step 1656, loss 0.108377, acc 0.95\n",
      "2017-11-06T18:07:13.266362: step 1657, loss 0.093587, acc 0.96875\n",
      "2017-11-06T18:07:17.290222: step 1658, loss 0.147786, acc 0.9375\n",
      "2017-11-06T18:07:21.264045: step 1659, loss 0.178539, acc 0.9375\n",
      "2017-11-06T18:07:25.260885: step 1660, loss 0.128736, acc 0.96875\n",
      "2017-11-06T18:07:29.288748: step 1661, loss 0.110643, acc 0.9375\n",
      "2017-11-06T18:07:33.309604: step 1662, loss 0.14467, acc 0.96875\n",
      "2017-11-06T18:07:37.666700: step 1663, loss 0.268362, acc 0.9375\n",
      "2017-11-06T18:07:41.804641: step 1664, loss 0.175553, acc 0.90625\n",
      "2017-11-06T18:07:45.848514: step 1665, loss 0.41162, acc 0.84375\n",
      "2017-11-06T18:07:49.843352: step 1666, loss 0.0444407, acc 0.96875\n",
      "2017-11-06T18:07:53.831185: step 1667, loss 0.204976, acc 0.9375\n",
      "2017-11-06T18:07:57.859050: step 1668, loss 0.219391, acc 0.90625\n",
      "2017-11-06T18:08:01.823865: step 1669, loss 0.222613, acc 0.90625\n",
      "2017-11-06T18:08:05.802454: step 1670, loss 0.136101, acc 0.9375\n",
      "2017-11-06T18:08:09.771273: step 1671, loss 0.171829, acc 0.90625\n",
      "2017-11-06T18:08:13.836163: step 1672, loss 0.275707, acc 0.875\n",
      "2017-11-06T18:08:17.896046: step 1673, loss 0.221204, acc 0.875\n",
      "2017-11-06T18:08:22.036990: step 1674, loss 0.112196, acc 0.96875\n",
      "2017-11-06T18:08:26.317030: step 1675, loss 0.0679617, acc 1\n",
      "2017-11-06T18:08:30.413940: step 1676, loss 0.0681475, acc 0.96875\n",
      "2017-11-06T18:08:34.648950: step 1677, loss 0.171264, acc 0.90625\n",
      "2017-11-06T18:08:38.737855: step 1678, loss 0.0433871, acc 0.96875\n",
      "2017-11-06T18:08:43.256066: step 1679, loss 0.195325, acc 0.90625\n",
      "2017-11-06T18:08:47.418023: step 1680, loss 0.17066, acc 0.90625\n",
      "2017-11-06T18:08:51.440881: step 1681, loss 0.248845, acc 0.90625\n",
      "2017-11-06T18:08:55.501767: step 1682, loss 0.271127, acc 0.875\n",
      "2017-11-06T18:08:59.509614: step 1683, loss 0.0691021, acc 0.96875\n",
      "2017-11-06T18:09:03.521466: step 1684, loss 0.159377, acc 0.9375\n",
      "2017-11-06T18:09:07.531315: step 1685, loss 0.201849, acc 0.875\n",
      "2017-11-06T18:09:11.530155: step 1686, loss 0.146346, acc 0.9375\n",
      "2017-11-06T18:09:15.514987: step 1687, loss 0.0795457, acc 0.9375\n",
      "2017-11-06T18:09:19.506823: step 1688, loss 0.444183, acc 0.84375\n",
      "2017-11-06T18:09:23.578717: step 1689, loss 0.286559, acc 0.84375\n",
      "2017-11-06T18:09:27.585564: step 1690, loss 0.229167, acc 0.84375\n",
      "2017-11-06T18:09:31.570397: step 1691, loss 0.156543, acc 0.90625\n",
      "2017-11-06T18:09:34.157233: step 1692, loss 0.0527294, acc 1\n",
      "2017-11-06T18:09:38.184095: step 1693, loss 0.0908734, acc 0.96875\n",
      "2017-11-06T18:09:42.212957: step 1694, loss 0.113961, acc 0.96875\n",
      "2017-11-06T18:09:46.418946: step 1695, loss 0.198836, acc 0.875\n",
      "2017-11-06T18:09:50.826079: step 1696, loss 0.200092, acc 0.875\n",
      "2017-11-06T18:09:54.846933: step 1697, loss 0.0730821, acc 0.96875\n",
      "2017-11-06T18:09:58.825761: step 1698, loss 0.0425644, acc 1\n",
      "2017-11-06T18:10:03.111807: step 1699, loss 0.0610726, acc 0.96875\n",
      "2017-11-06T18:10:07.117653: step 1700, loss 0.409783, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:10:09.737514: step 1700, loss 0.835044, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-06T18:10:15.377699: step 1701, loss 0.24417, acc 0.84375\n",
      "2017-11-06T18:10:19.435582: step 1702, loss 0.258749, acc 0.84375\n",
      "2017-11-06T18:10:23.433422: step 1703, loss 0.165698, acc 0.9375\n",
      "2017-11-06T18:10:27.484301: step 1704, loss 0.0915588, acc 0.9375\n",
      "2017-11-06T18:10:31.458125: step 1705, loss 0.220043, acc 0.875\n",
      "2017-11-06T18:10:35.710148: step 1706, loss 0.0951754, acc 0.9375\n",
      "2017-11-06T18:10:39.749016: step 1707, loss 0.214371, acc 0.90625\n",
      "2017-11-06T18:10:43.746856: step 1708, loss 0.348969, acc 0.84375\n",
      "2017-11-06T18:10:47.779724: step 1709, loss 0.144955, acc 0.90625\n",
      "2017-11-06T18:10:51.966696: step 1710, loss 0.201727, acc 0.90625\n",
      "2017-11-06T18:10:56.297775: step 1711, loss 0.0561149, acc 1\n",
      "2017-11-06T18:11:00.293614: step 1712, loss 0.243918, acc 0.875\n",
      "2017-11-06T18:11:04.346494: step 1713, loss 0.270501, acc 0.875\n",
      "2017-11-06T18:11:08.302089: step 1714, loss 0.0443699, acc 0.96875\n",
      "2017-11-06T18:11:12.341961: step 1715, loss 0.264451, acc 0.875\n",
      "2017-11-06T18:11:16.338801: step 1716, loss 0.0908581, acc 0.9375\n",
      "2017-11-06T18:11:20.351651: step 1717, loss 0.244753, acc 0.875\n",
      "2017-11-06T18:11:24.362501: step 1718, loss 0.109318, acc 0.96875\n",
      "2017-11-06T18:11:28.383359: step 1719, loss 0.0296705, acc 0.96875\n",
      "2017-11-06T18:11:32.426230: step 1720, loss 0.172169, acc 0.90625\n",
      "2017-11-06T18:11:36.435079: step 1721, loss 0.102173, acc 0.96875\n",
      "2017-11-06T18:11:40.564014: step 1722, loss 0.28758, acc 0.84375\n",
      "2017-11-06T18:11:44.694948: step 1723, loss 0.279996, acc 0.84375\n",
      "2017-11-06T18:11:48.753833: step 1724, loss 0.0853104, acc 0.9375\n",
      "2017-11-06T18:11:52.741666: step 1725, loss 0.124575, acc 0.9375\n",
      "2017-11-06T18:11:56.882608: step 1726, loss 0.120675, acc 0.96875\n",
      "2017-11-06T18:12:01.290741: step 1727, loss 0.216276, acc 0.90625\n",
      "2017-11-06T18:12:03.883582: step 1728, loss 0.183943, acc 0.85\n",
      "2017-11-06T18:12:07.911445: step 1729, loss 0.196309, acc 0.90625\n",
      "2017-11-06T18:12:11.935304: step 1730, loss 0.128925, acc 0.9375\n",
      "2017-11-06T18:12:15.947154: step 1731, loss 0.297531, acc 0.875\n",
      "2017-11-06T18:12:20.228020: step 1732, loss 0.158713, acc 0.9375\n",
      "2017-11-06T18:12:24.306916: step 1733, loss 0.122076, acc 0.9375\n",
      "2017-11-06T18:12:28.346788: step 1734, loss 0.11145, acc 0.9375\n",
      "2017-11-06T18:12:32.357638: step 1735, loss 0.197106, acc 0.875\n",
      "2017-11-06T18:12:36.500581: step 1736, loss 0.010183, acc 1\n",
      "2017-11-06T18:12:40.503425: step 1737, loss 0.352403, acc 0.875\n",
      "2017-11-06T18:12:44.494262: step 1738, loss 0.245775, acc 0.875\n",
      "2017-11-06T18:12:48.606183: step 1739, loss 0.25216, acc 0.9375\n",
      "2017-11-06T18:12:52.654058: step 1740, loss 0.212205, acc 0.875\n",
      "2017-11-06T18:12:56.662907: step 1741, loss 0.121978, acc 0.9375\n",
      "2017-11-06T18:13:00.670755: step 1742, loss 0.215207, acc 0.90625\n",
      "2017-11-06T18:13:05.047865: step 1743, loss 0.353065, acc 0.875\n",
      "2017-11-06T18:13:09.219829: step 1744, loss 0.201044, acc 0.90625\n",
      "2017-11-06T18:13:13.295725: step 1745, loss 0.377333, acc 0.875\n",
      "2017-11-06T18:13:17.357612: step 1746, loss 0.355587, acc 0.875\n",
      "2017-11-06T18:13:21.334437: step 1747, loss 0.541338, acc 0.8125\n",
      "2017-11-06T18:13:25.458368: step 1748, loss 0.171665, acc 0.90625\n",
      "2017-11-06T18:13:29.651346: step 1749, loss 0.116326, acc 0.96875\n",
      "2017-11-06T18:13:33.683212: step 1750, loss 0.204955, acc 0.96875\n",
      "2017-11-06T18:13:37.742096: step 1751, loss 0.177336, acc 0.9375\n",
      "2017-11-06T18:13:41.781966: step 1752, loss 0.389134, acc 0.8125\n",
      "2017-11-06T18:13:45.812830: step 1753, loss 0.326036, acc 0.84375\n",
      "2017-11-06T18:13:49.908740: step 1754, loss 0.362815, acc 0.875\n",
      "2017-11-06T18:13:53.954615: step 1755, loss 0.384027, acc 0.84375\n",
      "2017-11-06T18:13:58.011498: step 1756, loss 0.218966, acc 0.875\n",
      "2017-11-06T18:14:02.030353: step 1757, loss 0.132406, acc 0.9375\n",
      "2017-11-06T18:14:06.071991: step 1758, loss 0.197316, acc 0.90625\n",
      "2017-11-06T18:14:10.406069: step 1759, loss 0.195838, acc 0.96875\n",
      "2017-11-06T18:14:14.641078: step 1760, loss 0.0533355, acc 1\n",
      "2017-11-06T18:14:18.720977: step 1761, loss 0.152216, acc 0.9375\n",
      "2017-11-06T18:14:22.878588: step 1762, loss 0.331777, acc 0.84375\n",
      "2017-11-06T18:14:26.952483: step 1763, loss 0.118322, acc 0.9375\n",
      "2017-11-06T18:14:29.575347: step 1764, loss 0.141188, acc 0.9\n",
      "2017-11-06T18:14:33.822364: step 1765, loss 0.0793922, acc 0.96875\n",
      "2017-11-06T18:14:37.887253: step 1766, loss 0.612548, acc 0.78125\n",
      "2017-11-06T18:14:41.890097: step 1767, loss 0.246036, acc 0.96875\n",
      "2017-11-06T18:14:45.884935: step 1768, loss 0.115616, acc 0.9375\n",
      "2017-11-06T18:14:49.926807: step 1769, loss 0.235507, acc 0.9375\n",
      "2017-11-06T18:14:53.982688: step 1770, loss 0.211163, acc 0.90625\n",
      "2017-11-06T18:14:58.035569: step 1771, loss 0.231371, acc 0.875\n",
      "2017-11-06T18:15:02.052424: step 1772, loss 0.2397, acc 0.875\n",
      "2017-11-06T18:15:06.123315: step 1773, loss 0.207936, acc 0.90625\n",
      "2017-11-06T18:15:10.135167: step 1774, loss 0.224356, acc 0.90625\n",
      "2017-11-06T18:15:14.404199: step 1775, loss 0.0772514, acc 0.96875\n",
      "2017-11-06T18:15:18.777307: step 1776, loss 0.155928, acc 0.9375\n",
      "2017-11-06T18:15:22.792159: step 1777, loss 0.10979, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T18:15:26.829028: step 1778, loss 0.214459, acc 0.90625\n",
      "2017-11-06T18:15:30.877905: step 1779, loss 0.160854, acc 0.90625\n",
      "2017-11-06T18:15:34.842722: step 1780, loss 0.197855, acc 0.96875\n",
      "2017-11-06T18:15:38.871584: step 1781, loss 0.159219, acc 0.90625\n",
      "2017-11-06T18:15:42.900447: step 1782, loss 0.461117, acc 0.8125\n",
      "2017-11-06T18:15:46.914299: step 1783, loss 0.432637, acc 0.8125\n",
      "2017-11-06T18:15:50.898130: step 1784, loss 0.114836, acc 0.9375\n",
      "2017-11-06T18:15:54.908980: step 1785, loss 0.228639, acc 0.90625\n",
      "2017-11-06T18:15:58.868795: step 1786, loss 0.193518, acc 0.875\n",
      "2017-11-06T18:16:02.790580: step 1787, loss 0.147124, acc 0.875\n",
      "2017-11-06T18:16:06.724375: step 1788, loss 0.368495, acc 0.875\n",
      "2017-11-06T18:16:10.668177: step 1789, loss 0.203747, acc 0.90625\n",
      "2017-11-06T18:16:14.586962: step 1790, loss 0.476223, acc 0.875\n",
      "2017-11-06T18:16:18.612822: step 1791, loss 0.139091, acc 0.96875\n",
      "2017-11-06T18:16:22.925887: step 1792, loss 0.0950021, acc 0.96875\n",
      "2017-11-06T18:16:26.984771: step 1793, loss 0.181155, acc 0.9375\n",
      "2017-11-06T18:16:30.881540: step 1794, loss 0.186604, acc 0.9375\n",
      "2017-11-06T18:16:35.043498: step 1795, loss 0.294807, acc 0.90625\n",
      "2017-11-06T18:16:39.024325: step 1796, loss 0.230914, acc 0.9375\n",
      "2017-11-06T18:16:43.000150: step 1797, loss 0.10083, acc 0.96875\n",
      "2017-11-06T18:16:46.979978: step 1798, loss 0.471072, acc 0.84375\n",
      "2017-11-06T18:16:50.938791: step 1799, loss 0.205416, acc 0.9375\n",
      "2017-11-06T18:16:53.490604: step 1800, loss 0.221792, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:16:56.086449: step 1800, loss 0.887788, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-06T18:17:01.706191: step 1801, loss 0.299851, acc 0.875\n",
      "2017-11-06T18:17:05.672772: step 1802, loss 0.287914, acc 0.875\n",
      "2017-11-06T18:17:09.613572: step 1803, loss 0.0119283, acc 1\n",
      "2017-11-06T18:17:13.609412: step 1804, loss 0.261399, acc 0.84375\n",
      "2017-11-06T18:17:17.588238: step 1805, loss 0.129688, acc 0.9375\n",
      "2017-11-06T18:17:21.555057: step 1806, loss 0.345178, acc 0.875\n",
      "2017-11-06T18:17:25.756043: step 1807, loss 0.218751, acc 0.875\n",
      "2017-11-06T18:17:30.106133: step 1808, loss 0.00378389, acc 1\n",
      "2017-11-06T18:17:34.064946: step 1809, loss 0.497374, acc 0.8125\n",
      "2017-11-06T18:17:37.976725: step 1810, loss 0.223005, acc 0.90625\n",
      "2017-11-06T18:17:41.967580: step 1811, loss 0.152774, acc 0.9375\n",
      "2017-11-06T18:17:45.936381: step 1812, loss 0.20179, acc 0.90625\n",
      "2017-11-06T18:17:49.883187: step 1813, loss 0.116629, acc 0.96875\n",
      "2017-11-06T18:17:53.880026: step 1814, loss 0.196537, acc 0.875\n",
      "2017-11-06T18:17:57.933906: step 1815, loss 0.423024, acc 0.8125\n",
      "2017-11-06T18:18:01.871704: step 1816, loss 0.167258, acc 0.90625\n",
      "2017-11-06T18:18:05.811503: step 1817, loss 0.232968, acc 0.875\n",
      "2017-11-06T18:18:09.759308: step 1818, loss 0.174332, acc 0.875\n",
      "2017-11-06T18:18:13.740137: step 1819, loss 0.126036, acc 0.9375\n",
      "2017-11-06T18:18:17.669929: step 1820, loss 0.287956, acc 0.8125\n",
      "2017-11-06T18:18:21.690788: step 1821, loss 0.136165, acc 0.9375\n",
      "2017-11-06T18:18:25.907783: step 1822, loss 0.074696, acc 0.96875\n",
      "2017-11-06T18:18:29.974672: step 1823, loss 0.133569, acc 0.90625\n",
      "2017-11-06T18:18:34.541917: step 1824, loss 0.178583, acc 0.96875\n",
      "2017-11-06T18:18:38.590795: step 1825, loss 0.264033, acc 0.875\n",
      "2017-11-06T18:18:42.604647: step 1826, loss 0.253495, acc 0.875\n",
      "2017-11-06T18:18:46.625504: step 1827, loss 0.205767, acc 0.90625\n",
      "2017-11-06T18:18:50.560299: step 1828, loss 0.190054, acc 0.9375\n",
      "2017-11-06T18:18:54.550134: step 1829, loss 0.235804, acc 0.90625\n",
      "2017-11-06T18:18:58.564986: step 1830, loss 0.202172, acc 0.9375\n",
      "2017-11-06T18:19:02.521799: step 1831, loss 0.23818, acc 0.84375\n",
      "2017-11-06T18:19:06.559668: step 1832, loss 0.207276, acc 0.90625\n",
      "2017-11-06T18:19:10.566514: step 1833, loss 0.261163, acc 0.84375\n",
      "2017-11-06T18:19:14.625400: step 1834, loss 0.172599, acc 0.9375\n",
      "2017-11-06T18:19:18.666269: step 1835, loss 0.21422, acc 0.90625\n",
      "2017-11-06T18:19:21.258111: step 1836, loss 0.0682363, acc 0.95\n",
      "2017-11-06T18:19:25.266959: step 1837, loss 0.156467, acc 0.96875\n",
      "2017-11-06T18:19:29.268803: step 1838, loss 0.174636, acc 0.84375\n",
      "2017-11-06T18:19:33.322683: step 1839, loss 0.357398, acc 0.84375\n",
      "2017-11-06T18:19:37.622739: step 1840, loss 0.426828, acc 0.78125\n",
      "2017-11-06T18:19:41.927798: step 1841, loss 0.238135, acc 0.875\n",
      "2017-11-06T18:19:45.981680: step 1842, loss 0.365615, acc 0.84375\n",
      "2017-11-06T18:19:49.992528: step 1843, loss 0.25555, acc 0.84375\n",
      "2017-11-06T18:19:53.998375: step 1844, loss 0.044851, acc 0.96875\n",
      "2017-11-06T18:19:58.001219: step 1845, loss 0.160454, acc 0.90625\n",
      "2017-11-06T18:20:02.379329: step 1846, loss 0.223651, acc 0.90625\n",
      "2017-11-06T18:20:06.370016: step 1847, loss 0.193394, acc 0.9375\n",
      "2017-11-06T18:20:10.368855: step 1848, loss 0.20356, acc 0.9375\n",
      "2017-11-06T18:20:14.439748: step 1849, loss 0.226062, acc 0.9375\n",
      "2017-11-06T18:20:18.395559: step 1850, loss 0.128246, acc 0.9375\n",
      "2017-11-06T18:20:22.462449: step 1851, loss 0.173563, acc 0.875\n",
      "2017-11-06T18:20:26.623406: step 1852, loss 0.17503, acc 0.90625\n",
      "2017-11-06T18:20:30.613240: step 1853, loss 0.313752, acc 0.84375\n",
      "2017-11-06T18:20:34.886276: step 1854, loss 0.102087, acc 0.96875\n",
      "2017-11-06T18:20:38.926146: step 1855, loss 0.0902975, acc 0.96875\n",
      "2017-11-06T18:20:43.178169: step 1856, loss 0.0948109, acc 0.90625\n",
      "2017-11-06T18:20:47.419183: step 1857, loss 0.194882, acc 0.90625\n",
      "2017-11-06T18:20:51.429031: step 1858, loss 0.268485, acc 0.875\n",
      "2017-11-06T18:20:55.490916: step 1859, loss 0.312634, acc 0.84375\n",
      "2017-11-06T18:20:59.487757: step 1860, loss 0.11646, acc 0.96875\n",
      "2017-11-06T18:21:03.529629: step 1861, loss 0.100959, acc 0.96875\n",
      "2017-11-06T18:21:07.534475: step 1862, loss 0.208706, acc 0.875\n",
      "2017-11-06T18:21:11.553330: step 1863, loss 0.138335, acc 0.9375\n",
      "2017-11-06T18:21:15.549169: step 1864, loss 0.114507, acc 0.9375\n",
      "2017-11-06T18:21:19.645079: step 1865, loss 0.281021, acc 0.875\n",
      "2017-11-06T18:21:23.651926: step 1866, loss 0.0855394, acc 0.9375\n",
      "2017-11-06T18:21:27.689795: step 1867, loss 0.285217, acc 0.875\n",
      "2017-11-06T18:21:31.694640: step 1868, loss 0.0918577, acc 0.96875\n",
      "2017-11-06T18:21:35.632439: step 1869, loss 0.117837, acc 0.9375\n",
      "2017-11-06T18:21:39.633282: step 1870, loss 0.462251, acc 0.8125\n",
      "2017-11-06T18:21:43.640129: step 1871, loss 0.167236, acc 0.9375\n",
      "2017-11-06T18:21:46.192943: step 1872, loss 0.229655, acc 0.85\n",
      "2017-11-06T18:21:50.683133: step 1873, loss 0.094528, acc 0.9375\n",
      "2017-11-06T18:21:54.757027: step 1874, loss 0.23417, acc 0.90625\n",
      "2017-11-06T18:21:58.752866: step 1875, loss 0.466176, acc 0.78125\n",
      "2017-11-06T18:22:02.726691: step 1876, loss 0.147221, acc 0.9375\n",
      "2017-11-06T18:22:06.730535: step 1877, loss 0.0509325, acc 0.96875\n",
      "2017-11-06T18:22:10.743386: step 1878, loss 0.0755823, acc 0.9375\n",
      "2017-11-06T18:22:14.822284: step 1879, loss 0.26553, acc 0.8125\n",
      "2017-11-06T18:22:18.898180: step 1880, loss 0.140178, acc 0.9375\n",
      "2017-11-06T18:22:22.867001: step 1881, loss 0.0344452, acc 1\n",
      "2017-11-06T18:22:26.853833: step 1882, loss 0.160227, acc 0.875\n",
      "2017-11-06T18:22:30.915720: step 1883, loss 0.0656212, acc 1\n",
      "2017-11-06T18:22:35.109701: step 1884, loss 0.0957245, acc 0.9375\n",
      "2017-11-06T18:22:39.126554: step 1885, loss 0.202695, acc 0.90625\n",
      "2017-11-06T18:22:43.105381: step 1886, loss 0.0716069, acc 0.96875\n",
      "2017-11-06T18:22:47.123236: step 1887, loss 0.0185861, acc 1\n",
      "2017-11-06T18:22:51.119075: step 1888, loss 0.156917, acc 0.90625\n",
      "2017-11-06T18:22:55.500188: step 1889, loss 0.0755327, acc 0.96875\n",
      "2017-11-06T18:22:59.617113: step 1890, loss 0.172232, acc 0.875\n",
      "2017-11-06T18:23:03.653984: step 1891, loss 0.190093, acc 0.90625\n",
      "2017-11-06T18:23:07.719690: step 1892, loss 0.0568784, acc 0.96875\n",
      "2017-11-06T18:23:11.704522: step 1893, loss 0.0770808, acc 0.96875\n",
      "2017-11-06T18:23:15.770410: step 1894, loss 0.350553, acc 0.84375\n",
      "2017-11-06T18:23:19.861317: step 1895, loss 0.262674, acc 0.84375\n",
      "2017-11-06T18:23:23.971238: step 1896, loss 0.138302, acc 0.90625\n",
      "2017-11-06T18:23:28.297311: step 1897, loss 0.200294, acc 0.9375\n",
      "2017-11-06T18:23:32.341186: step 1898, loss 0.187751, acc 0.90625\n",
      "2017-11-06T18:23:36.344029: step 1899, loss 0.237656, acc 0.875\n",
      "2017-11-06T18:23:40.306845: step 1900, loss 0.274089, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:23:42.953728: step 1900, loss 0.805512, acc 0.783333\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-06T18:23:48.626501: step 1901, loss 0.0881558, acc 0.96875\n",
      "2017-11-06T18:23:52.665373: step 1902, loss 0.372138, acc 0.875\n",
      "2017-11-06T18:23:56.660210: step 1903, loss 0.147945, acc 0.90625\n",
      "2017-11-06T18:24:00.997291: step 1904, loss 0.333843, acc 0.875\n",
      "2017-11-06T18:24:05.167255: step 1905, loss 0.392734, acc 0.84375\n",
      "2017-11-06T18:24:09.185109: step 1906, loss 0.181504, acc 0.9375\n",
      "2017-11-06T18:24:13.144923: step 1907, loss 0.182225, acc 0.875\n",
      "2017-11-06T18:24:15.769788: step 1908, loss 0.0213303, acc 1\n",
      "2017-11-06T18:24:19.762625: step 1909, loss 0.244985, acc 0.90625\n",
      "2017-11-06T18:24:23.768471: step 1910, loss 0.190368, acc 0.90625\n",
      "2017-11-06T18:24:27.826355: step 1911, loss 0.111814, acc 0.90625\n",
      "2017-11-06T18:24:31.897248: step 1912, loss 0.0715504, acc 0.96875\n",
      "2017-11-06T18:24:36.159275: step 1913, loss 0.217372, acc 0.9375\n",
      "2017-11-06T18:24:40.159117: step 1914, loss 0.0779292, acc 0.96875\n",
      "2017-11-06T18:24:44.152957: step 1915, loss 0.195006, acc 0.90625\n",
      "2017-11-06T18:24:48.172813: step 1916, loss 0.178324, acc 0.9375\n",
      "2017-11-06T18:24:52.136628: step 1917, loss 0.25189, acc 0.84375\n",
      "2017-11-06T18:24:56.152482: step 1918, loss 0.145777, acc 0.9375\n",
      "2017-11-06T18:25:00.105290: step 1919, loss 0.244403, acc 0.90625\n",
      "2017-11-06T18:25:04.229220: step 1920, loss 0.17057, acc 0.96875\n",
      "2017-11-06T18:25:08.608334: step 1921, loss 0.108337, acc 0.9375\n",
      "2017-11-06T18:25:12.616182: step 1922, loss 0.0655056, acc 0.96875\n",
      "2017-11-06T18:25:16.600012: step 1923, loss 0.119427, acc 0.96875\n",
      "2017-11-06T18:25:20.560825: step 1924, loss 0.177688, acc 0.9375\n",
      "2017-11-06T18:25:24.558665: step 1925, loss 0.0841723, acc 0.96875\n",
      "2017-11-06T18:25:28.597535: step 1926, loss 0.284736, acc 0.84375\n",
      "2017-11-06T18:25:32.577363: step 1927, loss 0.408987, acc 0.78125\n",
      "2017-11-06T18:25:36.611229: step 1928, loss 0.160965, acc 0.90625\n",
      "2017-11-06T18:25:40.647097: step 1929, loss 0.388099, acc 0.78125\n",
      "2017-11-06T18:25:44.636932: step 1930, loss 0.441951, acc 0.875\n",
      "2017-11-06T18:25:48.625766: step 1931, loss 0.218867, acc 0.90625\n",
      "2017-11-06T18:25:52.613599: step 1932, loss 0.353797, acc 0.8125\n",
      "2017-11-06T18:25:56.616444: step 1933, loss 0.0380779, acc 1\n",
      "2017-11-06T18:26:00.596272: step 1934, loss 0.118945, acc 0.96875\n",
      "2017-11-06T18:26:04.664162: step 1935, loss 0.0520574, acc 0.96875\n",
      "2017-11-06T18:26:08.732934: step 1936, loss 0.21304, acc 0.90625\n",
      "2017-11-06T18:26:13.215119: step 1937, loss 0.140581, acc 0.96875\n",
      "2017-11-06T18:26:17.331043: step 1938, loss 0.199636, acc 0.90625\n",
      "2017-11-06T18:26:21.351900: step 1939, loss 0.201366, acc 0.9375\n",
      "2017-11-06T18:26:25.400779: step 1940, loss 0.191082, acc 0.90625\n",
      "2017-11-06T18:26:29.489682: step 1941, loss 0.146154, acc 0.90625\n",
      "2017-11-06T18:26:33.922274: step 1942, loss 0.0709906, acc 0.96875\n",
      "2017-11-06T18:26:37.964145: step 1943, loss 0.223729, acc 0.9375\n",
      "2017-11-06T18:26:40.516959: step 1944, loss 0.110762, acc 0.9\n",
      "2017-11-06T18:26:44.505793: step 1945, loss 0.20896, acc 0.90625\n",
      "2017-11-06T18:26:48.475614: step 1946, loss 0.0639391, acc 0.9375\n",
      "2017-11-06T18:26:52.471453: step 1947, loss 0.0852641, acc 0.96875\n",
      "2017-11-06T18:26:56.469294: step 1948, loss 0.251682, acc 0.875\n",
      "2017-11-06T18:27:00.454127: step 1949, loss 0.291851, acc 0.90625\n",
      "2017-11-06T18:27:04.583059: step 1950, loss 0.316917, acc 0.90625\n",
      "2017-11-06T18:27:08.567891: step 1951, loss 0.104271, acc 0.9375\n",
      "2017-11-06T18:27:12.579741: step 1952, loss 0.122867, acc 0.9375\n",
      "2017-11-06T18:27:16.979868: step 1953, loss 0.133846, acc 0.9375\n",
      "2017-11-06T18:27:21.157837: step 1954, loss 0.227712, acc 0.9375\n",
      "2017-11-06T18:27:25.213718: step 1955, loss 0.115031, acc 0.9375\n",
      "2017-11-06T18:27:29.225569: step 1956, loss 0.239014, acc 0.9375\n",
      "2017-11-06T18:27:33.262437: step 1957, loss 0.468489, acc 0.875\n",
      "2017-11-06T18:27:37.270285: step 1958, loss 0.179014, acc 0.9375\n",
      "2017-11-06T18:27:41.295146: step 1959, loss 0.102685, acc 0.96875\n",
      "2017-11-06T18:27:45.311999: step 1960, loss 0.107157, acc 0.96875\n",
      "2017-11-06T18:27:49.290826: step 1961, loss 0.0469434, acc 1\n",
      "2017-11-06T18:27:53.340703: step 1962, loss 0.314718, acc 0.875\n",
      "2017-11-06T18:27:57.315530: step 1963, loss 0.137981, acc 0.90625\n",
      "2017-11-06T18:28:01.345393: step 1964, loss 0.18064, acc 0.875\n",
      "2017-11-06T18:28:05.304204: step 1965, loss 0.155069, acc 0.9375\n",
      "2017-11-06T18:28:09.289035: step 1966, loss 0.147453, acc 0.90625\n",
      "2017-11-06T18:28:13.301887: step 1967, loss 0.24399, acc 0.90625\n",
      "2017-11-06T18:28:17.335753: step 1968, loss 0.344623, acc 0.84375\n",
      "2017-11-06T18:28:21.527732: step 1969, loss 0.222687, acc 0.90625\n",
      "2017-11-06T18:28:26.011918: step 1970, loss 0.182477, acc 0.9375\n",
      "2017-11-06T18:28:30.054809: step 1971, loss 0.26255, acc 0.875\n",
      "2017-11-06T18:28:34.287804: step 1972, loss 0.193404, acc 0.875\n",
      "2017-11-06T18:28:38.319668: step 1973, loss 0.322532, acc 0.84375\n",
      "2017-11-06T18:28:42.369546: step 1974, loss 0.12588, acc 0.90625\n",
      "2017-11-06T18:28:46.429431: step 1975, loss 0.156044, acc 0.9375\n",
      "2017-11-06T18:28:50.401254: step 1976, loss 0.358274, acc 0.875\n",
      "2017-11-06T18:28:54.471145: step 1977, loss 0.155804, acc 0.9375\n",
      "2017-11-06T18:28:58.457978: step 1978, loss 0.0784215, acc 0.96875\n",
      "2017-11-06T18:29:02.480836: step 1979, loss 0.0936776, acc 0.9375\n",
      "2017-11-06T18:29:05.123715: step 1980, loss 0.0513906, acc 1\n",
      "2017-11-06T18:29:09.143525: step 1981, loss 0.215928, acc 0.90625\n",
      "2017-11-06T18:29:13.213418: step 1982, loss 0.089272, acc 0.9375\n",
      "2017-11-06T18:29:17.162223: step 1983, loss 0.0821021, acc 0.96875\n",
      "2017-11-06T18:29:21.159063: step 1984, loss 0.139602, acc 0.9375\n",
      "2017-11-06T18:29:25.210943: step 1985, loss 0.21787, acc 0.9375\n",
      "2017-11-06T18:29:29.581047: step 1986, loss 0.211942, acc 0.875\n",
      "2017-11-06T18:29:33.713984: step 1987, loss 0.253139, acc 0.875\n",
      "2017-11-06T18:29:37.709823: step 1988, loss 0.280915, acc 0.8125\n",
      "2017-11-06T18:29:41.711667: step 1989, loss 0.353103, acc 0.90625\n",
      "2017-11-06T18:29:45.725519: step 1990, loss 0.133181, acc 0.9375\n",
      "2017-11-06T18:29:49.720358: step 1991, loss 0.135268, acc 0.9375\n",
      "2017-11-06T18:29:53.754225: step 1992, loss 0.199849, acc 0.875\n",
      "2017-11-06T18:29:57.845132: step 1993, loss 0.247146, acc 0.875\n",
      "2017-11-06T18:30:02.132176: step 1994, loss 0.194922, acc 0.90625\n",
      "2017-11-06T18:30:06.117008: step 1995, loss 0.145559, acc 0.90625\n",
      "2017-11-06T18:30:10.111847: step 1996, loss 0.351338, acc 0.875\n",
      "2017-11-06T18:30:14.116692: step 1997, loss 0.236322, acc 0.90625\n",
      "2017-11-06T18:30:18.184582: step 1998, loss 0.0144069, acc 1\n",
      "2017-11-06T18:30:22.155405: step 1999, loss 0.0906423, acc 0.9375\n",
      "2017-11-06T18:30:26.189270: step 2000, loss 0.141801, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:30:28.796122: step 2000, loss 0.960655, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-06T18:30:34.692809: step 2001, loss 0.308612, acc 0.84375\n",
      "2017-11-06T18:30:38.875782: step 2002, loss 0.259395, acc 0.90625\n",
      "2017-11-06T18:30:42.890634: step 2003, loss 0.221137, acc 0.9375\n",
      "2017-11-06T18:30:46.867460: step 2004, loss 0.114618, acc 0.9375\n",
      "2017-11-06T18:30:50.930347: step 2005, loss 0.19155, acc 0.875\n",
      "2017-11-06T18:30:54.879153: step 2006, loss 0.049129, acc 1\n",
      "2017-11-06T18:30:58.870989: step 2007, loss 0.206663, acc 0.875\n",
      "2017-11-06T18:31:02.854820: step 2008, loss 0.197435, acc 0.875\n",
      "2017-11-06T18:31:06.856663: step 2009, loss 0.262395, acc 0.875\n",
      "2017-11-06T18:31:10.959579: step 2010, loss 0.226313, acc 0.9375\n",
      "2017-11-06T18:31:14.986441: step 2011, loss 0.0981317, acc 0.9375\n",
      "2017-11-06T18:31:19.071344: step 2012, loss 0.049851, acc 1\n",
      "2017-11-06T18:31:23.084194: step 2013, loss 0.439899, acc 0.84375\n",
      "2017-11-06T18:31:27.187111: step 2014, loss 0.244556, acc 0.90625\n",
      "2017-11-06T18:31:31.185950: step 2015, loss 0.172049, acc 0.9375\n",
      "2017-11-06T18:31:33.783796: step 2016, loss 0.222838, acc 0.9\n",
      "2017-11-06T18:31:38.148898: step 2017, loss 0.0592845, acc 1\n",
      "2017-11-06T18:31:42.518002: step 2018, loss 0.12667, acc 0.9375\n",
      "2017-11-06T18:31:46.526851: step 2019, loss 0.00806916, acc 1\n",
      "2017-11-06T18:31:50.527693: step 2020, loss 0.0816756, acc 0.9375\n",
      "2017-11-06T18:31:54.531538: step 2021, loss 0.187503, acc 0.9375\n",
      "2017-11-06T18:31:58.611437: step 2022, loss 0.244429, acc 0.875\n",
      "2017-11-06T18:32:02.595268: step 2023, loss 0.0366573, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T18:32:06.572094: step 2024, loss 0.213909, acc 0.90625\n",
      "2017-11-06T18:32:10.603741: step 2025, loss 0.151206, acc 0.90625\n",
      "2017-11-06T18:32:14.692645: step 2026, loss 0.170267, acc 0.90625\n",
      "2017-11-06T18:32:18.675475: step 2027, loss 0.215367, acc 0.90625\n",
      "2017-11-06T18:32:22.658305: step 2028, loss 0.0925872, acc 0.96875\n",
      "2017-11-06T18:32:26.729197: step 2029, loss 0.104849, acc 0.90625\n",
      "2017-11-06T18:32:30.743049: step 2030, loss 0.170669, acc 0.90625\n",
      "2017-11-06T18:32:35.009081: step 2031, loss 0.236389, acc 0.84375\n",
      "2017-11-06T18:32:39.057958: step 2032, loss 0.148509, acc 0.9375\n",
      "2017-11-06T18:32:43.147863: step 2033, loss 0.0676239, acc 0.96875\n",
      "2017-11-06T18:32:47.552994: step 2034, loss 0.138854, acc 0.9375\n",
      "2017-11-06T18:32:51.565847: step 2035, loss 0.338367, acc 0.84375\n",
      "2017-11-06T18:32:55.562685: step 2036, loss 0.198788, acc 0.84375\n",
      "2017-11-06T18:32:59.573534: step 2037, loss 0.229645, acc 0.84375\n",
      "2017-11-06T18:33:03.531347: step 2038, loss 0.201925, acc 0.90625\n",
      "2017-11-06T18:33:07.584228: step 2039, loss 0.280841, acc 0.84375\n",
      "2017-11-06T18:33:11.587071: step 2040, loss 0.0316474, acc 1\n",
      "2017-11-06T18:33:15.655962: step 2041, loss 0.20224, acc 0.90625\n",
      "2017-11-06T18:33:19.701836: step 2042, loss 0.0559788, acc 0.96875\n",
      "2017-11-06T18:33:23.820764: step 2043, loss 0.174828, acc 0.9375\n",
      "2017-11-06T18:33:27.995731: step 2044, loss 0.177732, acc 0.90625\n",
      "2017-11-06T18:33:32.047609: step 2045, loss 0.319767, acc 0.84375\n",
      "2017-11-06T18:33:36.073469: step 2046, loss 0.216216, acc 0.875\n",
      "2017-11-06T18:33:40.035285: step 2047, loss 0.380151, acc 0.8125\n",
      "2017-11-06T18:33:44.187235: step 2048, loss 0.298925, acc 0.875\n",
      "2017-11-06T18:33:48.210093: step 2049, loss 0.310636, acc 0.8125\n",
      "2017-11-06T18:33:52.829375: step 2050, loss 0.132325, acc 0.9375\n",
      "2017-11-06T18:33:56.920282: step 2051, loss 0.235141, acc 0.9375\n",
      "2017-11-06T18:33:59.532137: step 2052, loss 0.139133, acc 0.95\n",
      "2017-11-06T18:34:03.595025: step 2053, loss 0.19792, acc 0.875\n",
      "2017-11-06T18:34:07.761986: step 2054, loss 0.130507, acc 0.9375\n",
      "2017-11-06T18:34:11.885916: step 2055, loss 0.240863, acc 0.90625\n",
      "2017-11-06T18:34:16.123927: step 2056, loss 0.193512, acc 0.875\n",
      "2017-11-06T18:34:20.219837: step 2057, loss 0.223607, acc 0.90625\n",
      "2017-11-06T18:34:24.327756: step 2058, loss 0.0725396, acc 0.96875\n",
      "2017-11-06T18:34:28.515732: step 2059, loss 0.176176, acc 0.90625\n",
      "2017-11-06T18:34:32.748740: step 2060, loss 0.14007, acc 0.9375\n",
      "2017-11-06T18:34:37.103834: step 2061, loss 0.0847571, acc 0.96875\n",
      "2017-11-06T18:34:41.543989: step 2062, loss 0.140522, acc 0.9375\n",
      "2017-11-06T18:34:45.598870: step 2063, loss 0.338176, acc 0.84375\n",
      "2017-11-06T18:34:49.736810: step 2064, loss 0.209205, acc 0.875\n",
      "2017-11-06T18:34:53.731648: step 2065, loss 0.0720317, acc 0.9375\n",
      "2017-11-06T18:34:58.186815: step 2066, loss 0.137771, acc 0.9375\n",
      "2017-11-06T18:35:02.261710: step 2067, loss 0.210116, acc 0.84375\n",
      "2017-11-06T18:35:06.301580: step 2068, loss 0.304589, acc 0.84375\n",
      "2017-11-06T18:35:10.301189: step 2069, loss 0.147397, acc 0.9375\n",
      "2017-11-06T18:35:14.359073: step 2070, loss 0.108909, acc 0.96875\n",
      "2017-11-06T18:35:18.365920: step 2071, loss 0.194192, acc 0.96875\n",
      "2017-11-06T18:35:22.342745: step 2072, loss 0.235543, acc 0.90625\n",
      "2017-11-06T18:35:26.344591: step 2073, loss 0.167037, acc 0.90625\n",
      "2017-11-06T18:35:30.409478: step 2074, loss 0.150304, acc 0.9375\n",
      "2017-11-06T18:35:34.334266: step 2075, loss 0.1729, acc 0.9375\n",
      "2017-11-06T18:35:38.310091: step 2076, loss 0.129413, acc 0.9375\n",
      "2017-11-06T18:35:42.327946: step 2077, loss 0.0338138, acc 1\n",
      "2017-11-06T18:35:46.332794: step 2078, loss 0.156757, acc 0.9375\n",
      "2017-11-06T18:35:50.399682: step 2079, loss 0.203971, acc 0.90625\n",
      "2017-11-06T18:35:54.392520: step 2080, loss 0.329299, acc 0.8125\n",
      "2017-11-06T18:35:58.390361: step 2081, loss 0.119707, acc 0.96875\n",
      "2017-11-06T18:36:02.753459: step 2082, loss 0.161363, acc 0.90625\n",
      "2017-11-06T18:36:06.987467: step 2083, loss 0.323673, acc 0.78125\n",
      "2017-11-06T18:36:10.956288: step 2084, loss 0.186292, acc 0.90625\n",
      "2017-11-06T18:36:14.928111: step 2085, loss 0.133942, acc 0.9375\n",
      "2017-11-06T18:36:18.951969: step 2086, loss 0.105007, acc 0.9375\n",
      "2017-11-06T18:36:23.035871: step 2087, loss 0.0532913, acc 0.96875\n",
      "2017-11-06T18:36:25.524639: step 2088, loss 0.058337, acc 1\n",
      "2017-11-06T18:36:29.458435: step 2089, loss 0.252122, acc 0.875\n",
      "2017-11-06T18:36:33.722464: step 2090, loss 0.215499, acc 0.90625\n",
      "2017-11-06T18:36:37.821377: step 2091, loss 0.0421768, acc 1\n",
      "2017-11-06T18:36:41.854242: step 2092, loss 0.115713, acc 0.9375\n",
      "2017-11-06T18:36:45.865092: step 2093, loss 0.231802, acc 0.875\n",
      "2017-11-06T18:36:49.832911: step 2094, loss 0.136688, acc 0.90625\n",
      "2017-11-06T18:36:53.889793: step 2095, loss 0.286104, acc 0.875\n",
      "2017-11-06T18:36:57.840601: step 2096, loss 0.176117, acc 0.90625\n",
      "2017-11-06T18:37:01.837440: step 2097, loss 0.254079, acc 0.90625\n",
      "2017-11-06T18:37:06.043430: step 2098, loss 0.354941, acc 0.84375\n",
      "2017-11-06T18:37:10.451562: step 2099, loss 0.02909, acc 1\n",
      "2017-11-06T18:37:14.581498: step 2100, loss 0.316615, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:37:17.200358: step 2100, loss 0.895797, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-06T18:37:22.408581: step 2101, loss 0.0468346, acc 1\n",
      "2017-11-06T18:37:26.401418: step 2102, loss 0.359922, acc 0.8125\n",
      "2017-11-06T18:37:30.389252: step 2103, loss 0.196488, acc 0.90625\n",
      "2017-11-06T18:37:34.387092: step 2104, loss 0.223709, acc 0.9375\n",
      "2017-11-06T18:37:38.353912: step 2105, loss 0.123117, acc 0.9375\n",
      "2017-11-06T18:37:42.347748: step 2106, loss 0.570738, acc 0.8125\n",
      "2017-11-06T18:37:46.440657: step 2107, loss 0.112988, acc 0.96875\n",
      "2017-11-06T18:37:50.447504: step 2108, loss 0.199778, acc 0.875\n",
      "2017-11-06T18:37:54.514393: step 2109, loss 0.22125, acc 0.90625\n",
      "2017-11-06T18:37:58.488219: step 2110, loss 0.137929, acc 0.96875\n",
      "2017-11-06T18:38:02.454035: step 2111, loss 0.265864, acc 0.84375\n",
      "2017-11-06T18:38:06.445871: step 2112, loss 0.207698, acc 0.90625\n",
      "2017-11-06T18:38:10.488564: step 2113, loss 0.158694, acc 0.90625\n",
      "2017-11-06T18:38:14.879685: step 2114, loss 0.222134, acc 0.90625\n",
      "2017-11-06T18:38:19.002614: step 2115, loss 0.249769, acc 0.8125\n",
      "2017-11-06T18:38:23.150561: step 2116, loss 0.0953788, acc 0.90625\n",
      "2017-11-06T18:38:27.295507: step 2117, loss 0.0327982, acc 1\n",
      "2017-11-06T18:38:31.277335: step 2118, loss 0.17728, acc 0.9375\n",
      "2017-11-06T18:38:35.479321: step 2119, loss 0.164075, acc 0.90625\n",
      "2017-11-06T18:38:39.537204: step 2120, loss 0.203017, acc 0.90625\n",
      "2017-11-06T18:38:43.552058: step 2121, loss 0.184737, acc 0.9375\n",
      "2017-11-06T18:38:47.508869: step 2122, loss 0.149769, acc 0.90625\n",
      "2017-11-06T18:38:51.501706: step 2123, loss 0.237559, acc 0.875\n",
      "2017-11-06T18:38:54.121568: step 2124, loss 0.223068, acc 0.9\n",
      "2017-11-06T18:38:58.151431: step 2125, loss 0.130348, acc 0.9375\n",
      "2017-11-06T18:39:02.144269: step 2126, loss 0.369077, acc 0.8125\n",
      "2017-11-06T18:39:06.191143: step 2127, loss 0.187686, acc 0.90625\n",
      "2017-11-06T18:39:10.182979: step 2128, loss 0.126252, acc 0.9375\n",
      "2017-11-06T18:39:14.163810: step 2129, loss 0.0591999, acc 1\n",
      "2017-11-06T18:39:18.438847: step 2130, loss 0.054551, acc 1\n",
      "2017-11-06T18:39:22.630825: step 2131, loss 0.291161, acc 0.84375\n",
      "2017-11-06T18:39:26.675699: step 2132, loss 0.361983, acc 0.8125\n",
      "2017-11-06T18:39:30.664532: step 2133, loss 0.142011, acc 0.90625\n",
      "2017-11-06T18:39:34.670379: step 2134, loss 0.127048, acc 0.9375\n",
      "2017-11-06T18:39:38.733266: step 2135, loss 0.302093, acc 0.90625\n",
      "2017-11-06T18:39:42.833179: step 2136, loss 0.0419845, acc 1\n",
      "2017-11-06T18:39:46.800998: step 2137, loss 0.159325, acc 0.90625\n",
      "2017-11-06T18:39:50.847875: step 2138, loss 0.217365, acc 0.90625\n",
      "2017-11-06T18:39:54.829703: step 2139, loss 0.171448, acc 0.9375\n",
      "2017-11-06T18:39:58.848578: step 2140, loss 0.144376, acc 0.90625\n",
      "2017-11-06T18:40:03.197649: step 2141, loss 0.136486, acc 0.9375\n",
      "2017-11-06T18:40:07.240521: step 2142, loss 0.282604, acc 0.84375\n",
      "2017-11-06T18:40:11.225372: step 2143, loss 0.164416, acc 0.90625\n",
      "2017-11-06T18:40:15.702821: step 2144, loss 0.185011, acc 0.90625\n",
      "2017-11-06T18:40:19.937330: step 2145, loss 0.15097, acc 0.90625\n",
      "2017-11-06T18:40:24.456040: step 2146, loss 0.0845563, acc 0.96875\n",
      "2017-11-06T18:40:28.696555: step 2147, loss 0.235967, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T18:40:32.736435: step 2148, loss 0.216706, acc 0.875\n",
      "2017-11-06T18:40:36.896896: step 2149, loss 0.281481, acc 0.84375\n",
      "2017-11-06T18:40:40.864715: step 2150, loss 0.203712, acc 0.875\n",
      "2017-11-06T18:40:45.072477: step 2151, loss 0.193709, acc 0.90625\n",
      "2017-11-06T18:40:49.074320: step 2152, loss 0.14962, acc 0.9375\n",
      "2017-11-06T18:40:53.128201: step 2153, loss 0.066529, acc 0.96875\n",
      "2017-11-06T18:40:57.119036: step 2154, loss 0.530542, acc 0.75\n",
      "2017-11-06T18:41:01.092860: step 2155, loss 0.0935345, acc 0.96875\n",
      "2017-11-06T18:41:05.091703: step 2156, loss 0.220421, acc 0.9375\n",
      "2017-11-06T18:41:09.090324: step 2157, loss 0.323711, acc 0.875\n",
      "2017-11-06T18:41:14.156924: step 2158, loss 0.360817, acc 0.84375\n",
      "2017-11-06T18:41:18.503013: step 2159, loss 0.0269249, acc 1\n",
      "2017-11-06T18:41:21.115868: step 2160, loss 0.617718, acc 0.75\n",
      "2017-11-06T18:41:25.136725: step 2161, loss 0.14919, acc 0.9375\n",
      "2017-11-06T18:41:29.254651: step 2162, loss 0.0900653, acc 0.96875\n",
      "2017-11-06T18:41:33.483657: step 2163, loss 0.220924, acc 0.9375\n",
      "2017-11-06T18:41:37.454478: step 2164, loss 0.0340445, acc 1\n",
      "2017-11-06T18:41:41.404285: step 2165, loss 0.204957, acc 0.90625\n",
      "2017-11-06T18:41:45.290046: step 2166, loss 0.201082, acc 0.90625\n",
      "2017-11-06T18:41:49.238852: step 2167, loss 0.1707, acc 0.9375\n",
      "2017-11-06T18:41:53.156635: step 2168, loss 0.173586, acc 0.9375\n",
      "2017-11-06T18:41:57.090432: step 2169, loss 0.295323, acc 0.90625\n",
      "2017-11-06T18:42:01.024226: step 2170, loss 0.111001, acc 0.9375\n",
      "2017-11-06T18:42:04.917992: step 2171, loss 0.263137, acc 0.90625\n",
      "2017-11-06T18:42:08.861795: step 2172, loss 0.128699, acc 0.9375\n",
      "2017-11-06T18:42:12.770572: step 2173, loss 0.432206, acc 0.78125\n",
      "2017-11-06T18:42:16.673345: step 2174, loss 0.215182, acc 0.875\n",
      "2017-11-06T18:42:20.619149: step 2175, loss 0.104813, acc 0.96875\n",
      "2017-11-06T18:42:24.680035: step 2176, loss 0.375367, acc 0.84375\n",
      "2017-11-06T18:42:28.663864: step 2177, loss 0.278241, acc 0.875\n",
      "2017-11-06T18:42:32.638690: step 2178, loss 0.215502, acc 0.90625\n",
      "2017-11-06T18:42:37.190924: step 2179, loss 0.149152, acc 0.9375\n",
      "2017-11-06T18:42:41.176756: step 2180, loss 0.241845, acc 0.84375\n",
      "2017-11-06T18:42:45.149578: step 2181, loss 0.209276, acc 0.90625\n",
      "2017-11-06T18:42:49.049351: step 2182, loss 0.230789, acc 0.84375\n",
      "2017-11-06T18:42:52.988148: step 2183, loss 0.132939, acc 0.9375\n",
      "2017-11-06T18:42:56.941957: step 2184, loss 0.238664, acc 0.875\n",
      "2017-11-06T18:43:00.869748: step 2185, loss 0.181325, acc 0.875\n",
      "2017-11-06T18:43:04.749504: step 2186, loss 0.305674, acc 0.90625\n",
      "2017-11-06T18:43:08.694310: step 2187, loss 0.270115, acc 0.90625\n",
      "2017-11-06T18:43:12.622099: step 2188, loss 0.243678, acc 0.90625\n",
      "2017-11-06T18:43:16.634951: step 2189, loss 0.0345069, acc 1\n",
      "2017-11-06T18:43:20.527717: step 2190, loss 0.121381, acc 0.96875\n",
      "2017-11-06T18:43:24.640639: step 2191, loss 0.220905, acc 0.90625\n",
      "2017-11-06T18:43:28.955704: step 2192, loss 0.0498546, acc 1\n",
      "2017-11-06T18:43:32.878491: step 2193, loss 0.377943, acc 0.78125\n",
      "2017-11-06T18:43:36.829300: step 2194, loss 0.212322, acc 0.90625\n",
      "2017-11-06T18:43:40.981250: step 2195, loss 0.113166, acc 0.9375\n",
      "2017-11-06T18:43:43.744212: step 2196, loss 0.252525, acc 0.85\n",
      "2017-11-06T18:43:47.657993: step 2197, loss 0.187304, acc 0.90625\n",
      "2017-11-06T18:43:51.623811: step 2198, loss 0.115969, acc 0.90625\n",
      "2017-11-06T18:43:55.535592: step 2199, loss 0.192197, acc 0.9375\n",
      "2017-11-06T18:43:59.456378: step 2200, loss 0.237354, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:44:01.998185: step 2200, loss 0.726491, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-06T18:44:07.409597: step 2201, loss 0.147957, acc 0.9375\n",
      "2017-11-06T18:44:11.356155: step 2202, loss 0.145775, acc 0.90625\n",
      "2017-11-06T18:44:15.259930: step 2203, loss 0.159447, acc 0.90625\n",
      "2017-11-06T18:44:19.176713: step 2204, loss 0.070342, acc 0.96875\n",
      "2017-11-06T18:44:23.107506: step 2205, loss 0.14034, acc 0.96875\n",
      "2017-11-06T18:44:27.048306: step 2206, loss 0.203095, acc 0.875\n",
      "2017-11-06T18:44:30.984103: step 2207, loss 0.217513, acc 0.875\n",
      "2017-11-06T18:44:35.182085: step 2208, loss 0.182932, acc 0.90625\n",
      "2017-11-06T18:44:39.167917: step 2209, loss 0.195236, acc 0.90625\n",
      "2017-11-06T18:44:43.129733: step 2210, loss 0.318753, acc 0.84375\n",
      "2017-11-06T18:44:47.362740: step 2211, loss 0.0859278, acc 0.9375\n",
      "2017-11-06T18:44:51.511688: step 2212, loss 0.138846, acc 0.9375\n",
      "2017-11-06T18:44:55.476505: step 2213, loss 0.291746, acc 0.90625\n",
      "2017-11-06T18:44:59.428313: step 2214, loss 0.21055, acc 0.90625\n",
      "2017-11-06T18:45:03.376118: step 2215, loss 0.146464, acc 0.96875\n",
      "2017-11-06T18:45:07.358948: step 2216, loss 0.561708, acc 0.8125\n",
      "2017-11-06T18:45:11.289741: step 2217, loss 0.325917, acc 0.875\n",
      "2017-11-06T18:45:15.234545: step 2218, loss 0.1857, acc 0.9375\n",
      "2017-11-06T18:45:19.266410: step 2219, loss 0.122592, acc 0.9375\n",
      "2017-11-06T18:45:23.229225: step 2220, loss 0.22975, acc 0.9375\n",
      "2017-11-06T18:45:27.236072: step 2221, loss 0.22381, acc 0.90625\n",
      "2017-11-06T18:45:31.252926: step 2222, loss 0.296191, acc 0.90625\n",
      "2017-11-06T18:45:35.216742: step 2223, loss 0.290679, acc 0.875\n",
      "2017-11-06T18:45:39.233596: step 2224, loss 0.209768, acc 0.875\n",
      "2017-11-06T18:45:43.178402: step 2225, loss 0.290388, acc 0.8125\n",
      "2017-11-06T18:45:47.202259: step 2226, loss 0.0848422, acc 0.96875\n",
      "2017-11-06T18:45:51.408247: step 2227, loss 0.0772048, acc 0.9375\n",
      "2017-11-06T18:45:55.713308: step 2228, loss 0.199802, acc 0.90625\n",
      "2017-11-06T18:45:59.725157: step 2229, loss 0.364969, acc 0.8125\n",
      "2017-11-06T18:46:03.861095: step 2230, loss 0.280847, acc 0.84375\n",
      "2017-11-06T18:46:08.307255: step 2231, loss 0.0808015, acc 0.96875\n",
      "2017-11-06T18:46:11.168289: step 2232, loss 0.365015, acc 0.85\n",
      "2017-11-06T18:46:15.179137: step 2233, loss 0.135359, acc 0.96875\n",
      "2017-11-06T18:46:19.288057: step 2234, loss 0.363305, acc 0.84375\n",
      "2017-11-06T18:46:23.358949: step 2235, loss 0.0882638, acc 0.96875\n",
      "2017-11-06T18:46:27.728053: step 2236, loss 0.121354, acc 0.9375\n",
      "2017-11-06T18:46:32.009096: step 2237, loss 0.206674, acc 0.90625\n",
      "2017-11-06T18:46:36.373196: step 2238, loss 0.0781915, acc 0.96875\n",
      "2017-11-06T18:46:40.303990: step 2239, loss 0.0840607, acc 0.96875\n",
      "2017-11-06T18:46:44.239786: step 2240, loss 0.0520948, acc 1\n",
      "2017-11-06T18:46:48.241691: step 2241, loss 0.0626003, acc 0.96875\n",
      "2017-11-06T18:46:52.217515: step 2242, loss 0.277446, acc 0.875\n",
      "2017-11-06T18:46:56.290409: step 2243, loss 0.213916, acc 0.9375\n",
      "2017-11-06T18:47:00.649507: step 2244, loss 0.197669, acc 0.90625\n",
      "2017-11-06T18:47:04.680370: step 2245, loss 0.174992, acc 0.90625\n",
      "2017-11-06T18:47:08.660200: step 2246, loss 0.202846, acc 0.90625\n",
      "2017-11-06T18:47:12.618781: step 2247, loss 0.246001, acc 0.875\n",
      "2017-11-06T18:47:16.638637: step 2248, loss 0.169366, acc 0.96875\n",
      "2017-11-06T18:47:20.586443: step 2249, loss 0.138738, acc 0.90625\n",
      "2017-11-06T18:47:24.573276: step 2250, loss 0.0798912, acc 0.96875\n",
      "2017-11-06T18:47:28.492060: step 2251, loss 0.381609, acc 0.8125\n",
      "2017-11-06T18:47:32.485897: step 2252, loss 0.289336, acc 0.875\n",
      "2017-11-06T18:47:36.442709: step 2253, loss 0.103818, acc 0.96875\n",
      "2017-11-06T18:47:40.389513: step 2254, loss 0.0953436, acc 0.96875\n",
      "2017-11-06T18:47:44.336317: step 2255, loss 0.147731, acc 0.90625\n",
      "2017-11-06T18:47:48.287124: step 2256, loss 0.279832, acc 0.875\n",
      "2017-11-06T18:47:52.213915: step 2257, loss 0.133821, acc 0.9375\n",
      "2017-11-06T18:47:56.218762: step 2258, loss 0.243816, acc 0.875\n",
      "2017-11-06T18:48:00.144550: step 2259, loss 0.193795, acc 0.875\n",
      "2017-11-06T18:48:04.458616: step 2260, loss 0.125792, acc 0.96875\n",
      "2017-11-06T18:48:08.612567: step 2261, loss 0.213776, acc 0.90625\n",
      "2017-11-06T18:48:12.522345: step 2262, loss 0.120463, acc 0.96875\n",
      "2017-11-06T18:48:16.524188: step 2263, loss 0.218755, acc 0.90625\n",
      "2017-11-06T18:48:20.477998: step 2264, loss 0.320734, acc 0.8125\n",
      "2017-11-06T18:48:24.457826: step 2265, loss 0.0983905, acc 0.9375\n",
      "2017-11-06T18:48:28.375609: step 2266, loss 0.12222, acc 0.96875\n",
      "2017-11-06T18:48:32.382457: step 2267, loss 0.244272, acc 0.875\n",
      "2017-11-06T18:48:35.167435: step 2268, loss 0.21587, acc 0.95\n",
      "2017-11-06T18:48:39.162274: step 2269, loss 0.273192, acc 0.9375\n",
      "2017-11-06T18:48:43.062044: step 2270, loss 0.191475, acc 0.875\n",
      "2017-11-06T18:48:47.006851: step 2271, loss 0.177599, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T18:48:50.974667: step 2272, loss 0.368513, acc 0.84375\n",
      "2017-11-06T18:48:54.916467: step 2273, loss 0.149061, acc 0.90625\n",
      "2017-11-06T18:48:58.842259: step 2274, loss 0.0586894, acc 1\n",
      "2017-11-06T18:49:02.747033: step 2275, loss 0.165887, acc 0.875\n",
      "2017-11-06T18:49:06.721856: step 2276, loss 0.0509879, acc 0.96875\n",
      "2017-11-06T18:49:11.121982: step 2277, loss 0.107033, acc 0.9375\n",
      "2017-11-06T18:49:15.125829: step 2278, loss 0.0949998, acc 0.96875\n",
      "2017-11-06T18:49:19.110659: step 2279, loss 0.224874, acc 0.90625\n",
      "2017-11-06T18:49:23.213574: step 2280, loss 0.148779, acc 0.9375\n",
      "2017-11-06T18:49:27.411557: step 2281, loss 0.178121, acc 0.875\n",
      "2017-11-06T18:49:31.378375: step 2282, loss 0.0956439, acc 0.96875\n",
      "2017-11-06T18:49:35.280149: step 2283, loss 0.294061, acc 0.90625\n",
      "2017-11-06T18:49:39.282992: step 2284, loss 0.175954, acc 0.90625\n",
      "2017-11-06T18:49:43.251812: step 2285, loss 0.112684, acc 0.90625\n",
      "2017-11-06T18:49:47.150582: step 2286, loss 0.321043, acc 0.875\n",
      "2017-11-06T18:49:51.135415: step 2287, loss 0.208495, acc 0.875\n",
      "2017-11-06T18:49:55.053197: step 2288, loss 0.0939571, acc 0.96875\n",
      "2017-11-06T18:49:58.986992: step 2289, loss 0.248343, acc 0.90625\n",
      "2017-11-06T18:50:03.309063: step 2290, loss 0.288938, acc 0.875\n",
      "2017-11-06T18:50:07.392965: step 2291, loss 0.10972, acc 0.96875\n",
      "2017-11-06T18:50:11.323526: step 2292, loss 0.12696, acc 0.90625\n",
      "2017-11-06T18:50:15.583554: step 2293, loss 0.128541, acc 0.9375\n",
      "2017-11-06T18:50:19.789543: step 2294, loss 0.218409, acc 0.875\n",
      "2017-11-06T18:50:23.729341: step 2295, loss 0.165001, acc 0.90625\n",
      "2017-11-06T18:50:27.690156: step 2296, loss 0.156687, acc 0.90625\n",
      "2017-11-06T18:50:31.594930: step 2297, loss 0.170231, acc 0.90625\n",
      "2017-11-06T18:50:35.804921: step 2298, loss 0.0567854, acc 0.96875\n",
      "2017-11-06T18:50:39.833785: step 2299, loss 0.129379, acc 0.90625\n",
      "2017-11-06T18:50:43.720546: step 2300, loss 0.351784, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:50:46.306385: step 2300, loss 0.866715, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-06T18:50:51.576860: step 2301, loss 0.0939165, acc 0.9375\n",
      "2017-11-06T18:50:55.545682: step 2302, loss 0.175911, acc 0.9375\n",
      "2017-11-06T18:50:59.572543: step 2303, loss 0.0671753, acc 0.96875\n",
      "2017-11-06T18:51:02.102339: step 2304, loss 0.330085, acc 0.8\n",
      "2017-11-06T18:51:06.122195: step 2305, loss 0.125881, acc 0.9375\n",
      "2017-11-06T18:51:10.061994: step 2306, loss 0.129121, acc 0.9375\n",
      "2017-11-06T18:51:13.989787: step 2307, loss 0.264084, acc 0.84375\n",
      "2017-11-06T18:51:18.069685: step 2308, loss 0.247791, acc 0.9375\n",
      "2017-11-06T18:51:22.391756: step 2309, loss 0.327316, acc 0.875\n",
      "2017-11-06T18:51:26.399603: step 2310, loss 0.0646387, acc 0.9375\n",
      "2017-11-06T18:51:30.386438: step 2311, loss 0.152704, acc 0.90625\n",
      "2017-11-06T18:51:34.350253: step 2312, loss 0.212383, acc 0.90625\n",
      "2017-11-06T18:51:38.291052: step 2313, loss 0.146728, acc 0.9375\n",
      "2017-11-06T18:51:42.293897: step 2314, loss 0.10972, acc 0.96875\n",
      "2017-11-06T18:51:46.245705: step 2315, loss 0.325524, acc 0.875\n",
      "2017-11-06T18:51:50.216528: step 2316, loss 0.0869113, acc 0.96875\n",
      "2017-11-06T18:51:54.178342: step 2317, loss 0.0332389, acc 1\n",
      "2017-11-06T18:51:58.200199: step 2318, loss 0.246431, acc 0.90625\n",
      "2017-11-06T18:52:02.214051: step 2319, loss 0.412339, acc 0.78125\n",
      "2017-11-06T18:52:06.247917: step 2320, loss 0.266784, acc 0.84375\n",
      "2017-11-06T18:52:10.207731: step 2321, loss 0.195614, acc 0.90625\n",
      "2017-11-06T18:52:14.242598: step 2322, loss 0.283818, acc 0.875\n",
      "2017-11-06T18:52:18.243441: step 2323, loss 0.119315, acc 0.9375\n",
      "2017-11-06T18:52:22.310331: step 2324, loss 0.483217, acc 0.78125\n",
      "2017-11-06T18:52:26.672430: step 2325, loss 0.175692, acc 0.9375\n",
      "2017-11-06T18:52:30.943465: step 2326, loss 0.226069, acc 0.90625\n",
      "2017-11-06T18:52:35.146451: step 2327, loss 0.201783, acc 0.875\n",
      "2017-11-06T18:52:39.199331: step 2328, loss 0.0746645, acc 0.9375\n",
      "2017-11-06T18:52:43.209180: step 2329, loss 0.148982, acc 0.875\n",
      "2017-11-06T18:52:47.220030: step 2330, loss 0.214411, acc 0.9375\n",
      "2017-11-06T18:52:51.260901: step 2331, loss 0.156112, acc 0.90625\n",
      "2017-11-06T18:52:55.235725: step 2332, loss 0.0485364, acc 1\n",
      "2017-11-06T18:52:59.237570: step 2333, loss 0.194445, acc 0.9375\n",
      "2017-11-06T18:53:03.263429: step 2334, loss 0.154248, acc 0.9375\n",
      "2017-11-06T18:53:07.202228: step 2335, loss 0.265495, acc 0.875\n",
      "2017-11-06T18:53:11.235916: step 2336, loss 0.237491, acc 0.90625\n",
      "2017-11-06T18:53:15.204735: step 2337, loss 0.405001, acc 0.78125\n",
      "2017-11-06T18:53:19.290639: step 2338, loss 0.286569, acc 0.90625\n",
      "2017-11-06T18:53:23.381545: step 2339, loss 0.396605, acc 0.8125\n",
      "2017-11-06T18:53:26.026425: step 2340, loss 0.432251, acc 0.8\n",
      "2017-11-06T18:53:30.160362: step 2341, loss 0.112472, acc 0.9375\n",
      "2017-11-06T18:53:34.519459: step 2342, loss 0.155716, acc 0.9375\n",
      "2017-11-06T18:53:38.618371: step 2343, loss 0.359375, acc 0.875\n",
      "2017-11-06T18:53:42.557170: step 2344, loss 0.413976, acc 0.875\n",
      "2017-11-06T18:53:46.581030: step 2345, loss 0.478874, acc 0.8125\n",
      "2017-11-06T18:53:50.558856: step 2346, loss 0.0764923, acc 0.96875\n",
      "2017-11-06T18:53:54.574709: step 2347, loss 0.204917, acc 0.9375\n",
      "2017-11-06T18:53:58.579555: step 2348, loss 0.335101, acc 0.875\n",
      "2017-11-06T18:54:02.608417: step 2349, loss 0.102494, acc 0.9375\n",
      "2017-11-06T18:54:06.593249: step 2350, loss 0.144397, acc 0.9375\n",
      "2017-11-06T18:54:10.534051: step 2351, loss 0.246681, acc 0.90625\n",
      "2017-11-06T18:54:14.606943: step 2352, loss 0.150889, acc 0.90625\n",
      "2017-11-06T18:54:18.592775: step 2353, loss 0.138723, acc 0.9375\n",
      "2017-11-06T18:54:22.606628: step 2354, loss 0.0478119, acc 1\n",
      "2017-11-06T18:54:26.600465: step 2355, loss 0.175193, acc 0.90625\n",
      "2017-11-06T18:54:30.599307: step 2356, loss 0.309959, acc 0.875\n",
      "2017-11-06T18:54:34.990426: step 2357, loss 0.111821, acc 0.9375\n",
      "2017-11-06T18:54:39.396557: step 2358, loss 0.180385, acc 0.90625\n",
      "2017-11-06T18:54:43.493468: step 2359, loss 0.157836, acc 0.96875\n",
      "2017-11-06T18:54:47.440272: step 2360, loss 0.0939411, acc 0.9375\n",
      "2017-11-06T18:54:51.428108: step 2361, loss 0.368072, acc 0.875\n",
      "2017-11-06T18:54:55.379914: step 2362, loss 0.326104, acc 0.875\n",
      "2017-11-06T18:54:59.453808: step 2363, loss 0.11275, acc 0.9375\n",
      "2017-11-06T18:55:03.408618: step 2364, loss 0.276767, acc 0.8125\n",
      "2017-11-06T18:55:07.418468: step 2365, loss 0.135325, acc 0.9375\n",
      "2017-11-06T18:55:11.419311: step 2366, loss 0.276922, acc 0.9375\n",
      "2017-11-06T18:55:15.489202: step 2367, loss 0.126989, acc 0.96875\n",
      "2017-11-06T18:55:19.525070: step 2368, loss 0.270703, acc 0.90625\n",
      "2017-11-06T18:55:23.495891: step 2369, loss 0.197047, acc 0.90625\n",
      "2017-11-06T18:55:27.539766: step 2370, loss 0.177234, acc 0.9375\n",
      "2017-11-06T18:55:31.580636: step 2371, loss 0.237975, acc 0.84375\n",
      "2017-11-06T18:55:35.525439: step 2372, loss 0.125791, acc 0.9375\n",
      "2017-11-06T18:55:39.549300: step 2373, loss 0.130402, acc 0.96875\n",
      "2017-11-06T18:55:43.990453: step 2374, loss 0.0637108, acc 0.96875\n",
      "2017-11-06T18:55:48.172425: step 2375, loss 0.180774, acc 0.90625\n",
      "2017-11-06T18:55:50.773273: step 2376, loss 0.534526, acc 0.85\n",
      "2017-11-06T18:55:54.792130: step 2377, loss 0.217129, acc 0.90625\n",
      "2017-11-06T18:55:58.804981: step 2378, loss 0.100631, acc 0.9375\n",
      "2017-11-06T18:56:02.736776: step 2379, loss 0.188471, acc 0.90625\n",
      "2017-11-06T18:56:06.718603: step 2380, loss 0.0763901, acc 0.96875\n",
      "2017-11-06T18:56:10.687199: step 2381, loss 0.0921545, acc 0.96875\n",
      "2017-11-06T18:56:14.692043: step 2382, loss 0.221969, acc 0.875\n",
      "2017-11-06T18:56:18.766940: step 2383, loss 0.0886724, acc 0.96875\n",
      "2017-11-06T18:56:22.757775: step 2384, loss 0.289102, acc 0.875\n",
      "2017-11-06T18:56:26.717588: step 2385, loss 0.242751, acc 0.875\n",
      "2017-11-06T18:56:30.774470: step 2386, loss 0.111946, acc 0.9375\n",
      "2017-11-06T18:56:35.003475: step 2387, loss 0.013843, acc 1\n",
      "2017-11-06T18:56:39.109393: step 2388, loss 0.165474, acc 0.875\n",
      "2017-11-06T18:56:43.133252: step 2389, loss 0.257652, acc 0.90625\n",
      "2017-11-06T18:56:47.317225: step 2390, loss 0.1925, acc 0.90625\n",
      "2017-11-06T18:56:51.638295: step 2391, loss 0.217659, acc 0.875\n",
      "2017-11-06T18:56:55.669159: step 2392, loss 0.141057, acc 0.96875\n",
      "2017-11-06T18:56:59.827717: step 2393, loss 0.337759, acc 0.875\n",
      "2017-11-06T18:57:03.833562: step 2394, loss 0.259713, acc 0.875\n",
      "2017-11-06T18:57:07.936478: step 2395, loss 0.132324, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T18:57:11.875279: step 2396, loss 0.196639, acc 0.875\n",
      "2017-11-06T18:57:15.907141: step 2397, loss 0.285378, acc 0.84375\n",
      "2017-11-06T18:57:19.921994: step 2398, loss 0.30075, acc 0.875\n",
      "2017-11-06T18:57:23.833774: step 2399, loss 0.221581, acc 0.84375\n",
      "2017-11-06T18:57:27.864638: step 2400, loss 0.084914, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T18:57:30.368417: step 2400, loss 0.77363, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-06T18:57:35.860012: step 2401, loss 0.0994535, acc 0.9375\n",
      "2017-11-06T18:57:39.852850: step 2402, loss 0.160665, acc 0.90625\n",
      "2017-11-06T18:57:43.807660: step 2403, loss 0.260406, acc 0.90625\n",
      "2017-11-06T18:57:47.822512: step 2404, loss 0.11909, acc 0.9375\n",
      "2017-11-06T18:57:51.912417: step 2405, loss 0.209106, acc 0.90625\n",
      "2017-11-06T18:57:56.290528: step 2406, loss 0.279309, acc 0.90625\n",
      "2017-11-06T18:58:00.316389: step 2407, loss 0.129341, acc 0.9375\n",
      "2017-11-06T18:58:04.277203: step 2408, loss 0.39776, acc 0.875\n",
      "2017-11-06T18:58:08.250025: step 2409, loss 0.0878796, acc 0.9375\n",
      "2017-11-06T18:58:12.225851: step 2410, loss 0.15187, acc 0.90625\n",
      "2017-11-06T18:58:16.214685: step 2411, loss 0.0849043, acc 0.9375\n",
      "2017-11-06T18:58:18.815533: step 2412, loss 0.108853, acc 0.95\n",
      "2017-11-06T18:58:22.959478: step 2413, loss 0.348771, acc 0.90625\n",
      "2017-11-06T18:58:27.060391: step 2414, loss 0.0229833, acc 1\n",
      "2017-11-06T18:58:31.070241: step 2415, loss 0.0985992, acc 0.96875\n",
      "2017-11-06T18:58:35.316257: step 2416, loss 0.194076, acc 0.84375\n",
      "2017-11-06T18:58:39.339117: step 2417, loss 0.0872978, acc 0.9375\n",
      "2017-11-06T18:58:43.279916: step 2418, loss 0.202277, acc 0.875\n",
      "2017-11-06T18:58:47.224718: step 2419, loss 0.40704, acc 0.8125\n",
      "2017-11-06T18:58:51.265610: step 2420, loss 0.18876, acc 0.84375\n",
      "2017-11-06T18:58:55.189378: step 2421, loss 0.280136, acc 0.90625\n",
      "2017-11-06T18:58:59.442400: step 2422, loss 0.106241, acc 0.9375\n",
      "2017-11-06T18:59:03.723442: step 2423, loss 0.189618, acc 0.90625\n",
      "2017-11-06T18:59:07.733291: step 2424, loss 0.220753, acc 0.90625\n",
      "2017-11-06T18:59:11.687884: step 2425, loss 0.224437, acc 0.84375\n",
      "2017-11-06T18:59:15.771785: step 2426, loss 0.184189, acc 0.90625\n",
      "2017-11-06T18:59:19.866694: step 2427, loss 0.453532, acc 0.75\n",
      "2017-11-06T18:59:23.811496: step 2428, loss 0.419131, acc 0.75\n",
      "2017-11-06T18:59:27.816343: step 2429, loss 0.116723, acc 0.9375\n",
      "2017-11-06T18:59:31.757142: step 2430, loss 0.0371478, acc 1\n",
      "2017-11-06T18:59:35.756984: step 2431, loss 0.169961, acc 0.875\n",
      "2017-11-06T18:59:39.736813: step 2432, loss 0.142161, acc 0.9375\n",
      "2017-11-06T18:59:43.685617: step 2433, loss 0.203184, acc 0.90625\n",
      "2017-11-06T18:59:47.689462: step 2434, loss 0.00554221, acc 1\n",
      "2017-11-06T18:59:51.684302: step 2435, loss 0.1358, acc 0.9375\n",
      "2017-11-06T18:59:55.683144: step 2436, loss 0.129131, acc 0.9375\n",
      "2017-11-06T18:59:59.718010: step 2437, loss 0.268725, acc 0.90625\n",
      "2017-11-06T19:00:04.139151: step 2438, loss 0.205885, acc 0.90625\n",
      "2017-11-06T19:00:08.448212: step 2439, loss 0.08827, acc 0.96875\n",
      "2017-11-06T19:00:12.432044: step 2440, loss 0.0535438, acc 0.96875\n",
      "2017-11-06T19:00:16.395861: step 2441, loss 0.18627, acc 0.875\n",
      "2017-11-06T19:00:20.375688: step 2442, loss 0.149586, acc 0.9375\n",
      "2017-11-06T19:00:24.373528: step 2443, loss 0.0796754, acc 0.96875\n",
      "2017-11-06T19:00:28.365364: step 2444, loss 0.337462, acc 0.875\n",
      "2017-11-06T19:00:32.357202: step 2445, loss 0.217799, acc 0.90625\n",
      "2017-11-06T19:00:36.576200: step 2446, loss 0.219883, acc 0.875\n",
      "2017-11-06T19:00:40.572038: step 2447, loss 0.377154, acc 0.84375\n",
      "2017-11-06T19:00:43.146867: step 2448, loss 0.198326, acc 0.9\n",
      "2017-11-06T19:00:47.144708: step 2449, loss 0.0772263, acc 0.96875\n",
      "2017-11-06T19:00:51.142548: step 2450, loss 0.115729, acc 0.90625\n",
      "2017-11-06T19:00:55.148395: step 2451, loss 0.124504, acc 0.9375\n",
      "2017-11-06T19:00:59.164249: step 2452, loss 0.190372, acc 0.96875\n",
      "2017-11-06T19:01:03.201118: step 2453, loss 0.186265, acc 0.90625\n",
      "2017-11-06T19:01:07.189972: step 2454, loss 0.303792, acc 0.90625\n",
      "2017-11-06T19:01:11.524053: step 2455, loss 0.202163, acc 0.875\n",
      "2017-11-06T19:01:15.735043: step 2456, loss 0.0668958, acc 0.96875\n",
      "2017-11-06T19:01:19.777916: step 2457, loss 0.0794845, acc 0.96875\n",
      "2017-11-06T19:01:23.728723: step 2458, loss 0.412264, acc 0.84375\n",
      "2017-11-06T19:01:27.713555: step 2459, loss 0.208596, acc 0.84375\n",
      "2017-11-06T19:01:31.711396: step 2460, loss 0.092618, acc 0.9375\n",
      "2017-11-06T19:01:35.808306: step 2461, loss 0.135956, acc 0.9375\n",
      "2017-11-06T19:01:39.989278: step 2462, loss 0.260238, acc 0.8125\n",
      "2017-11-06T19:01:44.100198: step 2463, loss 0.308379, acc 0.8125\n",
      "2017-11-06T19:01:47.999969: step 2464, loss 0.198896, acc 0.875\n",
      "2017-11-06T19:01:52.063857: step 2465, loss 0.0408361, acc 1\n",
      "2017-11-06T19:01:56.118738: step 2466, loss 0.297556, acc 0.84375\n",
      "2017-11-06T19:02:00.204641: step 2467, loss 0.170454, acc 0.90625\n",
      "2017-11-06T19:02:04.301552: step 2468, loss 0.277376, acc 0.84375\n",
      "2017-11-06T19:02:08.320408: step 2469, loss 0.289646, acc 0.8125\n",
      "2017-11-06T19:02:12.291020: step 2470, loss 0.0650931, acc 0.9375\n",
      "2017-11-06T19:02:16.631102: step 2471, loss 0.212869, acc 0.875\n",
      "2017-11-06T19:02:20.854105: step 2472, loss 0.27066, acc 0.84375\n",
      "2017-11-06T19:02:24.823923: step 2473, loss 0.0614656, acc 0.96875\n",
      "2017-11-06T19:02:28.851785: step 2474, loss 0.151368, acc 0.90625\n",
      "2017-11-06T19:02:32.938690: step 2475, loss 0.135155, acc 0.9375\n",
      "2017-11-06T19:02:37.112655: step 2476, loss 0.170888, acc 0.9375\n",
      "2017-11-06T19:02:41.035444: step 2477, loss 0.0467391, acc 0.96875\n",
      "2017-11-06T19:02:44.931211: step 2478, loss 0.38243, acc 0.8125\n",
      "2017-11-06T19:02:48.869009: step 2479, loss 0.218924, acc 0.84375\n",
      "2017-11-06T19:02:52.787793: step 2480, loss 0.299244, acc 0.875\n",
      "2017-11-06T19:02:56.762619: step 2481, loss 0.182995, acc 0.90625\n",
      "2017-11-06T19:03:00.693412: step 2482, loss 0.279871, acc 0.875\n",
      "2017-11-06T19:03:04.601187: step 2483, loss 0.268872, acc 0.84375\n",
      "2017-11-06T19:03:07.181393: step 2484, loss 0.183565, acc 0.85\n",
      "2017-11-06T19:03:11.145210: step 2485, loss 0.214527, acc 0.9375\n",
      "2017-11-06T19:03:15.143050: step 2486, loss 0.156964, acc 0.9375\n",
      "2017-11-06T19:03:19.077846: step 2487, loss 0.0884567, acc 0.96875\n",
      "2017-11-06T19:03:23.500989: step 2488, loss 0.0834394, acc 0.96875\n",
      "2017-11-06T19:03:27.807050: step 2489, loss 0.0960779, acc 0.9375\n",
      "2017-11-06T19:03:31.745848: step 2490, loss 0.202781, acc 0.875\n",
      "2017-11-06T19:03:35.661629: step 2491, loss 0.210959, acc 0.90625\n",
      "2017-11-06T19:03:39.576411: step 2492, loss 0.141008, acc 0.9375\n",
      "2017-11-06T19:03:43.675324: step 2493, loss 0.267935, acc 0.875\n",
      "2017-11-06T19:03:47.689176: step 2494, loss 0.0621872, acc 1\n",
      "2017-11-06T19:03:51.724042: step 2495, loss 0.191207, acc 0.9375\n",
      "2017-11-06T19:03:55.800939: step 2496, loss 0.131869, acc 0.9375\n",
      "2017-11-06T19:03:59.824798: step 2497, loss 0.150167, acc 0.9375\n",
      "2017-11-06T19:04:03.896692: step 2498, loss 0.272972, acc 0.875\n",
      "2017-11-06T19:04:07.912545: step 2499, loss 0.230843, acc 0.90625\n",
      "2017-11-06T19:04:12.035474: step 2500, loss 0.134012, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:04:14.601298: step 2500, loss 0.830344, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-06T19:04:20.353384: step 2501, loss 0.179648, acc 0.9375\n",
      "2017-11-06T19:04:24.421275: step 2502, loss 0.241543, acc 0.875\n",
      "2017-11-06T19:04:29.073581: step 2503, loss 0.301192, acc 0.8125\n",
      "2017-11-06T19:04:33.418669: step 2504, loss 0.0814525, acc 0.96875\n",
      "2017-11-06T19:04:37.656680: step 2505, loss 0.137062, acc 0.9375\n",
      "2017-11-06T19:04:41.634505: step 2506, loss 0.547074, acc 0.8125\n",
      "2017-11-06T19:04:45.631347: step 2507, loss 0.165393, acc 0.96875\n",
      "2017-11-06T19:04:49.643196: step 2508, loss 0.3385, acc 0.875\n",
      "2017-11-06T19:04:53.563982: step 2509, loss 0.411676, acc 0.90625\n",
      "2017-11-06T19:04:57.641880: step 2510, loss 0.0889974, acc 0.96875\n",
      "2017-11-06T19:05:01.604696: step 2511, loss 0.150333, acc 0.9375\n",
      "2017-11-06T19:05:05.633558: step 2512, loss 0.0764947, acc 0.9375\n",
      "2017-11-06T19:05:09.689440: step 2513, loss 0.104049, acc 0.9375\n",
      "2017-11-06T19:05:13.679255: step 2514, loss 0.187151, acc 0.90625\n",
      "2017-11-06T19:05:17.717124: step 2515, loss 0.411571, acc 0.75\n",
      "2017-11-06T19:05:21.693950: step 2516, loss 0.115332, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T19:05:25.714807: step 2517, loss 0.066554, acc 0.96875\n",
      "2017-11-06T19:05:29.698637: step 2518, loss 0.210303, acc 0.875\n",
      "2017-11-06T19:05:33.991687: step 2519, loss 0.221953, acc 0.90625\n",
      "2017-11-06T19:05:36.685602: step 2520, loss 0.300481, acc 0.8\n",
      "2017-11-06T19:05:40.746489: step 2521, loss 0.103536, acc 0.96875\n",
      "2017-11-06T19:05:44.724313: step 2522, loss 0.167651, acc 0.9375\n",
      "2017-11-06T19:05:48.664113: step 2523, loss 0.0725032, acc 0.96875\n",
      "2017-11-06T19:05:52.581898: step 2524, loss 0.189248, acc 0.9375\n",
      "2017-11-06T19:05:56.548716: step 2525, loss 0.175428, acc 0.90625\n",
      "2017-11-06T19:06:00.592590: step 2526, loss 0.114066, acc 0.90625\n",
      "2017-11-06T19:06:04.495362: step 2527, loss 0.191606, acc 0.9375\n",
      "2017-11-06T19:06:08.470186: step 2528, loss 0.0426479, acc 1\n",
      "2017-11-06T19:06:12.408986: step 2529, loss 0.143551, acc 0.9375\n",
      "2017-11-06T19:06:16.413831: step 2530, loss 0.163026, acc 0.9375\n",
      "2017-11-06T19:06:20.386655: step 2531, loss 0.0694174, acc 0.9375\n",
      "2017-11-06T19:06:24.484565: step 2532, loss 0.26747, acc 0.875\n",
      "2017-11-06T19:06:28.468395: step 2533, loss 0.129007, acc 0.9375\n",
      "2017-11-06T19:06:32.564306: step 2534, loss 0.0567465, acc 0.96875\n",
      "2017-11-06T19:06:36.823332: step 2535, loss 0.236033, acc 0.84375\n",
      "2017-11-06T19:06:41.240470: step 2536, loss 0.217382, acc 0.90625\n",
      "2017-11-06T19:06:45.246317: step 2537, loss 0.208093, acc 0.875\n",
      "2017-11-06T19:06:49.320213: step 2538, loss 0.149727, acc 0.875\n",
      "2017-11-06T19:06:53.255007: step 2539, loss 0.138861, acc 0.90625\n",
      "2017-11-06T19:06:57.162784: step 2540, loss 0.044479, acc 0.96875\n",
      "2017-11-06T19:07:01.216664: step 2541, loss 0.183935, acc 0.875\n",
      "2017-11-06T19:07:05.262541: step 2542, loss 0.2027, acc 0.90625\n",
      "2017-11-06T19:07:09.374461: step 2543, loss 0.203808, acc 0.90625\n",
      "2017-11-06T19:07:13.310258: step 2544, loss 0.187815, acc 0.875\n",
      "2017-11-06T19:07:17.263067: step 2545, loss 0.121114, acc 0.9375\n",
      "2017-11-06T19:07:21.252901: step 2546, loss 0.379675, acc 0.8125\n",
      "2017-11-06T19:07:25.248741: step 2547, loss 0.414149, acc 0.84375\n",
      "2017-11-06T19:07:29.205552: step 2548, loss 0.163812, acc 0.90625\n",
      "2017-11-06T19:07:33.239420: step 2549, loss 0.258876, acc 0.8125\n",
      "2017-11-06T19:07:37.193227: step 2550, loss 0.275678, acc 0.875\n",
      "2017-11-06T19:07:41.278130: step 2551, loss 0.299091, acc 0.875\n",
      "2017-11-06T19:07:45.583189: step 2552, loss 0.294793, acc 0.875\n",
      "2017-11-06T19:07:49.722130: step 2553, loss 0.0790011, acc 0.96875\n",
      "2017-11-06T19:07:53.668934: step 2554, loss 0.264137, acc 0.875\n",
      "2017-11-06T19:07:57.644760: step 2555, loss 0.467437, acc 0.8125\n",
      "2017-11-06T19:08:00.265621: step 2556, loss 0.245106, acc 0.85\n",
      "2017-11-06T19:08:04.238445: step 2557, loss 0.262858, acc 0.875\n",
      "2017-11-06T19:08:08.201262: step 2558, loss 0.237714, acc 0.875\n",
      "2017-11-06T19:08:12.186866: step 2559, loss 0.0889034, acc 0.9375\n",
      "2017-11-06T19:08:16.161689: step 2560, loss 0.238414, acc 0.875\n",
      "2017-11-06T19:08:20.159531: step 2561, loss 0.310325, acc 0.8125\n",
      "2017-11-06T19:08:24.241430: step 2562, loss 0.116605, acc 0.9375\n",
      "2017-11-06T19:08:28.359356: step 2563, loss 0.187052, acc 0.90625\n",
      "2017-11-06T19:08:32.326175: step 2564, loss 0.175754, acc 0.9375\n",
      "2017-11-06T19:08:36.516152: step 2565, loss 0.0763633, acc 0.96875\n",
      "2017-11-06T19:08:40.510990: step 2566, loss 0.281549, acc 0.84375\n",
      "2017-11-06T19:08:44.516837: step 2567, loss 0.155273, acc 0.90625\n",
      "2017-11-06T19:08:48.588730: step 2568, loss 0.19535, acc 0.875\n",
      "2017-11-06T19:08:52.958854: step 2569, loss 0.194638, acc 0.875\n",
      "2017-11-06T19:08:56.886626: step 2570, loss 0.0473743, acc 1\n",
      "2017-11-06T19:09:00.892473: step 2571, loss 0.0940857, acc 0.96875\n",
      "2017-11-06T19:09:04.923337: step 2572, loss 0.367932, acc 0.84375\n",
      "2017-11-06T19:09:08.966209: step 2573, loss 0.119597, acc 0.96875\n",
      "2017-11-06T19:09:12.960047: step 2574, loss 0.106431, acc 0.96875\n",
      "2017-11-06T19:09:17.036943: step 2575, loss 0.395436, acc 0.8125\n",
      "2017-11-06T19:09:20.982747: step 2576, loss 0.430237, acc 0.84375\n",
      "2017-11-06T19:09:25.023618: step 2577, loss 0.300481, acc 0.90625\n",
      "2017-11-06T19:09:29.012453: step 2578, loss 0.184231, acc 0.9375\n",
      "2017-11-06T19:09:32.956255: step 2579, loss 0.198587, acc 0.9375\n",
      "2017-11-06T19:09:36.911065: step 2580, loss 0.279189, acc 0.875\n",
      "2017-11-06T19:09:40.934925: step 2581, loss 0.00967055, acc 1\n",
      "2017-11-06T19:09:44.945774: step 2582, loss 0.275089, acc 0.9375\n",
      "2017-11-06T19:09:48.919598: step 2583, loss 0.124236, acc 0.96875\n",
      "2017-11-06T19:09:52.884416: step 2584, loss 0.204403, acc 0.90625\n",
      "2017-11-06T19:09:57.265529: step 2585, loss 0.213319, acc 0.9375\n",
      "2017-11-06T19:10:01.669657: step 2586, loss 0.294588, acc 0.875\n",
      "2017-11-06T19:10:05.709530: step 2587, loss 0.157282, acc 0.9375\n",
      "2017-11-06T19:10:09.691357: step 2588, loss 0.119122, acc 0.9375\n",
      "2017-11-06T19:10:13.627154: step 2589, loss 0.0743483, acc 0.96875\n",
      "2017-11-06T19:10:17.630998: step 2590, loss 0.126925, acc 0.90625\n",
      "2017-11-06T19:10:21.636844: step 2591, loss 0.0886499, acc 0.9375\n",
      "2017-11-06T19:10:24.260709: step 2592, loss 0.415446, acc 0.8\n",
      "2017-11-06T19:10:28.262553: step 2593, loss 0.226721, acc 0.9375\n",
      "2017-11-06T19:10:32.257393: step 2594, loss 0.138036, acc 0.96875\n",
      "2017-11-06T19:10:36.509412: step 2595, loss 0.112683, acc 0.9375\n",
      "2017-11-06T19:10:40.542278: step 2596, loss 0.143208, acc 0.9375\n",
      "2017-11-06T19:10:44.596158: step 2597, loss 0.258651, acc 0.90625\n",
      "2017-11-06T19:10:48.556973: step 2598, loss 0.265751, acc 0.8125\n",
      "2017-11-06T19:10:52.631868: step 2599, loss 0.130288, acc 0.90625\n",
      "2017-11-06T19:10:56.525634: step 2600, loss 0.131403, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:10:59.260578: step 2600, loss 0.8417, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2600\n",
      "\n",
      "2017-11-06T19:11:05.201956: step 2601, loss 0.479777, acc 0.78125\n",
      "2017-11-06T19:11:09.163771: step 2602, loss 0.33827, acc 0.84375\n",
      "2017-11-06T19:11:13.135367: step 2603, loss 0.356106, acc 0.84375\n",
      "2017-11-06T19:11:17.153220: step 2604, loss 0.122299, acc 0.9375\n",
      "2017-11-06T19:11:21.147058: step 2605, loss 0.0623499, acc 1\n",
      "2017-11-06T19:11:25.134891: step 2606, loss 0.100507, acc 0.9375\n",
      "2017-11-06T19:11:29.103712: step 2607, loss 0.0754198, acc 1\n",
      "2017-11-06T19:11:33.041510: step 2608, loss 0.209691, acc 0.90625\n",
      "2017-11-06T19:11:36.995319: step 2609, loss 0.178042, acc 0.9375\n",
      "2017-11-06T19:11:40.994160: step 2610, loss 0.130511, acc 0.96875\n",
      "2017-11-06T19:11:44.960978: step 2611, loss 0.112681, acc 0.9375\n",
      "2017-11-06T19:11:48.974832: step 2612, loss 0.0751485, acc 0.9375\n",
      "2017-11-06T19:11:52.951675: step 2613, loss 0.160122, acc 0.875\n",
      "2017-11-06T19:11:56.964510: step 2614, loss 0.112192, acc 0.96875\n",
      "2017-11-06T19:12:01.076430: step 2615, loss 0.188566, acc 0.90625\n",
      "2017-11-06T19:12:05.204362: step 2616, loss 0.166663, acc 0.90625\n",
      "2017-11-06T19:12:09.593481: step 2617, loss 0.259719, acc 0.9375\n",
      "2017-11-06T19:12:13.576311: step 2618, loss 0.257763, acc 0.84375\n",
      "2017-11-06T19:12:17.591164: step 2619, loss 0.442621, acc 0.84375\n",
      "2017-11-06T19:12:21.635037: step 2620, loss 0.177292, acc 0.90625\n",
      "2017-11-06T19:12:25.641884: step 2621, loss 0.273775, acc 0.90625\n",
      "2017-11-06T19:12:29.590690: step 2622, loss 0.284717, acc 0.90625\n",
      "2017-11-06T19:12:33.810689: step 2623, loss 0.194657, acc 0.90625\n",
      "2017-11-06T19:12:37.889587: step 2624, loss 0.0692014, acc 0.9375\n",
      "2017-11-06T19:12:41.899437: step 2625, loss 0.16062, acc 0.9375\n",
      "2017-11-06T19:12:45.854246: step 2626, loss 0.303223, acc 0.8125\n",
      "2017-11-06T19:12:49.815064: step 2627, loss 0.0680611, acc 1\n",
      "2017-11-06T19:12:52.377881: step 2628, loss 0.313581, acc 0.85\n",
      "2017-11-06T19:12:56.446772: step 2629, loss 0.0876288, acc 0.9375\n",
      "2017-11-06T19:13:00.449616: step 2630, loss 0.271106, acc 0.84375\n",
      "2017-11-06T19:13:04.461469: step 2631, loss 0.232957, acc 0.875\n",
      "2017-11-06T19:13:08.464311: step 2632, loss 0.311241, acc 0.84375\n",
      "2017-11-06T19:13:12.729341: step 2633, loss 0.115437, acc 0.9375\n",
      "2017-11-06T19:13:17.026395: step 2634, loss 0.0713333, acc 0.96875\n",
      "2017-11-06T19:13:20.989213: step 2635, loss 0.169702, acc 0.90625\n",
      "2017-11-06T19:13:25.185193: step 2636, loss 0.128031, acc 0.96875\n",
      "2017-11-06T19:13:29.386177: step 2637, loss 0.0503252, acc 0.96875\n",
      "2017-11-06T19:13:33.314968: step 2638, loss 0.0923129, acc 0.96875\n",
      "2017-11-06T19:13:37.349837: step 2639, loss 0.134854, acc 0.90625\n",
      "2017-11-06T19:13:41.328662: step 2640, loss 0.0729742, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T19:13:45.313494: step 2641, loss 0.0425504, acc 1\n",
      "2017-11-06T19:13:49.259297: step 2642, loss 0.122694, acc 0.9375\n",
      "2017-11-06T19:13:53.265144: step 2643, loss 0.0787025, acc 0.96875\n",
      "2017-11-06T19:13:57.220956: step 2644, loss 0.145068, acc 0.90625\n",
      "2017-11-06T19:14:01.223799: step 2645, loss 0.446447, acc 0.78125\n",
      "2017-11-06T19:14:05.238654: step 2646, loss 0.136831, acc 0.875\n",
      "2017-11-06T19:14:09.290530: step 2647, loss 0.393604, acc 0.8125\n",
      "2017-11-06T19:14:13.238198: step 2648, loss 0.404515, acc 0.84375\n",
      "2017-11-06T19:14:17.472207: step 2649, loss 0.229379, acc 0.875\n",
      "2017-11-06T19:14:21.735236: step 2650, loss 0.205417, acc 0.90625\n",
      "2017-11-06T19:14:25.759095: step 2651, loss 0.135917, acc 0.9375\n",
      "2017-11-06T19:14:29.743926: step 2652, loss 0.208471, acc 0.90625\n",
      "2017-11-06T19:14:33.968928: step 2653, loss 0.17894, acc 0.9375\n",
      "2017-11-06T19:14:37.998792: step 2654, loss 0.18181, acc 0.90625\n",
      "2017-11-06T19:14:42.014645: step 2655, loss 0.35824, acc 0.84375\n",
      "2017-11-06T19:14:45.954444: step 2656, loss 0.156016, acc 0.9375\n",
      "2017-11-06T19:14:49.934272: step 2657, loss 0.405122, acc 0.84375\n",
      "2017-11-06T19:14:53.892086: step 2658, loss 0.15482, acc 0.96875\n",
      "2017-11-06T19:14:57.865908: step 2659, loss 0.23075, acc 0.875\n",
      "2017-11-06T19:15:01.823720: step 2660, loss 0.145454, acc 0.90625\n",
      "2017-11-06T19:15:05.861589: step 2661, loss 0.0731471, acc 0.9375\n",
      "2017-11-06T19:15:09.834412: step 2662, loss 0.159483, acc 0.9375\n",
      "2017-11-06T19:15:13.845262: step 2663, loss 0.0946987, acc 0.90625\n",
      "2017-11-06T19:15:16.400077: step 2664, loss 0.200596, acc 0.9\n",
      "2017-11-06T19:15:20.372900: step 2665, loss 0.110182, acc 0.9375\n",
      "2017-11-06T19:15:24.818059: step 2666, loss 0.153329, acc 0.9375\n",
      "2017-11-06T19:15:28.981018: step 2667, loss 0.0852723, acc 0.96875\n",
      "2017-11-06T19:15:32.982859: step 2668, loss 0.106535, acc 0.9375\n",
      "2017-11-06T19:15:37.007720: step 2669, loss 0.033376, acc 1\n",
      "2017-11-06T19:15:41.096626: step 2670, loss 0.250635, acc 0.875\n",
      "2017-11-06T19:15:45.097468: step 2671, loss 0.261877, acc 0.875\n",
      "2017-11-06T19:15:49.094308: step 2672, loss 0.230926, acc 0.875\n",
      "2017-11-06T19:15:53.091148: step 2673, loss 0.12471, acc 0.9375\n",
      "2017-11-06T19:15:57.046959: step 2674, loss 0.31625, acc 0.8125\n",
      "2017-11-06T19:16:01.046801: step 2675, loss 0.252574, acc 0.875\n",
      "2017-11-06T19:16:05.085670: step 2676, loss 0.275877, acc 0.875\n",
      "2017-11-06T19:16:09.081510: step 2677, loss 0.35318, acc 0.8125\n",
      "2017-11-06T19:16:13.125383: step 2678, loss 0.0616119, acc 0.96875\n",
      "2017-11-06T19:16:17.133230: step 2679, loss 0.322213, acc 0.78125\n",
      "2017-11-06T19:16:21.119063: step 2680, loss 0.203394, acc 0.90625\n",
      "2017-11-06T19:16:25.152929: step 2681, loss 0.200398, acc 0.90625\n",
      "2017-11-06T19:16:29.399947: step 2682, loss 0.201049, acc 0.90625\n",
      "2017-11-06T19:16:33.807078: step 2683, loss 0.124179, acc 0.9375\n",
      "2017-11-06T19:16:37.984045: step 2684, loss 0.124506, acc 0.9375\n",
      "2017-11-06T19:16:41.932852: step 2685, loss 0.216828, acc 0.9375\n",
      "2017-11-06T19:16:45.936697: step 2686, loss 0.228984, acc 0.90625\n",
      "2017-11-06T19:16:49.888525: step 2687, loss 0.0637338, acc 0.9375\n",
      "2017-11-06T19:16:53.872355: step 2688, loss 0.0651787, acc 1\n",
      "2017-11-06T19:16:57.866173: step 2689, loss 0.142004, acc 0.90625\n",
      "2017-11-06T19:17:01.860011: step 2690, loss 0.195618, acc 0.875\n",
      "2017-11-06T19:17:05.847844: step 2691, loss 0.238037, acc 0.9375\n",
      "2017-11-06T19:17:09.935750: step 2692, loss 0.24785, acc 0.84375\n",
      "2017-11-06T19:17:13.994396: step 2693, loss 0.109684, acc 0.96875\n",
      "2017-11-06T19:17:17.999242: step 2694, loss 0.420257, acc 0.875\n",
      "2017-11-06T19:17:22.007089: step 2695, loss 0.182835, acc 0.90625\n",
      "2017-11-06T19:17:26.076982: step 2696, loss 0.0732891, acc 1\n",
      "2017-11-06T19:17:30.112849: step 2697, loss 0.251314, acc 0.84375\n",
      "2017-11-06T19:17:34.280810: step 2698, loss 0.288423, acc 0.875\n",
      "2017-11-06T19:17:38.583871: step 2699, loss 0.169848, acc 0.875\n",
      "2017-11-06T19:17:41.168705: step 2700, loss 0.00361689, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:17:43.891639: step 2700, loss 0.986542, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-06T19:17:49.466730: step 2701, loss 0.407879, acc 0.84375\n",
      "2017-11-06T19:17:53.500597: step 2702, loss 0.0563793, acc 0.96875\n",
      "2017-11-06T19:17:57.482425: step 2703, loss 0.262542, acc 0.9375\n",
      "2017-11-06T19:18:01.474263: step 2704, loss 0.346494, acc 0.875\n",
      "2017-11-06T19:18:05.473103: step 2705, loss 0.0536983, acc 1\n",
      "2017-11-06T19:18:09.469943: step 2706, loss 0.168422, acc 0.90625\n",
      "2017-11-06T19:18:13.490802: step 2707, loss 0.0520338, acc 0.96875\n",
      "2017-11-06T19:18:17.533673: step 2708, loss 0.10508, acc 0.96875\n",
      "2017-11-06T19:18:21.513501: step 2709, loss 0.0529755, acc 0.96875\n",
      "2017-11-06T19:18:25.677459: step 2710, loss 0.164178, acc 0.9375\n",
      "2017-11-06T19:18:29.749352: step 2711, loss 0.0418391, acc 1\n",
      "2017-11-06T19:18:34.107449: step 2712, loss 0.27867, acc 0.84375\n",
      "2017-11-06T19:18:38.257398: step 2713, loss 0.551225, acc 0.75\n",
      "2017-11-06T19:18:42.650519: step 2714, loss 0.0664374, acc 0.96875\n",
      "2017-11-06T19:18:46.645358: step 2715, loss 0.218855, acc 0.875\n",
      "2017-11-06T19:18:50.608173: step 2716, loss 0.129147, acc 0.9375\n",
      "2017-11-06T19:18:54.610018: step 2717, loss 0.244907, acc 0.9375\n",
      "2017-11-06T19:18:58.674907: step 2718, loss 0.15181, acc 0.90625\n",
      "2017-11-06T19:19:02.639723: step 2719, loss 0.138953, acc 0.96875\n",
      "2017-11-06T19:19:06.680593: step 2720, loss 0.0690961, acc 1\n",
      "2017-11-06T19:19:10.652417: step 2721, loss 0.172701, acc 0.875\n",
      "2017-11-06T19:19:14.637247: step 2722, loss 0.215555, acc 0.90625\n",
      "2017-11-06T19:19:18.666110: step 2723, loss 0.112687, acc 0.90625\n",
      "2017-11-06T19:19:22.668957: step 2724, loss 0.267278, acc 0.84375\n",
      "2017-11-06T19:19:26.675801: step 2725, loss 0.0667378, acc 0.9375\n",
      "2017-11-06T19:19:30.695659: step 2726, loss 0.0913995, acc 0.9375\n",
      "2017-11-06T19:19:34.643462: step 2727, loss 0.436006, acc 0.8125\n",
      "2017-11-06T19:19:38.684333: step 2728, loss 0.240965, acc 0.90625\n",
      "2017-11-06T19:19:42.674169: step 2729, loss 0.279879, acc 0.90625\n",
      "2017-11-06T19:19:46.976226: step 2730, loss 0.348302, acc 0.78125\n",
      "2017-11-06T19:19:51.195224: step 2731, loss 0.368853, acc 0.8125\n",
      "2017-11-06T19:19:55.223085: step 2732, loss 0.104927, acc 0.9375\n",
      "2017-11-06T19:19:59.239940: step 2733, loss 0.0637631, acc 0.96875\n",
      "2017-11-06T19:20:03.619051: step 2734, loss 0.158434, acc 0.9375\n",
      "2017-11-06T19:20:07.698951: step 2735, loss 0.13478, acc 0.90625\n",
      "2017-11-06T19:20:10.333822: step 2736, loss 0.270535, acc 0.8\n",
      "2017-11-06T19:20:14.346477: step 2737, loss 0.0499463, acc 0.96875\n",
      "2017-11-06T19:20:18.444389: step 2738, loss 0.154171, acc 0.9375\n",
      "2017-11-06T19:20:22.486261: step 2739, loss 0.0791319, acc 0.96875\n",
      "2017-11-06T19:20:26.502114: step 2740, loss 0.0283281, acc 1\n",
      "2017-11-06T19:20:30.500956: step 2741, loss 0.109272, acc 0.96875\n",
      "2017-11-06T19:20:34.761983: step 2742, loss 0.114698, acc 0.9375\n",
      "2017-11-06T19:20:38.833877: step 2743, loss 0.222228, acc 0.90625\n",
      "2017-11-06T19:20:42.844727: step 2744, loss 0.326539, acc 0.84375\n",
      "2017-11-06T19:20:46.769515: step 2745, loss 0.208944, acc 0.875\n",
      "2017-11-06T19:20:50.927470: step 2746, loss 0.0952078, acc 0.9375\n",
      "2017-11-06T19:20:55.288570: step 2747, loss 0.276329, acc 0.84375\n",
      "2017-11-06T19:20:59.273399: step 2748, loss 0.166342, acc 0.875\n",
      "2017-11-06T19:21:03.275244: step 2749, loss 0.115192, acc 0.9375\n",
      "2017-11-06T19:21:07.316114: step 2750, loss 0.0788361, acc 0.9375\n",
      "2017-11-06T19:21:11.273927: step 2751, loss 0.0581518, acc 0.96875\n",
      "2017-11-06T19:21:15.283776: step 2752, loss 0.247808, acc 0.875\n",
      "2017-11-06T19:21:19.352666: step 2753, loss 0.196053, acc 0.90625\n",
      "2017-11-06T19:21:23.336498: step 2754, loss 0.156195, acc 0.96875\n",
      "2017-11-06T19:21:27.393380: step 2755, loss 0.153846, acc 0.96875\n",
      "2017-11-06T19:21:31.424244: step 2756, loss 0.0801317, acc 0.9375\n",
      "2017-11-06T19:21:35.382056: step 2757, loss 0.329994, acc 0.875\n",
      "2017-11-06T19:21:39.459954: step 2758, loss 0.223637, acc 0.90625\n",
      "2017-11-06T19:21:43.403756: step 2759, loss 0.180576, acc 0.90625\n",
      "2017-11-06T19:21:47.462640: step 2760, loss 0.163437, acc 0.90625\n",
      "2017-11-06T19:21:51.477492: step 2761, loss 0.29649, acc 0.8125\n",
      "2017-11-06T19:21:55.566399: step 2762, loss 0.244061, acc 0.875\n",
      "2017-11-06T19:22:00.068597: step 2763, loss 0.174936, acc 0.875\n",
      "2017-11-06T19:22:04.042421: step 2764, loss 0.267612, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T19:22:08.085293: step 2765, loss 0.112806, acc 0.9375\n",
      "2017-11-06T19:22:12.083135: step 2766, loss 0.367035, acc 0.90625\n",
      "2017-11-06T19:22:16.019950: step 2767, loss 0.197193, acc 0.875\n",
      "2017-11-06T19:22:20.067808: step 2768, loss 0.111441, acc 0.96875\n",
      "2017-11-06T19:22:24.086663: step 2769, loss 0.0686513, acc 0.96875\n",
      "2017-11-06T19:22:28.128535: step 2770, loss 0.289231, acc 0.90625\n",
      "2017-11-06T19:22:32.128377: step 2771, loss 0.447252, acc 0.78125\n",
      "2017-11-06T19:22:34.940375: step 2772, loss 0.380758, acc 0.85\n",
      "2017-11-06T19:22:39.056300: step 2773, loss 0.13891, acc 0.90625\n",
      "2017-11-06T19:22:43.033125: step 2774, loss 0.272273, acc 0.875\n",
      "2017-11-06T19:22:46.976928: step 2775, loss 0.244966, acc 0.90625\n",
      "2017-11-06T19:22:50.979773: step 2776, loss 0.290039, acc 0.90625\n",
      "2017-11-06T19:22:54.980616: step 2777, loss 0.31033, acc 0.875\n",
      "2017-11-06T19:22:59.005476: step 2778, loss 0.199151, acc 0.90625\n",
      "2017-11-06T19:23:03.211463: step 2779, loss 0.0253491, acc 1\n",
      "2017-11-06T19:23:07.507515: step 2780, loss 0.182809, acc 0.84375\n",
      "2017-11-06T19:23:11.507358: step 2781, loss 0.115079, acc 0.90625\n",
      "2017-11-06T19:23:15.538996: step 2782, loss 0.322426, acc 0.84375\n",
      "2017-11-06T19:23:19.534836: step 2783, loss 0.20366, acc 0.875\n",
      "2017-11-06T19:23:23.660768: step 2784, loss 0.07619, acc 0.9375\n",
      "2017-11-06T19:23:27.925801: step 2785, loss 0.178475, acc 0.875\n",
      "2017-11-06T19:23:31.916634: step 2786, loss 0.106885, acc 0.9375\n",
      "2017-11-06T19:23:35.892459: step 2787, loss 0.272939, acc 0.84375\n",
      "2017-11-06T19:23:39.892301: step 2788, loss 0.255353, acc 0.84375\n",
      "2017-11-06T19:23:43.890142: step 2789, loss 0.210363, acc 0.90625\n",
      "2017-11-06T19:23:47.846953: step 2790, loss 0.0524751, acc 0.96875\n",
      "2017-11-06T19:23:51.856803: step 2791, loss 0.311746, acc 0.90625\n",
      "2017-11-06T19:23:55.898674: step 2792, loss 0.198965, acc 0.90625\n",
      "2017-11-06T19:23:59.890511: step 2793, loss 0.191711, acc 0.9375\n",
      "2017-11-06T19:24:03.822304: step 2794, loss 0.160844, acc 0.90625\n",
      "2017-11-06T19:24:08.005277: step 2795, loss 0.16716, acc 0.875\n",
      "2017-11-06T19:24:12.340357: step 2796, loss 0.292038, acc 0.875\n",
      "2017-11-06T19:24:16.398240: step 2797, loss 0.277579, acc 0.875\n",
      "2017-11-06T19:24:20.402084: step 2798, loss 0.292834, acc 0.875\n",
      "2017-11-06T19:24:24.487988: step 2799, loss 0.288562, acc 0.875\n",
      "2017-11-06T19:24:28.461813: step 2800, loss 0.081293, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:24:31.085677: step 2800, loss 0.750744, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-06T19:24:36.798593: step 2801, loss 0.0620416, acc 0.96875\n",
      "2017-11-06T19:24:40.898507: step 2802, loss 0.177878, acc 0.90625\n",
      "2017-11-06T19:24:44.952387: step 2803, loss 0.152427, acc 0.9375\n",
      "2017-11-06T19:24:48.959233: step 2804, loss 0.138234, acc 0.9375\n",
      "2017-11-06T19:24:52.958075: step 2805, loss 0.285583, acc 0.8125\n",
      "2017-11-06T19:24:56.969925: step 2806, loss 0.157915, acc 0.90625\n",
      "2017-11-06T19:25:00.973770: step 2807, loss 0.301311, acc 0.84375\n",
      "2017-11-06T19:25:03.604640: step 2808, loss 0.0968176, acc 0.95\n",
      "2017-11-06T19:25:07.602480: step 2809, loss 0.278669, acc 0.84375\n",
      "2017-11-06T19:25:11.639348: step 2810, loss 0.225787, acc 0.90625\n",
      "2017-11-06T19:25:16.044479: step 2811, loss 0.350465, acc 0.8125\n",
      "2017-11-06T19:25:20.126379: step 2812, loss 0.0422158, acc 1\n",
      "2017-11-06T19:25:24.103207: step 2813, loss 0.0697735, acc 0.96875\n",
      "2017-11-06T19:25:28.204119: step 2814, loss 0.0978038, acc 0.96875\n",
      "2017-11-06T19:25:32.235983: step 2815, loss 0.258534, acc 0.90625\n",
      "2017-11-06T19:25:36.224817: step 2816, loss 0.257515, acc 0.8125\n",
      "2017-11-06T19:25:40.255681: step 2817, loss 0.289061, acc 0.875\n",
      "2017-11-06T19:25:44.257526: step 2818, loss 0.284581, acc 0.8125\n",
      "2017-11-06T19:25:48.251363: step 2819, loss 0.071888, acc 0.9375\n",
      "2017-11-06T19:25:52.333263: step 2820, loss 0.163602, acc 0.9375\n",
      "2017-11-06T19:25:56.387143: step 2821, loss 0.0586052, acc 0.96875\n",
      "2017-11-06T19:26:00.443026: step 2822, loss 0.290308, acc 0.90625\n",
      "2017-11-06T19:26:04.586970: step 2823, loss 0.192102, acc 0.96875\n",
      "2017-11-06T19:26:08.619835: step 2824, loss 0.160359, acc 0.9375\n",
      "2017-11-06T19:26:12.701735: step 2825, loss 0.190563, acc 0.90625\n",
      "2017-11-06T19:26:16.705332: step 2826, loss 0.283918, acc 0.84375\n",
      "2017-11-06T19:26:21.113464: step 2827, loss 0.126931, acc 0.9375\n",
      "2017-11-06T19:26:25.288430: step 2828, loss 0.417878, acc 0.8125\n",
      "2017-11-06T19:26:29.316292: step 2829, loss 0.286323, acc 0.875\n",
      "2017-11-06T19:26:33.455233: step 2830, loss 0.0591827, acc 1\n",
      "2017-11-06T19:26:37.685239: step 2831, loss 0.157589, acc 0.875\n",
      "2017-11-06T19:26:41.734117: step 2832, loss 0.133686, acc 0.9375\n",
      "2017-11-06T19:26:45.726953: step 2833, loss 0.145881, acc 0.90625\n",
      "2017-11-06T19:26:49.687768: step 2834, loss 0.0826781, acc 1\n",
      "2017-11-06T19:26:53.754659: step 2835, loss 0.371872, acc 0.84375\n",
      "2017-11-06T19:26:57.683448: step 2836, loss 0.0650146, acc 0.96875\n",
      "2017-11-06T19:27:01.736328: step 2837, loss 0.262815, acc 0.90625\n",
      "2017-11-06T19:27:05.716158: step 2838, loss 0.0446631, acc 0.96875\n",
      "2017-11-06T19:27:09.746019: step 2839, loss 0.153096, acc 0.9375\n",
      "2017-11-06T19:27:13.876954: step 2840, loss 0.286963, acc 0.875\n",
      "2017-11-06T19:27:17.927833: step 2841, loss 0.268238, acc 0.84375\n",
      "2017-11-06T19:27:21.977711: step 2842, loss 0.227561, acc 0.875\n",
      "2017-11-06T19:27:26.391847: step 2843, loss 0.0639171, acc 0.96875\n",
      "2017-11-06T19:27:29.191836: step 2844, loss 0.495731, acc 0.85\n",
      "2017-11-06T19:27:33.205688: step 2845, loss 0.199455, acc 0.90625\n",
      "2017-11-06T19:27:37.182516: step 2846, loss 0.115155, acc 0.96875\n",
      "2017-11-06T19:27:41.201371: step 2847, loss 0.169018, acc 0.875\n",
      "2017-11-06T19:27:45.165186: step 2848, loss 0.0728481, acc 0.96875\n",
      "2017-11-06T19:27:49.237079: step 2849, loss 0.206145, acc 0.875\n",
      "2017-11-06T19:27:53.208901: step 2850, loss 0.111168, acc 0.9375\n",
      "2017-11-06T19:27:57.228757: step 2851, loss 0.252083, acc 0.875\n",
      "2017-11-06T19:28:01.248614: step 2852, loss 0.335046, acc 0.8125\n",
      "2017-11-06T19:28:05.211430: step 2853, loss 0.276567, acc 0.84375\n",
      "2017-11-06T19:28:09.136218: step 2854, loss 0.247189, acc 0.90625\n",
      "2017-11-06T19:28:13.070014: step 2855, loss 0.197911, acc 0.875\n",
      "2017-11-06T19:28:17.021821: step 2856, loss 0.183163, acc 0.90625\n",
      "2017-11-06T19:28:20.929598: step 2857, loss 0.241815, acc 0.875\n",
      "2017-11-06T19:28:25.025509: step 2858, loss 0.455851, acc 0.84375\n",
      "2017-11-06T19:28:29.238501: step 2859, loss 0.140186, acc 0.9375\n",
      "2017-11-06T19:28:33.820758: step 2860, loss 0.260298, acc 0.90625\n",
      "2017-11-06T19:28:37.863631: step 2861, loss 0.31442, acc 0.8125\n",
      "2017-11-06T19:28:41.800427: step 2862, loss 0.00911084, acc 1\n",
      "2017-11-06T19:28:45.718212: step 2863, loss 0.281679, acc 0.84375\n",
      "2017-11-06T19:28:49.692035: step 2864, loss 0.12921, acc 0.90625\n",
      "2017-11-06T19:28:53.680871: step 2865, loss 0.13272, acc 0.9375\n",
      "2017-11-06T19:28:57.587645: step 2866, loss 0.275189, acc 0.90625\n",
      "2017-11-06T19:29:01.556465: step 2867, loss 0.214262, acc 0.90625\n",
      "2017-11-06T19:29:05.490261: step 2868, loss 0.224036, acc 0.90625\n",
      "2017-11-06T19:29:09.474091: step 2869, loss 0.114729, acc 0.9375\n",
      "2017-11-06T19:29:13.399881: step 2870, loss 0.0705803, acc 0.96875\n",
      "2017-11-06T19:29:17.403470: step 2871, loss 0.192553, acc 0.9375\n",
      "2017-11-06T19:29:21.313250: step 2872, loss 0.106516, acc 0.9375\n",
      "2017-11-06T19:29:25.275065: step 2873, loss 0.131787, acc 0.9375\n",
      "2017-11-06T19:29:29.223871: step 2874, loss 0.0987734, acc 0.96875\n",
      "2017-11-06T19:29:33.179722: step 2875, loss 0.233463, acc 0.875\n",
      "2017-11-06T19:29:37.401721: step 2876, loss 0.127674, acc 0.96875\n",
      "2017-11-06T19:29:41.475616: step 2877, loss 0.186647, acc 0.9375\n",
      "2017-11-06T19:29:45.419418: step 2878, loss 0.208839, acc 0.90625\n",
      "2017-11-06T19:29:49.346208: step 2879, loss 0.0891024, acc 0.9375\n",
      "2017-11-06T19:29:51.856993: step 2880, loss 0.277127, acc 0.9\n",
      "2017-11-06T19:29:55.779781: step 2881, loss 0.0994292, acc 0.96875\n",
      "2017-11-06T19:29:59.719579: step 2882, loss 0.157057, acc 0.96875\n",
      "2017-11-06T19:30:03.924567: step 2883, loss 0.220782, acc 0.875\n",
      "2017-11-06T19:30:07.860363: step 2884, loss 0.194657, acc 0.9375\n",
      "2017-11-06T19:30:11.785152: step 2885, loss 0.134268, acc 0.9375\n",
      "2017-11-06T19:30:15.759977: step 2886, loss 0.113665, acc 0.96875\n",
      "2017-11-06T19:30:19.673757: step 2887, loss 0.350607, acc 0.84375\n",
      "2017-11-06T19:30:23.612556: step 2888, loss 0.280596, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T19:30:27.616401: step 2889, loss 0.250231, acc 0.90625\n",
      "2017-11-06T19:30:31.546194: step 2890, loss 0.348524, acc 0.84375\n",
      "2017-11-06T19:30:35.817230: step 2891, loss 0.131219, acc 0.96875\n",
      "2017-11-06T19:30:39.774039: step 2892, loss 0.310621, acc 0.90625\n",
      "2017-11-06T19:30:44.134137: step 2893, loss 0.164812, acc 0.90625\n",
      "2017-11-06T19:30:48.109962: step 2894, loss 0.295393, acc 0.90625\n",
      "2017-11-06T19:30:52.002728: step 2895, loss 0.14481, acc 0.96875\n",
      "2017-11-06T19:30:55.936523: step 2896, loss 0.15397, acc 0.96875\n",
      "2017-11-06T19:30:59.855310: step 2897, loss 0.0885671, acc 0.96875\n",
      "2017-11-06T19:31:03.824128: step 2898, loss 0.178317, acc 0.9375\n",
      "2017-11-06T19:31:07.767932: step 2899, loss 0.612726, acc 0.875\n",
      "2017-11-06T19:31:11.748759: step 2900, loss 0.0906085, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:31:14.324591: step 2900, loss 1.01685, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-06T19:31:19.567760: step 2901, loss 0.232964, acc 0.90625\n",
      "2017-11-06T19:31:23.565601: step 2902, loss 0.324184, acc 0.8125\n",
      "2017-11-06T19:31:27.507403: step 2903, loss 0.232611, acc 0.84375\n",
      "2017-11-06T19:31:31.498238: step 2904, loss 0.126984, acc 0.90625\n",
      "2017-11-06T19:31:35.593208: step 2905, loss 0.383628, acc 0.84375\n",
      "2017-11-06T19:31:39.696124: step 2906, loss 0.17048, acc 0.90625\n",
      "2017-11-06T19:31:43.717982: step 2907, loss 0.493654, acc 0.8125\n",
      "2017-11-06T19:31:47.897952: step 2908, loss 0.384218, acc 0.90625\n",
      "2017-11-06T19:31:52.104941: step 2909, loss 0.0657713, acc 1\n",
      "2017-11-06T19:31:56.065755: step 2910, loss 0.237286, acc 0.875\n",
      "2017-11-06T19:31:59.980536: step 2911, loss 0.191806, acc 0.90625\n",
      "2017-11-06T19:32:03.967369: step 2912, loss 0.284305, acc 0.84375\n",
      "2017-11-06T19:32:07.860135: step 2913, loss 0.127914, acc 0.9375\n",
      "2017-11-06T19:32:11.939033: step 2914, loss 0.175891, acc 0.875\n",
      "2017-11-06T19:32:15.880600: step 2915, loss 0.104375, acc 0.96875\n",
      "2017-11-06T19:32:18.430412: step 2916, loss 0.233484, acc 0.85\n",
      "2017-11-06T19:32:22.359203: step 2917, loss 0.169844, acc 0.875\n",
      "2017-11-06T19:32:26.298002: step 2918, loss 0.0448625, acc 0.96875\n",
      "2017-11-06T19:32:30.218788: step 2919, loss 0.214545, acc 0.90625\n",
      "2017-11-06T19:32:34.477815: step 2920, loss 0.208015, acc 0.875\n",
      "2017-11-06T19:32:38.453640: step 2921, loss 0.184489, acc 0.96875\n",
      "2017-11-06T19:32:42.427463: step 2922, loss 0.0894226, acc 0.96875\n",
      "2017-11-06T19:32:46.370265: step 2923, loss 0.136716, acc 0.9375\n",
      "2017-11-06T19:32:50.316068: step 2924, loss 0.199538, acc 0.9375\n",
      "2017-11-06T19:32:54.695180: step 2925, loss 0.0685163, acc 0.96875\n",
      "2017-11-06T19:32:58.833119: step 2926, loss 0.0730497, acc 0.96875\n",
      "2017-11-06T19:33:02.806945: step 2927, loss 0.0547531, acc 0.96875\n",
      "2017-11-06T19:33:06.771761: step 2928, loss 0.364737, acc 0.875\n",
      "2017-11-06T19:33:10.699552: step 2929, loss 0.119265, acc 0.9375\n",
      "2017-11-06T19:33:14.597321: step 2930, loss 0.0273896, acc 1\n",
      "2017-11-06T19:33:18.626183: step 2931, loss 0.208151, acc 0.875\n",
      "2017-11-06T19:33:22.680066: step 2932, loss 0.0687773, acc 0.96875\n",
      "2017-11-06T19:33:26.800992: step 2933, loss 0.188744, acc 0.875\n",
      "2017-11-06T19:33:30.826852: step 2934, loss 0.167603, acc 0.9375\n",
      "2017-11-06T19:33:34.785666: step 2935, loss 0.205322, acc 0.84375\n",
      "2017-11-06T19:33:38.759490: step 2936, loss 0.210291, acc 0.875\n",
      "2017-11-06T19:33:42.897429: step 2937, loss 0.292674, acc 0.90625\n",
      "2017-11-06T19:33:46.961317: step 2938, loss 0.122407, acc 0.9375\n",
      "2017-11-06T19:33:51.050222: step 2939, loss 0.361897, acc 0.8125\n",
      "2017-11-06T19:33:55.101100: step 2940, loss 0.33734, acc 0.90625\n",
      "2017-11-06T19:33:59.348118: step 2941, loss 0.139701, acc 0.90625\n",
      "2017-11-06T19:34:03.695207: step 2942, loss 0.226684, acc 0.90625\n",
      "2017-11-06T19:34:07.752090: step 2943, loss 0.438241, acc 0.78125\n",
      "2017-11-06T19:34:11.937063: step 2944, loss 0.15741, acc 0.90625\n",
      "2017-11-06T19:34:15.934904: step 2945, loss 0.102527, acc 0.96875\n",
      "2017-11-06T19:34:20.100864: step 2946, loss 0.363534, acc 0.8125\n",
      "2017-11-06T19:34:24.259819: step 2947, loss 0.195673, acc 0.90625\n",
      "2017-11-06T19:34:28.348724: step 2948, loss 0.198441, acc 0.84375\n",
      "2017-11-06T19:34:32.692811: step 2949, loss 0.331382, acc 0.84375\n",
      "2017-11-06T19:34:37.169992: step 2950, loss 0.291664, acc 0.875\n",
      "2017-11-06T19:34:41.171835: step 2951, loss 0.231404, acc 0.90625\n",
      "2017-11-06T19:34:43.735658: step 2952, loss 0.191201, acc 0.95\n",
      "2017-11-06T19:34:47.725494: step 2953, loss 0.265798, acc 0.90625\n",
      "2017-11-06T19:34:51.679305: step 2954, loss 0.124375, acc 0.9375\n",
      "2017-11-06T19:34:55.653127: step 2955, loss 0.133049, acc 0.90625\n",
      "2017-11-06T19:34:59.667978: step 2956, loss 0.196849, acc 0.90625\n",
      "2017-11-06T19:35:03.756883: step 2957, loss 0.31482, acc 0.875\n",
      "2017-11-06T19:35:08.145001: step 2958, loss 0.1259, acc 0.96875\n",
      "2017-11-06T19:35:12.133836: step 2959, loss 0.210088, acc 0.9375\n",
      "2017-11-06T19:35:16.114458: step 2960, loss 0.24936, acc 0.875\n",
      "2017-11-06T19:35:20.128309: step 2961, loss 0.153557, acc 0.9375\n",
      "2017-11-06T19:35:24.136157: step 2962, loss 0.168235, acc 0.9375\n",
      "2017-11-06T19:35:28.092971: step 2963, loss 0.164569, acc 0.9375\n",
      "2017-11-06T19:35:32.104821: step 2964, loss 0.160916, acc 0.90625\n",
      "2017-11-06T19:35:36.052625: step 2965, loss 0.140328, acc 0.9375\n",
      "2017-11-06T19:35:40.158542: step 2966, loss 0.143776, acc 0.9375\n",
      "2017-11-06T19:35:44.129363: step 2967, loss 0.11487, acc 0.9375\n",
      "2017-11-06T19:35:48.070164: step 2968, loss 0.192799, acc 0.9375\n",
      "2017-11-06T19:35:52.079012: step 2969, loss 0.155587, acc 0.90625\n",
      "2017-11-06T19:35:56.141899: step 2970, loss 0.0904729, acc 0.96875\n",
      "2017-11-06T19:36:00.122728: step 2971, loss 0.281828, acc 0.875\n",
      "2017-11-06T19:36:04.156594: step 2972, loss 0.0536919, acc 0.96875\n",
      "2017-11-06T19:36:08.231489: step 2973, loss 0.306024, acc 0.875\n",
      "2017-11-06T19:36:12.573574: step 2974, loss 0.100699, acc 0.90625\n",
      "2017-11-06T19:36:16.695504: step 2975, loss 0.175074, acc 0.9375\n",
      "2017-11-06T19:36:20.657318: step 2976, loss 0.168677, acc 0.9375\n",
      "2017-11-06T19:36:24.693186: step 2977, loss 0.162961, acc 0.90625\n",
      "2017-11-06T19:36:28.731055: step 2978, loss 0.241723, acc 0.9375\n",
      "2017-11-06T19:36:32.770925: step 2979, loss 0.273495, acc 0.875\n",
      "2017-11-06T19:36:36.953898: step 2980, loss 0.213376, acc 0.90625\n",
      "2017-11-06T19:36:40.973753: step 2981, loss 0.278059, acc 0.90625\n",
      "2017-11-06T19:36:44.939572: step 2982, loss 0.236764, acc 0.90625\n",
      "2017-11-06T19:36:48.907390: step 2983, loss 0.26615, acc 0.84375\n",
      "2017-11-06T19:36:52.885217: step 2984, loss 0.230473, acc 0.875\n",
      "2017-11-06T19:36:56.861044: step 2985, loss 0.207202, acc 0.875\n",
      "2017-11-06T19:37:00.811850: step 2986, loss 0.257879, acc 0.8125\n",
      "2017-11-06T19:37:04.792679: step 2987, loss 0.128648, acc 0.9375\n",
      "2017-11-06T19:37:07.336486: step 2988, loss 0.317861, acc 0.85\n",
      "2017-11-06T19:37:11.421388: step 2989, loss 0.115973, acc 0.9375\n",
      "2017-11-06T19:37:15.665403: step 2990, loss 0.271868, acc 0.84375\n",
      "2017-11-06T19:37:19.885402: step 2991, loss 0.34641, acc 0.84375\n",
      "2017-11-06T19:37:23.863229: step 2992, loss 0.252021, acc 0.84375\n",
      "2017-11-06T19:37:27.883084: step 2993, loss 0.0673312, acc 0.96875\n",
      "2017-11-06T19:37:31.868918: step 2994, loss 0.171467, acc 0.90625\n",
      "2017-11-06T19:37:35.905785: step 2995, loss 0.113076, acc 0.9375\n",
      "2017-11-06T19:37:39.923640: step 2996, loss 0.159937, acc 0.9375\n",
      "2017-11-06T19:37:43.948500: step 2997, loss 0.302912, acc 0.90625\n",
      "2017-11-06T19:37:47.962352: step 2998, loss 0.125272, acc 0.9375\n",
      "2017-11-06T19:37:51.979206: step 2999, loss 0.518945, acc 0.84375\n",
      "2017-11-06T19:37:55.963037: step 3000, loss 0.185316, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:37:58.492835: step 3000, loss 0.830304, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-06T19:38:03.776452: step 3001, loss 0.199448, acc 0.9375\n",
      "2017-11-06T19:38:07.782298: step 3002, loss 0.0672989, acc 0.96875\n",
      "2017-11-06T19:38:11.706086: step 3003, loss 0.263432, acc 0.875\n",
      "2017-11-06T19:38:15.670677: step 3004, loss 0.322414, acc 0.875\n",
      "2017-11-06T19:38:19.687531: step 3005, loss 0.0620882, acc 0.96875\n",
      "2017-11-06T19:38:24.101668: step 3006, loss 0.164928, acc 0.90625\n",
      "2017-11-06T19:38:28.081495: step 3007, loss 0.0687008, acc 0.9375\n",
      "2017-11-06T19:38:32.094347: step 3008, loss 0.1639, acc 0.96875\n",
      "2017-11-06T19:38:36.340364: step 3009, loss 0.140827, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T19:38:40.330198: step 3010, loss 0.146639, acc 0.90625\n",
      "2017-11-06T19:38:44.319033: step 3011, loss 0.0898502, acc 0.9375\n",
      "2017-11-06T19:38:48.345893: step 3012, loss 0.127373, acc 0.9375\n",
      "2017-11-06T19:38:52.232655: step 3013, loss 0.271975, acc 0.90625\n",
      "2017-11-06T19:38:56.196473: step 3014, loss 0.20794, acc 0.84375\n",
      "2017-11-06T19:39:00.199318: step 3015, loss 0.350765, acc 0.84375\n",
      "2017-11-06T19:39:04.155128: step 3016, loss 0.21254, acc 0.8125\n",
      "2017-11-06T19:39:08.164977: step 3017, loss 0.268546, acc 0.8125\n",
      "2017-11-06T19:39:12.154811: step 3018, loss 0.148627, acc 0.9375\n",
      "2017-11-06T19:39:16.173667: step 3019, loss 0.320721, acc 0.90625\n",
      "2017-11-06T19:39:20.116468: step 3020, loss 0.134996, acc 0.9375\n",
      "2017-11-06T19:39:24.170350: step 3021, loss 0.0984159, acc 0.9375\n",
      "2017-11-06T19:39:28.951746: step 3022, loss 0.17849, acc 0.875\n",
      "2017-11-06T19:39:33.055662: step 3023, loss 0.153965, acc 0.9375\n",
      "2017-11-06T19:39:35.605474: step 3024, loss 0.0716439, acc 0.95\n",
      "2017-11-06T19:39:39.589305: step 3025, loss 0.197535, acc 0.90625\n",
      "2017-11-06T19:39:43.601156: step 3026, loss 0.169999, acc 0.90625\n",
      "2017-11-06T19:39:47.699066: step 3027, loss 0.169417, acc 0.90625\n",
      "2017-11-06T19:39:51.649874: step 3028, loss 0.0972096, acc 0.96875\n",
      "2017-11-06T19:39:55.676735: step 3029, loss 0.591681, acc 0.8125\n",
      "2017-11-06T19:39:59.708600: step 3030, loss 0.24329, acc 0.84375\n",
      "2017-11-06T19:40:04.024668: step 3031, loss 0.303328, acc 0.84375\n",
      "2017-11-06T19:40:07.956460: step 3032, loss 0.20192, acc 0.875\n",
      "2017-11-06T19:40:11.942292: step 3033, loss 0.158375, acc 0.9375\n",
      "2017-11-06T19:40:15.999183: step 3034, loss 0.114197, acc 0.9375\n",
      "2017-11-06T19:40:20.152127: step 3035, loss 0.116224, acc 0.9375\n",
      "2017-11-06T19:40:24.152968: step 3036, loss 0.154511, acc 0.9375\n",
      "2017-11-06T19:40:28.124792: step 3037, loss 0.105373, acc 0.90625\n",
      "2017-11-06T19:40:32.188679: step 3038, loss 0.362297, acc 0.8125\n",
      "2017-11-06T19:40:36.755924: step 3039, loss 0.0527687, acc 1\n",
      "2017-11-06T19:40:40.757767: step 3040, loss 0.127594, acc 0.90625\n",
      "2017-11-06T19:40:44.778624: step 3041, loss 0.0612138, acc 0.96875\n",
      "2017-11-06T19:40:48.794477: step 3042, loss 0.116869, acc 0.9375\n",
      "2017-11-06T19:40:52.757293: step 3043, loss 0.0917007, acc 0.90625\n",
      "2017-11-06T19:40:56.743145: step 3044, loss 0.110949, acc 0.9375\n",
      "2017-11-06T19:41:00.760980: step 3045, loss 0.121753, acc 0.9375\n",
      "2017-11-06T19:41:04.798849: step 3046, loss 0.144386, acc 0.9375\n",
      "2017-11-06T19:41:08.820707: step 3047, loss 0.154953, acc 0.9375\n",
      "2017-11-06T19:41:13.770223: step 3048, loss 0.106753, acc 0.9375\n",
      "2017-11-06T19:41:18.216182: step 3049, loss 0.122197, acc 0.9375\n",
      "2017-11-06T19:41:22.277067: step 3050, loss 0.13453, acc 0.9375\n",
      "2017-11-06T19:41:26.345959: step 3051, loss 0.0825683, acc 0.96875\n",
      "2017-11-06T19:41:30.314778: step 3052, loss 0.208463, acc 0.90625\n",
      "2017-11-06T19:41:34.313620: step 3053, loss 0.271573, acc 0.875\n",
      "2017-11-06T19:41:38.509601: step 3054, loss 0.249589, acc 0.875\n",
      "2017-11-06T19:41:42.882709: step 3055, loss 0.265269, acc 0.875\n",
      "2017-11-06T19:41:46.869541: step 3056, loss 0.112343, acc 0.90625\n",
      "2017-11-06T19:41:50.891401: step 3057, loss 0.247523, acc 0.875\n",
      "2017-11-06T19:41:54.921262: step 3058, loss 0.272263, acc 0.875\n",
      "2017-11-06T19:41:58.886080: step 3059, loss 0.161418, acc 0.875\n",
      "2017-11-06T19:42:01.497936: step 3060, loss 0.203482, acc 0.9\n",
      "2017-11-06T19:42:05.516792: step 3061, loss 0.091254, acc 0.9375\n",
      "2017-11-06T19:42:09.526641: step 3062, loss 0.197977, acc 0.90625\n",
      "2017-11-06T19:42:13.488458: step 3063, loss 0.215505, acc 0.875\n",
      "2017-11-06T19:42:17.508312: step 3064, loss 0.28503, acc 0.84375\n",
      "2017-11-06T19:42:21.473129: step 3065, loss 0.309218, acc 0.84375\n",
      "2017-11-06T19:42:25.463965: step 3066, loss 0.235496, acc 0.875\n",
      "2017-11-06T19:42:29.523849: step 3067, loss 0.170243, acc 0.9375\n",
      "2017-11-06T19:42:33.762861: step 3068, loss 0.0269225, acc 1\n",
      "2017-11-06T19:42:37.847765: step 3069, loss 0.0264308, acc 1\n",
      "2017-11-06T19:42:41.864619: step 3070, loss 0.197359, acc 0.9375\n",
      "2017-11-06T19:42:46.277754: step 3071, loss 0.113343, acc 0.96875\n",
      "2017-11-06T19:42:50.359656: step 3072, loss 0.308476, acc 0.875\n",
      "2017-11-06T19:42:54.356494: step 3073, loss 0.0542709, acc 0.96875\n",
      "2017-11-06T19:42:58.329316: step 3074, loss 0.2314, acc 0.90625\n",
      "2017-11-06T19:43:02.368187: step 3075, loss 0.170811, acc 0.9375\n",
      "2017-11-06T19:43:06.306986: step 3076, loss 0.173147, acc 0.90625\n",
      "2017-11-06T19:43:10.321838: step 3077, loss 0.105901, acc 0.9375\n",
      "2017-11-06T19:43:14.313676: step 3078, loss 0.122842, acc 0.9375\n",
      "2017-11-06T19:43:18.333530: step 3079, loss 0.0709294, acc 0.96875\n",
      "2017-11-06T19:43:22.488484: step 3080, loss 0.0484769, acc 0.96875\n",
      "2017-11-06T19:43:27.056855: step 3081, loss 0.201165, acc 0.90625\n",
      "2017-11-06T19:43:31.338396: step 3082, loss 0.198988, acc 0.9375\n",
      "2017-11-06T19:43:35.473336: step 3083, loss 0.171159, acc 0.9375\n",
      "2017-11-06T19:43:39.506940: step 3084, loss 0.199812, acc 0.9375\n",
      "2017-11-06T19:43:43.496794: step 3085, loss 0.191383, acc 0.90625\n",
      "2017-11-06T19:43:47.498637: step 3086, loss 0.304546, acc 0.84375\n",
      "2017-11-06T19:43:51.970815: step 3087, loss 0.30443, acc 0.8125\n",
      "2017-11-06T19:43:56.080735: step 3088, loss 0.203868, acc 0.90625\n",
      "2017-11-06T19:44:00.119606: step 3089, loss 0.423868, acc 0.78125\n",
      "2017-11-06T19:44:04.059404: step 3090, loss 0.22319, acc 0.875\n",
      "2017-11-06T19:44:08.030226: step 3091, loss 0.244711, acc 0.8125\n",
      "2017-11-06T19:44:11.991040: step 3092, loss 0.074839, acc 0.96875\n",
      "2017-11-06T19:44:15.978873: step 3093, loss 0.310171, acc 0.90625\n",
      "2017-11-06T19:44:20.033072: step 3094, loss 0.348005, acc 0.875\n",
      "2017-11-06T19:44:24.031914: step 3095, loss 0.195423, acc 0.875\n",
      "2017-11-06T19:44:26.627758: step 3096, loss 0.0614287, acc 1\n",
      "2017-11-06T19:44:30.646614: step 3097, loss 0.0659016, acc 1\n",
      "2017-11-06T19:44:34.892630: step 3098, loss 0.239447, acc 0.90625\n",
      "2017-11-06T19:44:38.940507: step 3099, loss 0.0588993, acc 0.96875\n",
      "2017-11-06T19:44:42.998390: step 3100, loss 0.171792, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:44:45.568216: step 3100, loss 0.77563, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-06T19:44:50.956633: step 3101, loss 0.144366, acc 0.9375\n",
      "2017-11-06T19:44:55.172630: step 3102, loss 0.163865, acc 0.9375\n",
      "2017-11-06T19:44:59.424650: step 3103, loss 0.0751428, acc 0.96875\n",
      "2017-11-06T19:45:03.416487: step 3104, loss 0.0845975, acc 0.96875\n",
      "2017-11-06T19:45:07.355285: step 3105, loss 0.0606526, acc 1\n",
      "2017-11-06T19:45:11.313098: step 3106, loss 0.133478, acc 0.96875\n",
      "2017-11-06T19:45:15.308936: step 3107, loss 0.189669, acc 0.90625\n",
      "2017-11-06T19:45:19.371824: step 3108, loss 0.186536, acc 0.875\n",
      "2017-11-06T19:45:23.323632: step 3109, loss 0.239655, acc 0.9375\n",
      "2017-11-06T19:45:27.363501: step 3110, loss 0.423933, acc 0.8125\n",
      "2017-11-06T19:45:31.350335: step 3111, loss 0.242446, acc 0.90625\n",
      "2017-11-06T19:45:35.324158: step 3112, loss 0.149577, acc 0.9375\n",
      "2017-11-06T19:45:39.356023: step 3113, loss 0.178928, acc 0.90625\n",
      "2017-11-06T19:45:43.287817: step 3114, loss 0.248135, acc 0.875\n",
      "2017-11-06T19:45:47.300668: step 3115, loss 0.131765, acc 0.96875\n",
      "2017-11-06T19:45:51.349546: step 3116, loss 0.218745, acc 0.90625\n",
      "2017-11-06T19:45:55.297350: step 3117, loss 0.185413, acc 0.90625\n",
      "2017-11-06T19:45:59.382253: step 3118, loss 0.152206, acc 0.96875\n",
      "2017-11-06T19:46:03.707326: step 3119, loss 0.15944, acc 0.9375\n",
      "2017-11-06T19:46:07.760206: step 3120, loss 0.104557, acc 0.9375\n",
      "2017-11-06T19:46:11.781063: step 3121, loss 0.196808, acc 0.90625\n",
      "2017-11-06T19:46:15.731870: step 3122, loss 0.23487, acc 0.90625\n",
      "2017-11-06T19:46:19.750726: step 3123, loss 0.325351, acc 0.875\n",
      "2017-11-06T19:46:23.852642: step 3124, loss 0.268188, acc 0.90625\n",
      "2017-11-06T19:46:27.854483: step 3125, loss 0.18758, acc 0.90625\n",
      "2017-11-06T19:46:31.891352: step 3126, loss 0.266158, acc 0.875\n",
      "2017-11-06T19:46:36.234438: step 3127, loss 0.14941, acc 0.90625\n",
      "2017-11-06T19:46:40.298325: step 3128, loss 0.19407, acc 0.90625\n",
      "2017-11-06T19:46:44.318181: step 3129, loss 0.107407, acc 0.9375\n",
      "2017-11-06T19:46:48.311019: step 3130, loss 0.190493, acc 0.9375\n",
      "2017-11-06T19:46:52.357894: step 3131, loss 0.271579, acc 0.84375\n",
      "2017-11-06T19:46:54.980758: step 3132, loss 0.130112, acc 0.95\n",
      "2017-11-06T19:46:59.112694: step 3133, loss 0.226407, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T19:47:03.202600: step 3134, loss 0.0913941, acc 0.9375\n",
      "2017-11-06T19:47:07.562699: step 3135, loss 0.148112, acc 0.9375\n",
      "2017-11-06T19:47:11.742668: step 3136, loss 0.139653, acc 0.90625\n",
      "2017-11-06T19:47:15.895620: step 3137, loss 0.122206, acc 0.9375\n",
      "2017-11-06T19:47:19.845176: step 3138, loss 0.0626117, acc 0.96875\n",
      "2017-11-06T19:47:23.776971: step 3139, loss 0.105298, acc 0.96875\n",
      "2017-11-06T19:47:27.846863: step 3140, loss 0.1421, acc 0.875\n",
      "2017-11-06T19:47:31.753637: step 3141, loss 0.190914, acc 0.875\n",
      "2017-11-06T19:47:35.730463: step 3142, loss 0.0852833, acc 0.9375\n",
      "2017-11-06T19:47:39.755323: step 3143, loss 0.0169635, acc 1\n",
      "2017-11-06T19:47:43.769175: step 3144, loss 0.0141965, acc 1\n",
      "2017-11-06T19:47:47.705973: step 3145, loss 0.226138, acc 0.90625\n",
      "2017-11-06T19:47:51.727830: step 3146, loss 0.195359, acc 0.9375\n",
      "2017-11-06T19:47:55.751689: step 3147, loss 0.242972, acc 0.90625\n",
      "2017-11-06T19:47:59.722510: step 3148, loss 0.0757961, acc 0.96875\n",
      "2017-11-06T19:48:03.689329: step 3149, loss 0.329831, acc 0.78125\n",
      "2017-11-06T19:48:07.735204: step 3150, loss 0.104526, acc 0.9375\n",
      "2017-11-06T19:48:11.928183: step 3151, loss 0.159432, acc 0.9375\n",
      "2017-11-06T19:48:16.247252: step 3152, loss 0.192339, acc 0.90625\n",
      "2017-11-06T19:48:20.296129: step 3153, loss 0.275933, acc 0.875\n",
      "2017-11-06T19:48:24.311982: step 3154, loss 0.117026, acc 0.9375\n",
      "2017-11-06T19:48:28.326836: step 3155, loss 0.183925, acc 0.90625\n",
      "2017-11-06T19:48:32.306663: step 3156, loss 0.140517, acc 0.875\n",
      "2017-11-06T19:48:36.575696: step 3157, loss 0.0963275, acc 0.90625\n",
      "2017-11-06T19:48:40.625575: step 3158, loss 0.15468, acc 0.9375\n",
      "2017-11-06T19:48:44.607403: step 3159, loss 0.202035, acc 0.84375\n",
      "2017-11-06T19:48:48.610248: step 3160, loss 0.416254, acc 0.8125\n",
      "2017-11-06T19:48:52.603084: step 3161, loss 0.235448, acc 0.84375\n",
      "2017-11-06T19:48:56.578909: step 3162, loss 0.202987, acc 0.875\n",
      "2017-11-06T19:49:00.666814: step 3163, loss 0.114232, acc 0.90625\n",
      "2017-11-06T19:49:04.656649: step 3164, loss 0.483806, acc 0.8125\n",
      "2017-11-06T19:49:08.682509: step 3165, loss 0.35464, acc 0.78125\n",
      "2017-11-06T19:49:12.695362: step 3166, loss 0.345444, acc 0.84375\n",
      "2017-11-06T19:49:16.898347: step 3167, loss 0.0378874, acc 1\n",
      "2017-11-06T19:49:19.738365: step 3168, loss 0.163686, acc 0.9\n",
      "2017-11-06T19:49:23.887313: step 3169, loss 0.169395, acc 0.9375\n",
      "2017-11-06T19:49:28.134331: step 3170, loss 0.18176, acc 0.9375\n",
      "2017-11-06T19:49:32.159191: step 3171, loss 0.191678, acc 0.875\n",
      "2017-11-06T19:49:36.191056: step 3172, loss 0.127482, acc 0.9375\n",
      "2017-11-06T19:49:40.178889: step 3173, loss 0.229244, acc 0.875\n",
      "2017-11-06T19:49:44.153714: step 3174, loss 0.126358, acc 0.90625\n",
      "2017-11-06T19:49:48.137544: step 3175, loss 0.426946, acc 0.84375\n",
      "2017-11-06T19:49:52.142389: step 3176, loss 0.385421, acc 0.84375\n",
      "2017-11-06T19:49:56.188266: step 3177, loss 0.125775, acc 0.9375\n",
      "2017-11-06T19:50:00.305190: step 3178, loss 0.261129, acc 0.8125\n",
      "2017-11-06T19:50:04.551208: step 3179, loss 0.201153, acc 0.90625\n",
      "2017-11-06T19:50:08.615094: step 3180, loss 0.123399, acc 0.9375\n",
      "2017-11-06T19:50:12.661969: step 3181, loss 0.481311, acc 0.875\n",
      "2017-11-06T19:50:16.605772: step 3182, loss 0.128383, acc 0.96875\n",
      "2017-11-06T19:50:20.979732: step 3183, loss 0.110912, acc 0.96875\n",
      "2017-11-06T19:50:25.762631: step 3184, loss 0.133348, acc 0.9375\n",
      "2017-11-06T19:50:30.089713: step 3185, loss 0.129834, acc 0.90625\n",
      "2017-11-06T19:50:34.444808: step 3186, loss 0.035524, acc 1\n",
      "2017-11-06T19:50:38.500719: step 3187, loss 0.267225, acc 0.84375\n",
      "2017-11-06T19:50:42.461514: step 3188, loss 0.0862636, acc 0.9375\n",
      "2017-11-06T19:50:46.450349: step 3189, loss 0.0580129, acc 0.96875\n",
      "2017-11-06T19:50:50.458196: step 3190, loss 0.0408129, acc 0.96875\n",
      "2017-11-06T19:50:54.504071: step 3191, loss 0.254646, acc 0.875\n",
      "2017-11-06T19:50:58.555950: step 3192, loss 0.269775, acc 0.90625\n",
      "2017-11-06T19:51:02.545785: step 3193, loss 0.0841072, acc 0.96875\n",
      "2017-11-06T19:51:06.531617: step 3194, loss 0.177472, acc 0.9375\n",
      "2017-11-06T19:51:10.629529: step 3195, loss 0.319101, acc 0.90625\n",
      "2017-11-06T19:51:14.590345: step 3196, loss 0.23118, acc 0.875\n",
      "2017-11-06T19:51:18.610199: step 3197, loss 0.103262, acc 0.96875\n",
      "2017-11-06T19:51:22.601035: step 3198, loss 0.242482, acc 0.875\n",
      "2017-11-06T19:51:26.634901: step 3199, loss 0.272478, acc 0.84375\n",
      "2017-11-06T19:51:31.059044: step 3200, loss 0.225903, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:51:33.830014: step 3200, loss 0.903306, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-06T19:51:39.054761: step 3201, loss 0.220913, acc 0.90625\n",
      "2017-11-06T19:51:43.057605: step 3202, loss 0.183158, acc 0.90625\n",
      "2017-11-06T19:51:47.078462: step 3203, loss 0.127117, acc 0.90625\n",
      "2017-11-06T19:51:49.741355: step 3204, loss 0.270144, acc 0.85\n",
      "2017-11-06T19:51:53.722182: step 3205, loss 0.0627049, acc 0.96875\n",
      "2017-11-06T19:51:57.876257: step 3206, loss 0.159595, acc 0.9375\n",
      "2017-11-06T19:52:01.884104: step 3207, loss 0.107002, acc 0.9375\n",
      "2017-11-06T19:52:05.864933: step 3208, loss 0.19091, acc 0.84375\n",
      "2017-11-06T19:52:09.882788: step 3209, loss 0.17366, acc 0.9375\n",
      "2017-11-06T19:52:13.898641: step 3210, loss 0.130977, acc 0.90625\n",
      "2017-11-06T19:52:17.916495: step 3211, loss 0.186012, acc 0.875\n",
      "2017-11-06T19:52:21.957367: step 3212, loss 0.248198, acc 0.90625\n",
      "2017-11-06T19:52:25.961212: step 3213, loss 0.225101, acc 0.875\n",
      "2017-11-06T19:52:29.954049: step 3214, loss 0.152266, acc 0.9375\n",
      "2017-11-06T19:52:34.425226: step 3215, loss 0.149392, acc 0.9375\n",
      "2017-11-06T19:52:38.824353: step 3216, loss 0.114511, acc 0.96875\n",
      "2017-11-06T19:52:42.786169: step 3217, loss 0.182161, acc 0.875\n",
      "2017-11-06T19:52:46.792013: step 3218, loss 0.332881, acc 0.875\n",
      "2017-11-06T19:52:50.800862: step 3219, loss 0.241386, acc 0.90625\n",
      "2017-11-06T19:52:54.799703: step 3220, loss 0.189174, acc 0.90625\n",
      "2017-11-06T19:52:58.812554: step 3221, loss 0.065199, acc 0.96875\n",
      "2017-11-06T19:53:02.858429: step 3222, loss 0.044678, acc 1\n",
      "2017-11-06T19:53:06.853267: step 3223, loss 0.166779, acc 0.90625\n",
      "2017-11-06T19:53:10.874125: step 3224, loss 0.0465803, acc 0.96875\n",
      "2017-11-06T19:53:14.922001: step 3225, loss 0.210984, acc 0.875\n",
      "2017-11-06T19:53:18.972668: step 3226, loss 0.320994, acc 0.84375\n",
      "2017-11-06T19:53:23.151637: step 3227, loss 0.164123, acc 0.90625\n",
      "2017-11-06T19:53:27.238541: step 3228, loss 0.104112, acc 0.9375\n",
      "2017-11-06T19:53:31.273409: step 3229, loss 0.215643, acc 0.875\n",
      "2017-11-06T19:53:35.311277: step 3230, loss 0.115287, acc 0.9375\n",
      "2017-11-06T19:53:39.494250: step 3231, loss 0.146314, acc 0.9375\n",
      "2017-11-06T19:53:43.828330: step 3232, loss 0.293388, acc 0.875\n",
      "2017-11-06T19:53:47.953260: step 3233, loss 0.230424, acc 0.9375\n",
      "2017-11-06T19:53:52.031158: step 3234, loss 0.376219, acc 0.8125\n",
      "2017-11-06T19:53:56.153086: step 3235, loss 0.0817218, acc 0.96875\n",
      "2017-11-06T19:54:00.157932: step 3236, loss 0.331136, acc 0.875\n",
      "2017-11-06T19:54:04.215815: step 3237, loss 0.308668, acc 0.84375\n",
      "2017-11-06T19:54:08.239674: step 3238, loss 0.0827113, acc 0.96875\n",
      "2017-11-06T19:54:12.327579: step 3239, loss 0.0760392, acc 0.96875\n",
      "2017-11-06T19:54:14.955447: step 3240, loss 0.166728, acc 0.85\n",
      "2017-11-06T19:54:18.983308: step 3241, loss 0.253271, acc 0.875\n",
      "2017-11-06T19:54:22.959134: step 3242, loss 0.337493, acc 0.78125\n",
      "2017-11-06T19:54:26.983994: step 3243, loss 0.130849, acc 0.90625\n",
      "2017-11-06T19:54:30.916787: step 3244, loss 0.250139, acc 0.875\n",
      "2017-11-06T19:54:35.214841: step 3245, loss 0.125049, acc 0.9375\n",
      "2017-11-06T19:54:39.244706: step 3246, loss 0.109185, acc 0.9375\n",
      "2017-11-06T19:54:43.237542: step 3247, loss 0.143127, acc 0.90625\n",
      "2017-11-06T19:54:47.668690: step 3248, loss 0.119845, acc 0.96875\n",
      "2017-11-06T19:54:51.813636: step 3249, loss 0.168269, acc 0.9375\n",
      "2017-11-06T19:54:55.871520: step 3250, loss 0.133658, acc 0.96875\n",
      "2017-11-06T19:54:59.883369: step 3251, loss 0.297776, acc 0.875\n",
      "2017-11-06T19:55:03.962268: step 3252, loss 0.220391, acc 0.90625\n",
      "2017-11-06T19:55:07.931088: step 3253, loss 0.0216322, acc 1\n",
      "2017-11-06T19:55:11.975961: step 3254, loss 0.145708, acc 0.875\n",
      "2017-11-06T19:55:16.077877: step 3255, loss 0.180578, acc 0.90625\n",
      "2017-11-06T19:55:20.031687: step 3256, loss 0.186057, acc 0.875\n",
      "2017-11-06T19:55:24.060548: step 3257, loss 0.057608, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T19:55:28.071398: step 3258, loss 0.0971788, acc 0.90625\n",
      "2017-11-06T19:55:32.047223: step 3259, loss 0.342248, acc 0.84375\n",
      "2017-11-06T19:55:36.016043: step 3260, loss 0.115735, acc 0.9375\n",
      "2017-11-06T19:55:39.979860: step 3261, loss 0.198612, acc 0.90625\n",
      "2017-11-06T19:55:44.013745: step 3262, loss 0.393426, acc 0.875\n",
      "2017-11-06T19:55:48.007564: step 3263, loss 0.0482055, acc 0.96875\n",
      "2017-11-06T19:55:52.286604: step 3264, loss 0.293313, acc 0.875\n",
      "2017-11-06T19:55:56.533622: step 3265, loss 0.328317, acc 0.8125\n",
      "2017-11-06T19:56:00.535465: step 3266, loss 0.102403, acc 0.96875\n",
      "2017-11-06T19:56:04.570333: step 3267, loss 0.28904, acc 0.90625\n",
      "2017-11-06T19:56:08.544156: step 3268, loss 0.138443, acc 0.90625\n",
      "2017-11-06T19:56:12.537994: step 3269, loss 0.12577, acc 0.90625\n",
      "2017-11-06T19:56:16.542839: step 3270, loss 0.320515, acc 0.8125\n",
      "2017-11-06T19:56:20.566450: step 3271, loss 0.400204, acc 0.75\n",
      "2017-11-06T19:56:24.548282: step 3272, loss 0.0609734, acc 0.96875\n",
      "2017-11-06T19:56:28.540116: step 3273, loss 0.110715, acc 0.9375\n",
      "2017-11-06T19:56:32.675054: step 3274, loss 0.0554001, acc 0.96875\n",
      "2017-11-06T19:56:36.870036: step 3275, loss 0.112327, acc 0.90625\n",
      "2017-11-06T19:56:39.493899: step 3276, loss 0.107336, acc 0.9\n",
      "2017-11-06T19:56:43.561790: step 3277, loss 0.0270829, acc 1\n",
      "2017-11-06T19:56:47.599660: step 3278, loss 0.188882, acc 0.9375\n",
      "2017-11-06T19:56:51.663547: step 3279, loss 0.135384, acc 0.9375\n",
      "2017-11-06T19:56:55.694411: step 3280, loss 0.324929, acc 0.8125\n",
      "2017-11-06T19:57:00.114551: step 3281, loss 0.0862994, acc 0.9375\n",
      "2017-11-06T19:57:04.208461: step 3282, loss 0.287475, acc 0.84375\n",
      "2017-11-06T19:57:08.308373: step 3283, loss 0.314924, acc 0.875\n",
      "2017-11-06T19:57:12.374262: step 3284, loss 0.0902741, acc 0.96875\n",
      "2017-11-06T19:57:16.460166: step 3285, loss 0.0638212, acc 1\n",
      "2017-11-06T19:57:20.463011: step 3286, loss 0.141585, acc 0.9375\n",
      "2017-11-06T19:57:24.530902: step 3287, loss 0.215347, acc 0.90625\n",
      "2017-11-06T19:57:28.461694: step 3288, loss 0.215114, acc 0.875\n",
      "2017-11-06T19:57:32.492557: step 3289, loss 0.0924051, acc 0.96875\n",
      "2017-11-06T19:57:36.464379: step 3290, loss 0.11535, acc 0.9375\n",
      "2017-11-06T19:57:40.472227: step 3291, loss 0.0949697, acc 0.9375\n",
      "2017-11-06T19:57:44.452055: step 3292, loss 0.176435, acc 0.90625\n",
      "2017-11-06T19:57:48.433884: step 3293, loss 0.0596814, acc 0.96875\n",
      "2017-11-06T19:57:52.459745: step 3294, loss 0.546901, acc 0.78125\n",
      "2017-11-06T19:57:56.405548: step 3295, loss 0.252772, acc 0.90625\n",
      "2017-11-06T19:58:00.412395: step 3296, loss 0.149099, acc 0.9375\n",
      "2017-11-06T19:58:04.885574: step 3297, loss 0.27681, acc 0.84375\n",
      "2017-11-06T19:58:08.951463: step 3298, loss 0.153566, acc 0.90625\n",
      "2017-11-06T19:58:13.006363: step 3299, loss 0.176739, acc 0.9375\n",
      "2017-11-06T19:58:16.975164: step 3300, loss 0.160239, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T19:58:19.558000: step 3300, loss 0.761275, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-06T19:58:25.015571: step 3301, loss 0.203971, acc 0.9375\n",
      "2017-11-06T19:58:29.402688: step 3302, loss 0.144853, acc 0.9375\n",
      "2017-11-06T19:58:33.838841: step 3303, loss 0.079093, acc 0.9375\n",
      "2017-11-06T19:58:37.948761: step 3304, loss 0.323214, acc 0.84375\n",
      "2017-11-06T19:58:42.021654: step 3305, loss 0.188585, acc 0.90625\n",
      "2017-11-06T19:58:45.946443: step 3306, loss 0.257433, acc 0.90625\n",
      "2017-11-06T19:58:49.939281: step 3307, loss 0.213717, acc 0.84375\n",
      "2017-11-06T19:58:53.891088: step 3308, loss 0.0745918, acc 0.96875\n",
      "2017-11-06T19:58:57.908943: step 3309, loss 0.161221, acc 0.9375\n",
      "2017-11-06T19:59:01.867756: step 3310, loss 0.334923, acc 0.875\n",
      "2017-11-06T19:59:05.892616: step 3311, loss 0.0388318, acc 1\n",
      "2017-11-06T19:59:08.748645: step 3312, loss 0.238478, acc 0.85\n",
      "2017-11-06T19:59:12.946629: step 3313, loss 0.148425, acc 0.90625\n",
      "2017-11-06T19:59:16.988499: step 3314, loss 0.225557, acc 0.875\n",
      "2017-11-06T19:59:20.992139: step 3315, loss 0.187717, acc 0.9375\n",
      "2017-11-06T19:59:25.053023: step 3316, loss 0.0709899, acc 0.96875\n",
      "2017-11-06T19:59:29.060870: step 3317, loss 0.126697, acc 0.96875\n",
      "2017-11-06T19:59:33.044703: step 3318, loss 0.262763, acc 0.84375\n",
      "2017-11-06T19:59:37.015523: step 3319, loss 0.150485, acc 0.90625\n",
      "2017-11-06T19:59:41.022370: step 3320, loss 0.336991, acc 0.875\n",
      "2017-11-06T19:59:45.142297: step 3321, loss 0.172714, acc 0.90625\n",
      "2017-11-06T19:59:49.206185: step 3322, loss 0.203998, acc 0.875\n",
      "2017-11-06T19:59:53.240051: step 3323, loss 0.0763723, acc 0.96875\n",
      "2017-11-06T19:59:57.187856: step 3324, loss 0.492885, acc 0.84375\n",
      "2017-11-06T20:00:01.465897: step 3325, loss 0.285653, acc 0.90625\n",
      "2017-11-06T20:00:05.604838: step 3326, loss 0.25781, acc 0.90625\n",
      "2017-11-06T20:00:09.652713: step 3327, loss 0.250264, acc 0.8125\n",
      "2017-11-06T20:00:13.889724: step 3328, loss 0.275444, acc 0.875\n",
      "2017-11-06T20:00:18.158757: step 3329, loss 0.0784698, acc 0.96875\n",
      "2017-11-06T20:00:22.127578: step 3330, loss 0.133794, acc 0.9375\n",
      "2017-11-06T20:00:26.195468: step 3331, loss 0.0571829, acc 1\n",
      "2017-11-06T20:00:30.181299: step 3332, loss 0.261506, acc 0.84375\n",
      "2017-11-06T20:00:34.394293: step 3333, loss 0.387147, acc 0.84375\n",
      "2017-11-06T20:00:38.519223: step 3334, loss 0.134344, acc 0.90625\n",
      "2017-11-06T20:00:42.540083: step 3335, loss 0.150938, acc 0.875\n",
      "2017-11-06T20:00:46.532918: step 3336, loss 0.173611, acc 0.90625\n",
      "2017-11-06T20:00:50.543768: step 3337, loss 0.175519, acc 0.9375\n",
      "2017-11-06T20:00:54.492573: step 3338, loss 0.266482, acc 0.875\n",
      "2017-11-06T20:00:58.478406: step 3339, loss 0.00649687, acc 1\n",
      "2017-11-06T20:01:02.552301: step 3340, loss 0.304165, acc 0.875\n",
      "2017-11-06T20:01:06.557146: step 3341, loss 0.242721, acc 0.875\n",
      "2017-11-06T20:01:10.634043: step 3342, loss 0.280241, acc 0.875\n",
      "2017-11-06T20:01:14.749967: step 3343, loss 0.318891, acc 0.84375\n",
      "2017-11-06T20:01:18.949951: step 3344, loss 0.0712181, acc 0.96875\n",
      "2017-11-06T20:01:23.305047: step 3345, loss 0.459995, acc 0.875\n",
      "2017-11-06T20:01:27.299884: step 3346, loss 0.253542, acc 0.9375\n",
      "2017-11-06T20:01:31.359769: step 3347, loss 0.240009, acc 0.90625\n",
      "2017-11-06T20:01:34.026664: step 3348, loss 0.265341, acc 0.9\n",
      "2017-11-06T20:01:38.347735: step 3349, loss 0.0792991, acc 0.96875\n",
      "2017-11-06T20:01:42.477669: step 3350, loss 0.238647, acc 0.90625\n",
      "2017-11-06T20:01:46.470506: step 3351, loss 0.0926659, acc 0.96875\n",
      "2017-11-06T20:01:50.540397: step 3352, loss 0.128, acc 0.9375\n",
      "2017-11-06T20:01:54.577267: step 3353, loss 0.306184, acc 0.8125\n",
      "2017-11-06T20:01:58.610132: step 3354, loss 0.165643, acc 0.90625\n",
      "2017-11-06T20:02:02.678022: step 3355, loss 0.204573, acc 0.9375\n",
      "2017-11-06T20:02:06.681867: step 3356, loss 0.066151, acc 1\n",
      "2017-11-06T20:02:10.637679: step 3357, loss 0.069349, acc 0.96875\n",
      "2017-11-06T20:02:14.581480: step 3358, loss 0.0183561, acc 1\n",
      "2017-11-06T20:02:18.558078: step 3359, loss 0.0545262, acc 0.96875\n",
      "2017-11-06T20:02:22.498879: step 3360, loss 0.110109, acc 0.9375\n",
      "2017-11-06T20:02:26.819951: step 3361, loss 0.108236, acc 0.96875\n",
      "2017-11-06T20:02:30.902852: step 3362, loss 0.238096, acc 0.875\n",
      "2017-11-06T20:02:35.127871: step 3363, loss 0.0749857, acc 0.9375\n",
      "2017-11-06T20:02:39.153713: step 3364, loss 0.0890562, acc 0.9375\n",
      "2017-11-06T20:02:43.115528: step 3365, loss 0.251493, acc 0.875\n",
      "2017-11-06T20:02:47.076342: step 3366, loss 0.189839, acc 0.875\n",
      "2017-11-06T20:02:50.979115: step 3367, loss 0.173523, acc 0.90625\n",
      "2017-11-06T20:02:54.898900: step 3368, loss 0.184196, acc 0.90625\n",
      "2017-11-06T20:02:58.844704: step 3369, loss 0.294684, acc 0.875\n",
      "2017-11-06T20:03:02.770494: step 3370, loss 0.370592, acc 0.84375\n",
      "2017-11-06T20:03:06.696283: step 3371, loss 0.13307, acc 0.875\n",
      "2017-11-06T20:03:10.639086: step 3372, loss 0.166174, acc 0.875\n",
      "2017-11-06T20:03:14.574881: step 3373, loss 0.10092, acc 0.90625\n",
      "2017-11-06T20:03:18.579726: step 3374, loss 0.246942, acc 0.9375\n",
      "2017-11-06T20:03:22.692649: step 3375, loss 0.4092, acc 0.8125\n",
      "2017-11-06T20:03:26.658467: step 3376, loss 0.181007, acc 0.875\n",
      "2017-11-06T20:03:30.818424: step 3377, loss 0.120755, acc 0.90625\n",
      "2017-11-06T20:03:35.001395: step 3378, loss 0.148859, acc 0.9375\n",
      "2017-11-06T20:03:38.988228: step 3379, loss 0.18427, acc 0.875\n",
      "2017-11-06T20:03:43.138176: step 3380, loss 0.287766, acc 0.875\n",
      "2017-11-06T20:03:47.148026: step 3381, loss 0.198796, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T20:03:51.264951: step 3382, loss 0.1902, acc 0.84375\n",
      "2017-11-06T20:03:55.274800: step 3383, loss 0.167493, acc 0.90625\n",
      "2017-11-06T20:03:57.870645: step 3384, loss 0.302065, acc 0.8\n",
      "2017-11-06T20:04:01.894503: step 3385, loss 0.0919903, acc 0.9375\n",
      "2017-11-06T20:04:05.881336: step 3386, loss 0.224656, acc 0.9375\n",
      "2017-11-06T20:04:09.923208: step 3387, loss 0.181524, acc 0.90625\n",
      "2017-11-06T20:04:14.025123: step 3388, loss 0.0640279, acc 0.96875\n",
      "2017-11-06T20:04:18.090011: step 3389, loss 0.10288, acc 0.90625\n",
      "2017-11-06T20:04:22.058832: step 3390, loss 0.246924, acc 0.875\n",
      "2017-11-06T20:04:26.165750: step 3391, loss 0.149539, acc 0.9375\n",
      "2017-11-06T20:04:30.187607: step 3392, loss 0.236999, acc 0.90625\n",
      "2017-11-06T20:04:34.606748: step 3393, loss 0.190582, acc 0.90625\n",
      "2017-11-06T20:04:39.113949: step 3394, loss 0.089408, acc 0.90625\n",
      "2017-11-06T20:04:43.281911: step 3395, loss 0.171744, acc 0.875\n",
      "2017-11-06T20:04:47.287758: step 3396, loss 0.0652868, acc 0.96875\n",
      "2017-11-06T20:04:51.373661: step 3397, loss 0.201862, acc 0.875\n",
      "2017-11-06T20:04:55.293445: step 3398, loss 0.208038, acc 0.90625\n",
      "2017-11-06T20:04:59.253259: step 3399, loss 0.205049, acc 0.90625\n",
      "2017-11-06T20:05:03.205067: step 3400, loss 0.318711, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:05:05.739869: step 3400, loss 0.914151, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-06T20:05:10.963669: step 3401, loss 0.167035, acc 0.90625\n",
      "2017-11-06T20:05:14.896464: step 3402, loss 0.166827, acc 0.90625\n",
      "2017-11-06T20:05:18.865060: step 3403, loss 0.389838, acc 0.8125\n",
      "2017-11-06T20:05:22.818869: step 3404, loss 0.0683687, acc 0.96875\n",
      "2017-11-06T20:05:26.770677: step 3405, loss 0.284341, acc 0.875\n",
      "2017-11-06T20:05:30.734493: step 3406, loss 0.269179, acc 0.875\n",
      "2017-11-06T20:05:34.643271: step 3407, loss 0.122095, acc 0.96875\n",
      "2017-11-06T20:05:38.628102: step 3408, loss 0.117949, acc 0.9375\n",
      "2017-11-06T20:05:42.866114: step 3409, loss 0.131849, acc 0.90625\n",
      "2017-11-06T20:05:46.938007: step 3410, loss 0.112068, acc 0.96875\n",
      "2017-11-06T20:05:50.876807: step 3411, loss 0.166087, acc 0.90625\n",
      "2017-11-06T20:05:54.778578: step 3412, loss 0.102496, acc 0.9375\n",
      "2017-11-06T20:05:58.747400: step 3413, loss 0.188055, acc 0.875\n",
      "2017-11-06T20:06:02.673187: step 3414, loss 0.14949, acc 0.9375\n",
      "2017-11-06T20:06:06.621993: step 3415, loss 0.305526, acc 0.875\n",
      "2017-11-06T20:06:10.592050: step 3416, loss 0.412677, acc 0.8125\n",
      "2017-11-06T20:06:14.538855: step 3417, loss 0.295966, acc 0.875\n",
      "2017-11-06T20:06:18.482658: step 3418, loss 0.066194, acc 0.96875\n",
      "2017-11-06T20:06:22.468489: step 3419, loss 0.105957, acc 0.96875\n",
      "2017-11-06T20:06:24.945250: step 3420, loss 0.116746, acc 0.95\n",
      "2017-11-06T20:06:28.877042: step 3421, loss 0.0948593, acc 0.96875\n",
      "2017-11-06T20:06:32.877885: step 3422, loss 0.275845, acc 0.84375\n",
      "2017-11-06T20:06:37.011822: step 3423, loss 0.353149, acc 0.875\n",
      "2017-11-06T20:06:40.980642: step 3424, loss 0.129552, acc 0.875\n",
      "2017-11-06T20:06:44.970477: step 3425, loss 0.0870215, acc 0.9375\n",
      "2017-11-06T20:06:49.245515: step 3426, loss 0.165158, acc 0.9375\n",
      "2017-11-06T20:06:53.370445: step 3427, loss 0.113932, acc 0.9375\n",
      "2017-11-06T20:06:57.350274: step 3428, loss 0.117375, acc 0.96875\n",
      "2017-11-06T20:07:01.290073: step 3429, loss 0.627733, acc 0.75\n",
      "2017-11-06T20:07:05.292917: step 3430, loss 0.115612, acc 0.90625\n",
      "2017-11-06T20:07:09.241724: step 3431, loss 0.157288, acc 0.9375\n",
      "2017-11-06T20:07:13.236562: step 3432, loss 0.241078, acc 0.875\n",
      "2017-11-06T20:07:17.365495: step 3433, loss 0.133237, acc 0.9375\n",
      "2017-11-06T20:07:21.419376: step 3434, loss 0.115478, acc 0.9375\n",
      "2017-11-06T20:07:25.426225: step 3435, loss 0.0206331, acc 1\n",
      "2017-11-06T20:07:29.359017: step 3436, loss 0.124936, acc 0.9375\n",
      "2017-11-06T20:07:33.452926: step 3437, loss 0.0698347, acc 0.96875\n",
      "2017-11-06T20:07:37.519816: step 3438, loss 0.309256, acc 0.90625\n",
      "2017-11-06T20:07:41.502645: step 3439, loss 0.275577, acc 0.875\n",
      "2017-11-06T20:07:45.440444: step 3440, loss 0.246853, acc 0.90625\n",
      "2017-11-06T20:07:49.411265: step 3441, loss 0.175608, acc 0.90625\n",
      "2017-11-06T20:07:53.652279: step 3442, loss 0.0903072, acc 0.96875\n",
      "2017-11-06T20:07:57.827245: step 3443, loss 0.208633, acc 0.875\n",
      "2017-11-06T20:08:01.746030: step 3444, loss 0.131839, acc 0.9375\n",
      "2017-11-06T20:08:05.696837: step 3445, loss 0.0670208, acc 1\n",
      "2017-11-06T20:08:09.632635: step 3446, loss 0.175167, acc 0.96875\n",
      "2017-11-06T20:08:13.549417: step 3447, loss 0.0672268, acc 1\n",
      "2017-11-06T20:08:17.568273: step 3448, loss 0.329286, acc 0.9375\n",
      "2017-11-06T20:08:21.605924: step 3449, loss 0.185555, acc 0.90625\n",
      "2017-11-06T20:08:25.537719: step 3450, loss 0.157684, acc 0.875\n",
      "2017-11-06T20:08:29.492528: step 3451, loss 0.240242, acc 0.875\n",
      "2017-11-06T20:08:33.629468: step 3452, loss 0.225009, acc 0.9375\n",
      "2017-11-06T20:08:37.767408: step 3453, loss 0.144556, acc 0.96875\n",
      "2017-11-06T20:08:41.820288: step 3454, loss 0.249635, acc 0.90625\n",
      "2017-11-06T20:08:45.763089: step 3455, loss 0.102265, acc 0.96875\n",
      "2017-11-06T20:08:48.227841: step 3456, loss 0.304113, acc 0.85\n",
      "2017-11-06T20:08:52.176646: step 3457, loss 0.191622, acc 0.9375\n",
      "2017-11-06T20:08:56.156474: step 3458, loss 0.296109, acc 0.84375\n",
      "2017-11-06T20:09:00.565607: step 3459, loss 0.153771, acc 0.9375\n",
      "2017-11-06T20:09:04.652512: step 3460, loss 0.142607, acc 0.90625\n",
      "2017-11-06T20:09:08.596314: step 3461, loss 0.183496, acc 0.90625\n",
      "2017-11-06T20:09:12.560129: step 3462, loss 0.143533, acc 0.9375\n",
      "2017-11-06T20:09:16.550968: step 3463, loss 0.309678, acc 0.90625\n",
      "2017-11-06T20:09:20.518784: step 3464, loss 0.0306876, acc 0.96875\n",
      "2017-11-06T20:09:24.714766: step 3465, loss 0.301545, acc 0.84375\n",
      "2017-11-06T20:09:28.895737: step 3466, loss 0.0500211, acc 0.96875\n",
      "2017-11-06T20:09:32.848546: step 3467, loss 0.203156, acc 0.90625\n",
      "2017-11-06T20:09:36.803356: step 3468, loss 0.0386656, acc 1\n",
      "2017-11-06T20:09:40.835221: step 3469, loss 0.282462, acc 0.90625\n",
      "2017-11-06T20:09:44.799037: step 3470, loss 0.227237, acc 0.90625\n",
      "2017-11-06T20:09:48.798881: step 3471, loss 0.0731795, acc 0.96875\n",
      "2017-11-06T20:09:52.771703: step 3472, loss 0.280531, acc 0.875\n",
      "2017-11-06T20:09:56.742525: step 3473, loss 0.101201, acc 0.96875\n",
      "2017-11-06T20:10:00.885471: step 3474, loss 0.323268, acc 0.84375\n",
      "2017-11-06T20:10:05.316615: step 3475, loss 0.169522, acc 0.875\n",
      "2017-11-06T20:10:09.491582: step 3476, loss 0.235934, acc 0.8125\n",
      "2017-11-06T20:10:13.510437: step 3477, loss 0.0436486, acc 0.96875\n",
      "2017-11-06T20:10:17.517284: step 3478, loss 0.332045, acc 0.875\n",
      "2017-11-06T20:10:21.529135: step 3479, loss 0.127787, acc 0.9375\n",
      "2017-11-06T20:10:25.557997: step 3480, loss 0.310025, acc 0.875\n",
      "2017-11-06T20:10:29.509805: step 3481, loss 0.351939, acc 0.875\n",
      "2017-11-06T20:10:33.633736: step 3482, loss 0.224912, acc 0.9375\n",
      "2017-11-06T20:10:37.839725: step 3483, loss 0.474381, acc 0.875\n",
      "2017-11-06T20:10:41.842569: step 3484, loss 0.480223, acc 0.84375\n",
      "2017-11-06T20:10:45.816392: step 3485, loss 0.112566, acc 0.9375\n",
      "2017-11-06T20:10:49.796222: step 3486, loss 0.059559, acc 0.96875\n",
      "2017-11-06T20:10:53.804068: step 3487, loss 0.255604, acc 0.875\n",
      "2017-11-06T20:10:57.739865: step 3488, loss 0.112952, acc 0.96875\n",
      "2017-11-06T20:11:01.728698: step 3489, loss 0.159966, acc 0.9375\n",
      "2017-11-06T20:11:05.715532: step 3490, loss 0.198959, acc 0.875\n",
      "2017-11-06T20:11:09.896502: step 3491, loss 0.161425, acc 0.90625\n",
      "2017-11-06T20:11:12.761538: step 3492, loss 0.477323, acc 0.85\n",
      "2017-11-06T20:11:16.792404: step 3493, loss 0.33476, acc 0.84375\n",
      "2017-11-06T20:11:20.788067: step 3494, loss 0.11202, acc 0.9375\n",
      "2017-11-06T20:11:24.811927: step 3495, loss 0.119529, acc 0.9375\n",
      "2017-11-06T20:11:28.816774: step 3496, loss 0.0519565, acc 1\n",
      "2017-11-06T20:11:32.871653: step 3497, loss 0.0695811, acc 0.96875\n",
      "2017-11-06T20:11:36.795441: step 3498, loss 0.0416662, acc 0.96875\n",
      "2017-11-06T20:11:40.850323: step 3499, loss 0.170682, acc 0.90625\n",
      "2017-11-06T20:11:44.854167: step 3500, loss 0.117564, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:11:47.384966: step 3500, loss 1.13712, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-06T20:11:52.995552: step 3501, loss 0.409538, acc 0.84375\n",
      "2017-11-06T20:11:56.968375: step 3502, loss 0.00148581, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T20:12:01.011248: step 3503, loss 0.162155, acc 0.9375\n",
      "2017-11-06T20:12:05.040110: step 3504, loss 0.0597014, acc 0.96875\n",
      "2017-11-06T20:12:09.175048: step 3505, loss 0.13492, acc 0.96875\n",
      "2017-11-06T20:12:13.128858: step 3506, loss 0.141111, acc 0.9375\n",
      "2017-11-06T20:12:17.506969: step 3507, loss 0.113535, acc 0.9375\n",
      "2017-11-06T20:12:21.651914: step 3508, loss 0.0510435, acc 0.96875\n",
      "2017-11-06T20:12:25.628740: step 3509, loss 0.406288, acc 0.8125\n",
      "2017-11-06T20:12:29.571540: step 3510, loss 0.221621, acc 0.90625\n",
      "2017-11-06T20:12:33.687466: step 3511, loss 0.0967009, acc 0.96875\n",
      "2017-11-06T20:12:37.807393: step 3512, loss 0.136905, acc 0.90625\n",
      "2017-11-06T20:12:41.803232: step 3513, loss 0.17929, acc 0.875\n",
      "2017-11-06T20:12:45.851109: step 3514, loss 0.157774, acc 0.90625\n",
      "2017-11-06T20:12:49.870965: step 3515, loss 0.188483, acc 0.9375\n",
      "2017-11-06T20:12:53.885817: step 3516, loss 0.182035, acc 0.9375\n",
      "2017-11-06T20:12:57.861643: step 3517, loss 0.143173, acc 0.90625\n",
      "2017-11-06T20:13:01.878498: step 3518, loss 0.404099, acc 0.84375\n",
      "2017-11-06T20:13:05.892350: step 3519, loss 0.276635, acc 0.90625\n",
      "2017-11-06T20:13:09.952235: step 3520, loss 0.190421, acc 0.90625\n",
      "2017-11-06T20:13:13.916049: step 3521, loss 0.118172, acc 0.96875\n",
      "2017-11-06T20:13:17.945913: step 3522, loss 0.0666922, acc 0.96875\n",
      "2017-11-06T20:13:22.337033: step 3523, loss 0.264405, acc 0.78125\n",
      "2017-11-06T20:13:26.642093: step 3524, loss 0.205082, acc 0.90625\n",
      "2017-11-06T20:13:30.667953: step 3525, loss 0.290909, acc 0.875\n",
      "2017-11-06T20:13:34.654786: step 3526, loss 0.376152, acc 0.875\n",
      "2017-11-06T20:13:38.658630: step 3527, loss 0.158551, acc 0.9375\n",
      "2017-11-06T20:13:41.238463: step 3528, loss 0.196016, acc 0.95\n",
      "2017-11-06T20:13:45.251316: step 3529, loss 0.101134, acc 0.9375\n",
      "2017-11-06T20:13:49.232143: step 3530, loss 0.0229702, acc 1\n",
      "2017-11-06T20:13:53.223981: step 3531, loss 0.146547, acc 0.90625\n",
      "2017-11-06T20:13:57.214815: step 3532, loss 0.187255, acc 0.90625\n",
      "2017-11-06T20:14:01.262691: step 3533, loss 0.303445, acc 0.84375\n",
      "2017-11-06T20:14:05.169467: step 3534, loss 0.184448, acc 0.9375\n",
      "2017-11-06T20:14:09.187322: step 3535, loss 0.24104, acc 0.875\n",
      "2017-11-06T20:14:13.138130: step 3536, loss 0.212972, acc 0.875\n",
      "2017-11-06T20:14:17.132968: step 3537, loss 0.269981, acc 0.90625\n",
      "2017-11-06T20:14:21.141594: step 3538, loss 0.106222, acc 0.9375\n",
      "2017-11-06T20:14:25.257518: step 3539, loss 0.181812, acc 0.90625\n",
      "2017-11-06T20:14:29.609610: step 3540, loss 0.158005, acc 0.90625\n",
      "2017-11-06T20:14:33.899659: step 3541, loss 0.158183, acc 0.90625\n",
      "2017-11-06T20:14:37.959543: step 3542, loss 0.366146, acc 0.84375\n",
      "2017-11-06T20:14:41.982402: step 3543, loss 0.128053, acc 0.9375\n",
      "2017-11-06T20:14:45.926204: step 3544, loss 0.26118, acc 0.90625\n",
      "2017-11-06T20:14:49.905032: step 3545, loss 0.0684, acc 0.96875\n",
      "2017-11-06T20:14:53.888862: step 3546, loss 0.291093, acc 0.84375\n",
      "2017-11-06T20:14:57.883700: step 3547, loss 0.150468, acc 0.90625\n",
      "2017-11-06T20:15:01.870533: step 3548, loss 0.367291, acc 0.8125\n",
      "2017-11-06T20:15:05.829346: step 3549, loss 0.336578, acc 0.84375\n",
      "2017-11-06T20:15:09.796166: step 3550, loss 0.431913, acc 0.78125\n",
      "2017-11-06T20:15:13.766986: step 3551, loss 0.0671156, acc 0.96875\n",
      "2017-11-06T20:15:17.811860: step 3552, loss 0.188944, acc 0.90625\n",
      "2017-11-06T20:15:21.774676: step 3553, loss 0.046117, acc 0.96875\n",
      "2017-11-06T20:15:25.811545: step 3554, loss 0.141225, acc 0.90625\n",
      "2017-11-06T20:15:29.745339: step 3555, loss 0.0173555, acc 1\n",
      "2017-11-06T20:15:34.129455: step 3556, loss 0.23777, acc 0.875\n",
      "2017-11-06T20:15:38.209353: step 3557, loss 0.284341, acc 0.875\n",
      "2017-11-06T20:15:42.166165: step 3558, loss 0.355113, acc 0.8125\n",
      "2017-11-06T20:15:46.196029: step 3559, loss 0.0889535, acc 0.96875\n",
      "2017-11-06T20:15:50.181861: step 3560, loss 0.182091, acc 0.90625\n",
      "2017-11-06T20:15:54.113654: step 3561, loss 0.260573, acc 0.875\n",
      "2017-11-06T20:15:58.077470: step 3562, loss 0.483383, acc 0.6875\n",
      "2017-11-06T20:16:02.066305: step 3563, loss 0.307375, acc 0.84375\n",
      "2017-11-06T20:16:04.644136: step 3564, loss 0.136293, acc 0.9\n",
      "2017-11-06T20:16:08.656988: step 3565, loss 0.212351, acc 0.9375\n",
      "2017-11-06T20:16:12.700861: step 3566, loss 0.102563, acc 0.96875\n",
      "2017-11-06T20:16:16.741733: step 3567, loss 0.140504, acc 0.9375\n",
      "2017-11-06T20:16:20.741574: step 3568, loss 0.145765, acc 0.90625\n",
      "2017-11-06T20:16:24.737415: step 3569, loss 0.229766, acc 0.84375\n",
      "2017-11-06T20:16:28.711237: step 3570, loss 0.0187064, acc 1\n",
      "2017-11-06T20:16:32.837168: step 3571, loss 0.210686, acc 0.875\n",
      "2017-11-06T20:16:37.108203: step 3572, loss 0.151531, acc 0.90625\n",
      "2017-11-06T20:16:41.536350: step 3573, loss 0.154079, acc 0.9375\n",
      "2017-11-06T20:16:45.502169: step 3574, loss 0.387123, acc 0.84375\n",
      "2017-11-06T20:16:49.493003: step 3575, loss 0.0930136, acc 0.9375\n",
      "2017-11-06T20:16:53.437808: step 3576, loss 0.185086, acc 0.96875\n",
      "2017-11-06T20:16:57.473674: step 3577, loss 0.0936295, acc 0.96875\n",
      "2017-11-06T20:17:01.448498: step 3578, loss 0.330419, acc 0.84375\n",
      "2017-11-06T20:17:05.406311: step 3579, loss 0.168179, acc 0.90625\n",
      "2017-11-06T20:17:09.334101: step 3580, loss 0.183591, acc 0.96875\n",
      "2017-11-06T20:17:13.331942: step 3581, loss 0.268612, acc 0.875\n",
      "2017-11-06T20:17:17.525922: step 3582, loss 0.282378, acc 0.875\n",
      "2017-11-06T20:17:21.543651: step 3583, loss 0.14729, acc 0.90625\n",
      "2017-11-06T20:17:25.632556: step 3584, loss 0.363843, acc 0.875\n",
      "2017-11-06T20:17:29.759488: step 3585, loss 0.194121, acc 0.90625\n",
      "2017-11-06T20:17:33.798357: step 3586, loss 0.100792, acc 0.96875\n",
      "2017-11-06T20:17:37.823217: step 3587, loss 0.461329, acc 0.9375\n",
      "2017-11-06T20:17:41.847076: step 3588, loss 0.190762, acc 0.9375\n",
      "2017-11-06T20:17:46.337267: step 3589, loss 0.411745, acc 0.875\n",
      "2017-11-06T20:17:50.379140: step 3590, loss 0.3375, acc 0.90625\n",
      "2017-11-06T20:17:54.435021: step 3591, loss 0.226149, acc 0.9375\n",
      "2017-11-06T20:17:58.389833: step 3592, loss 0.587134, acc 0.84375\n",
      "2017-11-06T20:18:02.383670: step 3593, loss 0.313132, acc 0.84375\n",
      "2017-11-06T20:18:06.335476: step 3594, loss 0.308205, acc 0.875\n",
      "2017-11-06T20:18:10.324311: step 3595, loss 0.251346, acc 0.875\n",
      "2017-11-06T20:18:14.328157: step 3596, loss 0.395599, acc 0.8125\n",
      "2017-11-06T20:18:18.365024: step 3597, loss 0.101304, acc 0.9375\n",
      "2017-11-06T20:18:22.422909: step 3598, loss 0.127402, acc 0.9375\n",
      "2017-11-06T20:18:26.837062: step 3599, loss 0.397487, acc 0.84375\n",
      "2017-11-06T20:18:29.378850: step 3600, loss 0.17153, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:18:31.997710: step 3600, loss 0.834373, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\8\\1509959671\\checkpoints\\model-3600\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18248\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary.CategoricalVocabulary object at 0x00000113C97F8C88>\n",
      "Train/Dev split: 1140/60\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\n",
      "\n",
      "2017-11-06T20:18:42.902979: step 1, loss 5.99506, acc 0.25\n",
      "2017-11-06T20:18:46.974872: step 2, loss 2.38954, acc 0.46875\n",
      "2017-11-06T20:18:51.439044: step 3, loss 1.70674, acc 0.65625\n",
      "2017-11-06T20:18:55.471910: step 4, loss 1.32626, acc 0.6875\n",
      "2017-11-06T20:18:59.541801: step 5, loss 0.777159, acc 0.71875\n",
      "2017-11-06T20:19:03.507638: step 6, loss 0.616317, acc 0.9375\n",
      "2017-11-06T20:19:07.475438: step 7, loss 1.13343, acc 0.84375\n",
      "2017-11-06T20:19:11.463272: step 8, loss 1.48629, acc 0.875\n",
      "2017-11-06T20:19:15.486130: step 9, loss 1.70645, acc 0.90625\n",
      "2017-11-06T20:19:19.452948: step 10, loss 2.91699, acc 0.75\n",
      "2017-11-06T20:19:23.482812: step 11, loss 0.703845, acc 0.96875\n",
      "2017-11-06T20:19:27.457636: step 12, loss 3.41063, acc 0.8125\n",
      "2017-11-06T20:19:31.453476: step 13, loss 1.54781, acc 0.9375\n",
      "2017-11-06T20:19:35.457321: step 14, loss 4.00256, acc 0.8125\n",
      "2017-11-06T20:19:39.451160: step 15, loss 1.61272, acc 0.90625\n",
      "2017-11-06T20:19:43.435989: step 16, loss 2.64383, acc 0.84375\n",
      "2017-11-06T20:19:47.459850: step 17, loss 2.05752, acc 0.90625\n",
      "2017-11-06T20:19:51.425667: step 18, loss 4.13524, acc 0.78125\n",
      "2017-11-06T20:19:55.758746: step 19, loss 1.13893, acc 0.9375\n",
      "2017-11-06T20:19:59.918703: step 20, loss 3.02043, acc 0.84375\n",
      "2017-11-06T20:20:04.224761: step 21, loss 0.953445, acc 0.9375\n",
      "2017-11-06T20:20:08.242616: step 22, loss 1.03459, acc 0.90625\n",
      "2017-11-06T20:20:12.291493: step 23, loss 1.02996, acc 0.90625\n",
      "2017-11-06T20:20:16.347377: step 24, loss 1.59875, acc 0.75\n",
      "2017-11-06T20:20:20.397972: step 25, loss 0.579398, acc 0.84375\n",
      "2017-11-06T20:20:24.448852: step 26, loss 2.04803, acc 0.875\n",
      "2017-11-06T20:20:28.492723: step 27, loss 0.869067, acc 0.875\n",
      "2017-11-06T20:20:32.482559: step 28, loss 1.23391, acc 0.84375\n",
      "2017-11-06T20:20:36.716567: step 29, loss 0.632862, acc 0.9375\n",
      "2017-11-06T20:20:40.754437: step 30, loss 1.46699, acc 0.8125\n",
      "2017-11-06T20:20:44.825498: step 31, loss 2.2419, acc 0.75\n",
      "2017-11-06T20:20:48.780309: step 32, loss 0.56053, acc 0.84375\n",
      "2017-11-06T20:20:52.813174: step 33, loss 1.06687, acc 0.84375\n",
      "2017-11-06T20:20:56.762981: step 34, loss 0.373442, acc 0.84375\n",
      "2017-11-06T20:21:01.030011: step 35, loss 2.41763, acc 0.6875\n",
      "2017-11-06T20:21:03.856020: step 36, loss 0.455609, acc 0.85\n",
      "2017-11-06T20:21:07.978950: step 37, loss 0.716834, acc 0.875\n",
      "2017-11-06T20:21:12.006812: step 38, loss 1.2857, acc 0.6875\n",
      "2017-11-06T20:21:16.047684: step 39, loss 0.909788, acc 0.8125\n",
      "2017-11-06T20:21:20.090555: step 40, loss 0.69177, acc 0.84375\n",
      "2017-11-06T20:21:24.131427: step 41, loss 1.10517, acc 0.78125\n",
      "2017-11-06T20:21:28.130268: step 42, loss 1.7735, acc 0.8125\n",
      "2017-11-06T20:21:32.149123: step 43, loss 0.0865103, acc 0.96875\n",
      "2017-11-06T20:21:36.205005: step 44, loss 0.867219, acc 0.8125\n",
      "2017-11-06T20:21:40.294911: step 45, loss 0.904779, acc 0.78125\n",
      "2017-11-06T20:21:44.296755: step 46, loss 0.240506, acc 0.90625\n",
      "2017-11-06T20:21:48.256568: step 47, loss 0.968947, acc 0.90625\n",
      "2017-11-06T20:21:52.342472: step 48, loss 0.637693, acc 0.9375\n",
      "2017-11-06T20:21:56.385344: step 49, loss 1.02455, acc 0.8125\n",
      "2017-11-06T20:22:00.407203: step 50, loss 1.69461, acc 0.8125\n",
      "2017-11-06T20:22:04.423056: step 51, loss 0.542639, acc 0.90625\n",
      "2017-11-06T20:22:09.188818: step 52, loss 2.17506, acc 0.71875\n",
      "2017-11-06T20:22:13.544913: step 53, loss 0.617968, acc 0.78125\n",
      "2017-11-06T20:22:17.923524: step 54, loss 0.670904, acc 0.90625\n",
      "2017-11-06T20:22:22.108611: step 55, loss 0.530132, acc 0.875\n",
      "2017-11-06T20:22:26.171007: step 56, loss 1.01123, acc 0.84375\n",
      "2017-11-06T20:22:30.160842: step 57, loss 0.457701, acc 0.90625\n",
      "2017-11-06T20:22:34.448891: step 58, loss 0.743373, acc 0.8125\n",
      "2017-11-06T20:22:38.518781: step 59, loss 0.0145024, acc 1\n",
      "2017-11-06T20:22:42.541639: step 60, loss 0.677123, acc 0.90625\n",
      "2017-11-06T20:22:46.574505: step 61, loss 1.48769, acc 0.8125\n",
      "2017-11-06T20:22:50.613376: step 62, loss 1.70476, acc 0.84375\n",
      "2017-11-06T20:22:54.636233: step 63, loss 1.05858, acc 0.84375\n",
      "2017-11-06T20:22:58.670099: step 64, loss 1.94247, acc 0.6875\n",
      "2017-11-06T20:23:02.654931: step 65, loss 1.60833, acc 0.75\n",
      "2017-11-06T20:23:06.667782: step 66, loss 0.730102, acc 0.84375\n",
      "2017-11-06T20:23:10.801720: step 67, loss 1.32632, acc 0.84375\n",
      "2017-11-06T20:23:15.172825: step 68, loss 0.343928, acc 0.875\n",
      "2017-11-06T20:23:19.352795: step 69, loss 1.64075, acc 0.75\n",
      "2017-11-06T20:23:23.363411: step 70, loss 1.04733, acc 0.75\n",
      "2017-11-06T20:23:27.298209: step 71, loss 0.103398, acc 0.9375\n",
      "2017-11-06T20:23:29.800985: step 72, loss 1.91442, acc 0.8\n",
      "2017-11-06T20:23:33.747790: step 73, loss 0.901856, acc 0.875\n",
      "2017-11-06T20:23:37.679585: step 74, loss 0.775796, acc 0.8125\n",
      "2017-11-06T20:23:41.677424: step 75, loss 1.70527, acc 0.75\n",
      "2017-11-06T20:23:45.673265: step 76, loss 0.670635, acc 0.8125\n",
      "2017-11-06T20:23:49.661099: step 77, loss 1.04283, acc 0.875\n",
      "2017-11-06T20:23:53.671947: step 78, loss 0.969651, acc 0.8125\n",
      "2017-11-06T20:23:57.653776: step 79, loss 1.01563, acc 0.8125\n",
      "2017-11-06T20:24:01.660624: step 80, loss 0.0658811, acc 0.96875\n",
      "2017-11-06T20:24:05.664469: step 81, loss 0.8334, acc 0.875\n",
      "2017-11-06T20:24:09.652302: step 82, loss 0.965048, acc 0.8125\n",
      "2017-11-06T20:24:13.610115: step 83, loss 0.80682, acc 0.84375\n",
      "2017-11-06T20:24:17.829111: step 84, loss 1.06776, acc 0.84375\n",
      "2017-11-06T20:24:22.171197: step 85, loss 1.02395, acc 0.84375\n",
      "2017-11-06T20:24:26.331153: step 86, loss 1.67866, acc 0.75\n",
      "2017-11-06T20:24:30.293969: step 87, loss 1.4564, acc 0.8125\n",
      "2017-11-06T20:24:34.507962: step 88, loss 0.42281, acc 0.96875\n",
      "2017-11-06T20:24:38.467778: step 89, loss 1.24854, acc 0.78125\n",
      "2017-11-06T20:24:42.456610: step 90, loss 0.574442, acc 0.84375\n",
      "2017-11-06T20:24:46.418425: step 91, loss 0.678638, acc 0.84375\n",
      "2017-11-06T20:24:50.499679: step 92, loss 0.562633, acc 0.90625\n",
      "2017-11-06T20:24:54.458491: step 93, loss 0.737001, acc 0.875\n",
      "2017-11-06T20:24:58.395289: step 94, loss 0.264053, acc 0.9375\n",
      "2017-11-06T20:25:02.407139: step 95, loss 0.0894622, acc 0.96875\n",
      "2017-11-06T20:25:06.651660: step 96, loss 0.316849, acc 0.90625\n",
      "2017-11-06T20:25:10.634503: step 97, loss 0.211145, acc 0.90625\n",
      "2017-11-06T20:25:14.563398: step 98, loss 1.28449, acc 0.8125\n",
      "2017-11-06T20:25:18.544608: step 99, loss 1.35614, acc 0.8125\n",
      "2017-11-06T20:25:22.718090: step 100, loss 1.49814, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:25:25.647946: step 100, loss 1.57933, acc 0.833333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-100\n",
      "\n",
      "2017-11-06T20:25:31.071437: step 101, loss 0.300762, acc 0.90625\n",
      "2017-11-06T20:25:35.109307: step 102, loss 1.25227, acc 0.84375\n",
      "2017-11-06T20:25:39.090135: step 103, loss 0.186756, acc 0.9375\n",
      "2017-11-06T20:25:43.133008: step 104, loss 0.849104, acc 0.875\n",
      "2017-11-06T20:25:47.123843: step 105, loss 0.413662, acc 0.875\n",
      "2017-11-06T20:25:51.093664: step 106, loss 1.28295, acc 0.8125\n",
      "2017-11-06T20:25:55.112520: step 107, loss 0.355058, acc 0.9375\n",
      "2017-11-06T20:25:57.650323: step 108, loss 0.649367, acc 0.8\n",
      "2017-11-06T20:26:01.684189: step 109, loss 0.831449, acc 0.8125\n",
      "2017-11-06T20:26:05.680029: step 110, loss 0.854103, acc 0.78125\n",
      "2017-11-06T20:26:09.693881: step 111, loss 0.623949, acc 0.84375\n",
      "2017-11-06T20:26:13.685717: step 112, loss 0.597611, acc 0.875\n",
      "2017-11-06T20:26:17.671549: step 113, loss 0.56368, acc 0.875\n",
      "2017-11-06T20:26:21.614162: step 114, loss 0.392846, acc 0.84375\n",
      "2017-11-06T20:26:25.578979: step 115, loss 0.456159, acc 0.90625\n",
      "2017-11-06T20:26:29.938076: step 116, loss 0.530641, acc 0.90625\n",
      "2017-11-06T20:26:34.342205: step 117, loss 0.292698, acc 0.90625\n",
      "2017-11-06T20:26:38.364064: step 118, loss 0.17861, acc 0.875\n",
      "2017-11-06T20:26:42.358902: step 119, loss 1.00425, acc 0.75\n",
      "2017-11-06T20:26:46.379759: step 120, loss 0.805758, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T20:26:50.295541: step 121, loss 0.492173, acc 0.90625\n",
      "2017-11-06T20:26:54.486447: step 122, loss 1.02428, acc 0.84375\n",
      "2017-11-06T20:26:58.473281: step 123, loss 0.192021, acc 0.96875\n",
      "2017-11-06T20:27:02.542172: step 124, loss 0.765455, acc 0.90625\n",
      "2017-11-06T20:27:06.541014: step 125, loss 1.40544, acc 0.84375\n",
      "2017-11-06T20:27:10.494823: step 126, loss 0.451899, acc 0.875\n",
      "2017-11-06T20:27:14.541698: step 127, loss 0.775392, acc 0.9375\n",
      "2017-11-06T20:27:18.684642: step 128, loss 1.14169, acc 0.78125\n",
      "2017-11-06T20:27:22.666471: step 129, loss 0.0452823, acc 0.96875\n",
      "2017-11-06T20:27:26.632288: step 130, loss 0.720419, acc 0.90625\n",
      "2017-11-06T20:27:30.613117: step 131, loss 0.0269324, acc 1\n",
      "2017-11-06T20:27:34.901164: step 132, loss 0.302553, acc 0.875\n",
      "2017-11-06T20:27:39.059119: step 133, loss 0.720118, acc 0.9375\n",
      "2017-11-06T20:27:43.022937: step 134, loss 0.313443, acc 0.9375\n",
      "2017-11-06T20:27:47.056801: step 135, loss 0.674545, acc 0.9375\n",
      "2017-11-06T20:27:51.115685: step 136, loss 1.58674, acc 0.84375\n",
      "2017-11-06T20:27:55.132540: step 137, loss 0.656014, acc 0.90625\n",
      "2017-11-06T20:27:59.092353: step 138, loss 0.141627, acc 0.90625\n",
      "2017-11-06T20:28:03.082192: step 139, loss 0.470618, acc 0.875\n",
      "2017-11-06T20:28:07.067019: step 140, loss 0.542024, acc 0.9375\n",
      "2017-11-06T20:28:11.050850: step 141, loss 1.35555, acc 0.8125\n",
      "2017-11-06T20:28:15.034684: step 142, loss 0.13844, acc 0.96875\n",
      "2017-11-06T20:28:18.990492: step 143, loss 0.466742, acc 0.84375\n",
      "2017-11-06T20:28:21.536301: step 144, loss 0.685289, acc 0.85\n",
      "2017-11-06T20:28:25.698258: step 145, loss 1.32567, acc 0.78125\n",
      "2017-11-06T20:28:29.699100: step 146, loss 0.37947, acc 0.84375\n",
      "2017-11-06T20:28:33.864061: step 147, loss 0.619563, acc 0.84375\n",
      "2017-11-06T20:28:38.024015: step 148, loss 0.329506, acc 0.96875\n",
      "2017-11-06T20:28:42.430147: step 149, loss 0.683479, acc 0.90625\n",
      "2017-11-06T20:28:46.493033: step 150, loss 1.32081, acc 0.78125\n",
      "2017-11-06T20:28:50.456851: step 151, loss 0.617599, acc 0.90625\n",
      "2017-11-06T20:28:54.478835: step 152, loss 0.951703, acc 0.8125\n",
      "2017-11-06T20:28:58.407627: step 153, loss 0.902493, acc 0.90625\n",
      "2017-11-06T20:29:02.395460: step 154, loss 0.360872, acc 0.9375\n",
      "2017-11-06T20:29:06.332258: step 155, loss 0.0278737, acc 0.96875\n",
      "2017-11-06T20:29:10.301078: step 156, loss 0.224865, acc 0.9375\n",
      "2017-11-06T20:29:14.311927: step 157, loss 0.754027, acc 0.875\n",
      "2017-11-06T20:29:18.285752: step 158, loss 0.541299, acc 0.90625\n",
      "2017-11-06T20:29:22.286359: step 159, loss 0.348129, acc 0.9375\n",
      "2017-11-06T20:29:26.264185: step 160, loss 1.80348, acc 0.71875\n",
      "2017-11-06T20:29:30.249017: step 161, loss 1.86772, acc 0.71875\n",
      "2017-11-06T20:29:34.266872: step 162, loss 0.474328, acc 0.875\n",
      "2017-11-06T20:29:38.214676: step 163, loss 0.00876663, acc 1\n",
      "2017-11-06T20:29:42.123453: step 164, loss 0.900352, acc 0.8125\n",
      "2017-11-06T20:29:46.367469: step 165, loss 0.491338, acc 0.9375\n",
      "2017-11-06T20:29:50.633500: step 166, loss 0.398242, acc 0.96875\n",
      "2017-11-06T20:29:54.581305: step 167, loss 1.08965, acc 0.8125\n",
      "2017-11-06T20:29:58.586151: step 168, loss 0.904487, acc 0.84375\n",
      "2017-11-06T20:30:02.872196: step 169, loss 0.67252, acc 0.84375\n",
      "2017-11-06T20:30:06.847021: step 170, loss 0.690887, acc 0.8125\n",
      "2017-11-06T20:30:10.864875: step 171, loss 0.838517, acc 0.8125\n",
      "2017-11-06T20:30:14.825690: step 172, loss 0.374204, acc 0.875\n",
      "2017-11-06T20:30:18.846547: step 173, loss 0.724053, acc 0.78125\n",
      "2017-11-06T20:30:22.825374: step 174, loss 1.24606, acc 0.75\n",
      "2017-11-06T20:30:26.810205: step 175, loss 0.933442, acc 0.875\n",
      "2017-11-06T20:30:30.804042: step 176, loss 0.267883, acc 0.875\n",
      "2017-11-06T20:30:35.082083: step 177, loss 0.499316, acc 0.875\n",
      "2017-11-06T20:30:39.092933: step 178, loss 0.845836, acc 0.875\n",
      "2017-11-06T20:30:43.067757: step 179, loss 0.544244, acc 0.90625\n",
      "2017-11-06T20:30:45.628576: step 180, loss 0.0162996, acc 1\n",
      "2017-11-06T20:30:49.806545: step 181, loss 1.13189, acc 0.875\n",
      "2017-11-06T20:30:54.174649: step 182, loss 0.228347, acc 0.90625\n",
      "2017-11-06T20:30:58.240571: step 183, loss 0.199288, acc 0.9375\n",
      "2017-11-06T20:31:02.218398: step 184, loss 0.226796, acc 0.90625\n",
      "2017-11-06T20:31:06.206231: step 185, loss 0.219947, acc 0.96875\n",
      "2017-11-06T20:31:10.197067: step 186, loss 0.390364, acc 0.9375\n",
      "2017-11-06T20:31:14.298983: step 187, loss 1.93814, acc 0.75\n",
      "2017-11-06T20:31:18.300825: step 188, loss 0.667769, acc 0.84375\n",
      "2017-11-06T20:31:22.268645: step 189, loss 0.485179, acc 0.9375\n",
      "2017-11-06T20:31:26.242468: step 190, loss 0.980064, acc 0.875\n",
      "2017-11-06T20:31:30.258321: step 191, loss 0.604422, acc 0.96875\n",
      "2017-11-06T20:31:34.281180: step 192, loss 0.352269, acc 0.875\n",
      "2017-11-06T20:31:38.562222: step 193, loss 0.599925, acc 0.90625\n",
      "2017-11-06T20:31:42.680150: step 194, loss 0.334577, acc 0.875\n",
      "2017-11-06T20:31:46.629954: step 195, loss 0.162096, acc 0.96875\n",
      "2017-11-06T20:31:50.670827: step 196, loss 0.513486, acc 0.9375\n",
      "2017-11-06T20:31:54.706694: step 197, loss 0.99953, acc 0.78125\n",
      "2017-11-06T20:31:59.148849: step 198, loss 1.18225, acc 0.78125\n",
      "2017-11-06T20:32:03.164703: step 199, loss 0.311718, acc 0.875\n",
      "2017-11-06T20:32:07.138526: step 200, loss 0.433665, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:32:09.709353: step 200, loss 1.49883, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-200\n",
      "\n",
      "2017-11-06T20:32:15.394581: step 201, loss 0.786093, acc 0.90625\n",
      "2017-11-06T20:32:19.405431: step 202, loss 0.882904, acc 0.8125\n",
      "2017-11-06T20:32:23.423054: step 203, loss 0.777266, acc 0.78125\n",
      "2017-11-06T20:32:27.427899: step 204, loss 1.22082, acc 0.75\n",
      "2017-11-06T20:32:31.399721: step 205, loss 0.423542, acc 0.84375\n",
      "2017-11-06T20:32:35.673761: step 206, loss 0.267985, acc 0.9375\n",
      "2017-11-06T20:32:39.692613: step 207, loss 0.923919, acc 0.84375\n",
      "2017-11-06T20:32:43.690455: step 208, loss 0.409586, acc 0.90625\n",
      "2017-11-06T20:32:47.666279: step 209, loss 0.430243, acc 0.9375\n",
      "2017-11-06T20:32:51.647109: step 210, loss 0.507735, acc 0.9375\n",
      "2017-11-06T20:32:55.654955: step 211, loss 0.897601, acc 0.84375\n",
      "2017-11-06T20:32:59.703833: step 212, loss 1.20867, acc 0.84375\n",
      "2017-11-06T20:33:04.179012: step 213, loss 0.513467, acc 0.90625\n",
      "2017-11-06T20:33:08.289935: step 214, loss 0.303676, acc 0.96875\n",
      "2017-11-06T20:33:12.276766: step 215, loss 0.483358, acc 0.90625\n",
      "2017-11-06T20:33:14.842589: step 216, loss 0.275905, acc 0.95\n",
      "2017-11-06T20:33:18.922488: step 217, loss 0.774039, acc 0.875\n",
      "2017-11-06T20:33:23.001388: step 218, loss 0.203328, acc 0.9375\n",
      "2017-11-06T20:33:27.326460: step 219, loss 0.775429, acc 0.875\n",
      "2017-11-06T20:33:31.517437: step 220, loss 0.835322, acc 0.875\n",
      "2017-11-06T20:33:35.626358: step 221, loss 0.871289, acc 0.875\n",
      "2017-11-06T20:33:39.702254: step 222, loss 0.33914, acc 0.90625\n",
      "2017-11-06T20:33:43.776147: step 223, loss 0.584123, acc 0.875\n",
      "2017-11-06T20:33:47.906082: step 224, loss 0.701455, acc 0.84375\n",
      "2017-11-06T20:33:51.989984: step 225, loss 0.688821, acc 0.90625\n",
      "2017-11-06T20:33:56.121920: step 226, loss 0.553872, acc 0.9375\n",
      "2017-11-06T20:34:00.235843: step 227, loss 0.40949, acc 0.875\n",
      "2017-11-06T20:34:04.425820: step 228, loss 0.247503, acc 0.96875\n",
      "2017-11-06T20:34:08.829949: step 229, loss 0.858648, acc 0.8125\n",
      "2017-11-06T20:34:13.249089: step 230, loss 0.421002, acc 0.875\n",
      "2017-11-06T20:34:17.310975: step 231, loss 0.418669, acc 0.96875\n",
      "2017-11-06T20:34:21.497950: step 232, loss 0.710747, acc 0.8125\n",
      "2017-11-06T20:34:25.585855: step 233, loss 1.33335, acc 0.78125\n",
      "2017-11-06T20:34:29.749814: step 234, loss 0.544564, acc 0.875\n",
      "2017-11-06T20:34:34.111913: step 235, loss 0.588856, acc 0.875\n",
      "2017-11-06T20:34:38.415971: step 236, loss 0.423842, acc 0.96875\n",
      "2017-11-06T20:34:42.534898: step 237, loss 0.623453, acc 0.84375\n",
      "2017-11-06T20:34:46.669836: step 238, loss 0.665408, acc 0.84375\n",
      "2017-11-06T20:34:50.820785: step 239, loss 0.475861, acc 0.9375\n",
      "2017-11-06T20:34:54.982743: step 240, loss 0.645671, acc 0.875\n",
      "2017-11-06T20:34:59.037624: step 241, loss 0.574947, acc 0.875\n",
      "2017-11-06T20:35:03.056515: step 242, loss 0.592967, acc 0.9375\n",
      "2017-11-06T20:35:07.083376: step 243, loss 0.830726, acc 0.84375\n",
      "2017-11-06T20:35:11.141260: step 244, loss 0.501148, acc 0.9375\n",
      "2017-11-06T20:35:15.533380: step 245, loss 0.535578, acc 0.9375\n",
      "2017-11-06T20:35:19.653307: step 246, loss 0.0065422, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T20:35:23.717970: step 247, loss 0.271056, acc 0.90625\n",
      "2017-11-06T20:35:27.711808: step 248, loss 1.12284, acc 0.78125\n",
      "2017-11-06T20:35:31.718655: step 249, loss 0.0996211, acc 0.96875\n",
      "2017-11-06T20:35:35.716495: step 250, loss 0.42638, acc 0.9375\n",
      "2017-11-06T20:35:39.724344: step 251, loss 0.309685, acc 0.9375\n",
      "2017-11-06T20:35:42.353211: step 252, loss 0.493487, acc 0.95\n",
      "2017-11-06T20:35:46.347049: step 253, loss 0.179953, acc 0.90625\n",
      "2017-11-06T20:35:50.362902: step 254, loss 0.239854, acc 0.96875\n",
      "2017-11-06T20:35:54.368748: step 255, loss 0.650639, acc 0.78125\n",
      "2017-11-06T20:35:58.426632: step 256, loss 0.905671, acc 0.875\n",
      "2017-11-06T20:36:02.405459: step 257, loss 0.110693, acc 0.9375\n",
      "2017-11-06T20:36:06.460340: step 258, loss 0.0529162, acc 1\n",
      "2017-11-06T20:36:10.416150: step 259, loss 0.564359, acc 0.90625\n",
      "2017-11-06T20:36:14.427001: step 260, loss 0.691792, acc 0.75\n",
      "2017-11-06T20:36:18.582954: step 261, loss 0.226406, acc 0.96875\n",
      "2017-11-06T20:36:22.987083: step 262, loss 0.546338, acc 0.9375\n",
      "2017-11-06T20:36:27.008941: step 263, loss 0.886742, acc 0.8125\n",
      "2017-11-06T20:36:31.007781: step 264, loss 0.0558432, acc 0.96875\n",
      "2017-11-06T20:36:35.241790: step 265, loss 0.360372, acc 0.9375\n",
      "2017-11-06T20:36:39.279659: step 266, loss 0.452229, acc 0.9375\n",
      "2017-11-06T20:36:43.297514: step 267, loss 0.348746, acc 0.96875\n",
      "2017-11-06T20:36:47.264333: step 268, loss 0.435048, acc 0.90625\n",
      "2017-11-06T20:36:51.253167: step 269, loss 0.072017, acc 0.96875\n",
      "2017-11-06T20:36:55.299042: step 270, loss 0.629824, acc 0.875\n",
      "2017-11-06T20:36:59.349920: step 271, loss 0.0515574, acc 0.96875\n",
      "2017-11-06T20:37:03.296725: step 272, loss 0.301505, acc 0.96875\n",
      "2017-11-06T20:37:07.303571: step 273, loss 0.526819, acc 0.90625\n",
      "2017-11-06T20:37:11.309419: step 274, loss 0.710267, acc 0.90625\n",
      "2017-11-06T20:37:15.325271: step 275, loss 0.0225718, acc 1\n",
      "2017-11-06T20:37:19.442197: step 276, loss 0.918315, acc 0.84375\n",
      "2017-11-06T20:37:23.628171: step 277, loss 0.254515, acc 0.96875\n",
      "2017-11-06T20:37:27.986286: step 278, loss 0.448984, acc 0.90625\n",
      "2017-11-06T20:37:32.088182: step 279, loss 0.646933, acc 0.84375\n",
      "2017-11-06T20:37:36.122048: step 280, loss 0.990316, acc 0.8125\n",
      "2017-11-06T20:37:40.159917: step 281, loss 0.681856, acc 0.84375\n",
      "2017-11-06T20:37:44.205793: step 282, loss 0.829501, acc 0.875\n",
      "2017-11-06T20:37:48.218644: step 283, loss 0.604275, acc 0.90625\n",
      "2017-11-06T20:37:52.280530: step 284, loss 0.205265, acc 0.96875\n",
      "2017-11-06T20:37:56.359428: step 285, loss 0.48647, acc 0.84375\n",
      "2017-11-06T20:38:00.344259: step 286, loss 0.165409, acc 0.9375\n",
      "2017-11-06T20:38:04.400141: step 287, loss 0.165489, acc 0.90625\n",
      "2017-11-06T20:38:06.993984: step 288, loss 1.25545, acc 0.75\n",
      "2017-11-06T20:38:11.039860: step 289, loss 0.377343, acc 0.90625\n",
      "2017-11-06T20:38:14.998672: step 290, loss 0.488194, acc 0.875\n",
      "2017-11-06T20:38:19.053552: step 291, loss 0.0917158, acc 0.96875\n",
      "2017-11-06T20:38:23.145430: step 292, loss 0.762925, acc 0.8125\n",
      "2017-11-06T20:38:27.268359: step 293, loss 1.33848, acc 0.84375\n",
      "2017-11-06T20:38:31.674489: step 294, loss 0.392458, acc 0.90625\n",
      "2017-11-06T20:38:36.099634: step 295, loss 0.235196, acc 0.84375\n",
      "2017-11-06T20:38:40.117488: step 296, loss 0.635458, acc 0.8125\n",
      "2017-11-06T20:38:44.158360: step 297, loss 0.572738, acc 0.875\n",
      "2017-11-06T20:38:48.137187: step 298, loss 0.0251652, acc 1\n",
      "2017-11-06T20:38:52.242104: step 299, loss 0.533659, acc 0.96875\n",
      "2017-11-06T20:38:56.258958: step 300, loss 0.472777, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:38:58.800764: step 300, loss 1.75642, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-300\n",
      "\n",
      "2017-11-06T20:39:04.622913: step 301, loss 0.0616898, acc 0.96875\n",
      "2017-11-06T20:39:08.733853: step 302, loss 0.167049, acc 0.9375\n",
      "2017-11-06T20:39:12.739700: step 303, loss 0.258394, acc 0.9375\n",
      "2017-11-06T20:39:16.818599: step 304, loss 0.193911, acc 0.96875\n",
      "2017-11-06T20:39:20.899498: step 305, loss 0.159107, acc 0.96875\n",
      "2017-11-06T20:39:25.060454: step 306, loss 0.111763, acc 0.9375\n",
      "2017-11-06T20:39:29.140354: step 307, loss 0.697362, acc 0.8125\n",
      "2017-11-06T20:39:33.172218: step 308, loss 0.00461249, acc 1\n",
      "2017-11-06T20:39:37.572345: step 309, loss 0.378534, acc 0.875\n",
      "2017-11-06T20:39:41.675260: step 310, loss 0.688612, acc 0.875\n",
      "2017-11-06T20:39:45.691113: step 311, loss 0.32244, acc 0.9375\n",
      "2017-11-06T20:39:49.700963: step 312, loss 0.687907, acc 0.8125\n",
      "2017-11-06T20:39:53.733830: step 313, loss 0.156138, acc 0.9375\n",
      "2017-11-06T20:39:57.771697: step 314, loss 0.374843, acc 0.9375\n",
      "2017-11-06T20:40:02.104776: step 315, loss 0.85519, acc 0.875\n",
      "2017-11-06T20:40:06.207693: step 316, loss 1.01902, acc 0.6875\n",
      "2017-11-06T20:40:10.245561: step 317, loss 0.917123, acc 0.84375\n",
      "2017-11-06T20:40:14.296439: step 318, loss 0.0161834, acc 1\n",
      "2017-11-06T20:40:18.342315: step 319, loss 0.3255, acc 0.90625\n",
      "2017-11-06T20:40:22.347158: step 320, loss 0.904341, acc 0.875\n",
      "2017-11-06T20:40:26.387029: step 321, loss 0.403995, acc 0.8125\n",
      "2017-11-06T20:40:30.445913: step 322, loss 0.186284, acc 0.90625\n",
      "2017-11-06T20:40:34.733961: step 323, loss 0.778537, acc 0.84375\n",
      "2017-11-06T20:40:37.385844: step 324, loss 0.524755, acc 0.95\n",
      "2017-11-06T20:40:41.733934: step 325, loss 1.07667, acc 0.84375\n",
      "2017-11-06T20:40:46.043997: step 326, loss 0.495043, acc 0.96875\n",
      "2017-11-06T20:40:50.082885: step 327, loss 0.234693, acc 0.9375\n",
      "2017-11-06T20:40:54.164768: step 328, loss 0.589101, acc 0.8125\n",
      "2017-11-06T20:40:58.205637: step 329, loss 0.499598, acc 0.90625\n",
      "2017-11-06T20:41:02.274529: step 330, loss 0.393437, acc 0.9375\n",
      "2017-11-06T20:41:06.351426: step 331, loss 0.0683521, acc 0.9375\n",
      "2017-11-06T20:41:10.590438: step 332, loss 0.876125, acc 0.90625\n",
      "2017-11-06T20:41:15.685058: step 333, loss 0.234773, acc 0.90625\n",
      "2017-11-06T20:41:19.681897: step 334, loss 0.520796, acc 0.90625\n",
      "2017-11-06T20:41:23.804637: step 335, loss 0.644181, acc 0.90625\n",
      "2017-11-06T20:41:27.843505: step 336, loss 0.330449, acc 0.9375\n",
      "2017-11-06T20:41:31.827336: step 337, loss 0.456045, acc 0.84375\n",
      "2017-11-06T20:41:35.921245: step 338, loss 0.0228949, acc 1\n",
      "2017-11-06T20:41:39.908078: step 339, loss 0.430542, acc 0.90625\n",
      "2017-11-06T20:41:43.899914: step 340, loss 0.373418, acc 0.90625\n",
      "2017-11-06T20:41:48.272020: step 341, loss 0.487726, acc 0.875\n",
      "2017-11-06T20:41:52.599095: step 342, loss 1.58766, acc 0.78125\n",
      "2017-11-06T20:41:56.758050: step 343, loss 1.07795, acc 0.8125\n",
      "2017-11-06T20:42:00.855962: step 344, loss 0.397961, acc 0.96875\n",
      "2017-11-06T20:42:04.968884: step 345, loss 0.985224, acc 0.84375\n",
      "2017-11-06T20:42:08.997747: step 346, loss 0.589754, acc 0.84375\n",
      "2017-11-06T20:42:12.988583: step 347, loss 0.845101, acc 0.90625\n",
      "2017-11-06T20:42:17.010441: step 348, loss 0.853908, acc 0.90625\n",
      "2017-11-06T20:42:21.022291: step 349, loss 0.377784, acc 0.90625\n",
      "2017-11-06T20:42:25.056159: step 350, loss 0.559753, acc 0.90625\n",
      "2017-11-06T20:42:29.080036: step 351, loss 0.369409, acc 0.875\n",
      "2017-11-06T20:42:33.135899: step 352, loss 1.41779, acc 0.78125\n",
      "2017-11-06T20:42:37.309864: step 353, loss 0.243263, acc 0.90625\n",
      "2017-11-06T20:42:41.260671: step 354, loss 0.804185, acc 0.8125\n",
      "2017-11-06T20:42:45.292536: step 355, loss 0.835735, acc 0.875\n",
      "2017-11-06T20:42:49.290377: step 356, loss 0.44933, acc 0.875\n",
      "2017-11-06T20:42:53.616450: step 357, loss 0.426267, acc 0.875\n",
      "2017-11-06T20:42:57.815435: step 358, loss 0.178496, acc 0.9375\n",
      "2017-11-06T20:43:01.812274: step 359, loss 0.504666, acc 0.90625\n",
      "2017-11-06T20:43:04.402114: step 360, loss 0.47181, acc 0.9\n",
      "2017-11-06T20:43:08.518039: step 361, loss 0.43263, acc 0.90625\n",
      "2017-11-06T20:43:12.512877: step 362, loss 0.824839, acc 0.875\n",
      "2017-11-06T20:43:16.534735: step 363, loss 0.427084, acc 0.90625\n",
      "2017-11-06T20:43:20.559596: step 364, loss 0.265052, acc 0.9375\n",
      "2017-11-06T20:43:24.818621: step 365, loss 0.131417, acc 0.96875\n",
      "2017-11-06T20:43:29.014602: step 366, loss 0.600797, acc 0.90625\n",
      "2017-11-06T20:43:33.012443: step 367, loss 1.16031, acc 0.75\n",
      "2017-11-06T20:43:37.125365: step 368, loss 0.0815196, acc 0.96875\n",
      "2017-11-06T20:43:41.139218: step 369, loss 0.355879, acc 0.90625\n",
      "2017-11-06T20:43:45.175085: step 370, loss 0.762398, acc 0.84375\n",
      "2017-11-06T20:43:49.191941: step 371, loss 0.182497, acc 0.90625\n",
      "2017-11-06T20:43:53.209794: step 372, loss 0.614845, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T20:43:57.355740: step 373, loss 0.173223, acc 0.9375\n",
      "2017-11-06T20:44:01.754866: step 374, loss 0.235963, acc 0.90625\n",
      "2017-11-06T20:44:05.819754: step 375, loss 0.915202, acc 0.78125\n",
      "2017-11-06T20:44:09.862626: step 376, loss 1.15777, acc 0.84375\n",
      "2017-11-06T20:44:13.895493: step 377, loss 0.574074, acc 0.8125\n",
      "2017-11-06T20:44:17.891332: step 378, loss 0.160904, acc 0.9375\n",
      "2017-11-06T20:44:21.900180: step 379, loss 0.396436, acc 0.84375\n",
      "2017-11-06T20:44:25.908843: step 380, loss 0.246836, acc 0.96875\n",
      "2017-11-06T20:44:29.906684: step 381, loss 0.356638, acc 0.9375\n",
      "2017-11-06T20:44:34.100664: step 382, loss 0.511356, acc 0.90625\n",
      "2017-11-06T20:44:38.219590: step 383, loss 0.608842, acc 0.90625\n",
      "2017-11-06T20:44:42.186410: step 384, loss 0.308268, acc 0.9375\n",
      "2017-11-06T20:44:46.165236: step 385, loss 0.171081, acc 0.9375\n",
      "2017-11-06T20:44:50.202105: step 386, loss 0.329492, acc 0.875\n",
      "2017-11-06T20:44:54.246979: step 387, loss 0.25399, acc 0.90625\n",
      "2017-11-06T20:44:58.262833: step 388, loss 1.0235, acc 0.90625\n",
      "2017-11-06T20:45:02.406777: step 389, loss 0.276614, acc 0.90625\n",
      "2017-11-06T20:45:06.877955: step 390, loss 0.212891, acc 0.9375\n",
      "2017-11-06T20:45:10.975866: step 391, loss 0.182223, acc 0.9375\n",
      "2017-11-06T20:45:14.966701: step 392, loss 0.12388, acc 0.96875\n",
      "2017-11-06T20:45:18.983555: step 393, loss 0.080633, acc 0.96875\n",
      "2017-11-06T20:45:22.968386: step 394, loss 0.773877, acc 0.84375\n",
      "2017-11-06T20:45:27.042282: step 395, loss 0.118846, acc 0.9375\n",
      "2017-11-06T20:45:29.629119: step 396, loss 0.753359, acc 0.95\n",
      "2017-11-06T20:45:33.699010: step 397, loss 0.400624, acc 0.90625\n",
      "2017-11-06T20:45:37.728874: step 398, loss 0.0720937, acc 0.96875\n",
      "2017-11-06T20:45:41.763743: step 399, loss 0.0633828, acc 0.96875\n",
      "2017-11-06T20:45:45.814620: step 400, loss 0.405802, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:45:48.397455: step 400, loss 1.83102, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-400\n",
      "\n",
      "2017-11-06T20:45:53.726010: step 401, loss 0.205639, acc 0.8125\n",
      "2017-11-06T20:45:57.695831: step 402, loss 0.257785, acc 0.9375\n",
      "2017-11-06T20:46:01.698675: step 403, loss 0.523366, acc 0.875\n",
      "2017-11-06T20:46:05.772569: step 404, loss 0.514928, acc 0.90625\n",
      "2017-11-06T20:46:10.102647: step 405, loss 0.453261, acc 0.9375\n",
      "2017-11-06T20:46:14.397698: step 406, loss 0.536804, acc 0.90625\n",
      "2017-11-06T20:46:18.457583: step 407, loss 0.17876, acc 0.90625\n",
      "2017-11-06T20:46:22.408409: step 408, loss 0.270227, acc 0.84375\n",
      "2017-11-06T20:46:26.448260: step 409, loss 0.326297, acc 0.875\n",
      "2017-11-06T20:46:30.484128: step 410, loss 0.204006, acc 0.96875\n",
      "2017-11-06T20:46:34.733148: step 411, loss 0.221978, acc 0.9375\n",
      "2017-11-06T20:46:38.823053: step 412, loss 0.55903, acc 0.875\n",
      "2017-11-06T20:46:42.785869: step 413, loss 0.566612, acc 0.84375\n",
      "2017-11-06T20:46:46.772702: step 414, loss 0.0070828, acc 1\n",
      "2017-11-06T20:46:50.751529: step 415, loss 0.34029, acc 0.90625\n",
      "2017-11-06T20:46:54.741364: step 416, loss 0.505855, acc 0.90625\n",
      "2017-11-06T20:46:58.760219: step 417, loss 0.596832, acc 0.875\n",
      "2017-11-06T20:47:02.683007: step 418, loss 0.125724, acc 0.9375\n",
      "2017-11-06T20:47:06.704865: step 419, loss 0.182713, acc 0.9375\n",
      "2017-11-06T20:47:10.697701: step 420, loss 0.345282, acc 0.9375\n",
      "2017-11-06T20:47:14.921703: step 421, loss 0.0290117, acc 0.96875\n",
      "2017-11-06T20:47:19.288806: step 422, loss 0.185049, acc 0.9375\n",
      "2017-11-06T20:47:23.401728: step 423, loss 0.608898, acc 0.84375\n",
      "2017-11-06T20:47:27.376348: step 424, loss 0.768485, acc 0.875\n",
      "2017-11-06T20:47:31.425226: step 425, loss 1.12575, acc 0.8125\n",
      "2017-11-06T20:47:35.408054: step 426, loss 0.0426758, acc 0.96875\n",
      "2017-11-06T20:47:39.580020: step 427, loss 0.260171, acc 0.9375\n",
      "2017-11-06T20:47:43.589869: step 428, loss 0.302669, acc 0.96875\n",
      "2017-11-06T20:47:47.620732: step 429, loss 0.292653, acc 0.9375\n",
      "2017-11-06T20:47:51.620574: step 430, loss 0.481575, acc 0.875\n",
      "2017-11-06T20:47:55.674455: step 431, loss 0.770233, acc 0.8125\n",
      "2017-11-06T20:47:58.250286: step 432, loss 0.0853715, acc 0.95\n",
      "2017-11-06T20:48:02.296160: step 433, loss 0.241597, acc 0.90625\n",
      "2017-11-06T20:48:06.372056: step 434, loss 0.337072, acc 0.90625\n",
      "2017-11-06T20:48:10.569038: step 435, loss 0.108341, acc 0.96875\n",
      "2017-11-06T20:48:14.661947: step 436, loss 0.211818, acc 0.90625\n",
      "2017-11-06T20:48:18.824904: step 437, loss 0.329442, acc 0.9375\n",
      "2017-11-06T20:48:23.364130: step 438, loss 0.378244, acc 0.90625\n",
      "2017-11-06T20:48:27.831304: step 439, loss 0.197053, acc 0.9375\n",
      "2017-11-06T20:48:31.917208: step 440, loss 0.320933, acc 0.96875\n",
      "2017-11-06T20:48:36.253288: step 441, loss 0.628227, acc 0.90625\n",
      "2017-11-06T20:48:40.240121: step 442, loss 0.531311, acc 0.90625\n",
      "2017-11-06T20:48:44.332028: step 443, loss 0.284022, acc 0.9375\n",
      "2017-11-06T20:48:48.304852: step 444, loss 0.274549, acc 0.90625\n",
      "2017-11-06T20:48:52.364736: step 445, loss 0.0817994, acc 0.96875\n",
      "2017-11-06T20:48:56.472655: step 446, loss 0.302369, acc 0.84375\n",
      "2017-11-06T20:49:00.483504: step 447, loss 0.267835, acc 0.96875\n",
      "2017-11-06T20:49:04.536384: step 448, loss 0.673522, acc 0.875\n",
      "2017-11-06T20:49:08.590265: step 449, loss 0.330767, acc 0.9375\n",
      "2017-11-06T20:49:12.629134: step 450, loss 0.77584, acc 0.84375\n",
      "2017-11-06T20:49:16.740055: step 451, loss 0.33308, acc 0.9375\n",
      "2017-11-06T20:49:20.768919: step 452, loss 0.232889, acc 0.90625\n",
      "2017-11-06T20:49:24.781770: step 453, loss 0.183086, acc 0.9375\n",
      "2017-11-06T20:49:29.460094: step 454, loss 0.607037, acc 0.90625\n",
      "2017-11-06T20:49:33.619049: step 455, loss 0.715297, acc 0.8125\n",
      "2017-11-06T20:49:37.759991: step 456, loss 0.319197, acc 0.90625\n",
      "2017-11-06T20:49:41.781849: step 457, loss 0.222593, acc 0.9375\n",
      "2017-11-06T20:49:45.773685: step 458, loss 0.246376, acc 0.90625\n",
      "2017-11-06T20:49:49.809553: step 459, loss 0.420007, acc 0.90625\n",
      "2017-11-06T20:49:53.860431: step 460, loss 0.971707, acc 0.75\n",
      "2017-11-06T20:49:57.902304: step 461, loss 0.603316, acc 0.875\n",
      "2017-11-06T20:50:02.307433: step 462, loss 0.223028, acc 0.9375\n",
      "2017-11-06T20:50:06.326290: step 463, loss 0.78653, acc 0.78125\n",
      "2017-11-06T20:50:10.392177: step 464, loss 0.190335, acc 0.9375\n",
      "2017-11-06T20:50:14.441055: step 465, loss 0.873194, acc 0.84375\n",
      "2017-11-06T20:50:18.477922: step 466, loss 0.640186, acc 0.875\n",
      "2017-11-06T20:50:22.507787: step 467, loss 0.603079, acc 0.90625\n",
      "2017-11-06T20:50:25.176510: step 468, loss 0.434263, acc 0.9\n",
      "2017-11-06T20:50:29.193364: step 469, loss 0.209828, acc 0.84375\n",
      "2017-11-06T20:50:33.753605: step 470, loss 0.115294, acc 0.96875\n",
      "2017-11-06T20:50:38.028642: step 471, loss 0.410366, acc 0.90625\n",
      "2017-11-06T20:50:42.022480: step 472, loss 0.085512, acc 0.96875\n",
      "2017-11-06T20:50:46.013316: step 473, loss 0.0672593, acc 0.96875\n",
      "2017-11-06T20:50:50.009155: step 474, loss 0.802767, acc 0.78125\n",
      "2017-11-06T20:50:54.020005: step 475, loss 0.660642, acc 0.875\n",
      "2017-11-06T20:50:58.027852: step 476, loss 0.0516391, acc 0.96875\n",
      "2017-11-06T20:51:01.940633: step 477, loss 0.302615, acc 0.96875\n",
      "2017-11-06T20:51:05.899445: step 478, loss 0.542559, acc 0.90625\n",
      "2017-11-06T20:51:09.945321: step 479, loss 0.386441, acc 0.9375\n",
      "2017-11-06T20:51:13.932153: step 480, loss 0.446995, acc 0.90625\n",
      "2017-11-06T20:51:17.974025: step 481, loss 0.349682, acc 0.875\n",
      "2017-11-06T20:51:22.068934: step 482, loss 0.21427, acc 0.9375\n",
      "2017-11-06T20:51:26.141828: step 483, loss 0.776678, acc 0.84375\n",
      "2017-11-06T20:51:30.078626: step 484, loss 0.197411, acc 0.875\n",
      "2017-11-06T20:51:34.116495: step 485, loss 0.307711, acc 0.875\n",
      "2017-11-06T20:51:38.426557: step 486, loss 0.237314, acc 0.9375\n",
      "2017-11-06T20:51:42.687586: step 487, loss 0.231032, acc 0.9375\n",
      "2017-11-06T20:51:46.664410: step 488, loss 0.600465, acc 0.84375\n",
      "2017-11-06T20:51:50.689271: step 489, loss 0.292972, acc 0.875\n",
      "2017-11-06T20:51:54.725138: step 490, loss 0.389829, acc 0.9375\n",
      "2017-11-06T20:51:58.762007: step 491, loss 0.285123, acc 0.90625\n",
      "2017-11-06T20:52:02.778861: step 492, loss 0.222657, acc 0.90625\n",
      "2017-11-06T20:52:06.777702: step 493, loss 0.451723, acc 0.90625\n",
      "2017-11-06T20:52:10.816572: step 494, loss 0.228797, acc 0.90625\n",
      "2017-11-06T20:52:14.858444: step 495, loss 0.142817, acc 0.9375\n",
      "2017-11-06T20:52:18.867292: step 496, loss 0.359686, acc 0.90625\n",
      "2017-11-06T20:52:22.890151: step 497, loss 0.255953, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T20:52:26.921015: step 498, loss 1.09616, acc 0.84375\n",
      "2017-11-06T20:52:30.950878: step 499, loss 0.417158, acc 0.9375\n",
      "2017-11-06T20:52:35.222914: step 500, loss 0.341351, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:52:37.807751: step 500, loss 2.17625, acc 0.783333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-500\n",
      "\n",
      "2017-11-06T20:52:43.169417: step 501, loss 1.33232, acc 0.875\n",
      "2017-11-06T20:52:47.498493: step 502, loss 0.707189, acc 0.84375\n",
      "2017-11-06T20:52:51.544367: step 503, loss 0.252378, acc 0.9375\n",
      "2017-11-06T20:52:54.049147: step 504, loss 0.099704, acc 0.95\n",
      "2017-11-06T20:52:58.052993: step 505, loss 0.110923, acc 0.9375\n",
      "2017-11-06T20:53:02.090861: step 506, loss 0.330299, acc 0.875\n",
      "2017-11-06T20:53:06.226800: step 507, loss 0.344297, acc 0.90625\n",
      "2017-11-06T20:53:10.273675: step 508, loss 0.440556, acc 0.875\n",
      "2017-11-06T20:53:14.303539: step 509, loss 0.0339457, acc 0.96875\n",
      "2017-11-06T20:53:18.342408: step 510, loss 0.827394, acc 0.78125\n",
      "2017-11-06T20:53:22.396288: step 511, loss 0.373837, acc 0.90625\n",
      "2017-11-06T20:53:26.356896: step 512, loss 0.137913, acc 0.96875\n",
      "2017-11-06T20:53:30.358740: step 513, loss 0.109587, acc 0.9375\n",
      "2017-11-06T20:53:34.278524: step 514, loss 0.405377, acc 0.875\n",
      "2017-11-06T20:53:38.255350: step 515, loss 0.389928, acc 0.84375\n",
      "2017-11-06T20:53:42.250188: step 516, loss 0.406883, acc 0.90625\n",
      "2017-11-06T20:53:46.298065: step 517, loss 0.526312, acc 0.90625\n",
      "2017-11-06T20:53:50.698191: step 518, loss 0.496016, acc 0.9375\n",
      "2017-11-06T20:53:54.816118: step 519, loss 0.402577, acc 0.84375\n",
      "2017-11-06T20:53:58.821963: step 520, loss 0.0550992, acc 1\n",
      "2017-11-06T20:54:02.869840: step 521, loss 0.273851, acc 0.90625\n",
      "2017-11-06T20:54:06.853671: step 522, loss 0.305744, acc 0.90625\n",
      "2017-11-06T20:54:10.812483: step 523, loss 0.49663, acc 0.875\n",
      "2017-11-06T20:54:14.803320: step 524, loss 0.243878, acc 0.9375\n",
      "2017-11-06T20:54:18.775141: step 525, loss 0.379916, acc 0.90625\n",
      "2017-11-06T20:54:23.002145: step 526, loss 0.501757, acc 0.90625\n",
      "2017-11-06T20:54:27.294194: step 527, loss 0.0695906, acc 0.96875\n",
      "2017-11-06T20:54:31.271020: step 528, loss 0.077318, acc 0.96875\n",
      "2017-11-06T20:54:35.538053: step 529, loss 0.191397, acc 0.96875\n",
      "2017-11-06T20:54:39.588930: step 530, loss 0.907819, acc 0.90625\n",
      "2017-11-06T20:54:43.547743: step 531, loss 1.02863, acc 0.84375\n",
      "2017-11-06T20:54:47.588615: step 532, loss 0.860228, acc 0.84375\n",
      "2017-11-06T20:54:51.629485: step 533, loss 0.516773, acc 0.90625\n",
      "2017-11-06T20:54:55.950556: step 534, loss 1.40385, acc 0.78125\n",
      "2017-11-06T20:55:00.056473: step 535, loss 0.734914, acc 0.84375\n",
      "2017-11-06T20:55:04.043306: step 536, loss 0.25417, acc 0.90625\n",
      "2017-11-06T20:55:08.053156: step 537, loss 0.0277683, acc 1\n",
      "2017-11-06T20:55:12.157072: step 538, loss 0.298955, acc 0.875\n",
      "2017-11-06T20:55:16.127893: step 539, loss 0.379444, acc 0.90625\n",
      "2017-11-06T20:55:18.645681: step 540, loss 0.244537, acc 0.9\n",
      "2017-11-06T20:55:22.668640: step 541, loss 0.401352, acc 0.875\n",
      "2017-11-06T20:55:26.613444: step 542, loss 0.776854, acc 0.875\n",
      "2017-11-06T20:55:30.536230: step 543, loss 0.405249, acc 0.875\n",
      "2017-11-06T20:55:34.479031: step 544, loss 0.339304, acc 0.875\n",
      "2017-11-06T20:55:38.406822: step 545, loss 0.278917, acc 0.90625\n",
      "2017-11-06T20:55:42.348625: step 546, loss 0.497072, acc 0.90625\n",
      "2017-11-06T20:55:46.316443: step 547, loss 0.0658986, acc 0.96875\n",
      "2017-11-06T20:55:50.263266: step 548, loss 0.0782889, acc 0.96875\n",
      "2017-11-06T20:55:54.240075: step 549, loss 0.0559282, acc 0.96875\n",
      "2017-11-06T20:55:58.264932: step 550, loss 1.2319, acc 0.875\n",
      "2017-11-06T20:56:02.615023: step 551, loss 0.0814992, acc 0.96875\n",
      "2017-11-06T20:56:06.615868: step 552, loss 0.49323, acc 0.90625\n",
      "2017-11-06T20:56:10.540655: step 553, loss 0.452818, acc 0.875\n",
      "2017-11-06T20:56:14.477453: step 554, loss 0.49989, acc 0.96875\n",
      "2017-11-06T20:56:18.471291: step 555, loss 0.399839, acc 0.9375\n",
      "2017-11-06T20:56:22.402083: step 556, loss 0.555614, acc 0.9375\n",
      "2017-11-06T20:56:26.400708: step 557, loss 0.252068, acc 0.90625\n",
      "2017-11-06T20:56:30.322495: step 558, loss 0.984809, acc 0.875\n",
      "2017-11-06T20:56:34.563508: step 559, loss 0.449262, acc 0.875\n",
      "2017-11-06T20:56:38.550342: step 560, loss 0.359138, acc 0.90625\n",
      "2017-11-06T20:56:42.489140: step 561, loss 0.419379, acc 0.90625\n",
      "2017-11-06T20:56:46.430940: step 562, loss 0.233631, acc 0.9375\n",
      "2017-11-06T20:56:50.319705: step 563, loss 0.793615, acc 0.875\n",
      "2017-11-06T20:56:54.288525: step 564, loss 0.247613, acc 0.90625\n",
      "2017-11-06T20:56:58.250340: step 565, loss 0.459135, acc 0.875\n",
      "2017-11-06T20:57:02.199145: step 566, loss 0.135511, acc 0.9375\n",
      "2017-11-06T20:57:06.428149: step 567, loss 0.0617253, acc 0.96875\n",
      "2017-11-06T20:57:10.549077: step 568, loss 0.0613864, acc 0.96875\n",
      "2017-11-06T20:57:14.446847: step 569, loss 0.0677087, acc 0.9375\n",
      "2017-11-06T20:57:18.445688: step 570, loss 0.0145524, acc 1\n",
      "2017-11-06T20:57:22.492564: step 571, loss 0.391985, acc 0.875\n",
      "2017-11-06T20:57:26.866025: step 572, loss 0.483464, acc 0.84375\n",
      "2017-11-06T20:57:30.813830: step 573, loss 0.762632, acc 0.8125\n",
      "2017-11-06T20:57:34.712600: step 574, loss 0.557353, acc 0.84375\n",
      "2017-11-06T20:57:38.675416: step 575, loss 0.250152, acc 0.9375\n",
      "2017-11-06T20:57:41.218223: step 576, loss 1.12362, acc 0.8\n",
      "2017-11-06T20:57:45.142011: step 577, loss 0.0217156, acc 1\n",
      "2017-11-06T20:57:49.055792: step 578, loss 0.340976, acc 0.9375\n",
      "2017-11-06T20:57:52.938550: step 579, loss 0.544889, acc 0.875\n",
      "2017-11-06T20:57:56.879351: step 580, loss 0.131525, acc 0.9375\n",
      "2017-11-06T20:58:00.781123: step 581, loss 0.156452, acc 0.96875\n",
      "2017-11-06T20:58:04.725926: step 582, loss 0.399526, acc 0.9375\n",
      "2017-11-06T20:58:08.652717: step 583, loss 0.0223303, acc 1\n",
      "2017-11-06T20:58:12.959777: step 584, loss 0.470843, acc 0.875\n",
      "2017-11-06T20:58:17.100719: step 585, loss 0.28805, acc 0.90625\n",
      "2017-11-06T20:58:21.116572: step 586, loss 0.153311, acc 0.96875\n",
      "2017-11-06T20:58:25.147437: step 587, loss 0.689955, acc 0.8125\n",
      "2017-11-06T20:58:29.200316: step 588, loss 0.17836, acc 0.96875\n",
      "2017-11-06T20:58:33.254196: step 589, loss 0.300721, acc 0.90625\n",
      "2017-11-06T20:58:37.468191: step 590, loss 0.877501, acc 0.84375\n",
      "2017-11-06T20:58:41.526074: step 591, loss 0.654316, acc 0.90625\n",
      "2017-11-06T20:58:45.496895: step 592, loss 0.126427, acc 0.96875\n",
      "2017-11-06T20:58:49.501741: step 593, loss 0.427855, acc 0.90625\n",
      "2017-11-06T20:58:53.427531: step 594, loss 0.330534, acc 0.90625\n",
      "2017-11-06T20:58:57.390346: step 595, loss 0.327518, acc 0.90625\n",
      "2017-11-06T20:59:01.360167: step 596, loss 0.0782494, acc 0.96875\n",
      "2017-11-06T20:59:05.308974: step 597, loss 0.202349, acc 0.96875\n",
      "2017-11-06T20:59:09.233761: step 598, loss 0.463966, acc 0.90625\n",
      "2017-11-06T20:59:13.217594: step 599, loss 0.800671, acc 0.84375\n",
      "2017-11-06T20:59:17.384553: step 600, loss 0.65273, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T20:59:20.206558: step 600, loss 1.76979, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-600\n",
      "\n",
      "2017-11-06T20:59:26.229067: step 601, loss 0.311452, acc 0.875\n",
      "2017-11-06T20:59:30.264149: step 602, loss 0.359072, acc 0.90625\n",
      "2017-11-06T20:59:34.204949: step 603, loss 0.262116, acc 0.875\n",
      "2017-11-06T20:59:38.154757: step 604, loss 0.348051, acc 0.90625\n",
      "2017-11-06T20:59:42.074541: step 605, loss 0.26623, acc 0.90625\n",
      "2017-11-06T20:59:46.028350: step 606, loss 0.610484, acc 0.8125\n",
      "2017-11-06T20:59:50.020186: step 607, loss 0.48872, acc 0.84375\n",
      "2017-11-06T20:59:53.986004: step 608, loss 0.170262, acc 0.9375\n",
      "2017-11-06T20:59:57.952823: step 609, loss 0.549827, acc 0.90625\n",
      "2017-11-06T21:00:02.192837: step 610, loss 0.733712, acc 0.84375\n",
      "2017-11-06T21:00:06.122628: step 611, loss 0.316807, acc 0.90625\n",
      "2017-11-06T21:00:08.667436: step 612, loss 0.427069, acc 0.9\n",
      "2017-11-06T21:00:12.648265: step 613, loss 0.22133, acc 0.875\n",
      "2017-11-06T21:00:16.652109: step 614, loss 0.0503684, acc 0.96875\n",
      "2017-11-06T21:00:20.697984: step 615, loss 0.306909, acc 0.9375\n",
      "2017-11-06T21:00:25.057082: step 616, loss 0.196125, acc 0.96875\n",
      "2017-11-06T21:00:29.054923: step 617, loss 0.0655318, acc 0.96875\n",
      "2017-11-06T21:00:33.120811: step 618, loss 0.552655, acc 0.84375\n",
      "2017-11-06T21:00:37.273762: step 619, loss 0.0270868, acc 1\n",
      "2017-11-06T21:00:41.257593: step 620, loss 0.53576, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:00:45.243425: step 621, loss 0.0500806, acc 0.96875\n",
      "2017-11-06T21:00:49.220250: step 622, loss 0.095731, acc 0.9375\n",
      "2017-11-06T21:00:53.154047: step 623, loss 0.102979, acc 0.96875\n",
      "2017-11-06T21:00:57.150886: step 624, loss 0.409345, acc 0.90625\n",
      "2017-11-06T21:01:01.152729: step 625, loss 0.0584329, acc 0.96875\n",
      "2017-11-06T21:01:05.119548: step 626, loss 0.293873, acc 0.9375\n",
      "2017-11-06T21:01:09.102377: step 627, loss 0.879379, acc 0.875\n",
      "2017-11-06T21:01:13.075200: step 628, loss 0.263684, acc 0.90625\n",
      "2017-11-06T21:01:17.043019: step 629, loss 0.282618, acc 0.9375\n",
      "2017-11-06T21:01:21.070882: step 630, loss 0.580855, acc 0.875\n",
      "2017-11-06T21:01:25.100745: step 631, loss 0.217453, acc 0.96875\n",
      "2017-11-06T21:01:29.398799: step 632, loss 0.583085, acc 0.78125\n",
      "2017-11-06T21:01:33.641852: step 633, loss 0.365021, acc 0.875\n",
      "2017-11-06T21:01:37.808813: step 634, loss 0.109424, acc 0.9375\n",
      "2017-11-06T21:01:41.990784: step 635, loss 0.784562, acc 0.78125\n",
      "2017-11-06T21:01:45.918576: step 636, loss 0.336268, acc 0.9375\n",
      "2017-11-06T21:01:49.953442: step 637, loss 0.102184, acc 0.9375\n",
      "2017-11-06T21:01:53.955287: step 638, loss 0.307751, acc 0.90625\n",
      "2017-11-06T21:01:57.916100: step 639, loss 0.191811, acc 0.9375\n",
      "2017-11-06T21:02:02.018014: step 640, loss 0.0939447, acc 0.96875\n",
      "2017-11-06T21:02:05.954812: step 641, loss 0.276596, acc 0.90625\n",
      "2017-11-06T21:02:09.923632: step 642, loss 0.289171, acc 0.875\n",
      "2017-11-06T21:02:13.889449: step 643, loss 0.564722, acc 0.8125\n",
      "2017-11-06T21:02:17.884288: step 644, loss 0.386348, acc 0.90625\n",
      "2017-11-06T21:02:21.925161: step 645, loss 0.104711, acc 0.9375\n",
      "2017-11-06T21:02:25.859708: step 646, loss 0.0397612, acc 1\n",
      "2017-11-06T21:02:29.810516: step 647, loss 0.216731, acc 0.90625\n",
      "2017-11-06T21:02:32.549461: step 648, loss 0.0676393, acc 0.95\n",
      "2017-11-06T21:02:37.143726: step 649, loss 0.000493213, acc 1\n",
      "2017-11-06T21:02:41.108542: step 650, loss 0.325302, acc 0.9375\n",
      "2017-11-06T21:02:45.051345: step 651, loss 0.690699, acc 0.84375\n",
      "2017-11-06T21:02:49.056190: step 652, loss 0.90602, acc 0.84375\n",
      "2017-11-06T21:02:52.998993: step 653, loss 0.488571, acc 0.9375\n",
      "2017-11-06T21:02:56.978821: step 654, loss 0.345467, acc 0.9375\n",
      "2017-11-06T21:03:00.998676: step 655, loss 0.175747, acc 0.96875\n",
      "2017-11-06T21:03:04.991512: step 656, loss 0.162546, acc 0.9375\n",
      "2017-11-06T21:03:08.959332: step 657, loss 1.02386, acc 0.84375\n",
      "2017-11-06T21:03:12.962176: step 658, loss 0.325612, acc 0.9375\n",
      "2017-11-06T21:03:16.937000: step 659, loss 0.584665, acc 0.875\n",
      "2017-11-06T21:03:20.950852: step 660, loss 0.165576, acc 0.90625\n",
      "2017-11-06T21:03:25.140830: step 661, loss 0.625962, acc 0.8125\n",
      "2017-11-06T21:03:29.072624: step 662, loss 0.0216531, acc 1\n",
      "2017-11-06T21:03:33.068463: step 663, loss 0.298349, acc 0.875\n",
      "2017-11-06T21:03:37.132435: step 664, loss 0.288594, acc 0.875\n",
      "2017-11-06T21:03:41.719694: step 665, loss 0.307428, acc 0.90625\n",
      "2017-11-06T21:03:45.755562: step 666, loss 0.264142, acc 0.90625\n",
      "2017-11-06T21:03:50.016591: step 667, loss 0.0525836, acc 0.96875\n",
      "2017-11-06T21:03:54.118504: step 668, loss 0.344162, acc 0.875\n",
      "2017-11-06T21:03:58.335500: step 669, loss 0.291537, acc 0.875\n",
      "2017-11-06T21:04:02.408395: step 670, loss 0.273804, acc 0.90625\n",
      "2017-11-06T21:04:06.541331: step 671, loss 0.0550001, acc 0.96875\n",
      "2017-11-06T21:04:10.634239: step 672, loss 0.270678, acc 0.90625\n",
      "2017-11-06T21:04:14.832222: step 673, loss 0.318613, acc 0.9375\n",
      "2017-11-06T21:04:18.885102: step 674, loss 0.525979, acc 0.90625\n",
      "2017-11-06T21:04:23.027045: step 675, loss 0.139546, acc 0.9375\n",
      "2017-11-06T21:04:27.165986: step 676, loss 0.128862, acc 0.9375\n",
      "2017-11-06T21:04:31.305927: step 677, loss 0.504799, acc 0.875\n",
      "2017-11-06T21:04:35.682037: step 678, loss 0.485107, acc 0.90625\n",
      "2017-11-06T21:04:39.791959: step 679, loss 0.831686, acc 0.84375\n",
      "2017-11-06T21:04:43.899876: step 680, loss 0.139951, acc 0.96875\n",
      "2017-11-06T21:04:48.344034: step 681, loss 0.425936, acc 0.84375\n",
      "2017-11-06T21:04:52.292841: step 682, loss 0.275544, acc 0.96875\n",
      "2017-11-06T21:04:56.266663: step 683, loss 0.286498, acc 0.90625\n",
      "2017-11-06T21:04:58.842493: step 684, loss 0.02143, acc 1\n",
      "2017-11-06T21:05:02.831329: step 685, loss 0.241525, acc 0.9375\n",
      "2017-11-06T21:05:06.792143: step 686, loss 0.389194, acc 0.90625\n",
      "2017-11-06T21:05:10.820004: step 687, loss 0.215614, acc 0.96875\n",
      "2017-11-06T21:05:14.828852: step 688, loss 0.459277, acc 0.875\n",
      "2017-11-06T21:05:18.837700: step 689, loss 0.167399, acc 0.9375\n",
      "2017-11-06T21:05:22.867563: step 690, loss 0.917158, acc 0.84375\n",
      "2017-11-06T21:05:26.871178: step 691, loss 0.311958, acc 0.96875\n",
      "2017-11-06T21:05:30.887031: step 692, loss 0.0949953, acc 0.9375\n",
      "2017-11-06T21:05:34.863858: step 693, loss 0.2105, acc 0.90625\n",
      "2017-11-06T21:05:38.925782: step 694, loss 0.582111, acc 0.84375\n",
      "2017-11-06T21:05:42.990671: step 695, loss 0.266581, acc 0.875\n",
      "2017-11-06T21:05:46.984508: step 696, loss 0.315105, acc 0.90625\n",
      "2017-11-06T21:05:51.261549: step 697, loss 0.0933132, acc 0.96875\n",
      "2017-11-06T21:05:55.479545: step 698, loss 0.50179, acc 0.875\n",
      "2017-11-06T21:05:59.481389: step 699, loss 0.410097, acc 0.84375\n",
      "2017-11-06T21:06:03.428194: step 700, loss 0.314449, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:06:06.008025: step 700, loss 1.57159, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-700\n",
      "\n",
      "2017-11-06T21:06:11.451079: step 701, loss 0.331992, acc 0.90625\n",
      "2017-11-06T21:06:15.431908: step 702, loss 0.128572, acc 0.9375\n",
      "2017-11-06T21:06:19.382715: step 703, loss 0.276362, acc 0.90625\n",
      "2017-11-06T21:06:23.413580: step 704, loss 0.147245, acc 0.96875\n",
      "2017-11-06T21:06:27.424428: step 705, loss 0.647662, acc 0.84375\n",
      "2017-11-06T21:06:31.386244: step 706, loss 0.811435, acc 0.875\n",
      "2017-11-06T21:06:35.584227: step 707, loss 0.131607, acc 0.9375\n",
      "2017-11-06T21:06:39.589072: step 708, loss 0.332698, acc 0.9375\n",
      "2017-11-06T21:06:43.518864: step 709, loss 0.0418363, acc 0.96875\n",
      "2017-11-06T21:06:47.488686: step 710, loss 0.0739936, acc 0.96875\n",
      "2017-11-06T21:06:51.447498: step 711, loss 0.0510794, acc 1\n",
      "2017-11-06T21:06:55.581436: step 712, loss 0.739204, acc 0.84375\n",
      "2017-11-06T21:06:59.819447: step 713, loss 0.271177, acc 0.90625\n",
      "2017-11-06T21:07:03.818288: step 714, loss 0.193042, acc 0.9375\n",
      "2017-11-06T21:07:07.790113: step 715, loss 0.465908, acc 0.90625\n",
      "2017-11-06T21:07:11.803964: step 716, loss 0.470114, acc 0.84375\n",
      "2017-11-06T21:07:15.726750: step 717, loss 0.0675923, acc 0.96875\n",
      "2017-11-06T21:07:19.762617: step 718, loss 0.142192, acc 0.9375\n",
      "2017-11-06T21:07:23.903562: step 719, loss 0.207715, acc 0.9375\n",
      "2017-11-06T21:07:26.409340: step 720, loss 0.6344, acc 0.8\n",
      "2017-11-06T21:07:30.385166: step 721, loss 0.216957, acc 0.90625\n",
      "2017-11-06T21:07:34.423034: step 722, loss 0.0555472, acc 0.9375\n",
      "2017-11-06T21:07:38.437906: step 723, loss 0.344315, acc 0.90625\n",
      "2017-11-06T21:07:42.531885: step 724, loss 0.184362, acc 0.96875\n",
      "2017-11-06T21:07:46.503709: step 725, loss 0.383061, acc 0.90625\n",
      "2017-11-06T21:07:50.514557: step 726, loss 0.25768, acc 0.90625\n",
      "2017-11-06T21:07:54.482377: step 727, loss 0.0968776, acc 0.96875\n",
      "2017-11-06T21:07:58.458202: step 728, loss 0.0508247, acc 0.96875\n",
      "2017-11-06T21:08:02.729237: step 729, loss 0.327578, acc 0.84375\n",
      "2017-11-06T21:08:06.885189: step 730, loss 0.234336, acc 0.90625\n",
      "2017-11-06T21:08:10.901042: step 731, loss 0.304061, acc 0.90625\n",
      "2017-11-06T21:08:14.851850: step 732, loss 0.518995, acc 0.875\n",
      "2017-11-06T21:08:18.933750: step 733, loss 0.244051, acc 0.90625\n",
      "2017-11-06T21:08:23.025659: step 734, loss 0.229389, acc 0.96875\n",
      "2017-11-06T21:08:27.148347: step 735, loss 0.212811, acc 0.90625\n",
      "2017-11-06T21:08:31.098153: step 736, loss 0.304677, acc 0.875\n",
      "2017-11-06T21:08:35.414220: step 737, loss 0.240778, acc 0.90625\n",
      "2017-11-06T21:08:39.509130: step 738, loss 0.563496, acc 0.90625\n",
      "2017-11-06T21:08:43.531988: step 739, loss 0.292754, acc 0.9375\n",
      "2017-11-06T21:08:47.542838: step 740, loss 0.164537, acc 0.9375\n",
      "2017-11-06T21:08:51.571701: step 741, loss 0.0905291, acc 0.96875\n",
      "2017-11-06T21:08:55.612572: step 742, loss 0.19309, acc 0.9375\n",
      "2017-11-06T21:08:59.608412: step 743, loss 0.350438, acc 0.875\n",
      "2017-11-06T21:09:03.659289: step 744, loss 0.564136, acc 0.90625\n",
      "2017-11-06T21:09:07.921318: step 745, loss 0.0869051, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:09:12.203360: step 746, loss 0.031589, acc 1\n",
      "2017-11-06T21:09:16.183189: step 747, loss 0.275913, acc 0.96875\n",
      "2017-11-06T21:09:20.144004: step 748, loss 0.386494, acc 0.90625\n",
      "2017-11-06T21:09:24.167863: step 749, loss 0.232804, acc 0.9375\n",
      "2017-11-06T21:09:28.142687: step 750, loss 0.299143, acc 0.96875\n",
      "2017-11-06T21:09:32.178553: step 751, loss 0.751713, acc 0.78125\n",
      "2017-11-06T21:09:36.186401: step 752, loss 0.185281, acc 0.9375\n",
      "2017-11-06T21:09:40.233277: step 753, loss 0.298738, acc 0.90625\n",
      "2017-11-06T21:09:44.312175: step 754, loss 0.0439755, acc 0.96875\n",
      "2017-11-06T21:09:48.276993: step 755, loss 0.262571, acc 0.875\n",
      "2017-11-06T21:09:50.905860: step 756, loss 0.216941, acc 0.95\n",
      "2017-11-06T21:09:54.943729: step 757, loss 0.317822, acc 0.90625\n",
      "2017-11-06T21:09:58.890533: step 758, loss 0.285812, acc 0.90625\n",
      "2017-11-06T21:10:03.167572: step 759, loss 0.372276, acc 0.90625\n",
      "2017-11-06T21:10:07.172418: step 760, loss 0.0433939, acc 0.96875\n",
      "2017-11-06T21:10:11.184269: step 761, loss 0.226222, acc 0.9375\n",
      "2017-11-06T21:10:15.531358: step 762, loss 0.743806, acc 0.8125\n",
      "2017-11-06T21:10:19.639276: step 763, loss 0.583328, acc 0.90625\n",
      "2017-11-06T21:10:23.648125: step 764, loss 0.368138, acc 0.90625\n",
      "2017-11-06T21:10:27.666980: step 765, loss 0.37164, acc 0.90625\n",
      "2017-11-06T21:10:31.611784: step 766, loss 0.240501, acc 0.875\n",
      "2017-11-06T21:10:35.904834: step 767, loss 0.233419, acc 0.9375\n",
      "2017-11-06T21:10:39.861646: step 768, loss 0.227168, acc 0.9375\n",
      "2017-11-06T21:10:43.844477: step 769, loss 0.324391, acc 0.90625\n",
      "2017-11-06T21:10:47.878341: step 770, loss 0.86689, acc 0.8125\n",
      "2017-11-06T21:10:51.828151: step 771, loss 0.310647, acc 0.90625\n",
      "2017-11-06T21:10:55.820985: step 772, loss 0.591696, acc 0.84375\n",
      "2017-11-06T21:10:59.778797: step 773, loss 0.276176, acc 0.9375\n",
      "2017-11-06T21:11:03.747618: step 774, loss 0.228505, acc 0.9375\n",
      "2017-11-06T21:11:07.698424: step 775, loss 0.188053, acc 0.9375\n",
      "2017-11-06T21:11:11.669246: step 776, loss 0.0538353, acc 0.96875\n",
      "2017-11-06T21:11:15.611047: step 777, loss 0.364194, acc 0.875\n",
      "2017-11-06T21:11:19.951130: step 778, loss 0.509869, acc 0.875\n",
      "2017-11-06T21:11:24.170128: step 779, loss 0.174989, acc 0.9375\n",
      "2017-11-06T21:11:28.103676: step 780, loss 0.130955, acc 0.9375\n",
      "2017-11-06T21:11:32.060488: step 781, loss 0.565476, acc 0.84375\n",
      "2017-11-06T21:11:36.036313: step 782, loss 0.0722482, acc 0.9375\n",
      "2017-11-06T21:11:40.017142: step 783, loss 0.182643, acc 0.96875\n",
      "2017-11-06T21:11:43.990967: step 784, loss 0.0710727, acc 0.96875\n",
      "2017-11-06T21:11:48.068864: step 785, loss 0.197782, acc 0.875\n",
      "2017-11-06T21:11:52.106732: step 786, loss 0.254371, acc 0.875\n",
      "2017-11-06T21:11:56.169618: step 787, loss 0.35421, acc 0.875\n",
      "2017-11-06T21:12:00.145443: step 788, loss 0.226959, acc 0.90625\n",
      "2017-11-06T21:12:04.104256: step 789, loss 0.634522, acc 0.84375\n",
      "2017-11-06T21:12:08.072076: step 790, loss 0.594464, acc 0.84375\n",
      "2017-11-06T21:12:12.046900: step 791, loss 0.314955, acc 0.9375\n",
      "2017-11-06T21:12:14.565690: step 792, loss 0.150404, acc 0.9\n",
      "2017-11-06T21:12:18.518498: step 793, loss 0.154891, acc 0.90625\n",
      "2017-11-06T21:12:22.525345: step 794, loss 0.173343, acc 0.9375\n",
      "2017-11-06T21:12:27.046558: step 795, loss 0.147223, acc 0.9375\n",
      "2017-11-06T21:12:31.077422: step 796, loss 0.431206, acc 0.90625\n",
      "2017-11-06T21:12:35.286412: step 797, loss 0.385144, acc 0.875\n",
      "2017-11-06T21:12:39.344296: step 798, loss 0.407443, acc 0.9375\n",
      "2017-11-06T21:12:43.336134: step 799, loss 0.20264, acc 0.875\n",
      "2017-11-06T21:12:47.370999: step 800, loss 0.0781133, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:12:49.931819: step 800, loss 1.5766, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-800\n",
      "\n",
      "2017-11-06T21:12:55.612054: step 801, loss 0.690498, acc 0.84375\n",
      "2017-11-06T21:12:59.687951: step 802, loss 0.41873, acc 0.875\n",
      "2017-11-06T21:13:03.738829: step 803, loss 0.242583, acc 0.90625\n",
      "2017-11-06T21:13:07.739671: step 804, loss 0.111306, acc 0.9375\n",
      "2017-11-06T21:13:11.798556: step 805, loss 0.211488, acc 0.9375\n",
      "2017-11-06T21:13:15.751364: step 806, loss 0.337635, acc 0.90625\n",
      "2017-11-06T21:13:19.883300: step 807, loss 0.283916, acc 0.90625\n",
      "2017-11-06T21:13:24.105300: step 808, loss 0.063818, acc 0.96875\n",
      "2017-11-06T21:13:28.402354: step 809, loss 0.249788, acc 0.9375\n",
      "2017-11-06T21:13:32.814488: step 810, loss 0.238949, acc 0.875\n",
      "2017-11-06T21:13:36.906395: step 811, loss 0.397882, acc 0.9375\n",
      "2017-11-06T21:13:40.919247: step 812, loss 0.282287, acc 0.90625\n",
      "2017-11-06T21:13:44.909082: step 813, loss 0.0421067, acc 1\n",
      "2017-11-06T21:13:48.975971: step 814, loss 0.386051, acc 0.875\n",
      "2017-11-06T21:13:53.004834: step 815, loss 0.168443, acc 0.9375\n",
      "2017-11-06T21:13:57.049708: step 816, loss 0.280262, acc 0.90625\n",
      "2017-11-06T21:14:01.095583: step 817, loss 0.460649, acc 0.84375\n",
      "2017-11-06T21:14:05.151465: step 818, loss 0.235349, acc 0.9375\n",
      "2017-11-06T21:14:09.087261: step 819, loss 0.342975, acc 0.90625\n",
      "2017-11-06T21:14:13.153150: step 820, loss 0.111011, acc 0.96875\n",
      "2017-11-06T21:14:17.223042: step 821, loss 0.0797092, acc 0.9375\n",
      "2017-11-06T21:14:21.262913: step 822, loss 0.317294, acc 0.90625\n",
      "2017-11-06T21:14:25.355823: step 823, loss 0.399913, acc 0.875\n",
      "2017-11-06T21:14:29.394501: step 824, loss 0.158495, acc 0.9375\n",
      "2017-11-06T21:14:33.594484: step 825, loss 0.083767, acc 0.9375\n",
      "2017-11-06T21:14:38.142715: step 826, loss 0.56838, acc 0.90625\n",
      "2017-11-06T21:14:42.224615: step 827, loss 0.517284, acc 0.90625\n",
      "2017-11-06T21:14:44.759416: step 828, loss 0.0947251, acc 0.95\n",
      "2017-11-06T21:14:48.776272: step 829, loss 0.304237, acc 0.90625\n",
      "2017-11-06T21:14:52.852167: step 830, loss 0.145137, acc 0.96875\n",
      "2017-11-06T21:14:56.924060: step 831, loss 0.254856, acc 0.90625\n",
      "2017-11-06T21:15:00.876869: step 832, loss 0.032843, acc 1\n",
      "2017-11-06T21:15:04.867704: step 833, loss 0.479762, acc 0.8125\n",
      "2017-11-06T21:15:08.877555: step 834, loss 0.219991, acc 0.9375\n",
      "2017-11-06T21:15:12.921427: step 835, loss 0.0650423, acc 0.96875\n",
      "2017-11-06T21:15:17.021340: step 836, loss 0.342298, acc 0.90625\n",
      "2017-11-06T21:15:21.039196: step 837, loss 0.364483, acc 0.90625\n",
      "2017-11-06T21:15:25.101081: step 838, loss 0.0311121, acc 1\n",
      "2017-11-06T21:15:29.127943: step 839, loss 0.222112, acc 0.90625\n",
      "2017-11-06T21:15:33.139794: step 840, loss 0.525155, acc 0.875\n",
      "2017-11-06T21:15:37.157667: step 841, loss 0.255611, acc 0.90625\n",
      "2017-11-06T21:15:41.323609: step 842, loss 0.126574, acc 0.9375\n",
      "2017-11-06T21:15:45.642676: step 843, loss 0.195397, acc 0.9375\n",
      "2017-11-06T21:15:49.736627: step 844, loss 0.428208, acc 0.90625\n",
      "2017-11-06T21:15:53.729464: step 845, loss 0.246626, acc 0.9375\n",
      "2017-11-06T21:15:57.711293: step 846, loss 0.665881, acc 0.875\n",
      "2017-11-06T21:16:01.796195: step 847, loss 0.206128, acc 0.90625\n",
      "2017-11-06T21:16:05.880098: step 848, loss 0.21834, acc 0.90625\n",
      "2017-11-06T21:16:09.893950: step 849, loss 0.239165, acc 0.9375\n",
      "2017-11-06T21:16:14.006872: step 850, loss 0.159008, acc 0.9375\n",
      "2017-11-06T21:16:18.045742: step 851, loss 0.512151, acc 0.875\n",
      "2017-11-06T21:16:22.057593: step 852, loss 0.323329, acc 0.875\n",
      "2017-11-06T21:16:26.196534: step 853, loss 0.143561, acc 0.90625\n",
      "2017-11-06T21:16:30.257419: step 854, loss 0.0293891, acc 1\n",
      "2017-11-06T21:16:34.533457: step 855, loss 0.513549, acc 0.875\n",
      "2017-11-06T21:16:38.618361: step 856, loss 0.0138019, acc 1\n",
      "2017-11-06T21:16:42.643220: step 857, loss 0.416604, acc 0.90625\n",
      "2017-11-06T21:16:46.798172: step 858, loss 0.221688, acc 0.90625\n",
      "2017-11-06T21:16:51.092223: step 859, loss 0.543638, acc 0.8125\n",
      "2017-11-06T21:16:55.147104: step 860, loss 0.291371, acc 0.875\n",
      "2017-11-06T21:16:59.142944: step 861, loss 0.390137, acc 0.84375\n",
      "2017-11-06T21:17:03.161799: step 862, loss 0.0183469, acc 1\n",
      "2017-11-06T21:17:07.157637: step 863, loss 0.584034, acc 0.84375\n",
      "2017-11-06T21:17:09.699443: step 864, loss 0.311185, acc 0.85\n",
      "2017-11-06T21:17:13.726307: step 865, loss 0.28956, acc 0.9375\n",
      "2017-11-06T21:17:17.745161: step 866, loss 0.160124, acc 0.875\n",
      "2017-11-06T21:17:21.747004: step 867, loss 0.00327301, acc 1\n",
      "2017-11-06T21:17:26.010034: step 868, loss 0.38694, acc 0.90625\n",
      "2017-11-06T21:17:30.140728: step 869, loss 0.119011, acc 0.96875\n",
      "2017-11-06T21:17:34.140570: step 870, loss 0.345916, acc 0.90625\n",
      "2017-11-06T21:17:38.168432: step 871, loss 0.647274, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:17:42.222312: step 872, loss 0.219282, acc 0.96875\n",
      "2017-11-06T21:17:46.235164: step 873, loss 0.234548, acc 0.90625\n",
      "2017-11-06T21:17:50.343301: step 874, loss 0.200037, acc 0.90625\n",
      "2017-11-06T21:17:54.873520: step 875, loss 0.361959, acc 0.90625\n",
      "2017-11-06T21:17:58.922397: step 876, loss 0.544824, acc 0.8125\n",
      "2017-11-06T21:18:02.967271: step 877, loss 0.178571, acc 0.9375\n",
      "2017-11-06T21:18:06.988127: step 878, loss 0.336165, acc 0.875\n",
      "2017-11-06T21:18:11.055017: step 879, loss 0.128929, acc 0.9375\n",
      "2017-11-06T21:18:15.050856: step 880, loss 0.245881, acc 0.875\n",
      "2017-11-06T21:18:18.999664: step 881, loss 0.102364, acc 0.9375\n",
      "2017-11-06T21:18:23.117588: step 882, loss 0.365061, acc 0.84375\n",
      "2017-11-06T21:18:27.185479: step 883, loss 0.510307, acc 0.875\n",
      "2017-11-06T21:18:31.118273: step 884, loss 0.0680558, acc 0.96875\n",
      "2017-11-06T21:18:35.349279: step 885, loss 0.19945, acc 0.875\n",
      "2017-11-06T21:18:39.409165: step 886, loss 0.585953, acc 0.84375\n",
      "2017-11-06T21:18:43.347963: step 887, loss 0.363307, acc 0.90625\n",
      "2017-11-06T21:18:47.309778: step 888, loss 0.301934, acc 0.90625\n",
      "2017-11-06T21:18:51.283601: step 889, loss 0.224517, acc 0.90625\n",
      "2017-11-06T21:18:55.294451: step 890, loss 0.0937542, acc 0.9375\n",
      "2017-11-06T21:18:59.662555: step 891, loss 0.126331, acc 0.9375\n",
      "2017-11-06T21:19:03.818510: step 892, loss 0.158172, acc 0.9375\n",
      "2017-11-06T21:19:07.771317: step 893, loss 0.497894, acc 0.84375\n",
      "2017-11-06T21:19:11.736134: step 894, loss 0.59636, acc 0.8125\n",
      "2017-11-06T21:19:15.686941: step 895, loss 0.123248, acc 0.9375\n",
      "2017-11-06T21:19:19.705797: step 896, loss 0.34298, acc 0.96875\n",
      "2017-11-06T21:19:23.636591: step 897, loss 0.153289, acc 0.96875\n",
      "2017-11-06T21:19:27.576389: step 898, loss 0.381692, acc 0.84375\n",
      "2017-11-06T21:19:31.483165: step 899, loss 0.274562, acc 0.90625\n",
      "2017-11-06T21:19:33.982941: step 900, loss 0.376698, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:19:36.535755: step 900, loss 1.3582, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-900\n",
      "\n",
      "2017-11-06T21:19:42.059496: step 901, loss 0.224186, acc 0.90625\n",
      "2017-11-06T21:19:46.015305: step 902, loss 0.151021, acc 0.96875\n",
      "2017-11-06T21:19:49.975119: step 903, loss 0.273352, acc 0.90625\n",
      "2017-11-06T21:19:53.955023: step 904, loss 0.184066, acc 0.9375\n",
      "2017-11-06T21:19:57.875810: step 905, loss 0.347049, acc 0.90625\n",
      "2017-11-06T21:20:02.146844: step 906, loss 0.25927, acc 0.9375\n",
      "2017-11-06T21:20:06.497935: step 907, loss 0.155888, acc 0.9375\n",
      "2017-11-06T21:20:10.427728: step 908, loss 0.283332, acc 0.90625\n",
      "2017-11-06T21:20:14.411558: step 909, loss 0.402753, acc 0.875\n",
      "2017-11-06T21:20:18.374375: step 910, loss 0.262666, acc 0.84375\n",
      "2017-11-06T21:20:22.302166: step 911, loss 0.509454, acc 0.84375\n",
      "2017-11-06T21:20:26.292000: step 912, loss 0.328047, acc 0.875\n",
      "2017-11-06T21:20:30.219735: step 913, loss 0.261114, acc 0.9375\n",
      "2017-11-06T21:20:34.512784: step 914, loss 0.0682564, acc 0.96875\n",
      "2017-11-06T21:20:38.442579: step 915, loss 0.164827, acc 0.9375\n",
      "2017-11-06T21:20:42.340346: step 916, loss 0.123514, acc 0.9375\n",
      "2017-11-06T21:20:46.297160: step 917, loss 0.289233, acc 0.90625\n",
      "2017-11-06T21:20:50.227952: step 918, loss 0.0151741, acc 1\n",
      "2017-11-06T21:20:54.196772: step 919, loss 0.0126986, acc 1\n",
      "2017-11-06T21:20:58.117557: step 920, loss 0.29689, acc 0.875\n",
      "2017-11-06T21:21:02.083374: step 921, loss 0.0459562, acc 0.96875\n",
      "2017-11-06T21:21:06.060202: step 922, loss 0.293033, acc 0.90625\n",
      "2017-11-06T21:21:10.278197: step 923, loss 0.636439, acc 0.75\n",
      "2017-11-06T21:21:14.444157: step 924, loss 0.355408, acc 0.90625\n",
      "2017-11-06T21:21:18.417981: step 925, loss 0.335516, acc 0.90625\n",
      "2017-11-06T21:21:22.400812: step 926, loss 0.144327, acc 0.90625\n",
      "2017-11-06T21:21:26.355622: step 927, loss 0.262965, acc 0.96875\n",
      "2017-11-06T21:21:30.286415: step 928, loss 0.474424, acc 0.84375\n",
      "2017-11-06T21:21:34.253233: step 929, loss 0.214736, acc 0.9375\n",
      "2017-11-06T21:21:38.182024: step 930, loss 0.327159, acc 0.875\n",
      "2017-11-06T21:21:42.090802: step 931, loss 0.259754, acc 0.9375\n",
      "2017-11-06T21:21:46.065628: step 932, loss 0.198228, acc 0.90625\n",
      "2017-11-06T21:21:50.065468: step 933, loss 0.136185, acc 0.96875\n",
      "2017-11-06T21:21:54.136575: step 934, loss 0.125433, acc 0.96875\n",
      "2017-11-06T21:21:58.096389: step 935, loss 0.291018, acc 0.9375\n",
      "2017-11-06T21:22:00.626188: step 936, loss 0.0203338, acc 1\n",
      "2017-11-06T21:22:04.555978: step 937, loss 0.427841, acc 0.84375\n",
      "2017-11-06T21:22:08.510788: step 938, loss 0.139522, acc 0.96875\n",
      "2017-11-06T21:22:12.473605: step 939, loss 0.382914, acc 0.875\n",
      "2017-11-06T21:22:16.745640: step 940, loss 0.309571, acc 0.875\n",
      "2017-11-06T21:22:20.798519: step 941, loss 0.181145, acc 0.96875\n",
      "2017-11-06T21:22:24.790356: step 942, loss 0.399858, acc 0.8125\n",
      "2017-11-06T21:22:28.762180: step 943, loss 0.13145, acc 0.9375\n",
      "2017-11-06T21:22:32.787038: step 944, loss 0.0906921, acc 0.96875\n",
      "2017-11-06T21:22:36.912969: step 945, loss 0.434821, acc 0.875\n",
      "2017-11-06T21:22:40.826751: step 946, loss 0.368301, acc 0.875\n",
      "2017-11-06T21:22:44.757543: step 947, loss 0.430705, acc 0.84375\n",
      "2017-11-06T21:22:48.694340: step 948, loss 0.551661, acc 0.875\n",
      "2017-11-06T21:22:52.626134: step 949, loss 0.263919, acc 0.9375\n",
      "2017-11-06T21:22:56.561931: step 950, loss 0.175601, acc 0.9375\n",
      "2017-11-06T21:23:00.563774: step 951, loss 0.0333984, acc 1\n",
      "2017-11-06T21:23:04.506576: step 952, loss 0.283518, acc 0.9375\n",
      "2017-11-06T21:23:08.486404: step 953, loss 0.305322, acc 0.9375\n",
      "2017-11-06T21:23:12.450220: step 954, loss 0.204781, acc 0.9375\n",
      "2017-11-06T21:23:16.421041: step 955, loss 0.418381, acc 0.875\n",
      "2017-11-06T21:23:20.607016: step 956, loss 0.244113, acc 0.90625\n",
      "2017-11-06T21:23:25.054176: step 957, loss 0.305243, acc 0.9375\n",
      "2017-11-06T21:23:29.175862: step 958, loss 0.272524, acc 0.9375\n",
      "2017-11-06T21:23:33.106655: step 959, loss 0.0332366, acc 1\n",
      "2017-11-06T21:23:37.090485: step 960, loss 0.0158177, acc 1\n",
      "2017-11-06T21:23:41.074317: step 961, loss 0.239439, acc 0.875\n",
      "2017-11-06T21:23:45.023123: step 962, loss 0.412972, acc 0.84375\n",
      "2017-11-06T21:23:49.002950: step 963, loss 0.277681, acc 0.875\n",
      "2017-11-06T21:23:52.952757: step 964, loss 0.348622, acc 0.84375\n",
      "2017-11-06T21:23:56.954642: step 965, loss 0.200819, acc 0.9375\n",
      "2017-11-06T21:24:00.940474: step 966, loss 0.130516, acc 0.9375\n",
      "2017-11-06T21:24:04.901288: step 967, loss 0.434836, acc 0.84375\n",
      "2017-11-06T21:24:08.869107: step 968, loss 0.103035, acc 0.96875\n",
      "2017-11-06T21:24:12.804906: step 969, loss 0.143202, acc 0.90625\n",
      "2017-11-06T21:24:16.805747: step 970, loss 0.0962338, acc 0.96875\n",
      "2017-11-06T21:24:20.767563: step 971, loss 0.677393, acc 0.84375\n",
      "2017-11-06T21:24:23.263335: step 972, loss 0.158535, acc 0.9\n",
      "2017-11-06T21:24:27.619430: step 973, loss 0.088332, acc 0.9375\n",
      "2017-11-06T21:24:31.761375: step 974, loss 0.220226, acc 0.875\n",
      "2017-11-06T21:24:36.027406: step 975, loss 0.522724, acc 0.9375\n",
      "2017-11-06T21:24:40.021242: step 976, loss 0.00554113, acc 1\n",
      "2017-11-06T21:24:44.052107: step 977, loss 0.652064, acc 0.8125\n",
      "2017-11-06T21:24:47.998911: step 978, loss 0.0681497, acc 0.96875\n",
      "2017-11-06T21:24:51.934707: step 979, loss 0.332185, acc 0.9375\n",
      "2017-11-06T21:24:55.904528: step 980, loss 0.585903, acc 0.875\n",
      "2017-11-06T21:24:59.863360: step 981, loss 0.362146, acc 0.90625\n",
      "2017-11-06T21:25:03.834164: step 982, loss 0.0168626, acc 1\n",
      "2017-11-06T21:25:07.743940: step 983, loss 0.106228, acc 0.9375\n",
      "2017-11-06T21:25:11.684741: step 984, loss 0.0621586, acc 0.96875\n",
      "2017-11-06T21:25:15.663568: step 985, loss 0.255701, acc 0.90625\n",
      "2017-11-06T21:25:19.622381: step 986, loss 0.287199, acc 0.875\n",
      "2017-11-06T21:25:23.559178: step 987, loss 0.133516, acc 0.9375\n",
      "2017-11-06T21:25:27.555017: step 988, loss 0.371621, acc 0.90625\n",
      "2017-11-06T21:25:31.715975: step 989, loss 0.479601, acc 0.90625\n",
      "2017-11-06T21:25:35.959989: step 990, loss 0.21387, acc 0.90625\n",
      "2017-11-06T21:25:39.982850: step 991, loss 0.209144, acc 0.9375\n",
      "2017-11-06T21:25:43.926650: step 992, loss 0.220704, acc 0.9375\n",
      "2017-11-06T21:25:47.858464: step 993, loss 0.258294, acc 0.9375\n",
      "2017-11-06T21:25:51.864290: step 994, loss 0.115313, acc 0.90625\n",
      "2017-11-06T21:25:55.777070: step 995, loss 0.437493, acc 0.84375\n",
      "2017-11-06T21:25:59.743889: step 996, loss 0.501331, acc 0.84375\n",
      "2017-11-06T21:26:03.879828: step 997, loss 0.245358, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:26:07.914694: step 998, loss 0.0937521, acc 0.96875\n",
      "2017-11-06T21:26:11.887517: step 999, loss 0.48363, acc 0.84375\n",
      "2017-11-06T21:26:15.838325: step 1000, loss 0.284261, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:26:18.373125: step 1000, loss 1.23368, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1000\n",
      "\n",
      "2017-11-06T21:26:23.645971: step 1001, loss 0.486338, acc 0.875\n",
      "2017-11-06T21:26:27.651798: step 1002, loss 0.251997, acc 0.90625\n",
      "2017-11-06T21:26:31.564445: step 1003, loss 0.380644, acc 0.90625\n",
      "2017-11-06T21:26:35.862499: step 1004, loss 0.350374, acc 0.875\n",
      "2017-11-06T21:26:40.241610: step 1005, loss 0.195753, acc 0.96875\n",
      "2017-11-06T21:26:44.253461: step 1006, loss 0.315133, acc 0.9375\n",
      "2017-11-06T21:26:48.191259: step 1007, loss 0.0487197, acc 1\n",
      "2017-11-06T21:26:50.811122: step 1008, loss 0.348999, acc 0.85\n",
      "2017-11-06T21:26:54.812963: step 1009, loss 0.220886, acc 0.84375\n",
      "2017-11-06T21:26:58.752763: step 1010, loss 0.130534, acc 0.9375\n",
      "2017-11-06T21:27:02.715579: step 1011, loss 0.502414, acc 0.8125\n",
      "2017-11-06T21:27:06.693406: step 1012, loss 0.409846, acc 0.84375\n",
      "2017-11-06T21:27:10.629202: step 1013, loss 0.282087, acc 0.875\n",
      "2017-11-06T21:27:14.598024: step 1014, loss 0.0939855, acc 0.96875\n",
      "2017-11-06T21:27:18.540823: step 1015, loss 0.0481057, acc 0.96875\n",
      "2017-11-06T21:27:22.488630: step 1016, loss 0.11505, acc 0.96875\n",
      "2017-11-06T21:27:26.574532: step 1017, loss 0.0231027, acc 1\n",
      "2017-11-06T21:27:30.536347: step 1018, loss 0.35909, acc 0.84375\n",
      "2017-11-06T21:27:34.467142: step 1019, loss 0.154512, acc 0.9375\n",
      "2017-11-06T21:27:38.419948: step 1020, loss 0.138759, acc 0.9375\n",
      "2017-11-06T21:27:42.543879: step 1021, loss 0.0599504, acc 0.9375\n",
      "2017-11-06T21:27:46.858945: step 1022, loss 0.0482529, acc 0.96875\n",
      "2017-11-06T21:27:50.826764: step 1023, loss 0.193788, acc 0.90625\n",
      "2017-11-06T21:27:54.759559: step 1024, loss 0.185943, acc 0.9375\n",
      "2017-11-06T21:27:58.704361: step 1025, loss 0.593341, acc 0.8125\n",
      "2017-11-06T21:28:02.696199: step 1026, loss 0.0678039, acc 0.9375\n",
      "2017-11-06T21:28:06.667020: step 1027, loss 0.0990396, acc 0.9375\n",
      "2017-11-06T21:28:10.709892: step 1028, loss 0.396441, acc 0.875\n",
      "2017-11-06T21:28:14.682715: step 1029, loss 0.472808, acc 0.875\n",
      "2017-11-06T21:28:18.647532: step 1030, loss 0.295194, acc 0.9375\n",
      "2017-11-06T21:28:22.712420: step 1031, loss 0.162191, acc 0.9375\n",
      "2017-11-06T21:28:26.889387: step 1032, loss 0.355301, acc 0.875\n",
      "2017-11-06T21:28:30.902239: step 1033, loss 0.0455981, acc 0.96875\n",
      "2017-11-06T21:28:35.219307: step 1034, loss 0.120268, acc 0.9375\n",
      "2017-11-06T21:28:39.272186: step 1035, loss 0.281407, acc 0.9375\n",
      "2017-11-06T21:28:43.221993: step 1036, loss 0.274036, acc 0.84375\n",
      "2017-11-06T21:28:47.256860: step 1037, loss 0.275482, acc 0.90625\n",
      "2017-11-06T21:28:51.672998: step 1038, loss 0.303064, acc 0.90625\n",
      "2017-11-06T21:28:55.643819: step 1039, loss 0.234709, acc 0.90625\n",
      "2017-11-06T21:28:59.621646: step 1040, loss 0.128996, acc 0.96875\n",
      "2017-11-06T21:29:03.567450: step 1041, loss 0.320394, acc 0.875\n",
      "2017-11-06T21:29:07.646348: step 1042, loss 0.472935, acc 0.84375\n",
      "2017-11-06T21:29:11.626176: step 1043, loss 0.154948, acc 0.90625\n",
      "2017-11-06T21:29:14.223020: step 1044, loss 0.120773, acc 0.95\n",
      "2017-11-06T21:29:18.287909: step 1045, loss 0.142451, acc 0.9375\n",
      "2017-11-06T21:29:22.261733: step 1046, loss 0.436528, acc 0.84375\n",
      "2017-11-06T21:29:26.323618: step 1047, loss 0.483323, acc 0.8125\n",
      "2017-11-06T21:29:30.305262: step 1048, loss 0.241753, acc 0.875\n",
      "2017-11-06T21:29:34.353137: step 1049, loss 0.201156, acc 0.9375\n",
      "2017-11-06T21:29:38.347975: step 1050, loss 0.0854097, acc 0.96875\n",
      "2017-11-06T21:29:42.364829: step 1051, loss 0.0657867, acc 0.96875\n",
      "2017-11-06T21:29:46.332668: step 1052, loss 0.461698, acc 0.90625\n",
      "2017-11-06T21:29:50.319482: step 1053, loss 0.39369, acc 0.875\n",
      "2017-11-06T21:29:54.505456: step 1054, loss 0.347938, acc 0.90625\n",
      "2017-11-06T21:29:58.691430: step 1055, loss 0.0831324, acc 0.96875\n",
      "2017-11-06T21:30:03.092557: step 1056, loss 0.179837, acc 0.9375\n",
      "2017-11-06T21:30:07.141514: step 1057, loss 0.0767642, acc 0.96875\n",
      "2017-11-06T21:30:11.157368: step 1058, loss 0.213331, acc 0.875\n",
      "2017-11-06T21:30:15.133193: step 1059, loss 0.127326, acc 0.9375\n",
      "2017-11-06T21:30:19.155051: step 1060, loss 0.130268, acc 0.9375\n",
      "2017-11-06T21:30:23.168903: step 1061, loss 0.176484, acc 0.96875\n",
      "2017-11-06T21:30:27.161740: step 1062, loss 0.140727, acc 0.9375\n",
      "2017-11-06T21:30:31.134563: step 1063, loss 0.138078, acc 0.96875\n",
      "2017-11-06T21:30:35.345555: step 1064, loss 0.0318264, acc 1\n",
      "2017-11-06T21:30:39.427456: step 1065, loss 0.126141, acc 0.96875\n",
      "2017-11-06T21:30:43.453316: step 1066, loss 0.145614, acc 0.90625\n",
      "2017-11-06T21:30:47.418133: step 1067, loss 0.0197269, acc 1\n",
      "2017-11-06T21:30:51.384951: step 1068, loss 0.433141, acc 0.84375\n",
      "2017-11-06T21:30:55.441835: step 1069, loss 0.315935, acc 0.8125\n",
      "2017-11-06T21:30:59.628809: step 1070, loss 0.412184, acc 0.84375\n",
      "2017-11-06T21:31:03.888837: step 1071, loss 0.333732, acc 0.875\n",
      "2017-11-06T21:31:07.936712: step 1072, loss 0.106698, acc 0.9375\n",
      "2017-11-06T21:31:11.993595: step 1073, loss 0.229234, acc 0.875\n",
      "2017-11-06T21:31:16.014451: step 1074, loss 0.0502439, acc 1\n",
      "2017-11-06T21:31:20.018297: step 1075, loss 0.388486, acc 0.875\n",
      "2017-11-06T21:31:23.986116: step 1076, loss 0.311196, acc 0.875\n",
      "2017-11-06T21:31:27.947931: step 1077, loss 0.557899, acc 0.8125\n",
      "2017-11-06T21:31:31.917752: step 1078, loss 0.384746, acc 0.84375\n",
      "2017-11-06T21:31:36.113733: step 1079, loss 0.103729, acc 0.96875\n",
      "2017-11-06T21:31:38.719585: step 1080, loss 0.403967, acc 0.9\n",
      "2017-11-06T21:31:42.743443: step 1081, loss 0.0701376, acc 1\n",
      "2017-11-06T21:31:46.709262: step 1082, loss 0.0382019, acc 1\n",
      "2017-11-06T21:31:50.710107: step 1083, loss 0.0252234, acc 1\n",
      "2017-11-06T21:31:54.694936: step 1084, loss 0.11329, acc 0.96875\n",
      "2017-11-06T21:31:58.661754: step 1085, loss 0.343385, acc 0.90625\n",
      "2017-11-06T21:32:02.642583: step 1086, loss 0.226368, acc 0.90625\n",
      "2017-11-06T21:32:06.966655: step 1087, loss 0.265081, acc 0.9375\n",
      "2017-11-06T21:32:11.253701: step 1088, loss 0.423572, acc 0.8125\n",
      "2017-11-06T21:32:15.333600: step 1089, loss 0.265621, acc 0.90625\n",
      "2017-11-06T21:32:19.317433: step 1090, loss 0.393015, acc 0.90625\n",
      "2017-11-06T21:32:23.295257: step 1091, loss 0.264932, acc 0.90625\n",
      "2017-11-06T21:32:27.245063: step 1092, loss 0.150313, acc 0.9375\n",
      "2017-11-06T21:32:31.270747: step 1093, loss 0.186409, acc 0.9375\n",
      "2017-11-06T21:32:35.556794: step 1094, loss 0.196069, acc 0.90625\n",
      "2017-11-06T21:32:39.605669: step 1095, loss 0.260652, acc 0.90625\n",
      "2017-11-06T21:32:43.592502: step 1096, loss 0.340315, acc 0.875\n",
      "2017-11-06T21:32:47.555317: step 1097, loss 0.111116, acc 0.9375\n",
      "2017-11-06T21:32:51.553158: step 1098, loss 0.246987, acc 0.90625\n",
      "2017-11-06T21:32:55.559004: step 1099, loss 0.354258, acc 0.875\n",
      "2017-11-06T21:32:59.540834: step 1100, loss 0.206896, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:33:02.107659: step 1100, loss 1.12603, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1100\n",
      "\n",
      "2017-11-06T21:33:07.619691: step 1101, loss 0.0220214, acc 1\n",
      "2017-11-06T21:33:11.929753: step 1102, loss 0.167637, acc 0.9375\n",
      "2017-11-06T21:33:16.197786: step 1103, loss 0.0684747, acc 0.96875\n",
      "2017-11-06T21:33:20.253668: step 1104, loss 0.100252, acc 0.96875\n",
      "2017-11-06T21:33:24.477670: step 1105, loss 0.17571, acc 0.875\n",
      "2017-11-06T21:33:28.732693: step 1106, loss 0.222933, acc 0.9375\n",
      "2017-11-06T21:33:32.786574: step 1107, loss 0.137076, acc 0.90625\n",
      "2017-11-06T21:33:36.867473: step 1108, loss 0.218035, acc 0.9375\n",
      "2017-11-06T21:33:41.066458: step 1109, loss 0.417951, acc 0.90625\n",
      "2017-11-06T21:33:45.110330: step 1110, loss 0.179129, acc 0.9375\n",
      "2017-11-06T21:33:49.312315: step 1111, loss 0.17304, acc 0.9375\n",
      "2017-11-06T21:33:53.422235: step 1112, loss 0.318692, acc 0.875\n",
      "2017-11-06T21:33:57.621219: step 1113, loss 0.309643, acc 0.875\n",
      "2017-11-06T21:34:01.661090: step 1114, loss 0.0970131, acc 0.96875\n",
      "2017-11-06T21:34:05.744991: step 1115, loss 0.43344, acc 0.875\n",
      "2017-11-06T21:34:08.454917: step 1116, loss 0.312558, acc 0.9\n",
      "2017-11-06T21:34:12.579848: step 1117, loss 0.00337982, acc 1\n",
      "2017-11-06T21:34:17.060031: step 1118, loss 0.1258, acc 0.9375\n",
      "2017-11-06T21:34:21.383103: step 1119, loss 0.0903501, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:34:25.604102: step 1120, loss 0.150623, acc 0.9375\n",
      "2017-11-06T21:34:29.773064: step 1121, loss 0.085051, acc 0.96875\n",
      "2017-11-06T21:34:34.216221: step 1122, loss 0.358977, acc 0.875\n",
      "2017-11-06T21:34:38.424211: step 1123, loss 0.253576, acc 0.9375\n",
      "2017-11-06T21:34:42.607183: step 1124, loss 0.379108, acc 0.8125\n",
      "2017-11-06T21:34:46.640049: step 1125, loss 0.266712, acc 0.90625\n",
      "2017-11-06T21:34:50.751971: step 1126, loss 0.289499, acc 0.90625\n",
      "2017-11-06T21:34:54.788858: step 1127, loss 0.431015, acc 0.90625\n",
      "2017-11-06T21:34:58.807695: step 1128, loss 0.445539, acc 0.84375\n",
      "2017-11-06T21:35:02.851568: step 1129, loss 0.115456, acc 0.96875\n",
      "2017-11-06T21:35:06.811381: step 1130, loss 0.0327145, acc 1\n",
      "2017-11-06T21:35:10.899287: step 1131, loss 0.062812, acc 1\n",
      "2017-11-06T21:35:15.013209: step 1132, loss 0.354039, acc 0.84375\n",
      "2017-11-06T21:35:19.019057: step 1133, loss 0.192987, acc 0.90625\n",
      "2017-11-06T21:35:23.329118: step 1134, loss 0.188732, acc 0.875\n",
      "2017-11-06T21:35:27.516093: step 1135, loss 0.322776, acc 0.875\n",
      "2017-11-06T21:35:31.517743: step 1136, loss 0.0873916, acc 0.96875\n",
      "2017-11-06T21:35:35.456541: step 1137, loss 0.41495, acc 0.84375\n",
      "2017-11-06T21:35:39.485404: step 1138, loss 0.0757318, acc 0.96875\n",
      "2017-11-06T21:35:43.445218: step 1139, loss 0.326991, acc 0.875\n",
      "2017-11-06T21:35:47.425045: step 1140, loss 0.243837, acc 0.875\n",
      "2017-11-06T21:35:51.461914: step 1141, loss 0.272974, acc 0.90625\n",
      "2017-11-06T21:35:55.518798: step 1142, loss 0.103813, acc 0.96875\n",
      "2017-11-06T21:35:59.520640: step 1143, loss 0.336105, acc 0.90625\n",
      "2017-11-06T21:36:03.526487: step 1144, loss 0.319934, acc 0.84375\n",
      "2017-11-06T21:36:07.470288: step 1145, loss 0.179979, acc 0.9375\n",
      "2017-11-06T21:36:11.494148: step 1146, loss 0.231338, acc 0.96875\n",
      "2017-11-06T21:36:15.695612: step 1147, loss 0.407046, acc 0.84375\n",
      "2017-11-06T21:36:19.729478: step 1148, loss 0.538168, acc 0.90625\n",
      "2017-11-06T21:36:23.742330: step 1149, loss 0.226778, acc 0.96875\n",
      "2017-11-06T21:36:27.968332: step 1150, loss 0.318141, acc 0.9375\n",
      "2017-11-06T21:36:32.248375: step 1151, loss 0.404974, acc 0.90625\n",
      "2017-11-06T21:36:35.060371: step 1152, loss 0.570164, acc 0.9\n",
      "2017-11-06T21:36:39.149277: step 1153, loss 0.362993, acc 0.84375\n",
      "2017-11-06T21:36:43.237181: step 1154, loss 0.355836, acc 0.875\n",
      "2017-11-06T21:36:47.194994: step 1155, loss 0.106147, acc 0.9375\n",
      "2017-11-06T21:36:51.214850: step 1156, loss 0.126226, acc 0.9375\n",
      "2017-11-06T21:36:55.262726: step 1157, loss 0.189084, acc 0.9375\n",
      "2017-11-06T21:36:59.242554: step 1158, loss 0.180341, acc 0.90625\n",
      "2017-11-06T21:37:03.218380: step 1159, loss 0.163504, acc 0.9375\n",
      "2017-11-06T21:37:07.216220: step 1160, loss 0.099356, acc 0.9375\n",
      "2017-11-06T21:37:11.205054: step 1161, loss 0.0593435, acc 0.96875\n",
      "2017-11-06T21:37:15.134847: step 1162, loss 0.503854, acc 0.71875\n",
      "2017-11-06T21:37:19.188728: step 1163, loss 0.642229, acc 0.78125\n",
      "2017-11-06T21:37:23.164551: step 1164, loss 0.154893, acc 0.9375\n",
      "2017-11-06T21:37:27.293485: step 1165, loss 0.151826, acc 0.9375\n",
      "2017-11-06T21:37:31.292328: step 1166, loss 0.214926, acc 0.9375\n",
      "2017-11-06T21:37:35.741488: step 1167, loss 0.409076, acc 0.84375\n",
      "2017-11-06T21:37:39.763345: step 1168, loss 0.118109, acc 0.9375\n",
      "2017-11-06T21:37:43.765190: step 1169, loss 0.170794, acc 0.9375\n",
      "2017-11-06T21:37:47.792050: step 1170, loss 0.282195, acc 0.875\n",
      "2017-11-06T21:37:51.788890: step 1171, loss 0.465807, acc 0.875\n",
      "2017-11-06T21:37:55.787731: step 1172, loss 0.101115, acc 0.96875\n",
      "2017-11-06T21:37:59.761555: step 1173, loss 0.241054, acc 0.9375\n",
      "2017-11-06T21:38:03.801425: step 1174, loss 0.0775468, acc 0.9375\n",
      "2017-11-06T21:38:07.772247: step 1175, loss 0.109534, acc 0.96875\n",
      "2017-11-06T21:38:11.873161: step 1176, loss 0.0755595, acc 0.96875\n",
      "2017-11-06T21:38:15.970072: step 1177, loss 0.285943, acc 0.84375\n",
      "2017-11-06T21:38:20.025954: step 1178, loss 0.14906, acc 0.90625\n",
      "2017-11-06T21:38:24.284981: step 1179, loss 0.618612, acc 0.8125\n",
      "2017-11-06T21:38:28.589038: step 1180, loss 0.321198, acc 0.84375\n",
      "2017-11-06T21:38:32.713802: step 1181, loss 0.219292, acc 0.9375\n",
      "2017-11-06T21:38:36.988839: step 1182, loss 0.0736647, acc 0.96875\n",
      "2017-11-06T21:38:41.388966: step 1183, loss 0.239558, acc 0.9375\n",
      "2017-11-06T21:38:45.477870: step 1184, loss 0.33822, acc 0.84375\n",
      "2017-11-06T21:38:49.473710: step 1185, loss 0.283966, acc 0.78125\n",
      "2017-11-06T21:38:53.521589: step 1186, loss 0.332823, acc 0.90625\n",
      "2017-11-06T21:38:57.512421: step 1187, loss 0.180912, acc 0.9375\n",
      "2017-11-06T21:39:00.003192: step 1188, loss 0.11794, acc 0.95\n",
      "2017-11-06T21:39:04.111111: step 1189, loss 0.259158, acc 0.875\n",
      "2017-11-06T21:39:08.133969: step 1190, loss 0.125198, acc 0.96875\n",
      "2017-11-06T21:39:12.184847: step 1191, loss 0.04313, acc 1\n",
      "2017-11-06T21:39:16.249754: step 1192, loss 0.105109, acc 0.9375\n",
      "2017-11-06T21:39:20.282601: step 1193, loss 0.0760368, acc 0.96875\n",
      "2017-11-06T21:39:24.299455: step 1194, loss 0.159614, acc 0.9375\n",
      "2017-11-06T21:39:28.324315: step 1195, loss 0.186196, acc 0.9375\n",
      "2017-11-06T21:39:32.369189: step 1196, loss 0.236609, acc 0.9375\n",
      "2017-11-06T21:39:36.417065: step 1197, loss 0.05621, acc 0.96875\n",
      "2017-11-06T21:39:40.470947: step 1198, loss 0.0964558, acc 0.9375\n",
      "2017-11-06T21:39:44.678935: step 1199, loss 0.159383, acc 0.9375\n",
      "2017-11-06T21:39:49.012015: step 1200, loss 0.270826, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:39:51.539811: step 1200, loss 1.10659, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1200\n",
      "\n",
      "2017-11-06T21:39:56.791480: step 1201, loss 0.158907, acc 0.9375\n",
      "2017-11-06T21:40:00.990463: step 1202, loss 0.185204, acc 0.9375\n",
      "2017-11-06T21:40:04.996309: step 1203, loss 0.321431, acc 0.90625\n",
      "2017-11-06T21:40:09.028174: step 1204, loss 0.188319, acc 0.90625\n",
      "2017-11-06T21:40:13.039024: step 1205, loss 0.227296, acc 0.875\n",
      "2017-11-06T21:40:17.290036: step 1206, loss 0.174466, acc 0.9375\n",
      "2017-11-06T21:40:21.288879: step 1207, loss 0.1108, acc 0.9375\n",
      "2017-11-06T21:40:25.323744: step 1208, loss 0.274175, acc 0.90625\n",
      "2017-11-06T21:40:29.260542: step 1209, loss 0.251278, acc 0.84375\n",
      "2017-11-06T21:40:33.398482: step 1210, loss 0.289837, acc 0.875\n",
      "2017-11-06T21:40:37.490389: step 1211, loss 0.39913, acc 0.875\n",
      "2017-11-06T21:40:41.460210: step 1212, loss 0.453148, acc 0.84375\n",
      "2017-11-06T21:40:45.476066: step 1213, loss 0.262395, acc 0.875\n",
      "2017-11-06T21:40:49.593989: step 1214, loss 0.261175, acc 0.875\n",
      "2017-11-06T21:40:53.964096: step 1215, loss 0.172798, acc 0.9375\n",
      "2017-11-06T21:40:58.001964: step 1216, loss 0.208317, acc 0.9375\n",
      "2017-11-06T21:41:01.927753: step 1217, loss 0.247133, acc 0.90625\n",
      "2017-11-06T21:41:05.914586: step 1218, loss 0.458773, acc 0.875\n",
      "2017-11-06T21:41:10.076543: step 1219, loss 0.200078, acc 0.875\n",
      "2017-11-06T21:41:15.286245: step 1220, loss 0.347326, acc 0.84375\n",
      "2017-11-06T21:41:19.281083: step 1221, loss 0.137892, acc 0.90625\n",
      "2017-11-06T21:41:23.407014: step 1222, loss 0.386862, acc 0.84375\n",
      "2017-11-06T21:41:27.431876: step 1223, loss 0.221358, acc 0.9375\n",
      "2017-11-06T21:41:29.922645: step 1224, loss 0.563931, acc 0.8\n",
      "2017-11-06T21:41:33.923264: step 1225, loss 0.379757, acc 0.90625\n",
      "2017-11-06T21:41:37.958134: step 1226, loss 0.187157, acc 0.9375\n",
      "2017-11-06T21:41:41.942963: step 1227, loss 0.105501, acc 0.96875\n",
      "2017-11-06T21:41:45.914785: step 1228, loss 0.195029, acc 0.96875\n",
      "2017-11-06T21:41:49.955657: step 1229, loss 0.212283, acc 0.9375\n",
      "2017-11-06T21:41:53.986520: step 1230, loss 0.0923449, acc 0.9375\n",
      "2017-11-06T21:41:58.371637: step 1231, loss 0.0302636, acc 0.96875\n",
      "2017-11-06T21:42:02.472550: step 1232, loss 0.660827, acc 0.875\n",
      "2017-11-06T21:42:06.528432: step 1233, loss 0.0595728, acc 0.96875\n",
      "2017-11-06T21:42:10.680382: step 1234, loss 0.117681, acc 0.90625\n",
      "2017-11-06T21:42:14.741268: step 1235, loss 0.379812, acc 0.84375\n",
      "2017-11-06T21:42:18.796149: step 1236, loss 0.269609, acc 0.9375\n",
      "2017-11-06T21:42:22.801995: step 1237, loss 0.0554278, acc 0.96875\n",
      "2017-11-06T21:42:26.882895: step 1238, loss 0.121335, acc 0.90625\n",
      "2017-11-06T21:42:30.884738: step 1239, loss 0.0283465, acc 1\n",
      "2017-11-06T21:42:35.167783: step 1240, loss 0.0584917, acc 1\n",
      "2017-11-06T21:42:39.153614: step 1241, loss 0.201527, acc 0.90625\n",
      "2017-11-06T21:42:43.152455: step 1242, loss 0.152589, acc 0.90625\n",
      "2017-11-06T21:42:47.179318: step 1243, loss 0.364646, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:42:51.162146: step 1244, loss 0.117332, acc 0.9375\n",
      "2017-11-06T21:42:55.083933: step 1245, loss 0.488747, acc 0.84375\n",
      "2017-11-06T21:42:59.111795: step 1246, loss 0.266118, acc 0.9375\n",
      "2017-11-06T21:43:03.538941: step 1247, loss 0.309878, acc 0.875\n",
      "2017-11-06T21:43:07.681884: step 1248, loss 0.200474, acc 0.90625\n",
      "2017-11-06T21:43:11.692734: step 1249, loss 0.223225, acc 0.96875\n",
      "2017-11-06T21:43:15.669559: step 1250, loss 0.190956, acc 0.875\n",
      "2017-11-06T21:43:19.716435: step 1251, loss 0.562497, acc 0.75\n",
      "2017-11-06T21:43:23.926427: step 1252, loss 0.0691479, acc 0.96875\n",
      "2017-11-06T21:43:28.082380: step 1253, loss 0.280663, acc 0.90625\n",
      "2017-11-06T21:43:32.018177: step 1254, loss 0.575016, acc 0.78125\n",
      "2017-11-06T21:43:36.075059: step 1255, loss 0.144395, acc 0.90625\n",
      "2017-11-06T21:43:40.011856: step 1256, loss 0.192252, acc 0.96875\n",
      "2017-11-06T21:43:44.101762: step 1257, loss 0.47753, acc 0.84375\n",
      "2017-11-06T21:43:48.069581: step 1258, loss 0.254806, acc 0.90625\n",
      "2017-11-06T21:43:52.024391: step 1259, loss 0.204525, acc 0.90625\n",
      "2017-11-06T21:43:54.552188: step 1260, loss 0.289505, acc 0.9\n",
      "2017-11-06T21:43:58.503996: step 1261, loss 0.205205, acc 0.875\n",
      "2017-11-06T21:44:02.444795: step 1262, loss 0.11478, acc 0.9375\n",
      "2017-11-06T21:44:06.480663: step 1263, loss 0.0584797, acc 0.96875\n",
      "2017-11-06T21:44:10.835758: step 1264, loss 0.0634043, acc 1\n",
      "2017-11-06T21:44:14.862618: step 1265, loss 0.185519, acc 0.9375\n",
      "2017-11-06T21:44:18.818430: step 1266, loss 0.19819, acc 0.90625\n",
      "2017-11-06T21:44:22.803261: step 1267, loss 0.128494, acc 0.96875\n",
      "2017-11-06T21:44:26.801101: step 1268, loss 0.204257, acc 0.9375\n",
      "2017-11-06T21:44:30.772675: step 1269, loss 0.226, acc 0.9375\n",
      "2017-11-06T21:44:35.013688: step 1270, loss 0.0237404, acc 1\n",
      "2017-11-06T21:44:38.987512: step 1271, loss 0.404894, acc 0.84375\n",
      "2017-11-06T21:44:42.920306: step 1272, loss 0.230757, acc 0.90625\n",
      "2017-11-06T21:44:46.879119: step 1273, loss 0.299161, acc 0.84375\n",
      "2017-11-06T21:44:50.830927: step 1274, loss 0.197926, acc 0.875\n",
      "2017-11-06T21:44:54.795744: step 1275, loss 0.0669894, acc 0.96875\n",
      "2017-11-06T21:44:58.777573: step 1276, loss 0.15816, acc 0.9375\n",
      "2017-11-06T21:45:02.742391: step 1277, loss 0.193335, acc 0.9375\n",
      "2017-11-06T21:45:06.679188: step 1278, loss 0.274722, acc 0.9375\n",
      "2017-11-06T21:45:10.595971: step 1279, loss 0.29465, acc 0.90625\n",
      "2017-11-06T21:45:14.972080: step 1280, loss 0.058171, acc 1\n",
      "2017-11-06T21:45:19.021958: step 1281, loss 0.225778, acc 0.90625\n",
      "2017-11-06T21:45:22.925732: step 1282, loss 0.12577, acc 0.96875\n",
      "2017-11-06T21:45:26.854523: step 1283, loss 0.461256, acc 0.8125\n",
      "2017-11-06T21:45:30.784315: step 1284, loss 0.184117, acc 0.90625\n",
      "2017-11-06T21:45:34.706102: step 1285, loss 0.0536073, acc 0.96875\n",
      "2017-11-06T21:45:38.663915: step 1286, loss 0.277857, acc 0.875\n",
      "2017-11-06T21:45:42.590704: step 1287, loss 0.106182, acc 0.96875\n",
      "2017-11-06T21:45:46.496480: step 1288, loss 0.28032, acc 0.875\n",
      "2017-11-06T21:45:50.445285: step 1289, loss 0.196432, acc 0.90625\n",
      "2017-11-06T21:45:54.457136: step 1290, loss 0.259218, acc 0.875\n",
      "2017-11-06T21:45:58.399938: step 1291, loss 0.135963, acc 0.9375\n",
      "2017-11-06T21:46:02.344741: step 1292, loss 0.112603, acc 0.9375\n",
      "2017-11-06T21:46:06.295548: step 1293, loss 0.121182, acc 0.9375\n",
      "2017-11-06T21:46:10.229342: step 1294, loss 0.308369, acc 0.84375\n",
      "2017-11-06T21:46:14.174146: step 1295, loss 0.55863, acc 0.84375\n",
      "2017-11-06T21:46:16.756981: step 1296, loss 0.176049, acc 0.95\n",
      "2017-11-06T21:46:21.162111: step 1297, loss 0.172125, acc 0.9375\n",
      "2017-11-06T21:46:25.146942: step 1298, loss 0.115469, acc 0.96875\n",
      "2017-11-06T21:46:29.128772: step 1299, loss 0.0106266, acc 1\n",
      "2017-11-06T21:46:33.217677: step 1300, loss 0.163548, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:46:35.975637: step 1300, loss 1.12217, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1300\n",
      "\n",
      "2017-11-06T21:46:42.266278: step 1301, loss 0.221935, acc 0.9375\n",
      "2017-11-06T21:46:46.230094: step 1302, loss 0.401482, acc 0.875\n",
      "2017-11-06T21:46:50.164890: step 1303, loss 0.154568, acc 0.90625\n",
      "2017-11-06T21:46:54.126706: step 1304, loss 0.441047, acc 0.875\n",
      "2017-11-06T21:46:58.084519: step 1305, loss 0.239648, acc 0.90625\n",
      "2017-11-06T21:47:02.015310: step 1306, loss 0.0913854, acc 0.96875\n",
      "2017-11-06T21:47:05.994137: step 1307, loss 0.194927, acc 0.875\n",
      "2017-11-06T21:47:09.901914: step 1308, loss 0.132591, acc 0.9375\n",
      "2017-11-06T21:47:13.877739: step 1309, loss 0.265877, acc 0.9375\n",
      "2017-11-06T21:47:17.811535: step 1310, loss 0.188036, acc 0.9375\n",
      "2017-11-06T21:47:21.780354: step 1311, loss 0.206838, acc 0.84375\n",
      "2017-11-06T21:47:26.132447: step 1312, loss 0.253869, acc 0.9375\n",
      "2017-11-06T21:47:30.279394: step 1313, loss 0.149867, acc 0.9375\n",
      "2017-11-06T21:47:34.244006: step 1314, loss 0.090004, acc 0.96875\n",
      "2017-11-06T21:47:38.198817: step 1315, loss 0.0774594, acc 0.96875\n",
      "2017-11-06T21:47:42.180645: step 1316, loss 0.233519, acc 0.90625\n",
      "2017-11-06T21:47:46.129451: step 1317, loss 0.13483, acc 0.96875\n",
      "2017-11-06T21:47:50.110281: step 1318, loss 0.161592, acc 0.9375\n",
      "2017-11-06T21:47:54.085104: step 1319, loss 0.26586, acc 0.90625\n",
      "2017-11-06T21:47:58.042916: step 1320, loss 0.161625, acc 0.90625\n",
      "2017-11-06T21:48:02.018741: step 1321, loss 0.19512, acc 0.90625\n",
      "2017-11-06T21:48:05.992565: step 1322, loss 0.277687, acc 0.90625\n",
      "2017-11-06T21:48:09.931365: step 1323, loss 0.2732, acc 0.84375\n",
      "2017-11-06T21:48:13.891177: step 1324, loss 0.422569, acc 0.84375\n",
      "2017-11-06T21:48:17.868002: step 1325, loss 0.0660023, acc 0.96875\n",
      "2017-11-06T21:48:21.960952: step 1326, loss 0.0800933, acc 0.96875\n",
      "2017-11-06T21:48:26.093907: step 1327, loss 0.269883, acc 0.84375\n",
      "2017-11-06T21:48:30.299877: step 1328, loss 0.157625, acc 0.90625\n",
      "2017-11-06T21:48:34.836101: step 1329, loss 0.226392, acc 0.9375\n",
      "2017-11-06T21:48:38.829939: step 1330, loss 0.269741, acc 0.8125\n",
      "2017-11-06T21:48:42.806763: step 1331, loss 0.146688, acc 0.9375\n",
      "2017-11-06T21:48:45.308540: step 1332, loss 0.270351, acc 0.9\n",
      "2017-11-06T21:48:49.254344: step 1333, loss 0.101526, acc 0.9375\n",
      "2017-11-06T21:48:53.217160: step 1334, loss 0.161587, acc 0.9375\n",
      "2017-11-06T21:48:57.190983: step 1335, loss 0.187162, acc 0.9375\n",
      "2017-11-06T21:49:01.140790: step 1336, loss 0.199519, acc 0.90625\n",
      "2017-11-06T21:49:05.077588: step 1337, loss 0.231775, acc 0.90625\n",
      "2017-11-06T21:49:09.014385: step 1338, loss 0.335974, acc 0.84375\n",
      "2017-11-06T21:49:12.968194: step 1339, loss 0.128169, acc 0.9375\n",
      "2017-11-06T21:49:16.914998: step 1340, loss 0.104082, acc 0.96875\n",
      "2017-11-06T21:49:20.889824: step 1341, loss 0.255416, acc 0.875\n",
      "2017-11-06T21:49:24.852638: step 1342, loss 0.0614625, acc 0.96875\n",
      "2017-11-06T21:49:28.792438: step 1343, loss 0.156218, acc 0.9375\n",
      "2017-11-06T21:49:32.809292: step 1344, loss 0.228353, acc 0.875\n",
      "2017-11-06T21:49:37.174394: step 1345, loss 0.311908, acc 0.8125\n",
      "2017-11-06T21:49:41.327345: step 1346, loss 0.211418, acc 0.90625\n",
      "2017-11-06T21:49:45.326186: step 1347, loss 0.670053, acc 0.8125\n",
      "2017-11-06T21:49:49.340038: step 1348, loss 0.153435, acc 0.875\n",
      "2017-11-06T21:49:53.329872: step 1349, loss 0.0664156, acc 0.96875\n",
      "2017-11-06T21:49:57.330715: step 1350, loss 0.261632, acc 0.875\n",
      "2017-11-06T21:50:01.569728: step 1351, loss 0.15246, acc 0.96875\n",
      "2017-11-06T21:50:05.579577: step 1352, loss 0.359965, acc 0.84375\n",
      "2017-11-06T21:50:09.550398: step 1353, loss 0.0463362, acc 0.96875\n",
      "2017-11-06T21:50:13.550240: step 1354, loss 0.0435207, acc 0.96875\n",
      "2017-11-06T21:50:17.582105: step 1355, loss 0.0650751, acc 0.96875\n",
      "2017-11-06T21:50:21.532912: step 1356, loss 0.288695, acc 0.9375\n",
      "2017-11-06T21:50:25.550182: step 1357, loss 0.326243, acc 0.875\n",
      "2017-11-06T21:50:29.533012: step 1358, loss 0.160266, acc 0.90625\n",
      "2017-11-06T21:50:33.708792: step 1359, loss 0.27465, acc 0.90625\n",
      "2017-11-06T21:50:37.715639: step 1360, loss 0.125816, acc 0.90625\n",
      "2017-11-06T21:50:42.013693: step 1361, loss 0.0817367, acc 0.96875\n",
      "2017-11-06T21:50:46.142627: step 1362, loss 0.155309, acc 0.96875\n",
      "2017-11-06T21:50:50.075422: step 1363, loss 0.312195, acc 0.8125\n",
      "2017-11-06T21:50:54.063255: step 1364, loss 0.0258595, acc 1\n",
      "2017-11-06T21:50:58.009058: step 1365, loss 0.218462, acc 0.875\n",
      "2017-11-06T21:51:01.966870: step 1366, loss 0.179576, acc 0.90625\n",
      "2017-11-06T21:51:05.943696: step 1367, loss 0.147437, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:51:08.434466: step 1368, loss 0.188588, acc 0.9\n",
      "2017-11-06T21:51:12.407289: step 1369, loss 0.0763779, acc 0.96875\n",
      "2017-11-06T21:51:16.356095: step 1370, loss 0.243687, acc 0.8125\n",
      "2017-11-06T21:51:20.282885: step 1371, loss 0.283593, acc 0.90625\n",
      "2017-11-06T21:51:24.167645: step 1372, loss 0.101012, acc 0.96875\n",
      "2017-11-06T21:51:28.109446: step 1373, loss 0.248472, acc 0.875\n",
      "2017-11-06T21:51:32.074263: step 1374, loss 0.308029, acc 0.8125\n",
      "2017-11-06T21:51:36.057093: step 1375, loss 0.24488, acc 0.90625\n",
      "2017-11-06T21:51:40.018908: step 1376, loss 0.158734, acc 0.90625\n",
      "2017-11-06T21:51:43.968714: step 1377, loss 0.160551, acc 0.9375\n",
      "2017-11-06T21:51:48.341822: step 1378, loss 0.181424, acc 0.9375\n",
      "2017-11-06T21:51:52.415717: step 1379, loss 0.232783, acc 0.90625\n",
      "2017-11-06T21:51:56.354516: step 1380, loss 0.1343, acc 0.96875\n",
      "2017-11-06T21:52:00.279304: step 1381, loss 0.291345, acc 0.875\n",
      "2017-11-06T21:52:04.194088: step 1382, loss 0.0511463, acc 1\n",
      "2017-11-06T21:52:08.148895: step 1383, loss 0.131902, acc 0.9375\n",
      "2017-11-06T21:52:12.088695: step 1384, loss 0.302197, acc 0.875\n",
      "2017-11-06T21:52:16.045508: step 1385, loss 0.367683, acc 0.875\n",
      "2017-11-06T21:52:19.963291: step 1386, loss 0.162992, acc 0.96875\n",
      "2017-11-06T21:52:23.900088: step 1387, loss 0.170146, acc 0.96875\n",
      "2017-11-06T21:52:27.933013: step 1388, loss 0.199897, acc 0.90625\n",
      "2017-11-06T21:52:31.909838: step 1389, loss 0.050955, acc 1\n",
      "2017-11-06T21:52:36.171866: step 1390, loss 0.252866, acc 0.90625\n",
      "2017-11-06T21:52:40.187719: step 1391, loss 0.117724, acc 0.90625\n",
      "2017-11-06T21:52:44.185560: step 1392, loss 0.197815, acc 0.9375\n",
      "2017-11-06T21:52:48.156382: step 1393, loss 0.148975, acc 0.9375\n",
      "2017-11-06T21:52:52.386387: step 1394, loss 0.104227, acc 0.9375\n",
      "2017-11-06T21:52:56.609389: step 1395, loss 0.141174, acc 0.9375\n",
      "2017-11-06T21:53:00.587215: step 1396, loss 0.217586, acc 0.875\n",
      "2017-11-06T21:53:04.532017: step 1397, loss 0.0876272, acc 0.96875\n",
      "2017-11-06T21:53:08.496834: step 1398, loss 0.104111, acc 0.96875\n",
      "2017-11-06T21:53:12.431630: step 1399, loss 0.392152, acc 0.875\n",
      "2017-11-06T21:53:16.400451: step 1400, loss 0.13051, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T21:53:18.934251: step 1400, loss 1.12029, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1400\n",
      "\n",
      "2017-11-06T21:53:24.711351: step 1401, loss 0.63851, acc 0.8125\n",
      "2017-11-06T21:53:28.875310: step 1402, loss 0.00774212, acc 1\n",
      "2017-11-06T21:53:32.908002: step 1403, loss 0.165434, acc 0.90625\n",
      "2017-11-06T21:53:35.451809: step 1404, loss 0.477559, acc 0.9\n",
      "2017-11-06T21:53:39.444647: step 1405, loss 0.239847, acc 0.9375\n",
      "2017-11-06T21:53:43.435484: step 1406, loss 0.173534, acc 0.9375\n",
      "2017-11-06T21:53:47.354268: step 1407, loss 0.395129, acc 0.8125\n",
      "2017-11-06T21:53:51.394137: step 1408, loss 0.155481, acc 0.9375\n",
      "2017-11-06T21:53:55.369962: step 1409, loss 0.237813, acc 0.9375\n",
      "2017-11-06T21:53:59.745071: step 1410, loss 0.410618, acc 0.78125\n",
      "2017-11-06T21:54:03.837980: step 1411, loss 0.196561, acc 0.9375\n",
      "2017-11-06T21:54:07.809801: step 1412, loss 0.0637807, acc 0.9375\n",
      "2017-11-06T21:54:11.804640: step 1413, loss 0.448453, acc 0.8125\n",
      "2017-11-06T21:54:15.787470: step 1414, loss 0.176465, acc 0.875\n",
      "2017-11-06T21:54:19.764295: step 1415, loss 0.237257, acc 0.875\n",
      "2017-11-06T21:54:23.753130: step 1416, loss 0.321893, acc 0.8125\n",
      "2017-11-06T21:54:27.762979: step 1417, loss 0.587984, acc 0.8125\n",
      "2017-11-06T21:54:31.809856: step 1418, loss 0.285464, acc 0.90625\n",
      "2017-11-06T21:54:36.067881: step 1419, loss 0.11143, acc 0.96875\n",
      "2017-11-06T21:54:40.094741: step 1420, loss 0.0911215, acc 0.9375\n",
      "2017-11-06T21:54:44.044548: step 1421, loss 0.102266, acc 0.96875\n",
      "2017-11-06T21:54:47.991352: step 1422, loss 0.0342988, acc 1\n",
      "2017-11-06T21:54:51.994196: step 1423, loss 0.286341, acc 0.84375\n",
      "2017-11-06T21:54:56.008048: step 1424, loss 0.397246, acc 0.875\n",
      "2017-11-06T21:54:59.991880: step 1425, loss 0.333335, acc 0.875\n",
      "2017-11-06T21:55:04.262914: step 1426, loss 0.219003, acc 0.90625\n",
      "2017-11-06T21:55:08.461898: step 1427, loss 0.218353, acc 0.90625\n",
      "2017-11-06T21:55:12.395693: step 1428, loss 0.282466, acc 0.90625\n",
      "2017-11-06T21:55:16.404543: step 1429, loss 0.226765, acc 0.90625\n",
      "2017-11-06T21:55:20.361353: step 1430, loss 0.175041, acc 0.96875\n",
      "2017-11-06T21:55:24.372202: step 1431, loss 0.195381, acc 0.90625\n",
      "2017-11-06T21:55:28.328013: step 1432, loss 0.141917, acc 0.96875\n",
      "2017-11-06T21:55:32.345868: step 1433, loss 0.286677, acc 0.90625\n",
      "2017-11-06T21:55:36.311685: step 1434, loss 0.408952, acc 0.84375\n",
      "2017-11-06T21:55:40.254487: step 1435, loss 0.366348, acc 0.84375\n",
      "2017-11-06T21:55:44.245323: step 1436, loss 0.355794, acc 0.84375\n",
      "2017-11-06T21:55:48.229154: step 1437, loss 0.140787, acc 0.9375\n",
      "2017-11-06T21:55:52.204980: step 1438, loss 0.195266, acc 0.90625\n",
      "2017-11-06T21:55:56.210824: step 1439, loss 0.0786251, acc 0.96875\n",
      "2017-11-06T21:55:58.724611: step 1440, loss 0.0774226, acc 1\n",
      "2017-11-06T21:56:02.687430: step 1441, loss 0.341484, acc 0.9375\n",
      "2017-11-06T21:56:06.639234: step 1442, loss 0.230072, acc 0.90625\n",
      "2017-11-06T21:56:11.023349: step 1443, loss 0.0800876, acc 1\n",
      "2017-11-06T21:56:15.101247: step 1444, loss 0.291704, acc 0.84375\n",
      "2017-11-06T21:56:19.128109: step 1445, loss 0.233991, acc 0.8125\n",
      "2017-11-06T21:56:23.116942: step 1446, loss 0.155183, acc 0.90625\n",
      "2017-11-06T21:56:27.093769: step 1447, loss 0.133423, acc 0.9375\n",
      "2017-11-06T21:56:31.079601: step 1448, loss 0.147197, acc 0.875\n",
      "2017-11-06T21:56:35.375452: step 1449, loss 0.124364, acc 0.9375\n",
      "2017-11-06T21:56:39.386303: step 1450, loss 0.39526, acc 0.84375\n",
      "2017-11-06T21:56:43.403156: step 1451, loss 0.498141, acc 0.90625\n",
      "2017-11-06T21:56:47.359968: step 1452, loss 0.224766, acc 0.9375\n",
      "2017-11-06T21:56:51.322782: step 1453, loss 0.291489, acc 0.875\n",
      "2017-11-06T21:56:55.338636: step 1454, loss 0.133689, acc 0.96875\n",
      "2017-11-06T21:56:59.307456: step 1455, loss 0.198373, acc 0.9375\n",
      "2017-11-06T21:57:03.325311: step 1456, loss 0.347318, acc 0.78125\n",
      "2017-11-06T21:57:07.261107: step 1457, loss 0.335775, acc 0.875\n",
      "2017-11-06T21:57:11.257947: step 1458, loss 0.0997074, acc 0.96875\n",
      "2017-11-06T21:57:15.552999: step 1459, loss 0.0870471, acc 0.96875\n",
      "2017-11-06T21:57:19.687937: step 1460, loss 0.0871326, acc 0.96875\n",
      "2017-11-06T21:57:23.660760: step 1461, loss 0.310205, acc 0.84375\n",
      "2017-11-06T21:57:27.650595: step 1462, loss 0.260371, acc 0.84375\n",
      "2017-11-06T21:57:31.746506: step 1463, loss 0.091856, acc 0.96875\n",
      "2017-11-06T21:57:35.736341: step 1464, loss 0.18596, acc 0.875\n",
      "2017-11-06T21:57:39.706162: step 1465, loss 0.0685203, acc 1\n",
      "2017-11-06T21:57:43.679985: step 1466, loss 0.109693, acc 0.96875\n",
      "2017-11-06T21:57:47.655811: step 1467, loss 0.0288385, acc 1\n",
      "2017-11-06T21:57:51.673664: step 1468, loss 0.15019, acc 0.9375\n",
      "2017-11-06T21:57:55.667502: step 1469, loss 0.177569, acc 0.9375\n",
      "2017-11-06T21:57:59.639325: step 1470, loss 0.0819924, acc 0.9375\n",
      "2017-11-06T21:58:03.650175: step 1471, loss 0.191413, acc 0.9375\n",
      "2017-11-06T21:58:07.584970: step 1472, loss 0.164314, acc 0.90625\n",
      "2017-11-06T21:58:11.574806: step 1473, loss 0.262377, acc 0.90625\n",
      "2017-11-06T21:58:15.625683: step 1474, loss 0.366303, acc 0.84375\n",
      "2017-11-06T21:58:19.783638: step 1475, loss 0.191697, acc 0.96875\n",
      "2017-11-06T21:58:22.693706: step 1476, loss 0.0743489, acc 0.95\n",
      "2017-11-06T21:58:26.768601: step 1477, loss 0.133601, acc 0.9375\n",
      "2017-11-06T21:58:30.727414: step 1478, loss 0.181503, acc 0.9375\n",
      "2017-11-06T21:58:35.018464: step 1479, loss 0.431556, acc 0.84375\n",
      "2017-11-06T21:58:39.196989: step 1480, loss 0.0996851, acc 0.9375\n",
      "2017-11-06T21:58:43.172814: step 1481, loss 0.304722, acc 0.84375\n",
      "2017-11-06T21:58:47.184666: step 1482, loss 0.114434, acc 0.96875\n",
      "2017-11-06T21:58:51.124464: step 1483, loss 0.127988, acc 0.96875\n",
      "2017-11-06T21:58:55.150325: step 1484, loss 0.14699, acc 0.96875\n",
      "2017-11-06T21:58:59.109137: step 1485, loss 0.321805, acc 0.90625\n",
      "2017-11-06T21:59:03.086965: step 1486, loss 0.616768, acc 0.75\n",
      "2017-11-06T21:59:07.055786: step 1487, loss 0.0698486, acc 0.9375\n",
      "2017-11-06T21:59:11.064632: step 1488, loss 0.262297, acc 0.90625\n",
      "2017-11-06T21:59:15.046462: step 1489, loss 0.0723524, acc 0.9375\n",
      "2017-11-06T21:59:19.100342: step 1490, loss 0.0456027, acc 0.96875\n",
      "2017-11-06T21:59:23.248290: step 1491, loss 0.265573, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T21:59:27.810531: step 1492, loss 0.199567, acc 0.90625\n",
      "2017-11-06T21:59:31.891432: step 1493, loss 0.147894, acc 0.90625\n",
      "2017-11-06T21:59:35.894060: step 1494, loss 0.17062, acc 0.9375\n",
      "2017-11-06T21:59:39.887899: step 1495, loss 0.183231, acc 0.9375\n",
      "2017-11-06T21:59:43.847712: step 1496, loss 0.276856, acc 0.875\n",
      "2017-11-06T21:59:47.818534: step 1497, loss 0.21276, acc 0.875\n",
      "2017-11-06T21:59:51.855402: step 1498, loss 0.047566, acc 1\n",
      "2017-11-06T21:59:55.935301: step 1499, loss 0.108653, acc 0.90625\n",
      "2017-11-06T21:59:59.980175: step 1500, loss 0.303504, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:00:02.987312: step 1500, loss 0.859574, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1500\n",
      "\n",
      "2017-11-06T22:00:08.518174: step 1501, loss 0.163403, acc 0.90625\n",
      "2017-11-06T22:00:12.478989: step 1502, loss 0.25744, acc 0.90625\n",
      "2017-11-06T22:00:16.572898: step 1503, loss 0.160159, acc 0.96875\n",
      "2017-11-06T22:00:20.600760: step 1504, loss 0.323752, acc 0.875\n",
      "2017-11-06T22:00:24.539559: step 1505, loss 0.269382, acc 0.9375\n",
      "2017-11-06T22:00:28.597441: step 1506, loss 0.251577, acc 0.875\n",
      "2017-11-06T22:00:33.011578: step 1507, loss 0.31009, acc 0.875\n",
      "2017-11-06T22:00:37.347658: step 1508, loss 0.349723, acc 0.90625\n",
      "2017-11-06T22:00:41.503205: step 1509, loss 0.00995018, acc 1\n",
      "2017-11-06T22:00:45.511051: step 1510, loss 0.245944, acc 0.90625\n",
      "2017-11-06T22:00:49.499887: step 1511, loss 0.101652, acc 0.9375\n",
      "2017-11-06T22:00:52.064708: step 1512, loss 0.302405, acc 0.85\n",
      "2017-11-06T22:00:56.134600: step 1513, loss 0.0486773, acc 0.96875\n",
      "2017-11-06T22:01:00.121433: step 1514, loss 0.276604, acc 0.90625\n",
      "2017-11-06T22:01:04.162304: step 1515, loss 0.107309, acc 0.9375\n",
      "2017-11-06T22:01:08.257213: step 1516, loss 0.323734, acc 0.875\n",
      "2017-11-06T22:01:12.257056: step 1517, loss 0.271036, acc 0.875\n",
      "2017-11-06T22:01:16.345962: step 1518, loss 0.310002, acc 0.875\n",
      "2017-11-06T22:01:20.301772: step 1519, loss 0.275694, acc 0.875\n",
      "2017-11-06T22:01:24.350650: step 1520, loss 0.245737, acc 0.875\n",
      "2017-11-06T22:01:28.342485: step 1521, loss 0.393434, acc 0.8125\n",
      "2017-11-06T22:01:32.339325: step 1522, loss 0.403441, acc 0.84375\n",
      "2017-11-06T22:01:36.687415: step 1523, loss 0.103346, acc 0.9375\n",
      "2017-11-06T22:01:41.141580: step 1524, loss 0.390789, acc 0.84375\n",
      "2017-11-06T22:01:45.187454: step 1525, loss 0.170221, acc 0.9375\n",
      "2017-11-06T22:01:49.260348: step 1526, loss 0.154038, acc 0.90625\n",
      "2017-11-06T22:01:53.270197: step 1527, loss 0.289994, acc 0.90625\n",
      "2017-11-06T22:01:57.300061: step 1528, loss 0.386119, acc 0.875\n",
      "2017-11-06T22:02:01.357945: step 1529, loss 0.119832, acc 0.9375\n",
      "2017-11-06T22:02:05.345778: step 1530, loss 0.229768, acc 0.9375\n",
      "2017-11-06T22:02:09.372639: step 1531, loss 0.0632835, acc 0.96875\n",
      "2017-11-06T22:02:13.356489: step 1532, loss 0.26522, acc 0.90625\n",
      "2017-11-06T22:02:17.408348: step 1533, loss 0.0984547, acc 0.9375\n",
      "2017-11-06T22:02:21.343145: step 1534, loss 0.139409, acc 0.9375\n",
      "2017-11-06T22:02:25.327977: step 1535, loss 0.140736, acc 0.9375\n",
      "2017-11-06T22:02:29.315809: step 1536, loss 0.407902, acc 0.78125\n",
      "2017-11-06T22:02:33.481519: step 1537, loss 0.130062, acc 0.9375\n",
      "2017-11-06T22:02:37.625464: step 1538, loss 0.175898, acc 0.9375\n",
      "2017-11-06T22:02:41.826654: step 1539, loss 0.246825, acc 0.9375\n",
      "2017-11-06T22:02:46.182750: step 1540, loss 0.324123, acc 0.84375\n",
      "2017-11-06T22:02:50.153571: step 1541, loss 0.29468, acc 0.90625\n",
      "2017-11-06T22:02:54.199446: step 1542, loss 0.0798263, acc 0.9375\n",
      "2017-11-06T22:02:58.181275: step 1543, loss 0.391638, acc 0.84375\n",
      "2017-11-06T22:03:02.244163: step 1544, loss 0.249463, acc 0.8125\n",
      "2017-11-06T22:03:06.259014: step 1545, loss 0.097205, acc 0.9375\n",
      "2017-11-06T22:03:10.330908: step 1546, loss 0.101135, acc 0.96875\n",
      "2017-11-06T22:03:14.364774: step 1547, loss 0.00571749, acc 1\n",
      "2017-11-06T22:03:16.884565: step 1548, loss 0.0164201, acc 1\n",
      "2017-11-06T22:03:20.970468: step 1549, loss 0.0969296, acc 0.96875\n",
      "2017-11-06T22:03:25.153441: step 1550, loss 0.391227, acc 0.84375\n",
      "2017-11-06T22:03:29.165290: step 1551, loss 0.207094, acc 0.96875\n",
      "2017-11-06T22:03:33.098085: step 1552, loss 0.089441, acc 0.9375\n",
      "2017-11-06T22:03:37.121944: step 1553, loss 0.440409, acc 0.8125\n",
      "2017-11-06T22:03:41.245874: step 1554, loss 0.142911, acc 0.9375\n",
      "2017-11-06T22:03:45.356795: step 1555, loss 0.0812321, acc 0.96875\n",
      "2017-11-06T22:03:49.815964: step 1556, loss 0.230083, acc 0.84375\n",
      "2017-11-06T22:03:54.130029: step 1557, loss 0.302086, acc 0.90625\n",
      "2017-11-06T22:03:58.253959: step 1558, loss 0.188626, acc 0.9375\n",
      "2017-11-06T22:04:02.366881: step 1559, loss 0.121336, acc 0.96875\n",
      "2017-11-06T22:04:06.427766: step 1560, loss 0.14884, acc 0.9375\n",
      "2017-11-06T22:04:10.455628: step 1561, loss 0.0913768, acc 0.96875\n",
      "2017-11-06T22:04:14.567551: step 1562, loss 0.316206, acc 0.90625\n",
      "2017-11-06T22:04:18.632438: step 1563, loss 0.385877, acc 0.75\n",
      "2017-11-06T22:04:22.768377: step 1564, loss 0.0911138, acc 0.96875\n",
      "2017-11-06T22:04:26.805246: step 1565, loss 0.105955, acc 0.9375\n",
      "2017-11-06T22:04:30.962200: step 1566, loss 0.0111671, acc 1\n",
      "2017-11-06T22:04:35.490418: step 1567, loss 0.380485, acc 0.875\n",
      "2017-11-06T22:04:39.853517: step 1568, loss 0.183531, acc 0.9375\n",
      "2017-11-06T22:04:44.016475: step 1569, loss 0.312794, acc 0.90625\n",
      "2017-11-06T22:04:48.184437: step 1570, loss 0.0135193, acc 1\n",
      "2017-11-06T22:04:52.222305: step 1571, loss 0.163532, acc 0.96875\n",
      "2017-11-06T22:04:56.701489: step 1572, loss 0.101789, acc 0.96875\n",
      "2017-11-06T22:05:00.690323: step 1573, loss 0.38321, acc 0.875\n",
      "2017-11-06T22:05:04.668151: step 1574, loss 0.19247, acc 0.9375\n",
      "2017-11-06T22:05:08.692008: step 1575, loss 0.411135, acc 0.84375\n",
      "2017-11-06T22:05:12.682844: step 1576, loss 0.321868, acc 0.8125\n",
      "2017-11-06T22:05:16.754737: step 1577, loss 0.172334, acc 0.96875\n",
      "2017-11-06T22:05:20.741570: step 1578, loss 0.291319, acc 0.84375\n",
      "2017-11-06T22:05:24.763428: step 1579, loss 0.300057, acc 0.90625\n",
      "2017-11-06T22:05:28.759267: step 1580, loss 0.397915, acc 0.90625\n",
      "2017-11-06T22:05:32.758108: step 1581, loss 0.177271, acc 0.9375\n",
      "2017-11-06T22:05:36.737729: step 1582, loss 0.0626325, acc 0.96875\n",
      "2017-11-06T22:05:40.741572: step 1583, loss 0.072731, acc 0.96875\n",
      "2017-11-06T22:05:43.260361: step 1584, loss 0.0316792, acc 1\n",
      "2017-11-06T22:05:47.206167: step 1585, loss 0.697893, acc 0.78125\n",
      "2017-11-06T22:05:51.261047: step 1586, loss 0.233874, acc 0.875\n",
      "2017-11-06T22:05:55.265894: step 1587, loss 0.129785, acc 0.9375\n",
      "2017-11-06T22:05:59.500901: step 1588, loss 0.190903, acc 0.90625\n",
      "2017-11-06T22:06:03.803960: step 1589, loss 0.174783, acc 0.90625\n",
      "2017-11-06T22:06:07.760770: step 1590, loss 0.0624776, acc 0.96875\n",
      "2017-11-06T22:06:11.799640: step 1591, loss 0.228122, acc 0.875\n",
      "2017-11-06T22:06:15.759453: step 1592, loss 0.269727, acc 0.875\n",
      "2017-11-06T22:06:19.732276: step 1593, loss 0.0788514, acc 0.96875\n",
      "2017-11-06T22:06:23.782154: step 1594, loss 0.161908, acc 0.96875\n",
      "2017-11-06T22:06:27.802011: step 1595, loss 0.121869, acc 0.9375\n",
      "2017-11-06T22:06:31.783839: step 1596, loss 0.100618, acc 0.96875\n",
      "2017-11-06T22:06:36.085897: step 1597, loss 0.1651, acc 0.90625\n",
      "2017-11-06T22:06:40.076732: step 1598, loss 0.144285, acc 0.9375\n",
      "2017-11-06T22:06:44.027541: step 1599, loss 0.259569, acc 0.875\n",
      "2017-11-06T22:06:47.999363: step 1600, loss 0.0708238, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:06:50.533162: step 1600, loss 0.92153, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1600\n",
      "\n",
      "2017-11-06T22:06:55.864216: step 1601, loss 0.192901, acc 0.875\n",
      "2017-11-06T22:06:59.942113: step 1602, loss 0.284112, acc 0.84375\n",
      "2017-11-06T22:07:04.099067: step 1603, loss 0.0131617, acc 1\n",
      "2017-11-06T22:07:08.459165: step 1604, loss 0.185187, acc 0.9375\n",
      "2017-11-06T22:07:12.455005: step 1605, loss 0.220376, acc 0.90625\n",
      "2017-11-06T22:07:16.577934: step 1606, loss 0.449317, acc 0.875\n",
      "2017-11-06T22:07:20.575774: step 1607, loss 0.308069, acc 0.90625\n",
      "2017-11-06T22:07:24.586624: step 1608, loss 0.0693505, acc 0.96875\n",
      "2017-11-06T22:07:28.591470: step 1609, loss 0.16499, acc 0.9375\n",
      "2017-11-06T22:07:32.758430: step 1610, loss 0.146248, acc 0.90625\n",
      "2017-11-06T22:07:36.711241: step 1611, loss 0.0571925, acc 1\n",
      "2017-11-06T22:07:40.655041: step 1612, loss 0.0829988, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T22:07:44.608852: step 1613, loss 0.185144, acc 0.90625\n",
      "2017-11-06T22:07:48.538645: step 1614, loss 0.298621, acc 0.90625\n",
      "2017-11-06T22:07:52.533483: step 1615, loss 0.244482, acc 0.84375\n",
      "2017-11-06T22:07:56.486291: step 1616, loss 0.218599, acc 0.875\n",
      "2017-11-06T22:08:00.460115: step 1617, loss 0.171091, acc 0.9375\n",
      "2017-11-06T22:08:04.387906: step 1618, loss 0.132827, acc 0.9375\n",
      "2017-11-06T22:08:08.347719: step 1619, loss 0.303607, acc 0.875\n",
      "2017-11-06T22:08:11.073655: step 1620, loss 0.189742, acc 0.95\n",
      "2017-11-06T22:08:15.319674: step 1621, loss 0.159584, acc 0.9375\n",
      "2017-11-06T22:08:19.266476: step 1622, loss 0.109312, acc 0.9375\n",
      "2017-11-06T22:08:23.371393: step 1623, loss 0.230372, acc 0.875\n",
      "2017-11-06T22:08:27.463301: step 1624, loss 0.153319, acc 0.9375\n",
      "2017-11-06T22:08:31.412107: step 1625, loss 0.332104, acc 0.8125\n",
      "2017-11-06T22:08:35.708967: step 1626, loss 0.253514, acc 0.96875\n",
      "2017-11-06T22:08:39.736829: step 1627, loss 0.24834, acc 0.875\n",
      "2017-11-06T22:08:43.679630: step 1628, loss 0.135979, acc 0.90625\n",
      "2017-11-06T22:08:47.680473: step 1629, loss 0.0651621, acc 0.96875\n",
      "2017-11-06T22:08:51.664304: step 1630, loss 0.490504, acc 0.84375\n",
      "2017-11-06T22:08:55.596097: step 1631, loss 0.337085, acc 0.875\n",
      "2017-11-06T22:08:59.593938: step 1632, loss 0.0510638, acc 1\n",
      "2017-11-06T22:09:03.541743: step 1633, loss 0.0852932, acc 0.96875\n",
      "2017-11-06T22:09:07.481543: step 1634, loss 0.185224, acc 0.875\n",
      "2017-11-06T22:09:11.424344: step 1635, loss 0.327343, acc 0.8125\n",
      "2017-11-06T22:09:15.474222: step 1636, loss 0.3582, acc 0.84375\n",
      "2017-11-06T22:09:19.823312: step 1637, loss 0.235021, acc 0.90625\n",
      "2017-11-06T22:09:23.738114: step 1638, loss 0.0712361, acc 0.9375\n",
      "2017-11-06T22:09:27.668887: step 1639, loss 0.0250983, acc 1\n",
      "2017-11-06T22:09:31.614690: step 1640, loss 0.0391259, acc 0.96875\n",
      "2017-11-06T22:09:35.605525: step 1641, loss 0.385721, acc 0.8125\n",
      "2017-11-06T22:09:39.534318: step 1642, loss 0.194544, acc 0.875\n",
      "2017-11-06T22:09:43.488127: step 1643, loss 0.250351, acc 0.875\n",
      "2017-11-06T22:09:47.430930: step 1644, loss 0.158718, acc 0.9375\n",
      "2017-11-06T22:09:51.388741: step 1645, loss 0.260909, acc 0.90625\n",
      "2017-11-06T22:09:55.345552: step 1646, loss 0.133032, acc 0.9375\n",
      "2017-11-06T22:09:59.305366: step 1647, loss 0.0279824, acc 1\n",
      "2017-11-06T22:10:03.559388: step 1648, loss 0.33766, acc 0.875\n",
      "2017-11-06T22:10:07.584248: step 1649, loss 0.124877, acc 0.96875\n",
      "2017-11-06T22:10:11.583089: step 1650, loss 0.0461091, acc 1\n",
      "2017-11-06T22:10:15.496870: step 1651, loss 0.0896796, acc 0.90625\n",
      "2017-11-06T22:10:19.484704: step 1652, loss 0.086729, acc 0.96875\n",
      "2017-11-06T22:10:23.855810: step 1653, loss 0.432772, acc 0.84375\n",
      "2017-11-06T22:10:27.982743: step 1654, loss 0.199503, acc 0.875\n",
      "2017-11-06T22:10:31.924543: step 1655, loss 0.0944133, acc 0.96875\n",
      "2017-11-06T22:10:34.716527: step 1656, loss 0.297868, acc 0.9\n",
      "2017-11-06T22:10:38.797426: step 1657, loss 0.186779, acc 0.90625\n",
      "2017-11-06T22:10:42.762243: step 1658, loss 0.106154, acc 0.9375\n",
      "2017-11-06T22:10:46.670022: step 1659, loss 0.130572, acc 0.9375\n",
      "2017-11-06T22:10:50.659856: step 1660, loss 0.15133, acc 0.9375\n",
      "2017-11-06T22:10:54.634679: step 1661, loss 0.263805, acc 0.8125\n",
      "2017-11-06T22:10:58.523442: step 1662, loss 0.130932, acc 0.9375\n",
      "2017-11-06T22:11:02.523284: step 1663, loss 0.133477, acc 0.9375\n",
      "2017-11-06T22:11:06.457081: step 1664, loss 0.0990395, acc 0.96875\n",
      "2017-11-06T22:11:10.410891: step 1665, loss 0.15047, acc 0.9375\n",
      "2017-11-06T22:11:14.367702: step 1666, loss 0.0434277, acc 1\n",
      "2017-11-06T22:11:18.329515: step 1667, loss 0.0965255, acc 0.96875\n",
      "2017-11-06T22:11:22.273317: step 1668, loss 0.238588, acc 0.9375\n",
      "2017-11-06T22:11:26.302181: step 1669, loss 0.200615, acc 0.9375\n",
      "2017-11-06T22:11:30.591227: step 1670, loss 0.428297, acc 0.8125\n",
      "2017-11-06T22:11:34.545791: step 1671, loss 0.113377, acc 0.9375\n",
      "2017-11-06T22:11:38.477585: step 1672, loss 0.159356, acc 0.9375\n",
      "2017-11-06T22:11:42.459415: step 1673, loss 0.195681, acc 0.875\n",
      "2017-11-06T22:11:46.383204: step 1674, loss 0.439219, acc 0.78125\n",
      "2017-11-06T22:11:50.365033: step 1675, loss 0.062693, acc 0.96875\n",
      "2017-11-06T22:11:54.339856: step 1676, loss 0.142586, acc 0.9375\n",
      "2017-11-06T22:11:58.442771: step 1677, loss 0.127354, acc 0.9375\n",
      "2017-11-06T22:12:02.475638: step 1678, loss 0.0967028, acc 0.96875\n",
      "2017-11-06T22:12:06.441454: step 1679, loss 0.203474, acc 0.9375\n",
      "2017-11-06T22:12:10.392262: step 1680, loss 0.172984, acc 0.90625\n",
      "2017-11-06T22:12:14.298038: step 1681, loss 0.306932, acc 0.90625\n",
      "2017-11-06T22:12:18.229831: step 1682, loss 0.334538, acc 0.84375\n",
      "2017-11-06T22:12:22.187645: step 1683, loss 0.0700076, acc 0.96875\n",
      "2017-11-06T22:12:26.176478: step 1684, loss 0.121385, acc 0.9375\n",
      "2017-11-06T22:12:30.083253: step 1685, loss 0.186759, acc 0.90625\n",
      "2017-11-06T22:12:34.645497: step 1686, loss 0.141364, acc 0.9375\n",
      "2017-11-06T22:12:38.826465: step 1687, loss 0.0690426, acc 0.96875\n",
      "2017-11-06T22:12:42.759260: step 1688, loss 0.265916, acc 0.90625\n",
      "2017-11-06T22:12:46.706064: step 1689, loss 0.435686, acc 0.875\n",
      "2017-11-06T22:12:50.694959: step 1690, loss 0.0847066, acc 0.96875\n",
      "2017-11-06T22:12:54.654772: step 1691, loss 0.3654, acc 0.8125\n",
      "2017-11-06T22:12:57.152548: step 1692, loss 0.0379664, acc 1\n",
      "2017-11-06T22:13:01.166399: step 1693, loss 0.238853, acc 0.875\n",
      "2017-11-06T22:13:05.138221: step 1694, loss 0.120027, acc 0.9375\n",
      "2017-11-06T22:13:09.118050: step 1695, loss 0.214108, acc 0.90625\n",
      "2017-11-06T22:13:13.051845: step 1696, loss 0.337281, acc 0.78125\n",
      "2017-11-06T22:13:17.045682: step 1697, loss 0.0438414, acc 1\n",
      "2017-11-06T22:13:21.027511: step 1698, loss 0.0206977, acc 1\n",
      "2017-11-06T22:13:25.227495: step 1699, loss 0.147269, acc 0.90625\n",
      "2017-11-06T22:13:29.315400: step 1700, loss 0.305053, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:13:31.851202: step 1700, loss 0.924588, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1700\n",
      "\n",
      "2017-11-06T22:13:38.054892: step 1701, loss 0.183323, acc 0.90625\n",
      "2017-11-06T22:13:42.424997: step 1702, loss 0.510342, acc 0.8125\n",
      "2017-11-06T22:13:46.354789: step 1703, loss 0.19958, acc 0.875\n",
      "2017-11-06T22:13:50.298591: step 1704, loss 0.213775, acc 0.90625\n",
      "2017-11-06T22:13:54.271415: step 1705, loss 0.149948, acc 0.96875\n",
      "2017-11-06T22:13:58.217217: step 1706, loss 0.202327, acc 0.9375\n",
      "2017-11-06T22:14:02.208053: step 1707, loss 0.263945, acc 0.90625\n",
      "2017-11-06T22:14:06.172871: step 1708, loss 0.270131, acc 0.90625\n",
      "2017-11-06T22:14:10.188724: step 1709, loss 0.185494, acc 0.9375\n",
      "2017-11-06T22:14:14.120518: step 1710, loss 0.324163, acc 0.875\n",
      "2017-11-06T22:14:18.103350: step 1711, loss 0.228286, acc 0.9375\n",
      "2017-11-06T22:14:22.075170: step 1712, loss 0.0966596, acc 0.9375\n",
      "2017-11-06T22:14:26.060001: step 1713, loss 0.221918, acc 0.90625\n",
      "2017-11-06T22:14:29.989793: step 1714, loss 0.205516, acc 0.875\n",
      "2017-11-06T22:14:34.139743: step 1715, loss 0.456225, acc 0.875\n",
      "2017-11-06T22:14:38.173371: step 1716, loss 0.231518, acc 0.9375\n",
      "2017-11-06T22:14:42.106164: step 1717, loss 0.322171, acc 0.90625\n",
      "2017-11-06T22:14:46.473267: step 1718, loss 0.0133436, acc 1\n",
      "2017-11-06T22:14:50.590192: step 1719, loss 0.293413, acc 0.9375\n",
      "2017-11-06T22:14:54.612254: step 1720, loss 0.154624, acc 0.90625\n",
      "2017-11-06T22:14:58.604088: step 1721, loss 0.105, acc 0.96875\n",
      "2017-11-06T22:15:02.583916: step 1722, loss 0.273371, acc 0.84375\n",
      "2017-11-06T22:15:06.570749: step 1723, loss 0.451773, acc 0.8125\n",
      "2017-11-06T22:15:10.680669: step 1724, loss 0.167229, acc 0.9375\n",
      "2017-11-06T22:15:14.626473: step 1725, loss 0.0833185, acc 0.9375\n",
      "2017-11-06T22:15:18.588287: step 1726, loss 0.161815, acc 0.90625\n",
      "2017-11-06T22:15:22.602141: step 1727, loss 0.415327, acc 0.84375\n",
      "2017-11-06T22:15:25.108921: step 1728, loss 0.175077, acc 0.9\n",
      "2017-11-06T22:15:29.121772: step 1729, loss 0.119093, acc 0.9375\n",
      "2017-11-06T22:15:33.062573: step 1730, loss 0.252018, acc 0.90625\n",
      "2017-11-06T22:15:37.063415: step 1731, loss 0.162184, acc 0.9375\n",
      "2017-11-06T22:15:41.121299: step 1732, loss 0.160791, acc 0.9375\n",
      "2017-11-06T22:15:45.078110: step 1733, loss 0.090589, acc 0.96875\n",
      "2017-11-06T22:15:49.172021: step 1734, loss 0.108717, acc 0.9375\n",
      "2017-11-06T22:15:53.602167: step 1735, loss 0.209945, acc 0.90625\n",
      "2017-11-06T22:15:57.668057: step 1736, loss 0.0675365, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T22:16:01.626869: step 1737, loss 0.145252, acc 0.9375\n",
      "2017-11-06T22:16:05.624710: step 1738, loss 0.575543, acc 0.84375\n",
      "2017-11-06T22:16:09.633558: step 1739, loss 0.243992, acc 0.9375\n",
      "2017-11-06T22:16:13.609384: step 1740, loss 0.250688, acc 0.90625\n",
      "2017-11-06T22:16:17.659260: step 1741, loss 0.320836, acc 0.875\n",
      "2017-11-06T22:16:21.667108: step 1742, loss 0.136511, acc 0.96875\n",
      "2017-11-06T22:16:25.680959: step 1743, loss 0.328371, acc 0.875\n",
      "2017-11-06T22:16:29.628765: step 1744, loss 0.360849, acc 0.90625\n",
      "2017-11-06T22:16:33.815740: step 1745, loss 0.2848, acc 0.90625\n",
      "2017-11-06T22:16:37.932666: step 1746, loss 0.264665, acc 0.84375\n",
      "2017-11-06T22:16:41.884473: step 1747, loss 0.335013, acc 0.875\n",
      "2017-11-06T22:16:45.791250: step 1748, loss 0.20321, acc 0.9375\n",
      "2017-11-06T22:16:49.736052: step 1749, loss 0.0636063, acc 0.96875\n",
      "2017-11-06T22:16:53.676852: step 1750, loss 0.174337, acc 0.90625\n",
      "2017-11-06T22:16:58.087987: step 1751, loss 0.0686606, acc 0.96875\n",
      "2017-11-06T22:17:02.160881: step 1752, loss 0.396198, acc 0.84375\n",
      "2017-11-06T22:17:06.155719: step 1753, loss 0.308171, acc 0.875\n",
      "2017-11-06T22:17:10.188584: step 1754, loss 0.41825, acc 0.84375\n",
      "2017-11-06T22:17:14.150400: step 1755, loss 0.412697, acc 0.8125\n",
      "2017-11-06T22:17:18.168254: step 1756, loss 0.13359, acc 0.9375\n",
      "2017-11-06T22:17:22.163094: step 1757, loss 0.158938, acc 0.9375\n",
      "2017-11-06T22:17:26.255000: step 1758, loss 0.0466001, acc 1\n",
      "2017-11-06T22:17:30.267852: step 1759, loss 0.108351, acc 0.96875\n",
      "2017-11-06T22:17:34.330739: step 1760, loss 0.14981, acc 0.90625\n",
      "2017-11-06T22:17:38.334363: step 1761, loss 0.256717, acc 0.9375\n",
      "2017-11-06T22:17:42.292176: step 1762, loss 0.427477, acc 0.84375\n",
      "2017-11-06T22:17:46.285016: step 1763, loss 0.468226, acc 0.84375\n",
      "2017-11-06T22:17:49.423244: step 1764, loss 0.229695, acc 0.9\n",
      "2017-11-06T22:17:53.948458: step 1765, loss 0.0909855, acc 0.90625\n",
      "2017-11-06T22:17:58.099408: step 1766, loss 0.580668, acc 0.8125\n",
      "2017-11-06T22:18:02.318405: step 1767, loss 0.130238, acc 0.96875\n",
      "2017-11-06T22:18:06.497375: step 1768, loss 0.059678, acc 0.96875\n",
      "2017-11-06T22:18:10.502220: step 1769, loss 0.123904, acc 0.9375\n",
      "2017-11-06T22:18:14.464035: step 1770, loss 0.450004, acc 0.90625\n",
      "2017-11-06T22:18:18.477887: step 1771, loss 0.665481, acc 0.84375\n",
      "2017-11-06T22:18:22.534771: step 1772, loss 0.230317, acc 0.9375\n",
      "2017-11-06T22:18:26.705733: step 1773, loss 0.0334358, acc 1\n",
      "2017-11-06T22:18:30.727592: step 1774, loss 0.257065, acc 0.9375\n",
      "2017-11-06T22:18:34.949594: step 1775, loss 0.142233, acc 0.96875\n",
      "2017-11-06T22:18:38.971449: step 1776, loss 0.131745, acc 0.9375\n",
      "2017-11-06T22:18:42.989303: step 1777, loss 0.236865, acc 0.875\n",
      "2017-11-06T22:18:47.141254: step 1778, loss 0.207498, acc 0.875\n",
      "2017-11-06T22:18:51.108073: step 1779, loss 0.212007, acc 0.875\n",
      "2017-11-06T22:18:55.104912: step 1780, loss 0.185395, acc 0.90625\n",
      "2017-11-06T22:18:59.067752: step 1781, loss 0.309775, acc 0.90625\n",
      "2017-11-06T22:19:03.090610: step 1782, loss 0.269899, acc 0.875\n",
      "2017-11-06T22:19:07.293595: step 1783, loss 0.400794, acc 0.875\n",
      "2017-11-06T22:19:11.547619: step 1784, loss 0.0945709, acc 0.96875\n",
      "2017-11-06T22:19:15.508432: step 1785, loss 0.273353, acc 0.84375\n",
      "2017-11-06T22:19:19.507275: step 1786, loss 0.182408, acc 0.875\n",
      "2017-11-06T22:19:23.536136: step 1787, loss 0.0583011, acc 0.96875\n",
      "2017-11-06T22:19:27.527973: step 1788, loss 0.352285, acc 0.84375\n",
      "2017-11-06T22:19:31.514806: step 1789, loss 0.0360368, acc 1\n",
      "2017-11-06T22:19:35.556677: step 1790, loss 0.178225, acc 0.9375\n",
      "2017-11-06T22:19:39.550515: step 1791, loss 0.0841985, acc 0.96875\n",
      "2017-11-06T22:19:43.475303: step 1792, loss 0.26277, acc 0.875\n",
      "2017-11-06T22:19:47.448128: step 1793, loss 0.0705902, acc 0.9375\n",
      "2017-11-06T22:19:51.465983: step 1794, loss 0.0954871, acc 0.90625\n",
      "2017-11-06T22:19:55.405782: step 1795, loss 0.0899668, acc 0.9375\n",
      "2017-11-06T22:19:59.405624: step 1796, loss 0.270864, acc 0.875\n",
      "2017-11-06T22:20:03.681661: step 1797, loss 0.248647, acc 0.9375\n",
      "2017-11-06T22:20:07.666493: step 1798, loss 0.429151, acc 0.8125\n",
      "2017-11-06T22:20:11.755398: step 1799, loss 0.10145, acc 0.9375\n",
      "2017-11-06T22:20:14.680477: step 1800, loss 0.0553448, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:20:17.413418: step 1800, loss 0.929036, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1800\n",
      "\n",
      "2017-11-06T22:20:22.780596: step 1801, loss 0.143582, acc 0.9375\n",
      "2017-11-06T22:20:26.789443: step 1802, loss 0.155534, acc 0.90625\n",
      "2017-11-06T22:20:30.832316: step 1803, loss 0.208571, acc 0.90625\n",
      "2017-11-06T22:20:35.159821: step 1804, loss 0.105124, acc 0.96875\n",
      "2017-11-06T22:20:39.225710: step 1805, loss 0.109288, acc 0.96875\n",
      "2017-11-06T22:20:43.198533: step 1806, loss 0.183086, acc 0.9375\n",
      "2017-11-06T22:20:47.191370: step 1807, loss 0.269787, acc 0.875\n",
      "2017-11-06T22:20:51.261263: step 1808, loss 0.215875, acc 0.9375\n",
      "2017-11-06T22:20:55.390196: step 1809, loss 0.381549, acc 0.875\n",
      "2017-11-06T22:20:59.407051: step 1810, loss 0.22483, acc 0.875\n",
      "2017-11-06T22:21:03.509966: step 1811, loss 0.153583, acc 0.9375\n",
      "2017-11-06T22:21:07.560843: step 1812, loss 0.166677, acc 0.9375\n",
      "2017-11-06T22:21:11.574695: step 1813, loss 0.434154, acc 0.84375\n",
      "2017-11-06T22:21:15.566531: step 1814, loss 0.0916392, acc 0.9375\n",
      "2017-11-06T22:21:19.949646: step 1815, loss 0.236882, acc 0.9375\n",
      "2017-11-06T22:21:24.128616: step 1816, loss 0.115529, acc 0.96875\n",
      "2017-11-06T22:21:28.155477: step 1817, loss 0.214593, acc 0.90625\n",
      "2017-11-06T22:21:32.204354: step 1818, loss 0.221626, acc 0.9375\n",
      "2017-11-06T22:21:36.243223: step 1819, loss 0.128647, acc 0.96875\n",
      "2017-11-06T22:21:40.266082: step 1820, loss 0.113572, acc 0.9375\n",
      "2017-11-06T22:21:44.279934: step 1821, loss 0.309207, acc 0.84375\n",
      "2017-11-06T22:21:48.318805: step 1822, loss 0.111829, acc 0.9375\n",
      "2017-11-06T22:21:52.368681: step 1823, loss 0.140624, acc 0.9375\n",
      "2017-11-06T22:21:56.439574: step 1824, loss 0.159241, acc 0.90625\n",
      "2017-11-06T22:22:00.423404: step 1825, loss 0.0968602, acc 0.9375\n",
      "2017-11-06T22:22:04.492297: step 1826, loss 0.343201, acc 0.875\n",
      "2017-11-06T22:22:08.557183: step 1827, loss 0.0698118, acc 0.96875\n",
      "2017-11-06T22:22:12.603059: step 1828, loss 0.0603024, acc 1\n",
      "2017-11-06T22:22:16.637927: step 1829, loss 0.137096, acc 0.875\n",
      "2017-11-06T22:22:20.603743: step 1830, loss 0.19249, acc 0.9375\n",
      "2017-11-06T22:22:25.005871: step 1831, loss 0.23256, acc 0.875\n",
      "2017-11-06T22:22:29.206857: step 1832, loss 0.134014, acc 0.90625\n",
      "2017-11-06T22:22:33.385826: step 1833, loss 0.227849, acc 0.9375\n",
      "2017-11-06T22:22:37.581807: step 1834, loss 0.211747, acc 0.90625\n",
      "2017-11-06T22:22:41.621677: step 1835, loss 0.132155, acc 0.9375\n",
      "2017-11-06T22:22:44.136465: step 1836, loss 0.00768215, acc 1\n",
      "2017-11-06T22:22:48.116292: step 1837, loss 0.215413, acc 0.9375\n",
      "2017-11-06T22:22:52.119136: step 1838, loss 0.12508, acc 0.9375\n",
      "2017-11-06T22:22:56.193032: step 1839, loss 0.406629, acc 0.875\n",
      "2017-11-06T22:23:00.258939: step 1840, loss 0.436851, acc 0.84375\n",
      "2017-11-06T22:23:04.244771: step 1841, loss 0.175828, acc 0.9375\n",
      "2017-11-06T22:23:08.285623: step 1842, loss 0.143837, acc 0.90625\n",
      "2017-11-06T22:23:12.327495: step 1843, loss 0.092571, acc 0.96875\n",
      "2017-11-06T22:23:16.300318: step 1844, loss 0.170609, acc 0.90625\n",
      "2017-11-06T22:23:20.355199: step 1845, loss 0.242645, acc 0.9375\n",
      "2017-11-06T22:23:24.636241: step 1846, loss 0.201242, acc 0.875\n",
      "2017-11-06T22:23:29.048376: step 1847, loss 0.129441, acc 0.9375\n",
      "2017-11-06T22:23:33.436494: step 1848, loss 0.198081, acc 0.9375\n",
      "2017-11-06T22:23:37.377130: step 1849, loss 0.0552235, acc 1\n",
      "2017-11-06T22:23:41.374971: step 1850, loss 0.399325, acc 0.84375\n",
      "2017-11-06T22:23:45.344792: step 1851, loss 0.106559, acc 0.9375\n",
      "2017-11-06T22:23:49.444706: step 1852, loss 0.311204, acc 0.90625\n",
      "2017-11-06T22:23:53.464561: step 1853, loss 0.211642, acc 0.90625\n",
      "2017-11-06T22:23:57.535473: step 1854, loss 0.0561868, acc 1\n",
      "2017-11-06T22:24:01.450235: step 1855, loss 0.193235, acc 0.875\n",
      "2017-11-06T22:24:05.468090: step 1856, loss 0.154129, acc 0.9375\n",
      "2017-11-06T22:24:09.469934: step 1857, loss 0.13019, acc 0.96875\n",
      "2017-11-06T22:24:13.585858: step 1858, loss 0.38171, acc 0.90625\n",
      "2017-11-06T22:24:17.529661: step 1859, loss 0.414809, acc 0.84375\n",
      "2017-11-06T22:24:21.621567: step 1860, loss 0.0317308, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T22:24:25.615406: step 1861, loss 0.0186072, acc 1\n",
      "2017-11-06T22:24:29.608243: step 1862, loss 0.182714, acc 0.875\n",
      "2017-11-06T22:24:33.875274: step 1863, loss 0.30866, acc 0.875\n",
      "2017-11-06T22:24:38.463535: step 1864, loss 0.15883, acc 0.90625\n",
      "2017-11-06T22:24:42.566452: step 1865, loss 0.14129, acc 0.9375\n",
      "2017-11-06T22:24:46.544276: step 1866, loss 0.132318, acc 0.9375\n",
      "2017-11-06T22:24:50.542117: step 1867, loss 0.125466, acc 0.9375\n",
      "2017-11-06T22:24:54.560972: step 1868, loss 0.247688, acc 0.90625\n",
      "2017-11-06T22:24:58.579828: step 1869, loss 0.168517, acc 0.96875\n",
      "2017-11-06T22:25:02.640714: step 1870, loss 0.1691, acc 0.9375\n",
      "2017-11-06T22:25:06.547490: step 1871, loss 0.322551, acc 0.90625\n",
      "2017-11-06T22:25:09.161347: step 1872, loss 0.207281, acc 0.95\n",
      "2017-11-06T22:25:13.150181: step 1873, loss 0.11493, acc 0.9375\n",
      "2017-11-06T22:25:17.169036: step 1874, loss 0.30214, acc 0.84375\n",
      "2017-11-06T22:25:21.123846: step 1875, loss 0.266497, acc 0.875\n",
      "2017-11-06T22:25:25.304818: step 1876, loss 0.120385, acc 0.90625\n",
      "2017-11-06T22:25:29.351693: step 1877, loss 0.183039, acc 0.9375\n",
      "2017-11-06T22:25:33.402571: step 1878, loss 0.201273, acc 0.9375\n",
      "2017-11-06T22:25:37.430433: step 1879, loss 0.152207, acc 0.9375\n",
      "2017-11-06T22:25:41.738494: step 1880, loss 0.344471, acc 0.8125\n",
      "2017-11-06T22:25:45.951488: step 1881, loss 0.131619, acc 0.9375\n",
      "2017-11-06T22:25:50.001366: step 1882, loss 0.215828, acc 0.875\n",
      "2017-11-06T22:25:54.026226: step 1883, loss 0.26854, acc 0.875\n",
      "2017-11-06T22:25:58.001049: step 1884, loss 0.12933, acc 0.96875\n",
      "2017-11-06T22:26:02.061935: step 1885, loss 0.216022, acc 0.90625\n",
      "2017-11-06T22:26:06.094801: step 1886, loss 0.139295, acc 0.90625\n",
      "2017-11-06T22:26:10.091640: step 1887, loss 0.0112033, acc 1\n",
      "2017-11-06T22:26:14.075471: step 1888, loss 0.173868, acc 0.90625\n",
      "2017-11-06T22:26:18.068308: step 1889, loss 0.118169, acc 0.9375\n",
      "2017-11-06T22:26:22.122190: step 1890, loss 0.256711, acc 0.90625\n",
      "2017-11-06T22:26:26.107020: step 1891, loss 0.265948, acc 0.875\n",
      "2017-11-06T22:26:30.099857: step 1892, loss 0.144865, acc 0.9375\n",
      "2017-11-06T22:26:34.285831: step 1893, loss 0.237458, acc 0.90625\n",
      "2017-11-06T22:26:38.302544: step 1894, loss 0.21632, acc 0.9375\n",
      "2017-11-06T22:26:42.379441: step 1895, loss 0.0749832, acc 0.9375\n",
      "2017-11-06T22:26:46.616451: step 1896, loss 0.214635, acc 0.875\n",
      "2017-11-06T22:26:50.863470: step 1897, loss 0.136746, acc 0.90625\n",
      "2017-11-06T22:26:54.871317: step 1898, loss 0.615528, acc 0.78125\n",
      "2017-11-06T22:26:58.926197: step 1899, loss 0.252726, acc 0.90625\n",
      "2017-11-06T22:27:02.970071: step 1900, loss 0.544118, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:27:05.488862: step 1900, loss 0.872259, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-1900\n",
      "\n",
      "2017-11-06T22:27:10.812385: step 1901, loss 0.4046, acc 0.84375\n",
      "2017-11-06T22:27:14.826238: step 1902, loss 0.279034, acc 0.875\n",
      "2017-11-06T22:27:18.894127: step 1903, loss 0.111215, acc 0.9375\n",
      "2017-11-06T22:27:22.847939: step 1904, loss 0.227057, acc 0.9375\n",
      "2017-11-06T22:27:26.865792: step 1905, loss 0.36634, acc 0.875\n",
      "2017-11-06T22:27:30.929679: step 1906, loss 0.144211, acc 0.96875\n",
      "2017-11-06T22:27:35.060614: step 1907, loss 0.209094, acc 0.90625\n",
      "2017-11-06T22:27:37.654457: step 1908, loss 0.213326, acc 0.9\n",
      "2017-11-06T22:27:41.717345: step 1909, loss 0.391759, acc 0.875\n",
      "2017-11-06T22:27:45.729195: step 1910, loss 0.221427, acc 0.90625\n",
      "2017-11-06T22:27:49.775070: step 1911, loss 0.205478, acc 0.90625\n",
      "2017-11-06T22:27:54.271265: step 1912, loss 0.23751, acc 0.90625\n",
      "2017-11-06T22:27:58.371178: step 1913, loss 0.4068, acc 0.875\n",
      "2017-11-06T22:28:02.457081: step 1914, loss 0.180405, acc 0.9375\n",
      "2017-11-06T22:28:06.445916: step 1915, loss 0.151521, acc 0.9375\n",
      "2017-11-06T22:28:10.432750: step 1916, loss 0.301307, acc 0.84375\n",
      "2017-11-06T22:28:14.462612: step 1917, loss 0.147526, acc 0.9375\n",
      "2017-11-06T22:28:18.532503: step 1918, loss 0.245707, acc 0.875\n",
      "2017-11-06T22:28:22.706469: step 1919, loss 0.227783, acc 0.90625\n",
      "2017-11-06T22:28:26.955488: step 1920, loss 0.154209, acc 0.9375\n",
      "2017-11-06T22:28:30.868287: step 1921, loss 0.446276, acc 0.8125\n",
      "2017-11-06T22:28:35.148310: step 1922, loss 0.0754958, acc 1\n",
      "2017-11-06T22:28:39.199188: step 1923, loss 0.141056, acc 0.9375\n",
      "2017-11-06T22:28:43.207037: step 1924, loss 0.247844, acc 0.90625\n",
      "2017-11-06T22:28:47.217886: step 1925, loss 0.194439, acc 0.9375\n",
      "2017-11-06T22:28:51.191709: step 1926, loss 0.20836, acc 0.9375\n",
      "2017-11-06T22:28:55.218572: step 1927, loss 0.159404, acc 0.90625\n",
      "2017-11-06T22:28:59.609691: step 1928, loss 0.0314042, acc 0.96875\n",
      "2017-11-06T22:29:03.819742: step 1929, loss 0.36392, acc 0.875\n",
      "2017-11-06T22:29:07.810578: step 1930, loss 0.437418, acc 0.84375\n",
      "2017-11-06T22:29:11.802414: step 1931, loss 0.189456, acc 0.90625\n",
      "2017-11-06T22:29:15.798254: step 1932, loss 0.20471, acc 0.875\n",
      "2017-11-06T22:29:19.871148: step 1933, loss 0.250486, acc 0.875\n",
      "2017-11-06T22:29:23.880997: step 1934, loss 0.312265, acc 0.875\n",
      "2017-11-06T22:29:27.860825: step 1935, loss 0.24766, acc 0.90625\n",
      "2017-11-06T22:29:31.865670: step 1936, loss 0.150955, acc 0.9375\n",
      "2017-11-06T22:29:35.945569: step 1937, loss 0.420962, acc 0.84375\n",
      "2017-11-06T22:29:39.930164: step 1938, loss 0.171012, acc 0.90625\n",
      "2017-11-06T22:29:43.928005: step 1939, loss 0.0683439, acc 0.96875\n",
      "2017-11-06T22:29:47.933851: step 1940, loss 0.213972, acc 0.90625\n",
      "2017-11-06T22:29:51.901672: step 1941, loss 0.0630312, acc 0.96875\n",
      "2017-11-06T22:29:55.931534: step 1942, loss 0.14535, acc 0.9375\n",
      "2017-11-06T22:29:59.903357: step 1943, loss 0.114534, acc 0.96875\n",
      "2017-11-06T22:30:03.012565: step 1944, loss 0.380704, acc 0.85\n",
      "2017-11-06T22:30:07.473736: step 1945, loss 0.0734789, acc 0.9375\n",
      "2017-11-06T22:30:11.589659: step 1946, loss 0.0549082, acc 0.96875\n",
      "2017-11-06T22:30:15.672561: step 1947, loss 0.125847, acc 0.9375\n",
      "2017-11-06T22:30:19.703426: step 1948, loss 0.128857, acc 0.96875\n",
      "2017-11-06T22:30:23.721280: step 1949, loss 0.259829, acc 0.9375\n",
      "2017-11-06T22:30:27.823194: step 1950, loss 0.283496, acc 0.875\n",
      "2017-11-06T22:30:31.870069: step 1951, loss 0.19752, acc 0.90625\n",
      "2017-11-06T22:30:36.134102: step 1952, loss 0.311891, acc 0.90625\n",
      "2017-11-06T22:30:40.213999: step 1953, loss 0.107861, acc 0.96875\n",
      "2017-11-06T22:30:44.332925: step 1954, loss 0.18015, acc 0.9375\n",
      "2017-11-06T22:30:48.342774: step 1955, loss 0.105093, acc 0.9375\n",
      "2017-11-06T22:30:52.282574: step 1956, loss 0.23056, acc 0.875\n",
      "2017-11-06T22:30:56.308434: step 1957, loss 0.192144, acc 0.90625\n",
      "2017-11-06T22:31:00.388333: step 1958, loss 0.297116, acc 0.84375\n",
      "2017-11-06T22:31:04.384172: step 1959, loss 0.230382, acc 0.875\n",
      "2017-11-06T22:31:08.537123: step 1960, loss 0.0926841, acc 0.9375\n",
      "2017-11-06T22:31:12.905228: step 1961, loss 0.0575742, acc 0.96875\n",
      "2017-11-06T22:31:16.897063: step 1962, loss 0.356536, acc 0.875\n",
      "2017-11-06T22:31:20.884897: step 1963, loss 0.114862, acc 0.9375\n",
      "2017-11-06T22:31:24.887742: step 1964, loss 0.297464, acc 0.90625\n",
      "2017-11-06T22:31:28.891586: step 1965, loss 0.0598867, acc 0.9375\n",
      "2017-11-06T22:31:32.864409: step 1966, loss 0.360972, acc 0.875\n",
      "2017-11-06T22:31:37.089412: step 1967, loss 0.176204, acc 0.90625\n",
      "2017-11-06T22:31:41.260376: step 1968, loss 0.431549, acc 0.8125\n",
      "2017-11-06T22:31:45.283232: step 1969, loss 0.30076, acc 0.8125\n",
      "2017-11-06T22:31:49.288079: step 1970, loss 0.28189, acc 0.90625\n",
      "2017-11-06T22:31:53.276913: step 1971, loss 0.258118, acc 0.90625\n",
      "2017-11-06T22:31:57.269752: step 1972, loss 0.104559, acc 0.9375\n",
      "2017-11-06T22:32:01.234567: step 1973, loss 0.272668, acc 0.875\n",
      "2017-11-06T22:32:05.266432: step 1974, loss 0.119606, acc 0.96875\n",
      "2017-11-06T22:32:09.264272: step 1975, loss 0.141083, acc 0.9375\n",
      "2017-11-06T22:32:13.355179: step 1976, loss 0.236649, acc 0.9375\n",
      "2017-11-06T22:32:17.809344: step 1977, loss 0.184055, acc 0.90625\n",
      "2017-11-06T22:32:21.882239: step 1978, loss 0.00326954, acc 1\n",
      "2017-11-06T22:32:25.925111: step 1979, loss 0.0607443, acc 0.96875\n",
      "2017-11-06T22:32:28.408875: step 1980, loss 0.298254, acc 0.95\n",
      "2017-11-06T22:32:32.426730: step 1981, loss 0.152774, acc 0.90625\n",
      "2017-11-06T22:32:36.672507: step 1982, loss 0.0222859, acc 1\n",
      "2017-11-06T22:32:40.782427: step 1983, loss 0.202738, acc 0.875\n",
      "2017-11-06T22:32:44.816296: step 1984, loss 0.233873, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T22:32:48.855164: step 1985, loss 0.0777431, acc 0.96875\n",
      "2017-11-06T22:32:52.988100: step 1986, loss 0.277605, acc 0.875\n",
      "2017-11-06T22:32:56.998951: step 1987, loss 0.189772, acc 0.875\n",
      "2017-11-06T22:33:01.041823: step 1988, loss 0.295954, acc 0.875\n",
      "2017-11-06T22:33:05.042665: step 1989, loss 0.255146, acc 0.875\n",
      "2017-11-06T22:33:09.050513: step 1990, loss 0.282579, acc 0.84375\n",
      "2017-11-06T22:33:13.109397: step 1991, loss 0.107147, acc 0.9375\n",
      "2017-11-06T22:33:17.117245: step 1992, loss 0.212284, acc 0.9375\n",
      "2017-11-06T22:33:21.448322: step 1993, loss 0.362852, acc 0.875\n",
      "2017-11-06T22:33:26.089620: step 1994, loss 0.0589003, acc 1\n",
      "2017-11-06T22:33:30.088461: step 1995, loss 0.041656, acc 1\n",
      "2017-11-06T22:33:34.054279: step 1996, loss 0.079661, acc 0.96875\n",
      "2017-11-06T22:33:38.064129: step 1997, loss 0.243175, acc 0.90625\n",
      "2017-11-06T22:33:42.321154: step 1998, loss 0.109832, acc 0.96875\n",
      "2017-11-06T22:33:46.448085: step 1999, loss 0.149603, acc 0.9375\n",
      "2017-11-06T22:33:50.632059: step 2000, loss 0.274669, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:33:53.243914: step 2000, loss 1.0314, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2000\n",
      "\n",
      "2017-11-06T22:33:58.753829: step 2001, loss 0.293779, acc 0.90625\n",
      "2017-11-06T22:34:02.877760: step 2002, loss 0.0926781, acc 0.96875\n",
      "2017-11-06T22:34:07.038716: step 2003, loss 0.13892, acc 0.9375\n",
      "2017-11-06T22:34:11.232696: step 2004, loss 0.17362, acc 0.9375\n",
      "2017-11-06T22:34:15.454696: step 2005, loss 0.21733, acc 0.90625\n",
      "2017-11-06T22:34:19.599641: step 2006, loss 0.173868, acc 0.9375\n",
      "2017-11-06T22:34:23.749589: step 2007, loss 0.168712, acc 0.90625\n",
      "2017-11-06T22:34:28.373875: step 2008, loss 0.164394, acc 0.9375\n",
      "2017-11-06T22:34:32.648913: step 2009, loss 0.381399, acc 0.84375\n",
      "2017-11-06T22:34:37.051041: step 2010, loss 0.277794, acc 0.875\n",
      "2017-11-06T22:34:41.143951: step 2011, loss 0.174551, acc 0.9375\n",
      "2017-11-06T22:34:45.318915: step 2012, loss 0.0535593, acc 0.96875\n",
      "2017-11-06T22:34:49.351781: step 2013, loss 0.140774, acc 0.9375\n",
      "2017-11-06T22:34:53.395654: step 2014, loss 0.373393, acc 0.875\n",
      "2017-11-06T22:34:57.434524: step 2015, loss 0.131804, acc 0.90625\n",
      "2017-11-06T22:35:00.036373: step 2016, loss 0.345818, acc 0.8\n",
      "2017-11-06T22:35:04.056230: step 2017, loss 0.186587, acc 0.9375\n",
      "2017-11-06T22:35:07.995028: step 2018, loss 0.170304, acc 0.9375\n",
      "2017-11-06T22:35:12.051911: step 2019, loss 0.018124, acc 1\n",
      "2017-11-06T22:35:16.132810: step 2020, loss 0.0646818, acc 0.96875\n",
      "2017-11-06T22:35:20.182688: step 2021, loss 0.167769, acc 0.9375\n",
      "2017-11-06T22:35:24.261586: step 2022, loss 0.414432, acc 0.875\n",
      "2017-11-06T22:35:28.305459: step 2023, loss 0.144982, acc 0.9375\n",
      "2017-11-06T22:35:32.655550: step 2024, loss 0.441874, acc 0.875\n",
      "2017-11-06T22:35:36.922320: step 2025, loss 0.155813, acc 0.96875\n",
      "2017-11-06T22:35:40.949182: step 2026, loss 0.113048, acc 0.96875\n",
      "2017-11-06T22:35:45.008065: step 2027, loss 0.173141, acc 0.9375\n",
      "2017-11-06T22:35:49.092967: step 2028, loss 0.201368, acc 0.90625\n",
      "2017-11-06T22:35:53.156855: step 2029, loss 0.121904, acc 0.90625\n",
      "2017-11-06T22:35:57.197726: step 2030, loss 0.181003, acc 0.90625\n",
      "2017-11-06T22:36:01.182557: step 2031, loss 0.0653861, acc 0.96875\n",
      "2017-11-06T22:36:05.261455: step 2032, loss 0.1427, acc 0.9375\n",
      "2017-11-06T22:36:09.286317: step 2033, loss 0.18267, acc 0.90625\n",
      "2017-11-06T22:36:13.319181: step 2034, loss 0.159038, acc 0.9375\n",
      "2017-11-06T22:36:17.363054: step 2035, loss 0.216233, acc 0.875\n",
      "2017-11-06T22:36:21.372904: step 2036, loss 0.203003, acc 0.90625\n",
      "2017-11-06T22:36:25.432788: step 2037, loss 0.126291, acc 0.9375\n",
      "2017-11-06T22:36:29.410615: step 2038, loss 0.113278, acc 0.9375\n",
      "2017-11-06T22:36:33.712671: step 2039, loss 0.226528, acc 0.875\n",
      "2017-11-06T22:36:38.051755: step 2040, loss 0.0458649, acc 1\n",
      "2017-11-06T22:36:42.217716: step 2041, loss 0.274179, acc 0.84375\n",
      "2017-11-06T22:36:46.308621: step 2042, loss 0.067786, acc 0.9375\n",
      "2017-11-06T22:36:50.218400: step 2043, loss 0.31857, acc 0.8125\n",
      "2017-11-06T22:36:54.227267: step 2044, loss 0.315708, acc 0.84375\n",
      "2017-11-06T22:36:58.212079: step 2045, loss 0.166807, acc 0.9375\n",
      "2017-11-06T22:37:02.240942: step 2046, loss 0.107397, acc 0.96875\n",
      "2017-11-06T22:37:06.266804: step 2047, loss 0.151721, acc 0.9375\n",
      "2017-11-06T22:37:10.257890: step 2048, loss 0.186695, acc 0.9375\n",
      "2017-11-06T22:37:14.174671: step 2049, loss 0.193052, acc 0.875\n",
      "2017-11-06T22:37:18.120475: step 2050, loss 0.122724, acc 0.9375\n",
      "2017-11-06T22:37:22.008238: step 2051, loss 0.291129, acc 0.875\n",
      "2017-11-06T22:37:24.560050: step 2052, loss 0.462289, acc 0.85\n",
      "2017-11-06T22:37:28.483838: step 2053, loss 0.0848199, acc 0.96875\n",
      "2017-11-06T22:37:32.451658: step 2054, loss 0.0553455, acc 1\n",
      "2017-11-06T22:37:36.547570: step 2055, loss 0.267085, acc 0.84375\n",
      "2017-11-06T22:37:40.542407: step 2056, loss 0.250975, acc 0.875\n",
      "2017-11-06T22:37:44.911512: step 2057, loss 0.229721, acc 0.875\n",
      "2017-11-06T22:37:48.940374: step 2058, loss 0.0477159, acc 0.96875\n",
      "2017-11-06T22:37:52.887178: step 2059, loss 0.463404, acc 0.75\n",
      "2017-11-06T22:37:56.811967: step 2060, loss 0.258743, acc 0.90625\n",
      "2017-11-06T22:38:00.749765: step 2061, loss 0.00564105, acc 1\n",
      "2017-11-06T22:38:04.706577: step 2062, loss 0.130224, acc 0.9375\n",
      "2017-11-06T22:38:08.630364: step 2063, loss 0.354929, acc 0.84375\n",
      "2017-11-06T22:38:12.543144: step 2064, loss 0.126039, acc 0.9375\n",
      "2017-11-06T22:38:16.512965: step 2065, loss 0.105511, acc 0.96875\n",
      "2017-11-06T22:38:20.493793: step 2066, loss 0.204346, acc 0.90625\n",
      "2017-11-06T22:38:24.586702: step 2067, loss 0.159143, acc 0.875\n",
      "2017-11-06T22:38:28.606558: step 2068, loss 0.22262, acc 0.84375\n",
      "2017-11-06T22:38:32.559367: step 2069, loss 0.0842112, acc 0.96875\n",
      "2017-11-06T22:38:36.761354: step 2070, loss 0.0889096, acc 0.96875\n",
      "2017-11-06T22:38:40.763022: step 2071, loss 0.151074, acc 0.90625\n",
      "2017-11-06T22:38:44.769868: step 2072, loss 0.162839, acc 0.96875\n",
      "2017-11-06T22:38:48.993868: step 2073, loss 0.343854, acc 0.84375\n",
      "2017-11-06T22:38:53.215868: step 2074, loss 0.0753278, acc 1\n",
      "2017-11-06T22:38:57.203702: step 2075, loss 0.228272, acc 0.875\n",
      "2017-11-06T22:39:01.235567: step 2076, loss 0.0664001, acc 0.96875\n",
      "2017-11-06T22:39:05.246417: step 2077, loss 0.0680094, acc 0.96875\n",
      "2017-11-06T22:39:09.243256: step 2078, loss 0.189893, acc 0.9375\n",
      "2017-11-06T22:39:13.309201: step 2079, loss 0.0666979, acc 0.96875\n",
      "2017-11-06T22:39:17.326055: step 2080, loss 0.481709, acc 0.8125\n",
      "2017-11-06T22:39:21.242838: step 2081, loss 0.0128927, acc 1\n",
      "2017-11-06T22:39:25.251686: step 2082, loss 0.231972, acc 0.90625\n",
      "2017-11-06T22:39:29.198492: step 2083, loss 0.117756, acc 0.96875\n",
      "2017-11-06T22:39:33.115274: step 2084, loss 0.126731, acc 0.9375\n",
      "2017-11-06T22:39:37.042083: step 2085, loss 0.100324, acc 0.90625\n",
      "2017-11-06T22:39:41.107953: step 2086, loss 0.0824595, acc 0.9375\n",
      "2017-11-06T22:39:45.409010: step 2087, loss 0.300508, acc 0.875\n",
      "2017-11-06T22:39:47.933804: step 2088, loss 0.244448, acc 0.9\n",
      "2017-11-06T22:39:51.949656: step 2089, loss 0.189498, acc 0.9375\n",
      "2017-11-06T22:39:56.320762: step 2090, loss 0.194792, acc 0.90625\n",
      "2017-11-06T22:40:00.461707: step 2091, loss 0.106825, acc 0.96875\n",
      "2017-11-06T22:40:04.547608: step 2092, loss 0.345099, acc 0.875\n",
      "2017-11-06T22:40:08.499416: step 2093, loss 0.0594143, acc 0.96875\n",
      "2017-11-06T22:40:12.413197: step 2094, loss 0.158799, acc 0.9375\n",
      "2017-11-06T22:40:16.412040: step 2095, loss 0.415949, acc 0.875\n",
      "2017-11-06T22:40:20.377856: step 2096, loss 0.150746, acc 0.90625\n",
      "2017-11-06T22:40:24.327663: step 2097, loss 0.417842, acc 0.90625\n",
      "2017-11-06T22:40:28.270466: step 2098, loss 0.218107, acc 0.9375\n",
      "2017-11-06T22:40:32.236283: step 2099, loss 0.100897, acc 0.96875\n",
      "2017-11-06T22:40:36.464286: step 2100, loss 0.301791, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:40:39.029108: step 2100, loss 1.09943, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2100\n",
      "\n",
      "2017-11-06T22:40:44.504386: step 2101, loss 0.0373851, acc 1\n",
      "2017-11-06T22:40:48.464199: step 2102, loss 0.447763, acc 0.8125\n",
      "2017-11-06T22:40:52.441025: step 2103, loss 0.24491, acc 0.875\n",
      "2017-11-06T22:40:56.408844: step 2104, loss 0.0962248, acc 0.9375\n",
      "2017-11-06T22:41:00.668873: step 2105, loss 0.131663, acc 0.96875\n",
      "2017-11-06T22:41:04.750771: step 2106, loss 0.0855549, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T22:41:08.692572: step 2107, loss 0.176838, acc 0.9375\n",
      "2017-11-06T22:41:13.542017: step 2108, loss 0.212333, acc 0.875\n",
      "2017-11-06T22:41:17.961158: step 2109, loss 0.209958, acc 0.90625\n",
      "2017-11-06T22:41:22.067076: step 2110, loss 0.239515, acc 0.9375\n",
      "2017-11-06T22:41:26.103943: step 2111, loss 0.209669, acc 0.90625\n",
      "2017-11-06T22:41:30.074765: step 2112, loss 0.0914314, acc 0.9375\n",
      "2017-11-06T22:41:34.034580: step 2113, loss 0.104916, acc 0.96875\n",
      "2017-11-06T22:41:37.969196: step 2114, loss 0.289546, acc 0.875\n",
      "2017-11-06T22:41:41.909995: step 2115, loss 0.187143, acc 0.90625\n",
      "2017-11-06T22:41:45.838787: step 2116, loss 0.112936, acc 0.96875\n",
      "2017-11-06T22:41:49.831626: step 2117, loss 0.0539911, acc 0.96875\n",
      "2017-11-06T22:41:53.786434: step 2118, loss 0.396077, acc 0.875\n",
      "2017-11-06T22:41:57.702216: step 2119, loss 0.183206, acc 0.90625\n",
      "2017-11-06T22:42:01.697056: step 2120, loss 0.176824, acc 0.90625\n",
      "2017-11-06T22:42:05.858011: step 2121, loss 0.0844071, acc 0.96875\n",
      "2017-11-06T22:42:10.020970: step 2122, loss 0.275279, acc 0.84375\n",
      "2017-11-06T22:42:13.960769: step 2123, loss 0.41564, acc 0.84375\n",
      "2017-11-06T22:42:16.490566: step 2124, loss 0.22988, acc 0.9\n",
      "2017-11-06T22:42:20.551451: step 2125, loss 0.129356, acc 0.96875\n",
      "2017-11-06T22:42:24.546291: step 2126, loss 0.13628, acc 0.9375\n",
      "2017-11-06T22:42:28.568149: step 2127, loss 0.149723, acc 0.90625\n",
      "2017-11-06T22:42:32.562986: step 2128, loss 0.14806, acc 0.9375\n",
      "2017-11-06T22:42:36.811005: step 2129, loss 0.208844, acc 0.875\n",
      "2017-11-06T22:42:40.767818: step 2130, loss 0.102011, acc 0.96875\n",
      "2017-11-06T22:42:44.744643: step 2131, loss 0.188517, acc 0.90625\n",
      "2017-11-06T22:42:48.695449: step 2132, loss 0.149287, acc 0.9375\n",
      "2017-11-06T22:42:52.626242: step 2133, loss 0.0936364, acc 0.96875\n",
      "2017-11-06T22:42:56.603068: step 2134, loss 0.0236639, acc 1\n",
      "2017-11-06T22:43:00.538864: step 2135, loss 0.21196, acc 0.90625\n",
      "2017-11-06T22:43:04.519693: step 2136, loss 0.0639133, acc 0.96875\n",
      "2017-11-06T22:43:08.451489: step 2137, loss 0.123709, acc 0.9375\n",
      "2017-11-06T22:43:12.876631: step 2138, loss 0.116369, acc 0.9375\n",
      "2017-11-06T22:43:16.911499: step 2139, loss 0.3297, acc 0.84375\n",
      "2017-11-06T22:43:20.943658: step 2140, loss 0.069794, acc 0.96875\n",
      "2017-11-06T22:43:25.173663: step 2141, loss 0.16996, acc 0.9375\n",
      "2017-11-06T22:43:29.071435: step 2142, loss 0.39073, acc 0.8125\n",
      "2017-11-06T22:43:33.037250: step 2143, loss 0.169351, acc 0.9375\n",
      "2017-11-06T22:43:37.009073: step 2144, loss 0.227197, acc 0.90625\n",
      "2017-11-06T22:43:40.951874: step 2145, loss 0.0448619, acc 0.96875\n",
      "2017-11-06T22:43:44.875662: step 2146, loss 0.0910425, acc 0.96875\n",
      "2017-11-06T22:43:48.811458: step 2147, loss 0.225399, acc 0.84375\n",
      "2017-11-06T22:43:52.763268: step 2148, loss 0.259923, acc 0.875\n",
      "2017-11-06T22:43:56.736089: step 2149, loss 0.0796399, acc 0.9375\n",
      "2017-11-06T22:44:00.708912: step 2150, loss 0.188446, acc 0.9375\n",
      "2017-11-06T22:44:04.690744: step 2151, loss 0.221595, acc 0.90625\n",
      "2017-11-06T22:44:08.630541: step 2152, loss 0.113549, acc 0.96875\n",
      "2017-11-06T22:44:12.605365: step 2153, loss 0.101171, acc 0.9375\n",
      "2017-11-06T22:44:16.804349: step 2154, loss 0.200882, acc 0.875\n",
      "2017-11-06T22:44:20.998329: step 2155, loss 0.1379, acc 0.9375\n",
      "2017-11-06T22:44:25.014184: step 2156, loss 0.201198, acc 0.875\n",
      "2017-11-06T22:44:28.983002: step 2157, loss 0.203531, acc 0.9375\n",
      "2017-11-06T22:44:33.041886: step 2158, loss 0.17478, acc 0.9375\n",
      "2017-11-06T22:44:37.203843: step 2159, loss 0.103364, acc 0.96875\n",
      "2017-11-06T22:44:39.677356: step 2160, loss 0.393476, acc 0.8\n",
      "2017-11-06T22:44:43.656183: step 2161, loss 0.0975968, acc 0.9375\n",
      "2017-11-06T22:44:47.569964: step 2162, loss 0.158924, acc 0.9375\n",
      "2017-11-06T22:44:51.545791: step 2163, loss 0.268806, acc 0.875\n",
      "2017-11-06T22:44:55.483587: step 2164, loss 0.192382, acc 0.9375\n",
      "2017-11-06T22:44:59.383359: step 2165, loss 0.151179, acc 0.9375\n",
      "2017-11-06T22:45:03.370191: step 2166, loss 0.279345, acc 0.84375\n",
      "2017-11-06T22:45:07.327003: step 2167, loss 0.142037, acc 0.9375\n",
      "2017-11-06T22:45:11.218768: step 2168, loss 0.180148, acc 0.9375\n",
      "2017-11-06T22:45:15.202599: step 2169, loss 0.252175, acc 0.9375\n",
      "2017-11-06T22:45:19.174421: step 2170, loss 0.188942, acc 0.875\n",
      "2017-11-06T22:45:23.508537: step 2171, loss 0.20391, acc 0.9375\n",
      "2017-11-06T22:45:27.718526: step 2172, loss 0.230702, acc 0.90625\n",
      "2017-11-06T22:45:31.632307: step 2173, loss 0.274182, acc 0.84375\n",
      "2017-11-06T22:45:35.602128: step 2174, loss 0.10726, acc 0.9375\n",
      "2017-11-06T22:45:39.541928: step 2175, loss 0.0652212, acc 0.96875\n",
      "2017-11-06T22:45:43.521755: step 2176, loss 0.0669424, acc 0.96875\n",
      "2017-11-06T22:45:47.482570: step 2177, loss 0.22317, acc 0.8125\n",
      "2017-11-06T22:45:51.442383: step 2178, loss 0.122861, acc 0.90625\n",
      "2017-11-06T22:45:55.538293: step 2179, loss 0.0807339, acc 0.96875\n",
      "2017-11-06T22:45:59.595177: step 2180, loss 0.136275, acc 0.9375\n",
      "2017-11-06T22:46:03.692087: step 2181, loss 0.110217, acc 0.9375\n",
      "2017-11-06T22:46:07.837033: step 2182, loss 0.153678, acc 0.9375\n",
      "2017-11-06T22:46:11.994987: step 2183, loss 0.143978, acc 0.9375\n",
      "2017-11-06T22:46:16.179961: step 2184, loss 0.15188, acc 0.9375\n",
      "2017-11-06T22:46:20.305892: step 2185, loss 0.250585, acc 0.90625\n",
      "2017-11-06T22:46:24.286721: step 2186, loss 0.291623, acc 0.84375\n",
      "2017-11-06T22:46:28.634810: step 2187, loss 0.466703, acc 0.8125\n",
      "2017-11-06T22:46:32.969891: step 2188, loss 0.25575, acc 0.875\n",
      "2017-11-06T22:46:37.164873: step 2189, loss 0.254686, acc 0.9375\n",
      "2017-11-06T22:46:41.213749: step 2190, loss 0.118826, acc 0.9375\n",
      "2017-11-06T22:46:45.237607: step 2191, loss 0.180209, acc 0.90625\n",
      "2017-11-06T22:46:49.242452: step 2192, loss 0.0628105, acc 1\n",
      "2017-11-06T22:46:53.266312: step 2193, loss 0.114028, acc 0.9375\n",
      "2017-11-06T22:46:57.294174: step 2194, loss 0.327312, acc 0.875\n",
      "2017-11-06T22:47:01.310027: step 2195, loss 0.111824, acc 0.9375\n",
      "2017-11-06T22:47:03.858839: step 2196, loss 0.37046, acc 0.8\n",
      "2017-11-06T22:47:07.840687: step 2197, loss 0.26368, acc 0.875\n",
      "2017-11-06T22:47:11.821496: step 2198, loss 0.193994, acc 0.9375\n",
      "2017-11-06T22:47:15.859365: step 2199, loss 0.0768666, acc 0.96875\n",
      "2017-11-06T22:47:19.826185: step 2200, loss 0.026478, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:47:22.350977: step 2200, loss 0.918518, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2200\n",
      "\n",
      "2017-11-06T22:47:27.611815: step 2201, loss 0.0843213, acc 0.96875\n",
      "2017-11-06T22:47:31.644681: step 2202, loss 0.192257, acc 0.90625\n",
      "2017-11-06T22:47:36.051813: step 2203, loss 0.223547, acc 0.90625\n",
      "2017-11-06T22:47:40.254621: step 2204, loss 0.191097, acc 0.90625\n",
      "2017-11-06T22:47:44.195422: step 2205, loss 0.20925, acc 0.9375\n",
      "2017-11-06T22:47:48.236295: step 2206, loss 0.137423, acc 0.90625\n",
      "2017-11-06T22:47:52.202111: step 2207, loss 0.138521, acc 0.9375\n",
      "2017-11-06T22:47:56.211960: step 2208, loss 0.18928, acc 0.90625\n",
      "2017-11-06T22:48:00.194790: step 2209, loss 0.23176, acc 0.90625\n",
      "2017-11-06T22:48:04.181623: step 2210, loss 0.0847216, acc 0.9375\n",
      "2017-11-06T22:48:08.193475: step 2211, loss 0.0870838, acc 0.96875\n",
      "2017-11-06T22:48:12.128269: step 2212, loss 0.161425, acc 0.90625\n",
      "2017-11-06T22:48:16.093088: step 2213, loss 0.167512, acc 0.90625\n",
      "2017-11-06T22:48:20.079919: step 2214, loss 0.343809, acc 0.875\n",
      "2017-11-06T22:48:24.213857: step 2215, loss 0.223202, acc 0.90625\n",
      "2017-11-06T22:48:28.495899: step 2216, loss 0.394243, acc 0.8125\n",
      "2017-11-06T22:48:32.553782: step 2217, loss 0.24468, acc 0.875\n",
      "2017-11-06T22:48:36.789793: step 2218, loss 0.200823, acc 0.875\n",
      "2017-11-06T22:48:41.271977: step 2219, loss 0.195366, acc 0.9375\n",
      "2017-11-06T22:48:45.323856: step 2220, loss 0.181899, acc 0.90625\n",
      "2017-11-06T22:48:49.336707: step 2221, loss 0.449784, acc 0.875\n",
      "2017-11-06T22:48:53.363569: step 2222, loss 0.217649, acc 0.90625\n",
      "2017-11-06T22:48:57.380422: step 2223, loss 0.162527, acc 0.9375\n",
      "2017-11-06T22:49:01.363252: step 2224, loss 0.106915, acc 0.96875\n",
      "2017-11-06T22:49:05.294047: step 2225, loss 0.177394, acc 0.9375\n",
      "2017-11-06T22:49:09.370942: step 2226, loss 0.0833026, acc 0.96875\n",
      "2017-11-06T22:49:13.338761: step 2227, loss 0.192222, acc 0.96875\n",
      "2017-11-06T22:49:17.283566: step 2228, loss 0.318069, acc 0.84375\n",
      "2017-11-06T22:49:21.262392: step 2229, loss 0.299449, acc 0.84375\n",
      "2017-11-06T22:49:25.255228: step 2230, loss 0.162879, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T22:49:29.290095: step 2231, loss 0.133967, acc 0.9375\n",
      "2017-11-06T22:49:31.799879: step 2232, loss 0.16675, acc 0.95\n",
      "2017-11-06T22:49:35.790715: step 2233, loss 0.152441, acc 0.90625\n",
      "2017-11-06T22:49:39.756533: step 2234, loss 0.215242, acc 0.90625\n",
      "2017-11-06T22:49:43.950513: step 2235, loss 0.00613154, acc 1\n",
      "2017-11-06T22:49:48.314613: step 2236, loss 0.112245, acc 0.9375\n",
      "2017-11-06T22:49:52.339473: step 2237, loss 0.0763567, acc 0.96875\n",
      "2017-11-06T22:49:56.384347: step 2238, loss 0.22917, acc 0.9375\n",
      "2017-11-06T22:50:00.449236: step 2239, loss 0.21028, acc 0.90625\n",
      "2017-11-06T22:50:04.670236: step 2240, loss 0.161922, acc 0.90625\n",
      "2017-11-06T22:50:08.678082: step 2241, loss 0.100512, acc 0.96875\n",
      "2017-11-06T22:50:12.717953: step 2242, loss 0.341712, acc 0.875\n",
      "2017-11-06T22:50:16.765829: step 2243, loss 0.298806, acc 0.90625\n",
      "2017-11-06T22:50:20.804699: step 2244, loss 0.0659608, acc 0.96875\n",
      "2017-11-06T22:50:24.803540: step 2245, loss 0.192424, acc 0.9375\n",
      "2017-11-06T22:50:28.794376: step 2246, loss 0.161345, acc 0.9375\n",
      "2017-11-06T22:50:32.962337: step 2247, loss 0.358751, acc 0.84375\n",
      "2017-11-06T22:50:37.121293: step 2248, loss 0.186916, acc 0.96875\n",
      "2017-11-06T22:50:41.137127: step 2249, loss 0.192231, acc 0.90625\n",
      "2017-11-06T22:50:45.188006: step 2250, loss 0.0446054, acc 0.96875\n",
      "2017-11-06T22:50:49.362972: step 2251, loss 0.268988, acc 0.90625\n",
      "2017-11-06T22:50:53.790118: step 2252, loss 0.130184, acc 0.9375\n",
      "2017-11-06T22:50:57.872019: step 2253, loss 0.155836, acc 0.90625\n",
      "2017-11-06T22:51:01.893876: step 2254, loss 0.23966, acc 0.9375\n",
      "2017-11-06T22:51:05.857693: step 2255, loss 0.29519, acc 0.875\n",
      "2017-11-06T22:51:09.875547: step 2256, loss 0.210751, acc 0.875\n",
      "2017-11-06T22:51:13.878392: step 2257, loss 0.107199, acc 0.9375\n",
      "2017-11-06T22:51:17.863225: step 2258, loss 0.285792, acc 0.875\n",
      "2017-11-06T22:51:21.832044: step 2259, loss 0.147669, acc 0.9375\n",
      "2017-11-06T22:51:25.814873: step 2260, loss 0.471344, acc 0.78125\n",
      "2017-11-06T22:51:29.848739: step 2261, loss 0.240446, acc 0.84375\n",
      "2017-11-06T22:51:33.867596: step 2262, loss 0.0931027, acc 0.96875\n",
      "2017-11-06T22:51:37.840418: step 2263, loss 0.307717, acc 0.875\n",
      "2017-11-06T22:51:41.882289: step 2264, loss 0.465399, acc 0.84375\n",
      "2017-11-06T22:51:45.917156: step 2265, loss 0.244843, acc 0.90625\n",
      "2017-11-06T22:51:49.893982: step 2266, loss 0.297324, acc 0.875\n",
      "2017-11-06T22:51:53.987892: step 2267, loss 0.32845, acc 0.84375\n",
      "2017-11-06T22:51:56.794886: step 2268, loss 0.387804, acc 0.9\n",
      "2017-11-06T22:52:01.009881: step 2269, loss 0.191403, acc 0.9375\n",
      "2017-11-06T22:52:05.005722: step 2270, loss 0.0839299, acc 0.96875\n",
      "2017-11-06T22:52:09.062602: step 2271, loss 0.163048, acc 0.875\n",
      "2017-11-06T22:52:13.138498: step 2272, loss 0.286103, acc 0.84375\n",
      "2017-11-06T22:52:17.159356: step 2273, loss 0.075071, acc 0.96875\n",
      "2017-11-06T22:52:21.140186: step 2274, loss 0.180417, acc 0.875\n",
      "2017-11-06T22:52:25.183059: step 2275, loss 0.137115, acc 0.9375\n",
      "2017-11-06T22:52:29.188904: step 2276, loss 0.0517876, acc 0.96875\n",
      "2017-11-06T22:52:33.446931: step 2277, loss 0.0912391, acc 0.96875\n",
      "2017-11-06T22:52:37.612891: step 2278, loss 0.136116, acc 0.9375\n",
      "2017-11-06T22:52:41.689786: step 2279, loss 0.277014, acc 0.84375\n",
      "2017-11-06T22:52:45.735660: step 2280, loss 0.164962, acc 0.90625\n",
      "2017-11-06T22:52:49.739505: step 2281, loss 0.128766, acc 0.90625\n",
      "2017-11-06T22:52:53.805394: step 2282, loss 0.157531, acc 0.9375\n",
      "2017-11-06T22:52:57.781219: step 2283, loss 0.137975, acc 0.90625\n",
      "2017-11-06T22:53:02.109295: step 2284, loss 0.108133, acc 0.9375\n",
      "2017-11-06T22:53:06.334297: step 2285, loss 0.143857, acc 0.9375\n",
      "2017-11-06T22:53:10.364161: step 2286, loss 0.362363, acc 0.875\n",
      "2017-11-06T22:53:14.403029: step 2287, loss 0.236134, acc 0.8125\n",
      "2017-11-06T22:53:18.433894: step 2288, loss 0.176711, acc 0.90625\n",
      "2017-11-06T22:53:22.367689: step 2289, loss 0.194177, acc 0.875\n",
      "2017-11-06T22:53:26.485616: step 2290, loss 0.129014, acc 0.96875\n",
      "2017-11-06T22:53:30.467463: step 2291, loss 0.0912187, acc 0.96875\n",
      "2017-11-06T22:53:34.504313: step 2292, loss 0.124511, acc 0.9375\n",
      "2017-11-06T22:53:38.489143: step 2293, loss 0.186392, acc 0.9375\n",
      "2017-11-06T22:53:42.523783: step 2294, loss 0.20343, acc 0.90625\n",
      "2017-11-06T22:53:46.598678: step 2295, loss 0.346062, acc 0.875\n",
      "2017-11-06T22:53:50.512459: step 2296, loss 0.289536, acc 0.90625\n",
      "2017-11-06T22:53:54.556334: step 2297, loss 0.156195, acc 0.9375\n",
      "2017-11-06T22:53:58.552172: step 2298, loss 0.327153, acc 0.875\n",
      "2017-11-06T22:54:02.624065: step 2299, loss 0.0751327, acc 0.96875\n",
      "2017-11-06T22:54:06.819046: step 2300, loss 0.212315, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T22:54:09.721109: step 2300, loss 0.990585, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2300\n",
      "\n",
      "2017-11-06T22:54:15.055365: step 2301, loss 0.104565, acc 0.96875\n",
      "2017-11-06T22:54:19.057206: step 2302, loss 0.161586, acc 0.96875\n",
      "2017-11-06T22:54:23.192145: step 2303, loss 0.175216, acc 0.90625\n",
      "2017-11-06T22:54:26.041169: step 2304, loss 0.481794, acc 0.8\n",
      "2017-11-06T22:54:30.162097: step 2305, loss 0.044955, acc 0.96875\n",
      "2017-11-06T22:54:34.409114: step 2306, loss 0.102867, acc 0.9375\n",
      "2017-11-06T22:54:38.450986: step 2307, loss 0.0971691, acc 0.9375\n",
      "2017-11-06T22:54:42.413802: step 2308, loss 0.189971, acc 0.90625\n",
      "2017-11-06T22:54:46.390628: step 2309, loss 0.227082, acc 0.90625\n",
      "2017-11-06T22:54:50.309412: step 2310, loss 0.194533, acc 0.9375\n",
      "2017-11-06T22:54:54.282235: step 2311, loss 0.108347, acc 0.96875\n",
      "2017-11-06T22:54:58.238047: step 2312, loss 0.162563, acc 0.90625\n",
      "2017-11-06T22:55:02.207867: step 2313, loss 0.20697, acc 0.90625\n",
      "2017-11-06T22:55:06.134657: step 2314, loss 0.221013, acc 0.875\n",
      "2017-11-06T22:55:10.107481: step 2315, loss 0.131412, acc 0.90625\n",
      "2017-11-06T22:55:14.505605: step 2316, loss 0.0713578, acc 0.9375\n",
      "2017-11-06T22:55:18.558484: step 2317, loss 0.0440209, acc 1\n",
      "2017-11-06T22:55:22.512294: step 2318, loss 0.123989, acc 0.9375\n",
      "2017-11-06T22:55:26.483198: step 2319, loss 0.197732, acc 0.9375\n",
      "2017-11-06T22:55:30.395978: step 2320, loss 0.0232073, acc 1\n",
      "2017-11-06T22:55:34.342784: step 2321, loss 0.03938, acc 1\n",
      "2017-11-06T22:55:38.379650: step 2322, loss 0.425214, acc 0.84375\n",
      "2017-11-06T22:55:42.372487: step 2323, loss 0.103925, acc 0.9375\n",
      "2017-11-06T22:55:46.294274: step 2324, loss 0.456838, acc 0.78125\n",
      "2017-11-06T22:55:50.214060: step 2325, loss 0.239697, acc 0.90625\n",
      "2017-11-06T22:55:54.191886: step 2326, loss 0.223523, acc 0.90625\n",
      "2017-11-06T22:55:58.120677: step 2327, loss 0.0915016, acc 0.9375\n",
      "2017-11-06T22:56:02.026453: step 2328, loss 0.308073, acc 0.84375\n",
      "2017-11-06T22:56:06.015286: step 2329, loss 0.245044, acc 0.875\n",
      "2017-11-06T22:56:09.932070: step 2330, loss 0.424128, acc 0.75\n",
      "2017-11-06T22:56:13.879875: step 2331, loss 0.0921514, acc 0.96875\n",
      "2017-11-06T22:56:18.049838: step 2332, loss 0.202829, acc 0.90625\n",
      "2017-11-06T22:56:22.298857: step 2333, loss 0.182612, acc 0.90625\n",
      "2017-11-06T22:56:26.267677: step 2334, loss 0.079022, acc 0.96875\n",
      "2017-11-06T22:56:30.220486: step 2335, loss 0.317958, acc 0.875\n",
      "2017-11-06T22:56:34.486517: step 2336, loss 0.311804, acc 0.8125\n",
      "2017-11-06T22:56:38.484358: step 2337, loss 0.195801, acc 0.875\n",
      "2017-11-06T22:56:42.423957: step 2338, loss 0.0740658, acc 0.96875\n",
      "2017-11-06T22:56:46.373763: step 2339, loss 0.226205, acc 0.875\n",
      "2017-11-06T22:56:48.901560: step 2340, loss 0.200165, acc 0.9\n",
      "2017-11-06T22:56:52.826348: step 2341, loss 0.216754, acc 0.90625\n",
      "2017-11-06T22:56:56.784160: step 2342, loss 0.0690209, acc 0.96875\n",
      "2017-11-06T22:57:00.728963: step 2343, loss 0.142574, acc 0.90625\n",
      "2017-11-06T22:57:04.692779: step 2344, loss 0.141496, acc 0.96875\n",
      "2017-11-06T22:57:08.635581: step 2345, loss 0.356766, acc 0.875\n",
      "2017-11-06T22:57:12.554365: step 2346, loss 0.0646259, acc 0.96875\n",
      "2017-11-06T22:57:16.531192: step 2347, loss 0.0978746, acc 0.9375\n",
      "2017-11-06T22:57:20.506016: step 2348, loss 0.201984, acc 0.90625\n",
      "2017-11-06T22:57:24.817078: step 2349, loss 0.137664, acc 0.9375\n",
      "2017-11-06T22:57:29.060096: step 2350, loss 0.162369, acc 0.9375\n",
      "2017-11-06T22:57:33.024911: step 2351, loss 0.0944587, acc 0.96875\n",
      "2017-11-06T22:57:37.010743: step 2352, loss 0.0559965, acc 0.96875\n",
      "2017-11-06T22:57:41.082637: step 2353, loss 0.0892509, acc 0.9375\n",
      "2017-11-06T22:57:44.993415: step 2354, loss 0.144671, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T22:57:48.951229: step 2355, loss 0.270189, acc 0.875\n",
      "2017-11-06T22:57:52.864007: step 2356, loss 0.345001, acc 0.875\n",
      "2017-11-06T22:57:56.811814: step 2357, loss 0.196886, acc 0.90625\n",
      "2017-11-06T22:58:00.775629: step 2358, loss 0.145966, acc 0.9375\n",
      "2017-11-06T22:58:04.689412: step 2359, loss 0.136843, acc 0.9375\n",
      "2017-11-06T22:58:08.675241: step 2360, loss 0.209056, acc 0.9375\n",
      "2017-11-06T22:58:12.622046: step 2361, loss 0.34087, acc 0.84375\n",
      "2017-11-06T22:58:16.569851: step 2362, loss 0.126257, acc 0.9375\n",
      "2017-11-06T22:58:20.517658: step 2363, loss 0.090895, acc 0.96875\n",
      "2017-11-06T22:58:24.611566: step 2364, loss 0.183071, acc 0.90625\n",
      "2017-11-06T22:58:28.822557: step 2365, loss 0.123752, acc 0.9375\n",
      "2017-11-06T22:58:33.313749: step 2366, loss 0.187991, acc 0.9375\n",
      "2017-11-06T22:58:37.324600: step 2367, loss 0.0693077, acc 0.96875\n",
      "2017-11-06T22:58:41.322439: step 2368, loss 0.373548, acc 0.875\n",
      "2017-11-06T22:58:45.250229: step 2369, loss 0.260003, acc 0.90625\n",
      "2017-11-06T22:58:49.201037: step 2370, loss 0.193966, acc 0.90625\n",
      "2017-11-06T22:58:53.120822: step 2371, loss 0.234348, acc 0.84375\n",
      "2017-11-06T22:58:57.101651: step 2372, loss 0.128336, acc 0.9375\n",
      "2017-11-06T22:59:01.009428: step 2373, loss 0.0737125, acc 0.96875\n",
      "2017-11-06T22:59:05.072314: step 2374, loss 0.113048, acc 0.9375\n",
      "2017-11-06T22:59:09.079161: step 2375, loss 0.371606, acc 0.875\n",
      "2017-11-06T22:59:11.634978: step 2376, loss 0.416214, acc 0.8\n",
      "2017-11-06T22:59:15.530746: step 2377, loss 0.274943, acc 0.90625\n",
      "2017-11-06T22:59:19.475548: step 2378, loss 0.165965, acc 0.9375\n",
      "2017-11-06T22:59:23.430360: step 2379, loss 0.126382, acc 0.875\n",
      "2017-11-06T22:59:27.412188: step 2380, loss 0.067782, acc 0.96875\n",
      "2017-11-06T22:59:31.399089: step 2381, loss 0.290937, acc 0.84375\n",
      "2017-11-06T22:59:35.516014: step 2382, loss 0.30161, acc 0.875\n",
      "2017-11-06T22:59:39.716999: step 2383, loss 0.190454, acc 0.90625\n",
      "2017-11-06T22:59:43.707544: step 2384, loss 0.257461, acc 0.875\n",
      "2017-11-06T22:59:47.677364: step 2385, loss 0.115454, acc 0.9375\n",
      "2017-11-06T22:59:51.632174: step 2386, loss 0.108445, acc 0.9375\n",
      "2017-11-06T22:59:55.639022: step 2387, loss 0.211675, acc 0.9375\n",
      "2017-11-06T22:59:59.521780: step 2388, loss 0.137179, acc 0.96875\n",
      "2017-11-06T23:00:03.767798: step 2389, loss 0.112053, acc 0.96875\n",
      "2017-11-06T23:00:07.733616: step 2390, loss 0.0538711, acc 1\n",
      "2017-11-06T23:00:11.669412: step 2391, loss 0.362014, acc 0.875\n",
      "2017-11-06T23:00:15.591202: step 2392, loss 0.0540345, acc 0.96875\n",
      "2017-11-06T23:00:19.553014: step 2393, loss 0.155795, acc 0.9375\n",
      "2017-11-06T23:00:23.517832: step 2394, loss 0.234899, acc 0.90625\n",
      "2017-11-06T23:00:27.501662: step 2395, loss 0.204034, acc 0.875\n",
      "2017-11-06T23:00:31.449468: step 2396, loss 0.173111, acc 0.90625\n",
      "2017-11-06T23:00:35.732509: step 2397, loss 0.3204, acc 0.90625\n",
      "2017-11-06T23:00:39.815411: step 2398, loss 0.19393, acc 0.9375\n",
      "2017-11-06T23:00:44.180513: step 2399, loss 0.253641, acc 0.875\n",
      "2017-11-06T23:00:48.199368: step 2400, loss 0.192479, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:00:50.759187: step 2400, loss 0.980436, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2400\n",
      "\n",
      "2017-11-06T23:00:56.058659: step 2401, loss 0.283193, acc 0.875\n",
      "2017-11-06T23:00:59.990452: step 2402, loss 0.17794, acc 0.9375\n",
      "2017-11-06T23:01:03.947264: step 2403, loss 0.127684, acc 0.9375\n",
      "2017-11-06T23:01:07.991137: step 2404, loss 0.0206695, acc 1\n",
      "2017-11-06T23:01:12.054024: step 2405, loss 0.304986, acc 0.84375\n",
      "2017-11-06T23:01:16.034853: step 2406, loss 0.416843, acc 0.78125\n",
      "2017-11-06T23:01:20.052707: step 2407, loss 0.0437443, acc 1\n",
      "2017-11-06T23:01:24.063557: step 2408, loss 0.17774, acc 0.90625\n",
      "2017-11-06T23:01:28.082413: step 2409, loss 0.160037, acc 0.90625\n",
      "2017-11-06T23:01:32.174321: step 2410, loss 0.177856, acc 0.90625\n",
      "2017-11-06T23:01:36.333275: step 2411, loss 0.196035, acc 0.9375\n",
      "2017-11-06T23:01:38.993165: step 2412, loss 0.056133, acc 1\n",
      "2017-11-06T23:01:42.995008: step 2413, loss 0.107146, acc 0.9375\n",
      "2017-11-06T23:01:47.377123: step 2414, loss 0.15238, acc 0.90625\n",
      "2017-11-06T23:01:51.503055: step 2415, loss 0.19534, acc 0.875\n",
      "2017-11-06T23:01:55.522910: step 2416, loss 0.261546, acc 0.90625\n",
      "2017-11-06T23:01:59.532760: step 2417, loss 0.0955824, acc 0.96875\n",
      "2017-11-06T23:02:03.581636: step 2418, loss 0.239267, acc 0.90625\n",
      "2017-11-06T23:02:07.544452: step 2419, loss 0.456184, acc 0.875\n",
      "2017-11-06T23:02:11.509271: step 2420, loss 0.136651, acc 0.9375\n",
      "2017-11-06T23:02:15.475087: step 2421, loss 0.154576, acc 0.90625\n",
      "2017-11-06T23:02:19.449912: step 2422, loss 0.114903, acc 0.9375\n",
      "2017-11-06T23:02:23.385708: step 2423, loss 0.132449, acc 0.9375\n",
      "2017-11-06T23:02:27.420576: step 2424, loss 0.202881, acc 0.90625\n",
      "2017-11-06T23:02:31.397401: step 2425, loss 0.272765, acc 0.84375\n",
      "2017-11-06T23:02:35.624404: step 2426, loss 0.308224, acc 0.90625\n",
      "2017-11-06T23:02:39.585220: step 2427, loss 0.190489, acc 0.9375\n",
      "2017-11-06T23:02:43.584821: step 2428, loss 0.2283, acc 0.84375\n",
      "2017-11-06T23:02:47.598673: step 2429, loss 0.10696, acc 0.96875\n",
      "2017-11-06T23:02:51.792653: step 2430, loss 0.108921, acc 0.9375\n",
      "2017-11-06T23:02:56.074696: step 2431, loss 0.145332, acc 0.96875\n",
      "2017-11-06T23:03:00.032507: step 2432, loss 0.284242, acc 0.9375\n",
      "2017-11-06T23:03:03.984316: step 2433, loss 0.208673, acc 0.9375\n",
      "2017-11-06T23:03:07.972149: step 2434, loss 0.0449547, acc 0.96875\n",
      "2017-11-06T23:03:11.950976: step 2435, loss 0.174736, acc 0.9375\n",
      "2017-11-06T23:03:15.907789: step 2436, loss 0.259475, acc 0.9375\n",
      "2017-11-06T23:03:19.916636: step 2437, loss 0.575945, acc 0.8125\n",
      "2017-11-06T23:03:24.046571: step 2438, loss 0.0756434, acc 0.96875\n",
      "2017-11-06T23:03:28.190515: step 2439, loss 0.131193, acc 0.9375\n",
      "2017-11-06T23:03:32.197362: step 2440, loss 0.152335, acc 0.9375\n",
      "2017-11-06T23:03:36.191200: step 2441, loss 0.230635, acc 0.875\n",
      "2017-11-06T23:03:40.285109: step 2442, loss 0.337834, acc 0.90625\n",
      "2017-11-06T23:03:44.330984: step 2443, loss 0.0609573, acc 0.96875\n",
      "2017-11-06T23:03:48.523962: step 2444, loss 0.121334, acc 0.9375\n",
      "2017-11-06T23:03:52.605863: step 2445, loss 0.195777, acc 0.90625\n",
      "2017-11-06T23:03:56.908921: step 2446, loss 0.272496, acc 0.84375\n",
      "2017-11-06T23:04:01.358082: step 2447, loss 0.423198, acc 0.78125\n",
      "2017-11-06T23:04:03.938915: step 2448, loss 0.220596, acc 0.85\n",
      "2017-11-06T23:04:08.115884: step 2449, loss 0.114152, acc 0.90625\n",
      "2017-11-06T23:04:12.131737: step 2450, loss 0.0755381, acc 0.9375\n",
      "2017-11-06T23:04:16.368748: step 2451, loss 0.151111, acc 0.90625\n",
      "2017-11-06T23:04:20.367589: step 2452, loss 0.364713, acc 0.90625\n",
      "2017-11-06T23:04:24.574578: step 2453, loss 0.0756666, acc 0.96875\n",
      "2017-11-06T23:04:28.678494: step 2454, loss 0.10305, acc 0.9375\n",
      "2017-11-06T23:04:32.886484: step 2455, loss 0.225228, acc 0.90625\n",
      "2017-11-06T23:04:37.178534: step 2456, loss 0.168734, acc 0.96875\n",
      "2017-11-06T23:04:41.334487: step 2457, loss 0.00680305, acc 1\n",
      "2017-11-06T23:04:45.335329: step 2458, loss 0.291878, acc 0.875\n",
      "2017-11-06T23:04:49.483277: step 2459, loss 0.340958, acc 0.875\n",
      "2017-11-06T23:04:53.495128: step 2460, loss 0.153356, acc 0.9375\n",
      "2017-11-06T23:04:57.480960: step 2461, loss 0.127078, acc 0.9375\n",
      "2017-11-06T23:05:01.605891: step 2462, loss 0.156307, acc 0.9375\n",
      "2017-11-06T23:05:06.005018: step 2463, loss 0.167379, acc 0.90625\n",
      "2017-11-06T23:05:10.023872: step 2464, loss 0.135052, acc 0.96875\n",
      "2017-11-06T23:05:13.987688: step 2465, loss 0.282635, acc 0.90625\n",
      "2017-11-06T23:05:18.005543: step 2466, loss 0.0785061, acc 1\n",
      "2017-11-06T23:05:21.956350: step 2467, loss 0.129465, acc 0.9375\n",
      "2017-11-06T23:05:25.966200: step 2468, loss 0.0335751, acc 1\n",
      "2017-11-06T23:05:29.983055: step 2469, loss 0.118662, acc 0.96875\n",
      "2017-11-06T23:05:34.060039: step 2470, loss 0.103402, acc 0.9375\n",
      "2017-11-06T23:05:38.024856: step 2471, loss 0.265248, acc 0.90625\n",
      "2017-11-06T23:05:41.993421: step 2472, loss 0.255643, acc 0.90625\n",
      "2017-11-06T23:05:45.968245: step 2473, loss 0.0722981, acc 0.96875\n",
      "2017-11-06T23:05:50.018123: step 2474, loss 0.205052, acc 0.875\n",
      "2017-11-06T23:05:54.018966: step 2475, loss 0.155167, acc 0.9375\n",
      "2017-11-06T23:05:58.000796: step 2476, loss 0.0848156, acc 0.96875\n",
      "2017-11-06T23:06:01.948601: step 2477, loss 0.0330331, acc 0.96875\n",
      "2017-11-06T23:06:05.951443: step 2478, loss 0.247336, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T23:06:10.371585: step 2479, loss 0.35393, acc 0.875\n",
      "2017-11-06T23:06:14.447481: step 2480, loss 0.11445, acc 0.9375\n",
      "2017-11-06T23:06:18.474342: step 2481, loss 0.172691, acc 0.9375\n",
      "2017-11-06T23:06:22.474203: step 2482, loss 0.263446, acc 0.90625\n",
      "2017-11-06T23:06:26.460016: step 2483, loss 0.148236, acc 0.9375\n",
      "2017-11-06T23:06:29.056862: step 2484, loss 0.25551, acc 0.85\n",
      "2017-11-06T23:06:33.096732: step 2485, loss 0.27203, acc 0.90625\n",
      "2017-11-06T23:06:37.223664: step 2486, loss 0.121236, acc 0.9375\n",
      "2017-11-06T23:06:41.170469: step 2487, loss 0.0434067, acc 0.96875\n",
      "2017-11-06T23:06:45.114271: step 2488, loss 0.147363, acc 0.90625\n",
      "2017-11-06T23:06:49.118116: step 2489, loss 0.158139, acc 0.9375\n",
      "2017-11-06T23:06:53.079931: step 2490, loss 0.160408, acc 0.96875\n",
      "2017-11-06T23:06:57.104791: step 2491, loss 0.157984, acc 0.90625\n",
      "2017-11-06T23:07:01.094626: step 2492, loss 0.302049, acc 0.8125\n",
      "2017-11-06T23:07:05.061446: step 2493, loss 0.174448, acc 0.9375\n",
      "2017-11-06T23:07:09.049278: step 2494, loss 0.111587, acc 0.96875\n",
      "2017-11-06T23:07:13.139184: step 2495, loss 0.0963456, acc 1\n",
      "2017-11-06T23:07:17.543313: step 2496, loss 0.337466, acc 0.875\n",
      "2017-11-06T23:07:21.487115: step 2497, loss 0.223956, acc 0.875\n",
      "2017-11-06T23:07:25.485957: step 2498, loss 0.204298, acc 0.90625\n",
      "2017-11-06T23:07:29.502811: step 2499, loss 0.272841, acc 0.875\n",
      "2017-11-06T23:07:33.443611: step 2500, loss 0.0370525, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:07:35.991421: step 2500, loss 1.07329, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2500\n",
      "\n",
      "2017-11-06T23:07:41.453936: step 2501, loss 0.0183827, acc 1\n",
      "2017-11-06T23:07:45.408746: step 2502, loss 0.155116, acc 0.90625\n",
      "2017-11-06T23:07:49.402583: step 2503, loss 0.0993586, acc 0.96875\n",
      "2017-11-06T23:07:53.330374: step 2504, loss 0.0683988, acc 0.96875\n",
      "2017-11-06T23:07:57.304198: step 2505, loss 0.147318, acc 0.9375\n",
      "2017-11-06T23:08:01.253004: step 2506, loss 0.463353, acc 0.8125\n",
      "2017-11-06T23:08:05.253847: step 2507, loss 0.0567354, acc 0.96875\n",
      "2017-11-06T23:08:09.242681: step 2508, loss 0.413429, acc 0.84375\n",
      "2017-11-06T23:08:13.208499: step 2509, loss 0.151485, acc 0.9375\n",
      "2017-11-06T23:08:17.106269: step 2510, loss 0.0998436, acc 0.9375\n",
      "2017-11-06T23:08:21.475373: step 2511, loss 0.100645, acc 0.96875\n",
      "2017-11-06T23:08:25.763421: step 2512, loss 0.181741, acc 0.90625\n",
      "2017-11-06T23:08:29.785277: step 2513, loss 0.158036, acc 0.9375\n",
      "2017-11-06T23:08:34.019286: step 2514, loss 0.20926, acc 0.875\n",
      "2017-11-06T23:08:38.077169: step 2515, loss 0.188164, acc 0.90625\n",
      "2017-11-06T23:08:42.064812: step 2516, loss 0.158728, acc 0.9375\n",
      "2017-11-06T23:08:46.103683: step 2517, loss 0.0974933, acc 0.96875\n",
      "2017-11-06T23:08:50.055491: step 2518, loss 0.36318, acc 0.8125\n",
      "2017-11-06T23:08:54.053332: step 2519, loss 0.4516, acc 0.8125\n",
      "2017-11-06T23:08:56.662184: step 2520, loss 0.14832, acc 0.95\n",
      "2017-11-06T23:09:00.661026: step 2521, loss 0.0704257, acc 0.9375\n",
      "2017-11-06T23:09:04.622841: step 2522, loss 0.25047, acc 0.90625\n",
      "2017-11-06T23:09:08.624684: step 2523, loss 0.29173, acc 0.90625\n",
      "2017-11-06T23:09:12.637535: step 2524, loss 0.127353, acc 0.96875\n",
      "2017-11-06T23:09:16.567327: step 2525, loss 0.277078, acc 0.90625\n",
      "2017-11-06T23:09:20.590186: step 2526, loss 0.0082691, acc 1\n",
      "2017-11-06T23:09:24.762150: step 2527, loss 0.112125, acc 0.9375\n",
      "2017-11-06T23:09:29.135258: step 2528, loss 0.203688, acc 0.9375\n",
      "2017-11-06T23:09:33.155114: step 2529, loss 0.295162, acc 0.90625\n",
      "2017-11-06T23:09:37.122934: step 2530, loss 0.131911, acc 0.90625\n",
      "2017-11-06T23:09:41.166807: step 2531, loss 0.150783, acc 0.96875\n",
      "2017-11-06T23:09:45.141631: step 2532, loss 0.24106, acc 0.875\n",
      "2017-11-06T23:09:49.099444: step 2533, loss 0.143083, acc 0.9375\n",
      "2017-11-06T23:09:53.151322: step 2534, loss 0.0918111, acc 0.96875\n",
      "2017-11-06T23:09:57.187190: step 2535, loss 0.184176, acc 0.9375\n",
      "2017-11-06T23:10:01.547288: step 2536, loss 0.168902, acc 0.875\n",
      "2017-11-06T23:10:05.544129: step 2537, loss 0.440479, acc 0.8125\n",
      "2017-11-06T23:10:09.548973: step 2538, loss 0.12343, acc 0.96875\n",
      "2017-11-06T23:10:13.534805: step 2539, loss 0.317629, acc 0.84375\n",
      "2017-11-06T23:10:17.513633: step 2540, loss 0.0994241, acc 0.96875\n",
      "2017-11-06T23:10:21.502467: step 2541, loss 0.294312, acc 0.875\n",
      "2017-11-06T23:10:25.543337: step 2542, loss 0.267325, acc 0.84375\n",
      "2017-11-06T23:10:29.575202: step 2543, loss 0.132391, acc 0.9375\n",
      "2017-11-06T23:10:34.240518: step 2544, loss 0.184474, acc 0.9375\n",
      "2017-11-06T23:10:38.239377: step 2545, loss 0.0819691, acc 1\n",
      "2017-11-06T23:10:42.246207: step 2546, loss 0.323104, acc 0.875\n",
      "2017-11-06T23:10:46.255055: step 2547, loss 0.248418, acc 0.875\n",
      "2017-11-06T23:10:50.243892: step 2548, loss 0.149433, acc 0.96875\n",
      "2017-11-06T23:10:54.224736: step 2549, loss 0.212835, acc 0.875\n",
      "2017-11-06T23:10:58.253583: step 2550, loss 0.337385, acc 0.875\n",
      "2017-11-06T23:11:02.211392: step 2551, loss 0.169448, acc 0.90625\n",
      "2017-11-06T23:11:06.219241: step 2552, loss 0.263092, acc 0.9375\n",
      "2017-11-06T23:11:10.229089: step 2553, loss 0.0812147, acc 0.96875\n",
      "2017-11-06T23:11:14.264957: step 2554, loss 0.162254, acc 0.9375\n",
      "2017-11-06T23:11:18.286814: step 2555, loss 0.291667, acc 0.84375\n",
      "2017-11-06T23:11:20.956712: step 2556, loss 0.257042, acc 0.8\n",
      "2017-11-06T23:11:24.954552: step 2557, loss 0.159314, acc 0.90625\n",
      "2017-11-06T23:11:29.006431: step 2558, loss 0.11779, acc 0.9375\n",
      "2017-11-06T23:11:33.053307: step 2559, loss 0.10657, acc 0.96875\n",
      "2017-11-06T23:11:37.421410: step 2560, loss 0.200986, acc 0.90625\n",
      "2017-11-06T23:11:41.688201: step 2561, loss 0.136541, acc 0.9375\n",
      "2017-11-06T23:11:45.689042: step 2562, loss 0.159851, acc 0.9375\n",
      "2017-11-06T23:11:49.744924: step 2563, loss 0.226718, acc 0.9375\n",
      "2017-11-06T23:11:53.758776: step 2564, loss 0.0640873, acc 1\n",
      "2017-11-06T23:11:57.809654: step 2565, loss 0.20594, acc 0.90625\n",
      "2017-11-06T23:12:01.793486: step 2566, loss 0.227253, acc 0.90625\n",
      "2017-11-06T23:12:05.800332: step 2567, loss 0.0640815, acc 0.96875\n",
      "2017-11-06T23:12:09.862218: step 2568, loss 0.301146, acc 0.8125\n",
      "2017-11-06T23:12:13.902089: step 2569, loss 0.135506, acc 0.90625\n",
      "2017-11-06T23:12:17.957971: step 2570, loss 0.149146, acc 0.96875\n",
      "2017-11-06T23:12:21.944804: step 2571, loss 0.179009, acc 0.875\n",
      "2017-11-06T23:12:26.032710: step 2572, loss 0.401844, acc 0.875\n",
      "2017-11-06T23:12:30.096596: step 2573, loss 0.046931, acc 1\n",
      "2017-11-06T23:12:34.406658: step 2574, loss 0.0900562, acc 0.9375\n",
      "2017-11-06T23:12:38.445528: step 2575, loss 0.303421, acc 0.84375\n",
      "2017-11-06T23:12:42.836647: step 2576, loss 0.132938, acc 0.90625\n",
      "2017-11-06T23:12:47.082665: step 2577, loss 0.0486097, acc 0.96875\n",
      "2017-11-06T23:12:51.103524: step 2578, loss 0.214923, acc 0.8125\n",
      "2017-11-06T23:12:55.092356: step 2579, loss 0.197177, acc 0.90625\n",
      "2017-11-06T23:12:59.078189: step 2580, loss 0.201695, acc 0.90625\n",
      "2017-11-06T23:13:03.083035: step 2581, loss 0.0574321, acc 0.96875\n",
      "2017-11-06T23:13:07.063863: step 2582, loss 0.13606, acc 0.9375\n",
      "2017-11-06T23:13:11.113741: step 2583, loss 0.0809177, acc 0.96875\n",
      "2017-11-06T23:13:15.131595: step 2584, loss 0.40453, acc 0.90625\n",
      "2017-11-06T23:13:19.103417: step 2585, loss 0.317071, acc 0.9375\n",
      "2017-11-06T23:13:23.209335: step 2586, loss 0.620735, acc 0.875\n",
      "2017-11-06T23:13:27.653492: step 2587, loss 0.193094, acc 0.90625\n",
      "2017-11-06T23:13:31.684356: step 2588, loss 0.13443, acc 0.9375\n",
      "2017-11-06T23:13:35.790274: step 2589, loss 0.0867106, acc 0.96875\n",
      "2017-11-06T23:13:39.815133: step 2590, loss 0.10131, acc 0.96875\n",
      "2017-11-06T23:13:43.876019: step 2591, loss 0.0763576, acc 0.96875\n",
      "2017-11-06T23:13:46.637981: step 2592, loss 0.398235, acc 0.85\n",
      "2017-11-06T23:13:51.120166: step 2593, loss 0.30704, acc 0.84375\n",
      "2017-11-06T23:13:55.171046: step 2594, loss 0.034322, acc 1\n",
      "2017-11-06T23:13:59.171887: step 2595, loss 0.148844, acc 0.90625\n",
      "2017-11-06T23:14:03.202752: step 2596, loss 0.223763, acc 0.90625\n",
      "2017-11-06T23:14:07.154559: step 2597, loss 0.203038, acc 0.875\n",
      "2017-11-06T23:14:11.139391: step 2598, loss 0.155509, acc 0.9375\n",
      "2017-11-06T23:14:15.187267: step 2599, loss 0.141106, acc 0.875\n",
      "2017-11-06T23:14:19.149082: step 2600, loss 0.193859, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:14:21.697893: step 2600, loss 1.02407, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2600\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T23:14:27.120763: step 2601, loss 0.118573, acc 0.96875\n",
      "2017-11-06T23:14:31.153628: step 2602, loss 0.252661, acc 0.90625\n",
      "2017-11-06T23:14:35.414655: step 2603, loss 0.133946, acc 0.96875\n",
      "2017-11-06T23:14:39.460530: step 2604, loss 0.329134, acc 0.84375\n",
      "2017-11-06T23:14:43.479167: step 2605, loss 0.197175, acc 0.875\n",
      "2017-11-06T23:14:47.479008: step 2606, loss 0.150486, acc 0.9375\n",
      "2017-11-06T23:14:51.498864: step 2607, loss 0.109334, acc 0.96875\n",
      "2017-11-06T23:14:56.000063: step 2608, loss 0.138227, acc 0.96875\n",
      "2017-11-06T23:15:00.018919: step 2609, loss 0.195629, acc 0.90625\n",
      "2017-11-06T23:15:04.021763: step 2610, loss 0.200956, acc 0.875\n",
      "2017-11-06T23:15:08.011597: step 2611, loss 0.0955493, acc 0.9375\n",
      "2017-11-06T23:15:12.068480: step 2612, loss 0.0335036, acc 0.96875\n",
      "2017-11-06T23:15:16.066320: step 2613, loss 0.375129, acc 0.875\n",
      "2017-11-06T23:15:20.125205: step 2614, loss 0.231126, acc 0.875\n",
      "2017-11-06T23:15:24.228120: step 2615, loss 0.26899, acc 0.90625\n",
      "2017-11-06T23:15:28.250980: step 2616, loss 0.14184, acc 0.9375\n",
      "2017-11-06T23:15:32.270835: step 2617, loss 0.301154, acc 0.8125\n",
      "2017-11-06T23:15:36.262671: step 2618, loss 0.0599268, acc 0.96875\n",
      "2017-11-06T23:15:40.318553: step 2619, loss 0.14662, acc 0.9375\n",
      "2017-11-06T23:15:44.295381: step 2620, loss 0.448161, acc 0.84375\n",
      "2017-11-06T23:15:48.292218: step 2621, loss 0.0989452, acc 0.9375\n",
      "2017-11-06T23:15:52.272046: step 2622, loss 0.301695, acc 0.875\n",
      "2017-11-06T23:15:56.274890: step 2623, loss 0.0196384, acc 1\n",
      "2017-11-06T23:16:00.757075: step 2624, loss 0.171177, acc 0.9375\n",
      "2017-11-06T23:16:04.873000: step 2625, loss 0.525648, acc 0.75\n",
      "2017-11-06T23:16:08.855830: step 2626, loss 0.296524, acc 0.90625\n",
      "2017-11-06T23:16:12.905707: step 2627, loss 0.0895911, acc 0.9375\n",
      "2017-11-06T23:16:15.420495: step 2628, loss 0.226359, acc 0.85\n",
      "2017-11-06T23:16:19.459364: step 2629, loss 0.197212, acc 0.90625\n",
      "2017-11-06T23:16:23.466212: step 2630, loss 0.248031, acc 0.90625\n",
      "2017-11-06T23:16:27.451043: step 2631, loss 0.0681867, acc 0.9375\n",
      "2017-11-06T23:16:31.478904: step 2632, loss 0.220612, acc 0.875\n",
      "2017-11-06T23:16:35.781962: step 2633, loss 0.0501149, acc 0.96875\n",
      "2017-11-06T23:16:39.897886: step 2634, loss 0.093107, acc 0.96875\n",
      "2017-11-06T23:16:43.977785: step 2635, loss 0.192775, acc 0.9375\n",
      "2017-11-06T23:16:48.018657: step 2636, loss 0.13447, acc 0.96875\n",
      "2017-11-06T23:16:52.010493: step 2637, loss 0.0279885, acc 1\n",
      "2017-11-06T23:16:56.077385: step 2638, loss 0.126227, acc 0.9375\n",
      "2017-11-06T23:17:00.220326: step 2639, loss 0.0816974, acc 0.96875\n",
      "2017-11-06T23:17:04.530389: step 2640, loss 0.0830749, acc 0.96875\n",
      "2017-11-06T23:17:08.828443: step 2641, loss 0.0944593, acc 0.96875\n",
      "2017-11-06T23:17:12.849300: step 2642, loss 0.0129769, acc 1\n",
      "2017-11-06T23:17:16.825125: step 2643, loss 0.14825, acc 0.875\n",
      "2017-11-06T23:17:20.857991: step 2644, loss 0.247936, acc 0.90625\n",
      "2017-11-06T23:17:24.844823: step 2645, loss 0.213527, acc 0.90625\n",
      "2017-11-06T23:17:28.870684: step 2646, loss 0.140932, acc 0.96875\n",
      "2017-11-06T23:17:32.916559: step 2647, loss 0.314379, acc 0.90625\n",
      "2017-11-06T23:17:36.950425: step 2648, loss 0.193955, acc 0.84375\n",
      "2017-11-06T23:17:41.038330: step 2649, loss 0.355484, acc 0.875\n",
      "2017-11-06T23:17:45.032932: step 2650, loss 0.196213, acc 0.9375\n",
      "2017-11-06T23:17:49.018765: step 2651, loss 0.311708, acc 0.875\n",
      "2017-11-06T23:17:53.022610: step 2652, loss 0.242393, acc 0.90625\n",
      "2017-11-06T23:17:57.001436: step 2653, loss 0.132353, acc 0.9375\n",
      "2017-11-06T23:18:01.053316: step 2654, loss 0.144814, acc 0.90625\n",
      "2017-11-06T23:18:04.980105: step 2655, loss 0.259354, acc 0.9375\n",
      "2017-11-06T23:18:09.161076: step 2656, loss 0.134665, acc 0.9375\n",
      "2017-11-06T23:18:13.598229: step 2657, loss 0.364394, acc 0.84375\n",
      "2017-11-06T23:18:17.573053: step 2658, loss 0.186864, acc 0.90625\n",
      "2017-11-06T23:18:21.645947: step 2659, loss 0.23366, acc 0.875\n",
      "2017-11-06T23:18:25.804904: step 2660, loss 0.213098, acc 0.90625\n",
      "2017-11-06T23:18:29.798739: step 2661, loss 0.129301, acc 0.90625\n",
      "2017-11-06T23:18:34.017737: step 2662, loss 0.119514, acc 0.96875\n",
      "2017-11-06T23:18:38.120655: step 2663, loss 0.220198, acc 0.875\n",
      "2017-11-06T23:18:40.680473: step 2664, loss 0.115826, acc 0.95\n",
      "2017-11-06T23:18:44.730350: step 2665, loss 0.0119925, acc 1\n",
      "2017-11-06T23:18:48.712179: step 2666, loss 0.0610412, acc 0.96875\n",
      "2017-11-06T23:18:52.721027: step 2667, loss 0.0374455, acc 1\n",
      "2017-11-06T23:18:56.688847: step 2668, loss 0.141755, acc 0.9375\n",
      "2017-11-06T23:19:00.737723: step 2669, loss 0.0131316, acc 1\n",
      "2017-11-06T23:19:04.745571: step 2670, loss 0.269049, acc 0.9375\n",
      "2017-11-06T23:19:08.775434: step 2671, loss 0.096355, acc 0.9375\n",
      "2017-11-06T23:19:12.738250: step 2672, loss 0.233705, acc 0.9375\n",
      "2017-11-06T23:19:17.110356: step 2673, loss 0.0787761, acc 0.96875\n",
      "2017-11-06T23:19:21.320348: step 2674, loss 0.349889, acc 0.78125\n",
      "2017-11-06T23:19:25.375229: step 2675, loss 0.131756, acc 0.9375\n",
      "2017-11-06T23:19:29.328039: step 2676, loss 0.0509618, acc 1\n",
      "2017-11-06T23:19:33.360903: step 2677, loss 0.199379, acc 0.90625\n",
      "2017-11-06T23:19:37.359744: step 2678, loss 0.151008, acc 0.90625\n",
      "2017-11-06T23:19:41.357585: step 2679, loss 0.12412, acc 0.90625\n",
      "2017-11-06T23:19:45.406463: step 2680, loss 0.265026, acc 0.875\n",
      "2017-11-06T23:19:49.422315: step 2681, loss 0.242684, acc 0.90625\n",
      "2017-11-06T23:19:53.418155: step 2682, loss 0.235743, acc 0.875\n",
      "2017-11-06T23:19:57.464030: step 2683, loss 0.180238, acc 0.875\n",
      "2017-11-06T23:20:01.752076: step 2684, loss 0.10644, acc 0.96875\n",
      "2017-11-06T23:20:05.796950: step 2685, loss 0.219381, acc 0.875\n",
      "2017-11-06T23:20:09.797793: step 2686, loss 0.290457, acc 0.84375\n",
      "2017-11-06T23:20:13.841668: step 2687, loss 0.117606, acc 0.9375\n",
      "2017-11-06T23:20:17.888542: step 2688, loss 0.102024, acc 0.9375\n",
      "2017-11-06T23:20:22.202607: step 2689, loss 0.123377, acc 0.90625\n",
      "2017-11-06T23:20:26.530682: step 2690, loss 0.200114, acc 0.875\n",
      "2017-11-06T23:20:30.545535: step 2691, loss 0.273873, acc 0.875\n",
      "2017-11-06T23:20:34.874612: step 2692, loss 0.128531, acc 0.9375\n",
      "2017-11-06T23:20:39.009550: step 2693, loss 0.393436, acc 0.84375\n",
      "2017-11-06T23:20:43.053202: step 2694, loss 0.338789, acc 0.875\n",
      "2017-11-06T23:20:47.135103: step 2695, loss 0.091285, acc 0.96875\n",
      "2017-11-06T23:20:51.087912: step 2696, loss 0.145071, acc 0.9375\n",
      "2017-11-06T23:20:55.120777: step 2697, loss 0.102355, acc 0.9375\n",
      "2017-11-06T23:20:59.156646: step 2698, loss 0.0729015, acc 0.96875\n",
      "2017-11-06T23:21:03.251555: step 2699, loss 0.359758, acc 0.875\n",
      "2017-11-06T23:21:05.884425: step 2700, loss 0.136125, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:21:08.511292: step 2700, loss 0.955453, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2700\n",
      "\n",
      "2017-11-06T23:21:13.759642: step 2701, loss 0.0794936, acc 0.96875\n",
      "2017-11-06T23:21:17.805515: step 2702, loss 0.148825, acc 0.875\n",
      "2017-11-06T23:21:21.905428: step 2703, loss 0.0604426, acc 0.96875\n",
      "2017-11-06T23:21:26.088401: step 2704, loss 0.18477, acc 0.90625\n",
      "2017-11-06T23:21:30.530557: step 2705, loss 0.0309847, acc 1\n",
      "2017-11-06T23:21:34.602451: step 2706, loss 0.142049, acc 0.90625\n",
      "2017-11-06T23:21:38.638318: step 2707, loss 0.124376, acc 0.96875\n",
      "2017-11-06T23:21:42.718217: step 2708, loss 0.215151, acc 0.9375\n",
      "2017-11-06T23:21:46.829138: step 2709, loss 0.105135, acc 0.9375\n",
      "2017-11-06T23:21:50.917042: step 2710, loss 0.120648, acc 0.9375\n",
      "2017-11-06T23:21:54.949909: step 2711, loss 0.179304, acc 0.90625\n",
      "2017-11-06T23:21:59.039816: step 2712, loss 0.252199, acc 0.875\n",
      "2017-11-06T23:22:03.023645: step 2713, loss 0.395611, acc 0.84375\n",
      "2017-11-06T23:22:07.006475: step 2714, loss 0.135259, acc 0.90625\n",
      "2017-11-06T23:22:10.974295: step 2715, loss 0.0373916, acc 1\n",
      "2017-11-06T23:22:14.907089: step 2716, loss 0.240967, acc 0.875\n",
      "2017-11-06T23:22:18.833878: step 2717, loss 0.264252, acc 0.875\n",
      "2017-11-06T23:22:22.778681: step 2718, loss 0.281567, acc 0.875\n",
      "2017-11-06T23:22:26.763513: step 2719, loss 0.210298, acc 0.90625\n",
      "2017-11-06T23:22:30.716321: step 2720, loss 0.101594, acc 0.9375\n",
      "2017-11-06T23:22:35.448684: step 2721, loss 0.0685897, acc 0.96875\n",
      "2017-11-06T23:22:39.496560: step 2722, loss 0.190058, acc 0.9375\n",
      "2017-11-06T23:22:43.442364: step 2723, loss 0.255604, acc 0.9375\n",
      "2017-11-06T23:22:47.495243: step 2724, loss 0.125257, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T23:22:51.520103: step 2725, loss 0.354749, acc 0.84375\n",
      "2017-11-06T23:22:55.540960: step 2726, loss 0.104722, acc 0.90625\n",
      "2017-11-06T23:22:59.495771: step 2727, loss 0.257445, acc 0.90625\n",
      "2017-11-06T23:23:03.451581: step 2728, loss 0.128688, acc 0.90625\n",
      "2017-11-06T23:23:07.419401: step 2729, loss 0.328558, acc 0.875\n",
      "2017-11-06T23:23:11.374212: step 2730, loss 0.256208, acc 0.90625\n",
      "2017-11-06T23:23:15.316011: step 2731, loss 0.254392, acc 0.8125\n",
      "2017-11-06T23:23:19.242802: step 2732, loss 0.166568, acc 0.90625\n",
      "2017-11-06T23:23:23.326703: step 2733, loss 0.153142, acc 0.90625\n",
      "2017-11-06T23:23:27.649775: step 2734, loss 0.0686141, acc 0.96875\n",
      "2017-11-06T23:23:31.558552: step 2735, loss 0.228514, acc 0.90625\n",
      "2017-11-06T23:23:34.133382: step 2736, loss 0.273407, acc 0.85\n",
      "2017-11-06T23:23:38.237298: step 2737, loss 0.199054, acc 0.9375\n",
      "2017-11-06T23:23:42.542358: step 2738, loss 0.0132072, acc 1\n",
      "2017-11-06T23:23:46.493938: step 2739, loss 0.0609268, acc 0.96875\n",
      "2017-11-06T23:23:50.533917: step 2740, loss 0.113952, acc 0.96875\n",
      "2017-11-06T23:23:54.459707: step 2741, loss 0.175734, acc 0.875\n",
      "2017-11-06T23:23:58.425524: step 2742, loss 0.16484, acc 0.90625\n",
      "2017-11-06T23:24:02.345310: step 2743, loss 0.165965, acc 0.875\n",
      "2017-11-06T23:24:06.244080: step 2744, loss 0.16859, acc 0.9375\n",
      "2017-11-06T23:24:10.163867: step 2745, loss 0.174325, acc 0.875\n",
      "2017-11-06T23:24:14.089654: step 2746, loss 0.150418, acc 0.90625\n",
      "2017-11-06T23:24:18.025451: step 2747, loss 0.298794, acc 0.875\n",
      "2017-11-06T23:24:21.963249: step 2748, loss 0.113688, acc 0.9375\n",
      "2017-11-06T23:24:25.989110: step 2749, loss 0.104051, acc 0.96875\n",
      "2017-11-06T23:24:29.916900: step 2750, loss 0.0894606, acc 0.96875\n",
      "2017-11-06T23:24:34.068851: step 2751, loss 0.120724, acc 0.9375\n",
      "2017-11-06T23:24:38.114725: step 2752, loss 0.082681, acc 0.96875\n",
      "2017-11-06T23:24:42.090550: step 2753, loss 0.228475, acc 0.90625\n",
      "2017-11-06T23:24:46.524701: step 2754, loss 0.172254, acc 0.875\n",
      "2017-11-06T23:24:50.594593: step 2755, loss 0.116253, acc 0.96875\n",
      "2017-11-06T23:24:54.565414: step 2756, loss 0.173035, acc 0.875\n",
      "2017-11-06T23:24:58.509216: step 2757, loss 0.35673, acc 0.8125\n",
      "2017-11-06T23:25:02.456021: step 2758, loss 0.0740772, acc 0.96875\n",
      "2017-11-06T23:25:06.443855: step 2759, loss 0.10961, acc 0.9375\n",
      "2017-11-06T23:25:10.411674: step 2760, loss 0.151936, acc 0.9375\n",
      "2017-11-06T23:25:14.341466: step 2761, loss 0.291555, acc 0.875\n",
      "2017-11-06T23:25:18.313289: step 2762, loss 0.0521695, acc 1\n",
      "2017-11-06T23:25:22.285111: step 2763, loss 0.304063, acc 0.875\n",
      "2017-11-06T23:25:26.274946: step 2764, loss 0.456109, acc 0.8125\n",
      "2017-11-06T23:25:30.218749: step 2765, loss 0.114205, acc 0.96875\n",
      "2017-11-06T23:25:34.141535: step 2766, loss 0.144457, acc 0.9375\n",
      "2017-11-06T23:25:38.107355: step 2767, loss 0.159708, acc 0.875\n",
      "2017-11-06T23:25:42.035143: step 2768, loss 0.13985, acc 0.96875\n",
      "2017-11-06T23:25:45.951926: step 2769, loss 0.0859757, acc 0.96875\n",
      "2017-11-06T23:25:50.223962: step 2770, loss 0.349333, acc 0.90625\n",
      "2017-11-06T23:25:54.495023: step 2771, loss 0.260029, acc 0.875\n",
      "2017-11-06T23:25:57.038832: step 2772, loss 0.0386508, acc 1\n",
      "2017-11-06T23:26:00.965639: step 2773, loss 0.0700109, acc 0.96875\n",
      "2017-11-06T23:26:05.023503: step 2774, loss 0.098101, acc 0.9375\n",
      "2017-11-06T23:26:09.034353: step 2775, loss 0.0901352, acc 0.96875\n",
      "2017-11-06T23:26:13.000171: step 2776, loss 0.106618, acc 0.96875\n",
      "2017-11-06T23:26:17.006018: step 2777, loss 0.182049, acc 0.9375\n",
      "2017-11-06T23:26:20.920799: step 2778, loss 0.343229, acc 0.875\n",
      "2017-11-06T23:26:24.940655: step 2779, loss 0.169876, acc 0.90625\n",
      "2017-11-06T23:26:28.954507: step 2780, loss 0.158102, acc 0.90625\n",
      "2017-11-06T23:26:33.089445: step 2781, loss 0.151597, acc 0.875\n",
      "2017-11-06T23:26:37.199365: step 2782, loss 0.234681, acc 0.9375\n",
      "2017-11-06T23:26:41.161180: step 2783, loss 0.0714499, acc 0.96875\n",
      "2017-11-06T23:26:45.102751: step 2784, loss 0.139092, acc 0.90625\n",
      "2017-11-06T23:26:49.055560: step 2785, loss 0.135174, acc 0.90625\n",
      "2017-11-06T23:26:53.017374: step 2786, loss 0.0586752, acc 0.96875\n",
      "2017-11-06T23:26:57.457528: step 2787, loss 0.0401738, acc 1\n",
      "2017-11-06T23:27:01.575455: step 2788, loss 0.265953, acc 0.9375\n",
      "2017-11-06T23:27:05.534268: step 2789, loss 0.222572, acc 0.90625\n",
      "2017-11-06T23:27:09.489080: step 2790, loss 0.11108, acc 0.96875\n",
      "2017-11-06T23:27:13.402858: step 2791, loss 0.242726, acc 0.875\n",
      "2017-11-06T23:27:17.390692: step 2792, loss 0.212765, acc 0.90625\n",
      "2017-11-06T23:27:21.322486: step 2793, loss 0.226052, acc 0.90625\n",
      "2017-11-06T23:27:25.290305: step 2794, loss 0.1565, acc 0.9375\n",
      "2017-11-06T23:27:29.262127: step 2795, loss 0.281801, acc 0.90625\n",
      "2017-11-06T23:27:33.216937: step 2796, loss 0.209449, acc 0.90625\n",
      "2017-11-06T23:27:37.196765: step 2797, loss 0.241394, acc 0.84375\n",
      "2017-11-06T23:27:41.225628: step 2798, loss 0.307209, acc 0.84375\n",
      "2017-11-06T23:27:45.182439: step 2799, loss 0.237822, acc 0.90625\n",
      "2017-11-06T23:27:49.140252: step 2800, loss 0.0567389, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:27:51.700070: step 2800, loss 0.964452, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2800\n",
      "\n",
      "2017-11-06T23:27:57.114810: step 2801, loss 0.264557, acc 0.90625\n",
      "2017-11-06T23:28:01.403935: step 2802, loss 0.129473, acc 0.9375\n",
      "2017-11-06T23:28:05.616929: step 2803, loss 0.165315, acc 0.90625\n",
      "2017-11-06T23:28:09.562732: step 2804, loss 0.155667, acc 0.9375\n",
      "2017-11-06T23:28:13.465525: step 2805, loss 0.0754304, acc 1\n",
      "2017-11-06T23:28:17.444333: step 2806, loss 0.177308, acc 0.9375\n",
      "2017-11-06T23:28:21.421159: step 2807, loss 0.159117, acc 0.875\n",
      "2017-11-06T23:28:24.231155: step 2808, loss 0.513872, acc 0.8\n",
      "2017-11-06T23:28:28.229996: step 2809, loss 0.16699, acc 0.9375\n",
      "2017-11-06T23:28:32.128768: step 2810, loss 0.0829968, acc 0.9375\n",
      "2017-11-06T23:28:36.346764: step 2811, loss 0.265139, acc 0.84375\n",
      "2017-11-06T23:28:40.307579: step 2812, loss 0.0810081, acc 0.96875\n",
      "2017-11-06T23:28:44.230365: step 2813, loss 0.135306, acc 0.96875\n",
      "2017-11-06T23:28:48.182174: step 2814, loss 0.0852139, acc 0.9375\n",
      "2017-11-06T23:28:52.134982: step 2815, loss 0.24614, acc 0.90625\n",
      "2017-11-06T23:28:56.141829: step 2816, loss 0.326094, acc 0.84375\n",
      "2017-11-06T23:29:00.078627: step 2817, loss 0.118086, acc 0.9375\n",
      "2017-11-06T23:29:04.042443: step 2818, loss 0.0955559, acc 0.9375\n",
      "2017-11-06T23:29:08.381526: step 2819, loss 0.0343055, acc 1\n",
      "2017-11-06T23:29:12.518466: step 2820, loss 0.164954, acc 0.9375\n",
      "2017-11-06T23:29:16.476277: step 2821, loss 0.0808657, acc 0.96875\n",
      "2017-11-06T23:29:20.438093: step 2822, loss 0.259928, acc 0.875\n",
      "2017-11-06T23:29:24.434935: step 2823, loss 0.273407, acc 0.8125\n",
      "2017-11-06T23:29:28.385739: step 2824, loss 0.310287, acc 0.875\n",
      "2017-11-06T23:29:32.345553: step 2825, loss 0.0746826, acc 0.96875\n",
      "2017-11-06T23:29:36.280349: step 2826, loss 0.181449, acc 0.875\n",
      "2017-11-06T23:29:40.225152: step 2827, loss 0.0707456, acc 0.9375\n",
      "2017-11-06T23:29:44.176712: step 2828, loss 0.388207, acc 0.8125\n",
      "2017-11-06T23:29:48.159541: step 2829, loss 0.120521, acc 0.96875\n",
      "2017-11-06T23:29:52.102342: step 2830, loss 0.142268, acc 0.9375\n",
      "2017-11-06T23:29:56.112191: step 2831, loss 0.144328, acc 0.9375\n",
      "2017-11-06T23:30:00.093020: step 2832, loss 0.154282, acc 0.96875\n",
      "2017-11-06T23:30:04.270990: step 2833, loss 0.230023, acc 0.90625\n",
      "2017-11-06T23:30:08.275833: step 2834, loss 0.278223, acc 0.875\n",
      "2017-11-06T23:30:12.435790: step 2835, loss 0.264391, acc 0.875\n",
      "2017-11-06T23:30:16.673801: step 2836, loss 0.116135, acc 0.9375\n",
      "2017-11-06T23:30:20.619605: step 2837, loss 0.0473734, acc 0.96875\n",
      "2017-11-06T23:30:24.592428: step 2838, loss 0.0645806, acc 0.96875\n",
      "2017-11-06T23:30:28.587266: step 2839, loss 0.0937402, acc 0.9375\n",
      "2017-11-06T23:30:32.587108: step 2840, loss 0.262402, acc 0.9375\n",
      "2017-11-06T23:30:36.762074: step 2841, loss 0.274374, acc 0.9375\n",
      "2017-11-06T23:30:40.724890: step 2842, loss 0.0255975, acc 1\n",
      "2017-11-06T23:30:44.629666: step 2843, loss 0.194705, acc 0.875\n",
      "2017-11-06T23:30:47.184481: step 2844, loss 0.175615, acc 0.9\n",
      "2017-11-06T23:30:51.158303: step 2845, loss 0.218678, acc 0.84375\n",
      "2017-11-06T23:30:55.109110: step 2846, loss 0.245133, acc 0.90625\n",
      "2017-11-06T23:30:59.086939: step 2847, loss 0.0437713, acc 0.96875\n",
      "2017-11-06T23:31:03.063763: step 2848, loss 0.293418, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T23:31:07.027579: step 2849, loss 0.124032, acc 0.9375\n",
      "2017-11-06T23:31:10.988394: step 2850, loss 0.197537, acc 0.9375\n",
      "2017-11-06T23:31:14.967222: step 2851, loss 0.129636, acc 0.90625\n",
      "2017-11-06T23:31:19.238255: step 2852, loss 0.174192, acc 0.90625\n",
      "2017-11-06T23:31:23.435237: step 2853, loss 0.0724144, acc 0.96875\n",
      "2017-11-06T23:31:27.368032: step 2854, loss 0.0764063, acc 1\n",
      "2017-11-06T23:31:31.345859: step 2855, loss 0.0671577, acc 1\n",
      "2017-11-06T23:31:35.410747: step 2856, loss 0.156775, acc 0.90625\n",
      "2017-11-06T23:31:39.503655: step 2857, loss 0.15989, acc 0.9375\n",
      "2017-11-06T23:31:43.543528: step 2858, loss 0.318261, acc 0.90625\n",
      "2017-11-06T23:31:47.497335: step 2859, loss 0.205541, acc 0.90625\n",
      "2017-11-06T23:31:51.536204: step 2860, loss 0.1317, acc 0.9375\n",
      "2017-11-06T23:31:55.508026: step 2861, loss 0.167137, acc 0.90625\n",
      "2017-11-06T23:31:59.605938: step 2862, loss 0.126924, acc 0.96875\n",
      "2017-11-06T23:32:03.539733: step 2863, loss 0.0815721, acc 1\n",
      "2017-11-06T23:32:07.551584: step 2864, loss 0.0145828, acc 1\n",
      "2017-11-06T23:32:11.542419: step 2865, loss 0.130742, acc 0.90625\n",
      "2017-11-06T23:32:15.568281: step 2866, loss 0.158733, acc 0.9375\n",
      "2017-11-06T23:32:19.555115: step 2867, loss 0.286224, acc 0.875\n",
      "2017-11-06T23:32:23.820144: step 2868, loss 0.144054, acc 0.90625\n",
      "2017-11-06T23:32:28.144218: step 2869, loss 0.110951, acc 0.9375\n",
      "2017-11-06T23:32:32.110034: step 2870, loss 0.144019, acc 0.875\n",
      "2017-11-06T23:32:36.368060: step 2871, loss 0.25379, acc 0.875\n",
      "2017-11-06T23:32:40.357894: step 2872, loss 0.0916561, acc 0.9375\n",
      "2017-11-06T23:32:44.364522: step 2873, loss 0.130381, acc 0.90625\n",
      "2017-11-06T23:32:48.355360: step 2874, loss 0.0759422, acc 0.96875\n",
      "2017-11-06T23:32:52.277144: step 2875, loss 0.104092, acc 0.96875\n",
      "2017-11-06T23:32:56.296001: step 2876, loss 0.0328926, acc 0.96875\n",
      "2017-11-06T23:33:00.269823: step 2877, loss 0.115222, acc 0.90625\n",
      "2017-11-06T23:33:04.316698: step 2878, loss 0.147579, acc 0.9375\n",
      "2017-11-06T23:33:08.340557: step 2879, loss 0.180681, acc 0.875\n",
      "2017-11-06T23:33:10.854344: step 2880, loss 0.265943, acc 0.9\n",
      "2017-11-06T23:33:14.798146: step 2881, loss 0.04163, acc 0.96875\n",
      "2017-11-06T23:33:18.885213: step 2882, loss 0.148889, acc 0.9375\n",
      "2017-11-06T23:33:22.999134: step 2883, loss 0.433014, acc 0.84375\n",
      "2017-11-06T23:33:27.374243: step 2884, loss 0.203938, acc 0.875\n",
      "2017-11-06T23:33:31.803390: step 2885, loss 0.0538874, acc 0.96875\n",
      "2017-11-06T23:33:35.798229: step 2886, loss 0.107045, acc 0.9375\n",
      "2017-11-06T23:33:39.810080: step 2887, loss 0.223275, acc 0.90625\n",
      "2017-11-06T23:33:43.976040: step 2888, loss 0.291753, acc 0.8125\n",
      "2017-11-06T23:33:48.198040: step 2889, loss 0.237789, acc 0.90625\n",
      "2017-11-06T23:33:52.275937: step 2890, loss 0.16465, acc 0.90625\n",
      "2017-11-06T23:33:56.434892: step 2891, loss 0.104705, acc 0.9375\n",
      "2017-11-06T23:34:00.582840: step 2892, loss 0.161571, acc 0.90625\n",
      "2017-11-06T23:34:04.816847: step 2893, loss 0.06629, acc 0.96875\n",
      "2017-11-06T23:34:08.939778: step 2894, loss 0.283689, acc 0.90625\n",
      "2017-11-06T23:34:13.102735: step 2895, loss 0.0315697, acc 1\n",
      "2017-11-06T23:34:17.155615: step 2896, loss 0.155857, acc 0.9375\n",
      "2017-11-06T23:34:21.309566: step 2897, loss 0.211918, acc 0.90625\n",
      "2017-11-06T23:34:25.482532: step 2898, loss 0.0885312, acc 0.96875\n",
      "2017-11-06T23:34:29.663502: step 2899, loss 0.266937, acc 0.875\n",
      "2017-11-06T23:34:34.054623: step 2900, loss 0.103479, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:34:37.323945: step 2900, loss 1.04518, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-2900\n",
      "\n",
      "2017-11-06T23:34:42.603634: step 2901, loss 0.244824, acc 0.90625\n",
      "2017-11-06T23:34:46.667521: step 2902, loss 0.293413, acc 0.90625\n",
      "2017-11-06T23:34:50.629336: step 2903, loss 0.366312, acc 0.84375\n",
      "2017-11-06T23:34:54.605163: step 2904, loss 0.0711436, acc 0.96875\n",
      "2017-11-06T23:34:58.616012: step 2905, loss 0.151156, acc 0.9375\n",
      "2017-11-06T23:35:02.650878: step 2906, loss 0.0610064, acc 1\n",
      "2017-11-06T23:35:06.682743: step 2907, loss 0.293692, acc 0.90625\n",
      "2017-11-06T23:35:10.631550: step 2908, loss 0.20821, acc 0.90625\n",
      "2017-11-06T23:35:14.607374: step 2909, loss 0.0871297, acc 0.9375\n",
      "2017-11-06T23:35:18.625230: step 2910, loss 0.0946199, acc 0.96875\n",
      "2017-11-06T23:35:22.667101: step 2911, loss 0.147293, acc 0.9375\n",
      "2017-11-06T23:35:26.710975: step 2912, loss 0.0642339, acc 0.96875\n",
      "2017-11-06T23:35:30.644770: step 2913, loss 0.191524, acc 0.96875\n",
      "2017-11-06T23:35:34.681637: step 2914, loss 0.517199, acc 0.78125\n",
      "2017-11-06T23:35:38.713503: step 2915, loss 0.0503575, acc 1\n",
      "2017-11-06T23:35:41.566530: step 2916, loss 0.0745647, acc 1\n",
      "2017-11-06T23:35:45.755354: step 2917, loss 0.266111, acc 0.875\n",
      "2017-11-06T23:35:49.753194: step 2918, loss 0.0750666, acc 0.96875\n",
      "2017-11-06T23:35:53.736027: step 2919, loss 0.172576, acc 0.875\n",
      "2017-11-06T23:35:57.732864: step 2920, loss 0.0953837, acc 0.96875\n",
      "2017-11-06T23:36:01.754724: step 2921, loss 0.060039, acc 0.96875\n",
      "2017-11-06T23:36:05.838624: step 2922, loss 0.0892626, acc 0.96875\n",
      "2017-11-06T23:36:09.936535: step 2923, loss 0.145904, acc 0.9375\n",
      "2017-11-06T23:36:13.874333: step 2924, loss 0.195115, acc 0.875\n",
      "2017-11-06T23:36:17.854162: step 2925, loss 0.162977, acc 0.90625\n",
      "2017-11-06T23:36:21.858005: step 2926, loss 0.0284791, acc 1\n",
      "2017-11-06T23:36:25.890871: step 2927, loss 0.101947, acc 0.9375\n",
      "2017-11-06T23:36:29.926740: step 2928, loss 0.221518, acc 0.875\n",
      "2017-11-06T23:36:34.184765: step 2929, loss 0.160628, acc 0.90625\n",
      "2017-11-06T23:36:38.219632: step 2930, loss 0.168791, acc 0.90625\n",
      "2017-11-06T23:36:42.254499: step 2931, loss 0.320879, acc 0.84375\n",
      "2017-11-06T23:36:46.562560: step 2932, loss 0.177743, acc 0.90625\n",
      "2017-11-06T23:36:50.796570: step 2933, loss 0.173305, acc 0.875\n",
      "2017-11-06T23:36:54.787404: step 2934, loss 0.0925255, acc 0.96875\n",
      "2017-11-06T23:36:58.782242: step 2935, loss 0.269754, acc 0.875\n",
      "2017-11-06T23:37:02.788089: step 2936, loss 0.145245, acc 0.9375\n",
      "2017-11-06T23:37:06.836969: step 2937, loss 0.0843355, acc 1\n",
      "2017-11-06T23:37:10.865828: step 2938, loss 0.0431275, acc 0.96875\n",
      "2017-11-06T23:37:14.848658: step 2939, loss 0.558971, acc 0.8125\n",
      "2017-11-06T23:37:18.870515: step 2940, loss 0.33845, acc 0.875\n",
      "2017-11-06T23:37:22.814318: step 2941, loss 0.167527, acc 0.9375\n",
      "2017-11-06T23:37:26.924238: step 2942, loss 0.420906, acc 0.875\n",
      "2017-11-06T23:37:30.939091: step 2943, loss 0.376345, acc 0.875\n",
      "2017-11-06T23:37:34.885895: step 2944, loss 0.182773, acc 0.90625\n",
      "2017-11-06T23:37:38.942777: step 2945, loss 0.106217, acc 0.9375\n",
      "2017-11-06T23:37:43.050698: step 2946, loss 0.281965, acc 0.90625\n",
      "2017-11-06T23:37:47.094570: step 2947, loss 0.209889, acc 0.875\n",
      "2017-11-06T23:37:51.288550: step 2948, loss 0.49838, acc 0.84375\n",
      "2017-11-06T23:37:55.630635: step 2949, loss 0.233637, acc 0.9375\n",
      "2017-11-06T23:37:59.594452: step 2950, loss 0.196797, acc 0.96875\n",
      "2017-11-06T23:38:03.573279: step 2951, loss 0.246877, acc 0.90625\n",
      "2017-11-06T23:38:06.111083: step 2952, loss 0.289485, acc 0.85\n",
      "2017-11-06T23:38:10.147952: step 2953, loss 0.280018, acc 0.875\n",
      "2017-11-06T23:38:14.149793: step 2954, loss 0.256137, acc 0.90625\n",
      "2017-11-06T23:38:18.156640: step 2955, loss 0.0540344, acc 0.96875\n",
      "2017-11-06T23:38:22.288577: step 2956, loss 0.240718, acc 0.9375\n",
      "2017-11-06T23:38:26.683699: step 2957, loss 0.45861, acc 0.875\n",
      "2017-11-06T23:38:31.220924: step 2958, loss 0.174467, acc 0.90625\n",
      "2017-11-06T23:38:36.164437: step 2959, loss 0.221844, acc 0.9375\n",
      "2017-11-06T23:38:40.954840: step 2960, loss 0.050233, acc 1\n",
      "2017-11-06T23:38:45.485864: step 2961, loss 0.0741088, acc 0.96875\n",
      "2017-11-06T23:38:49.900000: step 2962, loss 0.231932, acc 0.90625\n",
      "2017-11-06T23:38:54.025932: step 2963, loss 0.206949, acc 0.90625\n",
      "2017-11-06T23:38:58.335995: step 2964, loss 0.132119, acc 0.90625\n",
      "2017-11-06T23:39:02.540984: step 2965, loss 0.166413, acc 0.96875\n",
      "2017-11-06T23:39:06.556837: step 2966, loss 0.113849, acc 0.9375\n",
      "2017-11-06T23:39:10.613718: step 2967, loss 0.298956, acc 0.90625\n",
      "2017-11-06T23:39:14.577535: step 2968, loss 0.282787, acc 0.90625\n",
      "2017-11-06T23:39:18.558364: step 2969, loss 0.0867321, acc 0.96875\n",
      "2017-11-06T23:39:22.524181: step 2970, loss 0.0747165, acc 0.96875\n",
      "2017-11-06T23:39:26.560048: step 2971, loss 0.247811, acc 0.90625\n",
      "2017-11-06T23:39:30.538876: step 2972, loss 0.0442646, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T23:39:34.548725: step 2973, loss 0.0989748, acc 0.9375\n",
      "2017-11-06T23:39:38.525550: step 2974, loss 0.116022, acc 0.9375\n",
      "2017-11-06T23:39:42.548409: step 2975, loss 0.23387, acc 0.875\n",
      "2017-11-06T23:39:46.513226: step 2976, loss 0.166785, acc 0.9375\n",
      "2017-11-06T23:39:50.571110: step 2977, loss 0.189361, acc 0.9375\n",
      "2017-11-06T23:39:54.529925: step 2978, loss 0.155567, acc 0.90625\n",
      "2017-11-06T23:39:58.542774: step 2979, loss 0.162718, acc 0.90625\n",
      "2017-11-06T23:40:03.057982: step 2980, loss 0.212275, acc 0.875\n",
      "2017-11-06T23:40:07.351032: step 2981, loss 0.269343, acc 0.875\n",
      "2017-11-06T23:40:11.352877: step 2982, loss 0.169988, acc 0.90625\n",
      "2017-11-06T23:40:15.346715: step 2983, loss 0.213979, acc 0.875\n",
      "2017-11-06T23:40:19.341552: step 2984, loss 0.135938, acc 0.90625\n",
      "2017-11-06T23:40:23.330387: step 2985, loss 0.300134, acc 0.84375\n",
      "2017-11-06T23:40:27.306212: step 2986, loss 0.240375, acc 0.90625\n",
      "2017-11-06T23:40:31.326068: step 2987, loss 0.231816, acc 0.9375\n",
      "2017-11-06T23:40:34.033993: step 2988, loss 0.352368, acc 0.9\n",
      "2017-11-06T23:40:38.133905: step 2989, loss 0.0222953, acc 1\n",
      "2017-11-06T23:40:42.092737: step 2990, loss 0.169246, acc 0.90625\n",
      "2017-11-06T23:40:46.114575: step 2991, loss 0.211206, acc 0.90625\n",
      "2017-11-06T23:40:50.126426: step 2992, loss 0.162793, acc 0.9375\n",
      "2017-11-06T23:40:54.078234: step 2993, loss 0.0660658, acc 0.96875\n",
      "2017-11-06T23:40:58.031044: step 2994, loss 0.160291, acc 0.9375\n",
      "2017-11-06T23:41:02.024881: step 2995, loss 0.0728093, acc 0.96875\n",
      "2017-11-06T23:41:06.002707: step 2996, loss 0.216703, acc 0.9375\n",
      "2017-11-06T23:41:10.724062: step 2997, loss 0.101001, acc 0.96875\n",
      "2017-11-06T23:41:15.869720: step 2998, loss 0.0517726, acc 0.96875\n",
      "2017-11-06T23:41:19.987643: step 2999, loss 0.299743, acc 0.875\n",
      "2017-11-06T23:41:24.093561: step 3000, loss 0.192024, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:41:26.700414: step 3000, loss 0.975267, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-3000\n",
      "\n",
      "2017-11-06T23:41:31.979341: step 3001, loss 0.0971702, acc 0.9375\n",
      "2017-11-06T23:41:36.082256: step 3002, loss 0.214522, acc 0.90625\n",
      "2017-11-06T23:41:40.139139: step 3003, loss 0.372065, acc 0.8125\n",
      "2017-11-06T23:41:44.159996: step 3004, loss 0.204881, acc 0.90625\n",
      "2017-11-06T23:41:48.193628: step 3005, loss 0.146195, acc 0.90625\n",
      "2017-11-06T23:41:52.272526: step 3006, loss 0.157606, acc 0.9375\n",
      "2017-11-06T23:41:56.291382: step 3007, loss 0.0602575, acc 0.96875\n",
      "2017-11-06T23:42:00.299230: step 3008, loss 0.092105, acc 0.96875\n",
      "2017-11-06T23:42:04.351109: step 3009, loss 0.331283, acc 0.875\n",
      "2017-11-06T23:42:08.406990: step 3010, loss 0.393584, acc 0.84375\n",
      "2017-11-06T23:42:12.576953: step 3011, loss 0.0566985, acc 0.96875\n",
      "2017-11-06T23:42:17.151204: step 3012, loss 0.270261, acc 0.90625\n",
      "2017-11-06T23:42:21.237107: step 3013, loss 0.0661296, acc 0.96875\n",
      "2017-11-06T23:42:25.259965: step 3014, loss 0.411524, acc 0.8125\n",
      "2017-11-06T23:42:29.353874: step 3015, loss 0.393261, acc 0.8125\n",
      "2017-11-06T23:42:33.582879: step 3016, loss 0.378545, acc 0.8125\n",
      "2017-11-06T23:42:37.679792: step 3017, loss 0.29818, acc 0.875\n",
      "2017-11-06T23:42:41.730668: step 3018, loss 0.0729336, acc 0.96875\n",
      "2017-11-06T23:42:45.728508: step 3019, loss 0.128839, acc 0.90625\n",
      "2017-11-06T23:42:49.692325: step 3020, loss 0.067318, acc 0.96875\n",
      "2017-11-06T23:42:53.788236: step 3021, loss 0.332375, acc 0.90625\n",
      "2017-11-06T23:42:57.722031: step 3022, loss 0.092787, acc 0.9375\n",
      "2017-11-06T23:43:01.673839: step 3023, loss 0.240599, acc 0.90625\n",
      "2017-11-06T23:43:04.206638: step 3024, loss 0.28356, acc 0.9\n",
      "2017-11-06T23:43:08.156446: step 3025, loss 0.222028, acc 0.90625\n",
      "2017-11-06T23:43:12.061219: step 3026, loss 0.0966514, acc 0.96875\n",
      "2017-11-06T23:43:16.037044: step 3027, loss 0.0473038, acc 0.96875\n",
      "2017-11-06T23:43:20.254041: step 3028, loss 0.108385, acc 0.96875\n",
      "2017-11-06T23:43:24.554096: step 3029, loss 0.416021, acc 0.84375\n",
      "2017-11-06T23:43:28.684031: step 3030, loss 0.097426, acc 0.96875\n",
      "2017-11-06T23:43:32.611821: step 3031, loss 0.222123, acc 0.9375\n",
      "2017-11-06T23:43:36.567632: step 3032, loss 0.199041, acc 0.90625\n",
      "2017-11-06T23:43:40.489419: step 3033, loss 0.305243, acc 0.90625\n",
      "2017-11-06T23:43:44.447232: step 3034, loss 0.0792142, acc 0.9375\n",
      "2017-11-06T23:43:48.429061: step 3035, loss 0.158055, acc 0.9375\n",
      "2017-11-06T23:43:52.379867: step 3036, loss 0.0565026, acc 0.96875\n",
      "2017-11-06T23:43:56.363698: step 3037, loss 0.192311, acc 0.875\n",
      "2017-11-06T23:44:00.340524: step 3038, loss 0.123898, acc 0.9375\n",
      "2017-11-06T23:44:04.279323: step 3039, loss 0.142187, acc 0.9375\n",
      "2017-11-06T23:44:08.300180: step 3040, loss 0.202675, acc 0.9375\n",
      "2017-11-06T23:44:12.264997: step 3041, loss 0.065053, acc 0.96875\n",
      "2017-11-06T23:44:16.356905: step 3042, loss 0.0682603, acc 0.96875\n",
      "2017-11-06T23:44:20.366753: step 3043, loss 0.19089, acc 0.875\n",
      "2017-11-06T23:44:24.445652: step 3044, loss 0.282366, acc 0.90625\n",
      "2017-11-06T23:44:28.899816: step 3045, loss 0.169573, acc 0.9375\n",
      "2017-11-06T23:44:32.907664: step 3046, loss 0.0713913, acc 0.96875\n",
      "2017-11-06T23:44:37.094658: step 3047, loss 0.177172, acc 0.90625\n",
      "2017-11-06T23:44:41.092480: step 3048, loss 0.0941541, acc 0.9375\n",
      "2017-11-06T23:44:44.997254: step 3049, loss 0.240317, acc 0.875\n",
      "2017-11-06T23:44:48.983896: step 3050, loss 0.056143, acc 0.96875\n",
      "2017-11-06T23:44:52.930700: step 3051, loss 0.361213, acc 0.84375\n",
      "2017-11-06T23:44:56.941552: step 3052, loss 0.1595, acc 0.90625\n",
      "2017-11-06T23:45:01.012442: step 3053, loss 0.19339, acc 0.90625\n",
      "2017-11-06T23:45:05.012285: step 3054, loss 0.342409, acc 0.875\n",
      "2017-11-06T23:45:09.009125: step 3055, loss 0.0619068, acc 0.96875\n",
      "2017-11-06T23:45:12.988953: step 3056, loss 0.141393, acc 0.9375\n",
      "2017-11-06T23:45:17.001805: step 3057, loss 0.196435, acc 0.90625\n",
      "2017-11-06T23:45:20.975627: step 3058, loss 0.26176, acc 0.84375\n",
      "2017-11-06T23:45:24.916428: step 3059, loss 0.191598, acc 0.90625\n",
      "2017-11-06T23:45:27.466239: step 3060, loss 0.078698, acc 0.95\n",
      "2017-11-06T23:45:31.672228: step 3061, loss 0.139038, acc 0.9375\n",
      "2017-11-06T23:45:35.904237: step 3062, loss 0.14786, acc 0.9375\n",
      "2017-11-06T23:45:39.846036: step 3063, loss 0.246785, acc 0.9375\n",
      "2017-11-06T23:45:43.825864: step 3064, loss 0.0985329, acc 0.96875\n",
      "2017-11-06T23:45:47.801689: step 3065, loss 0.259804, acc 0.875\n",
      "2017-11-06T23:45:51.754497: step 3066, loss 0.199455, acc 0.90625\n",
      "2017-11-06T23:45:55.696299: step 3067, loss 0.16631, acc 0.90625\n",
      "2017-11-06T23:45:59.655110: step 3068, loss 0.0422482, acc 1\n",
      "2017-11-06T23:46:03.632937: step 3069, loss 0.113923, acc 0.9375\n",
      "2017-11-06T23:46:07.650792: step 3070, loss 0.143113, acc 0.9375\n",
      "2017-11-06T23:46:11.635624: step 3071, loss 0.176171, acc 0.90625\n",
      "2017-11-06T23:46:15.554408: step 3072, loss 0.189395, acc 0.90625\n",
      "2017-11-06T23:46:19.591276: step 3073, loss 0.217479, acc 0.875\n",
      "2017-11-06T23:46:23.523071: step 3074, loss 0.201475, acc 0.875\n",
      "2017-11-06T23:46:27.520911: step 3075, loss 0.216735, acc 0.90625\n",
      "2017-11-06T23:46:31.483728: step 3076, loss 0.052431, acc 0.96875\n",
      "2017-11-06T23:46:35.846826: step 3077, loss 0.0863581, acc 0.96875\n",
      "2017-11-06T23:46:40.243951: step 3078, loss 0.0699877, acc 0.96875\n",
      "2017-11-06T23:46:44.205766: step 3079, loss 0.171976, acc 0.96875\n",
      "2017-11-06T23:46:48.146566: step 3080, loss 0.127573, acc 0.90625\n",
      "2017-11-06T23:46:52.070355: step 3081, loss 0.334545, acc 0.8125\n",
      "2017-11-06T23:46:56.017158: step 3082, loss 0.176148, acc 0.90625\n",
      "2017-11-06T23:46:59.960962: step 3083, loss 0.180537, acc 0.875\n",
      "2017-11-06T23:47:03.927779: step 3084, loss 0.0160675, acc 1\n",
      "2017-11-06T23:47:07.869581: step 3085, loss 0.176254, acc 0.9375\n",
      "2017-11-06T23:47:11.815383: step 3086, loss 0.184706, acc 0.9375\n",
      "2017-11-06T23:47:15.765190: step 3087, loss 0.347457, acc 0.875\n",
      "2017-11-06T23:47:19.737012: step 3088, loss 0.0779457, acc 0.9375\n",
      "2017-11-06T23:47:23.701830: step 3089, loss 0.296809, acc 0.90625\n",
      "2017-11-06T23:47:27.677676: step 3090, loss 0.148044, acc 0.9375\n",
      "2017-11-06T23:47:31.699512: step 3091, loss 0.198095, acc 0.90625\n",
      "2017-11-06T23:47:35.643315: step 3092, loss 0.165186, acc 0.9375\n",
      "2017-11-06T23:47:39.626144: step 3093, loss 0.092667, acc 0.9375\n",
      "2017-11-06T23:47:44.095321: step 3094, loss 0.396159, acc 0.8125\n",
      "2017-11-06T23:47:48.246106: step 3095, loss 0.184468, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T23:47:50.770900: step 3096, loss 0.401297, acc 0.8\n",
      "2017-11-06T23:47:54.696690: step 3097, loss 0.263115, acc 0.875\n",
      "2017-11-06T23:47:58.704537: step 3098, loss 0.0910193, acc 0.9375\n",
      "2017-11-06T23:48:02.767427: step 3099, loss 0.011269, acc 1\n",
      "2017-11-06T23:48:06.799290: step 3100, loss 0.0528492, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:48:09.352104: step 3100, loss 0.956632, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-3100\n",
      "\n",
      "2017-11-06T23:48:14.728610: step 3101, loss 0.180423, acc 0.90625\n",
      "2017-11-06T23:48:18.705435: step 3102, loss 0.132307, acc 0.96875\n",
      "2017-11-06T23:48:22.881422: step 3103, loss 0.268619, acc 0.875\n",
      "2017-11-06T23:48:26.954316: step 3104, loss 0.147869, acc 0.9375\n",
      "2017-11-06T23:48:30.932141: step 3105, loss 0.0720589, acc 0.96875\n",
      "2017-11-06T23:48:35.137130: step 3106, loss 0.240355, acc 0.90625\n",
      "2017-11-06T23:48:39.109954: step 3107, loss 0.185937, acc 0.90625\n",
      "2017-11-06T23:48:43.105792: step 3108, loss 0.0806801, acc 0.9375\n",
      "2017-11-06T23:48:47.212710: step 3109, loss 0.343495, acc 0.84375\n",
      "2017-11-06T23:48:51.564803: step 3110, loss 0.201315, acc 0.9375\n",
      "2017-11-06T23:48:55.534622: step 3111, loss 0.256814, acc 0.875\n",
      "2017-11-06T23:48:59.542471: step 3112, loss 0.22787, acc 0.90625\n",
      "2017-11-06T23:49:03.503285: step 3113, loss 0.1358, acc 0.96875\n",
      "2017-11-06T23:49:07.478110: step 3114, loss 0.338538, acc 0.8125\n",
      "2017-11-06T23:49:11.495964: step 3115, loss 0.430914, acc 0.8125\n",
      "2017-11-06T23:49:15.464786: step 3116, loss 0.0499898, acc 1\n",
      "2017-11-06T23:49:19.450616: step 3117, loss 0.248645, acc 0.8125\n",
      "2017-11-06T23:49:23.454461: step 3118, loss 0.0462396, acc 0.96875\n",
      "2017-11-06T23:49:27.449299: step 3119, loss 0.0654491, acc 0.9375\n",
      "2017-11-06T23:49:31.413117: step 3120, loss 0.249496, acc 0.90625\n",
      "2017-11-06T23:49:35.359921: step 3121, loss 0.127509, acc 0.9375\n",
      "2017-11-06T23:49:39.387782: step 3122, loss 0.227047, acc 0.84375\n",
      "2017-11-06T23:49:43.395631: step 3123, loss 0.0305819, acc 1\n",
      "2017-11-06T23:49:47.382463: step 3124, loss 0.175951, acc 0.9375\n",
      "2017-11-06T23:49:51.396315: step 3125, loss 0.206062, acc 0.90625\n",
      "2017-11-06T23:49:55.875498: step 3126, loss 0.139563, acc 0.9375\n",
      "2017-11-06T23:49:59.924377: step 3127, loss 0.177072, acc 0.90625\n",
      "2017-11-06T23:50:04.232435: step 3128, loss 0.0801929, acc 0.96875\n",
      "2017-11-06T23:50:08.212263: step 3129, loss 0.134983, acc 0.90625\n",
      "2017-11-06T23:50:12.212107: step 3130, loss 0.236005, acc 0.84375\n",
      "2017-11-06T23:50:16.250976: step 3131, loss 0.267695, acc 0.875\n",
      "2017-11-06T23:50:18.759758: step 3132, loss 0.279484, acc 0.85\n",
      "2017-11-06T23:50:22.802631: step 3133, loss 0.226827, acc 0.875\n",
      "2017-11-06T23:50:26.839499: step 3134, loss 0.166036, acc 0.90625\n",
      "2017-11-06T23:50:30.818326: step 3135, loss 0.113746, acc 0.9375\n",
      "2017-11-06T23:50:35.074350: step 3136, loss 0.14257, acc 0.90625\n",
      "2017-11-06T23:50:39.042170: step 3137, loss 0.181585, acc 0.875\n",
      "2017-11-06T23:50:43.076035: step 3138, loss 0.137936, acc 0.90625\n",
      "2017-11-06T23:50:47.068758: step 3139, loss 0.206812, acc 0.90625\n",
      "2017-11-06T23:50:51.031574: step 3140, loss 0.174759, acc 0.90625\n",
      "2017-11-06T23:50:55.009400: step 3141, loss 0.317876, acc 0.875\n",
      "2017-11-06T23:50:59.247412: step 3142, loss 0.118979, acc 0.9375\n",
      "2017-11-06T23:51:03.555472: step 3143, loss 0.139279, acc 0.9375\n",
      "2017-11-06T23:51:07.497273: step 3144, loss 0.0794744, acc 0.9375\n",
      "2017-11-06T23:51:11.537144: step 3145, loss 0.170099, acc 0.9375\n",
      "2017-11-06T23:51:15.501962: step 3146, loss 0.143211, acc 0.875\n",
      "2017-11-06T23:51:19.457772: step 3147, loss 0.109337, acc 0.9375\n",
      "2017-11-06T23:51:23.436601: step 3148, loss 0.160237, acc 0.90625\n",
      "2017-11-06T23:51:27.496484: step 3149, loss 0.0834276, acc 0.96875\n",
      "2017-11-06T23:51:31.464303: step 3150, loss 0.161906, acc 0.9375\n",
      "2017-11-06T23:51:35.457140: step 3151, loss 0.0622352, acc 0.96875\n",
      "2017-11-06T23:51:39.441973: step 3152, loss 0.111457, acc 0.96875\n",
      "2017-11-06T23:51:43.447817: step 3153, loss 0.161638, acc 0.90625\n",
      "2017-11-06T23:51:47.428646: step 3154, loss 0.390101, acc 0.8125\n",
      "2017-11-06T23:51:51.453506: step 3155, loss 0.103546, acc 0.9375\n",
      "2017-11-06T23:51:55.399310: step 3156, loss 0.285036, acc 0.875\n",
      "2017-11-06T23:51:59.353120: step 3157, loss 0.0901036, acc 0.96875\n",
      "2017-11-06T23:52:03.410002: step 3158, loss 0.100085, acc 0.9375\n",
      "2017-11-06T23:52:07.845153: step 3159, loss 0.382303, acc 0.84375\n",
      "2017-11-06T23:52:11.914044: step 3160, loss 0.46194, acc 0.78125\n",
      "2017-11-06T23:52:15.911885: step 3161, loss 0.0959662, acc 0.96875\n",
      "2017-11-06T23:52:19.952756: step 3162, loss 0.16198, acc 0.84375\n",
      "2017-11-06T23:52:24.010639: step 3163, loss 0.190385, acc 0.90625\n",
      "2017-11-06T23:52:28.163590: step 3164, loss 0.148772, acc 0.90625\n",
      "2017-11-06T23:52:32.177442: step 3165, loss 0.193361, acc 0.9375\n",
      "2017-11-06T23:52:36.602586: step 3166, loss 0.302703, acc 0.875\n",
      "2017-11-06T23:52:40.689490: step 3167, loss 0.0370992, acc 1\n",
      "2017-11-06T23:52:43.233297: step 3168, loss 0.330383, acc 0.85\n",
      "2017-11-06T23:52:47.328207: step 3169, loss 0.181873, acc 0.90625\n",
      "2017-11-06T23:52:51.362074: step 3170, loss 0.0426179, acc 0.96875\n",
      "2017-11-06T23:52:55.362916: step 3171, loss 0.206279, acc 0.90625\n",
      "2017-11-06T23:52:59.405789: step 3172, loss 0.136664, acc 0.96875\n",
      "2017-11-06T23:53:03.590763: step 3173, loss 0.201913, acc 0.90625\n",
      "2017-11-06T23:53:07.620628: step 3174, loss 0.0773271, acc 0.96875\n",
      "2017-11-06T23:53:12.124826: step 3175, loss 0.290731, acc 0.875\n",
      "2017-11-06T23:53:16.199722: step 3176, loss 0.278923, acc 0.84375\n",
      "2017-11-06T23:53:20.190558: step 3177, loss 0.140139, acc 0.9375\n",
      "2017-11-06T23:53:24.363523: step 3178, loss 0.211979, acc 0.875\n",
      "2017-11-06T23:53:28.648567: step 3179, loss 0.161709, acc 0.90625\n",
      "2017-11-06T23:53:32.634399: step 3180, loss 0.248267, acc 0.9375\n",
      "2017-11-06T23:53:36.620231: step 3181, loss 0.144041, acc 0.9375\n",
      "2017-11-06T23:53:40.585050: step 3182, loss 0.141158, acc 0.9375\n",
      "2017-11-06T23:53:44.620916: step 3183, loss 0.228688, acc 0.90625\n",
      "2017-11-06T23:53:48.571483: step 3184, loss 0.162726, acc 0.9375\n",
      "2017-11-06T23:53:52.571325: step 3185, loss 0.224531, acc 0.875\n",
      "2017-11-06T23:53:56.589181: step 3186, loss 0.179852, acc 0.90625\n",
      "2017-11-06T23:54:00.558000: step 3187, loss 0.0771367, acc 0.9375\n",
      "2017-11-06T23:54:04.526820: step 3188, loss 0.0658433, acc 0.96875\n",
      "2017-11-06T23:54:08.567693: step 3189, loss 0.0307326, acc 0.96875\n",
      "2017-11-06T23:54:12.612566: step 3190, loss 0.186755, acc 0.96875\n",
      "2017-11-06T23:54:16.968660: step 3191, loss 0.295124, acc 0.90625\n",
      "2017-11-06T23:54:21.149631: step 3192, loss 0.153747, acc 0.9375\n",
      "2017-11-06T23:54:25.215522: step 3193, loss 0.204224, acc 0.9375\n",
      "2017-11-06T23:54:29.202353: step 3194, loss 0.249155, acc 0.90625\n",
      "2017-11-06T23:54:33.351301: step 3195, loss 0.382939, acc 0.875\n",
      "2017-11-06T23:54:37.482236: step 3196, loss 0.0816466, acc 0.96875\n",
      "2017-11-06T23:54:41.478077: step 3197, loss 0.113786, acc 0.96875\n",
      "2017-11-06T23:54:45.452900: step 3198, loss 0.194625, acc 0.90625\n",
      "2017-11-06T23:54:49.487768: step 3199, loss 0.341272, acc 0.8125\n",
      "2017-11-06T23:54:53.452585: step 3200, loss 0.0812584, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-06T23:54:55.990387: step 3200, loss 0.905445, acc 0.733333\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-3200\n",
      "\n",
      "2017-11-06T23:55:01.198166: step 3201, loss 0.235716, acc 0.84375\n",
      "2017-11-06T23:55:05.142968: step 3202, loss 0.283437, acc 0.84375\n",
      "2017-11-06T23:55:09.154818: step 3203, loss 0.229759, acc 0.90625\n",
      "2017-11-06T23:55:11.765674: step 3204, loss 0.235446, acc 0.9\n",
      "2017-11-06T23:55:15.724486: step 3205, loss 0.0395357, acc 0.96875\n",
      "2017-11-06T23:55:19.766358: step 3206, loss 0.115873, acc 0.9375\n",
      "2017-11-06T23:55:24.182498: step 3207, loss 0.0922529, acc 0.9375\n",
      "2017-11-06T23:55:28.178337: step 3208, loss 0.0809898, acc 0.9375\n",
      "2017-11-06T23:55:32.190185: step 3209, loss 0.150714, acc 0.9375\n",
      "2017-11-06T23:55:36.214045: step 3210, loss 0.0687052, acc 0.96875\n",
      "2017-11-06T23:55:40.192871: step 3211, loss 0.230848, acc 0.90625\n",
      "2017-11-06T23:55:44.165695: step 3212, loss 0.239661, acc 0.90625\n",
      "2017-11-06T23:55:48.168540: step 3213, loss 0.268906, acc 0.875\n",
      "2017-11-06T23:55:52.157374: step 3214, loss 0.0837674, acc 0.96875\n",
      "2017-11-06T23:55:56.105178: step 3215, loss 0.0905465, acc 0.96875\n",
      "2017-11-06T23:56:00.107021: step 3216, loss 0.457439, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06T23:56:04.119872: step 3217, loss 0.285274, acc 0.875\n",
      "2017-11-06T23:56:08.091695: step 3218, loss 0.242861, acc 0.875\n",
      "2017-11-06T23:56:12.107550: step 3219, loss 0.0785174, acc 0.9375\n",
      "2017-11-06T23:56:16.114396: step 3220, loss 0.11173, acc 0.9375\n",
      "2017-11-06T23:56:20.143259: step 3221, loss 0.221908, acc 0.90625\n",
      "2017-11-06T23:56:24.127088: step 3222, loss 0.0934554, acc 0.9375\n",
      "2017-11-06T23:56:28.643298: step 3223, loss 0.28963, acc 0.875\n",
      "2017-11-06T23:56:32.977377: step 3224, loss 0.0621238, acc 0.96875\n",
      "2017-11-06T23:56:37.194373: step 3225, loss 0.256465, acc 0.875\n",
      "2017-11-06T23:56:41.255259: step 3226, loss 0.165863, acc 0.96875\n",
      "2017-11-06T23:56:45.245094: step 3227, loss 0.0269174, acc 1\n",
      "2017-11-06T23:56:49.274760: step 3228, loss 0.3158, acc 0.84375\n",
      "2017-11-06T23:56:53.329641: step 3229, loss 0.232053, acc 0.9375\n",
      "2017-11-06T23:56:57.344495: step 3230, loss 0.154571, acc 0.9375\n",
      "2017-11-06T23:57:01.470426: step 3231, loss 0.191423, acc 0.90625\n",
      "2017-11-06T23:57:05.558330: step 3232, loss 0.0886784, acc 0.9375\n",
      "2017-11-06T23:57:09.618215: step 3233, loss 0.246412, acc 0.84375\n",
      "2017-11-06T23:57:13.596042: step 3234, loss 0.146153, acc 0.96875\n",
      "2017-11-06T23:57:17.590880: step 3235, loss 0.0192191, acc 1\n",
      "2017-11-06T23:57:21.505663: step 3236, loss 0.0977238, acc 0.9375\n",
      "2017-11-06T23:57:25.526519: step 3237, loss 0.157961, acc 0.9375\n",
      "2017-11-06T23:57:29.503344: step 3238, loss 0.404419, acc 0.84375\n",
      "2017-11-06T23:57:33.808403: step 3239, loss 0.0869423, acc 0.96875\n",
      "2017-11-06T23:57:36.556355: step 3240, loss 0.109803, acc 0.95\n",
      "2017-11-06T23:57:40.599228: step 3241, loss 0.107704, acc 0.96875\n",
      "2017-11-06T23:57:44.614082: step 3242, loss 0.150448, acc 0.90625\n",
      "2017-11-06T23:57:48.703987: step 3243, loss 0.0813444, acc 0.9375\n",
      "2017-11-06T23:57:52.688819: step 3244, loss 0.293259, acc 0.84375\n",
      "2017-11-06T23:57:56.692663: step 3245, loss 0.192452, acc 0.90625\n",
      "2017-11-06T23:58:00.673491: step 3246, loss 0.025333, acc 1\n",
      "2017-11-06T23:58:04.627301: step 3247, loss 0.122809, acc 0.9375\n",
      "2017-11-06T23:58:08.640152: step 3248, loss 0.095802, acc 0.9375\n",
      "2017-11-06T23:58:12.624983: step 3249, loss 0.260694, acc 0.875\n",
      "2017-11-06T23:58:16.605812: step 3250, loss 0.150905, acc 0.9375\n",
      "2017-11-06T23:58:20.572631: step 3251, loss 0.269951, acc 0.90625\n",
      "2017-11-06T23:58:24.764610: step 3252, loss 0.304773, acc 0.8125\n",
      "2017-11-06T23:58:28.919581: step 3253, loss 0.0214587, acc 1\n",
      "2017-11-06T23:58:32.954428: step 3254, loss 0.270572, acc 0.875\n",
      "2017-11-06T23:58:37.281503: step 3255, loss 0.16092, acc 0.90625\n",
      "2017-11-06T23:58:41.756683: step 3256, loss 0.171057, acc 0.90625\n",
      "2017-11-06T23:58:45.748519: step 3257, loss 0.0349106, acc 1\n",
      "2017-11-06T23:58:49.680313: step 3258, loss 0.115985, acc 0.96875\n",
      "2017-11-06T23:58:53.711179: step 3259, loss 0.189999, acc 0.90625\n",
      "2017-11-06T23:58:57.663986: step 3260, loss 0.107275, acc 0.96875\n",
      "2017-11-06T23:59:01.694850: step 3261, loss 0.2, acc 0.9375\n",
      "2017-11-06T23:59:05.686686: step 3262, loss 0.404836, acc 0.78125\n",
      "2017-11-06T23:59:09.704542: step 3263, loss 0.111127, acc 0.9375\n",
      "2017-11-06T23:59:13.640337: step 3264, loss 0.249415, acc 0.90625\n",
      "2017-11-06T23:59:17.640179: step 3265, loss 0.220118, acc 0.84375\n",
      "2017-11-06T23:59:21.672044: step 3266, loss 0.093773, acc 0.9375\n",
      "2017-11-06T23:59:25.627856: step 3267, loss 0.169678, acc 0.9375\n",
      "2017-11-06T23:59:29.636704: step 3268, loss 0.244294, acc 0.875\n",
      "2017-11-06T23:59:33.575503: step 3269, loss 0.215003, acc 0.84375\n",
      "2017-11-06T23:59:37.576345: step 3270, loss 0.322063, acc 0.8125\n",
      "2017-11-06T23:59:41.599203: step 3271, loss 0.123164, acc 0.96875\n",
      "2017-11-06T23:59:46.067378: step 3272, loss 0.120046, acc 0.90625\n",
      "2017-11-06T23:59:50.145036: step 3273, loss 0.124664, acc 0.90625\n",
      "2017-11-06T23:59:54.098846: step 3274, loss 0.0976433, acc 0.96875\n",
      "2017-11-06T23:59:58.029639: step 3275, loss 0.198893, acc 0.90625\n",
      "2017-11-07T00:00:00.664510: step 3276, loss 0.172698, acc 0.9\n",
      "2017-11-07T00:00:04.731400: step 3277, loss 0.0499473, acc 1\n",
      "2017-11-07T00:00:08.721235: step 3278, loss 0.186778, acc 0.875\n",
      "2017-11-07T00:00:12.722078: step 3279, loss 0.0616296, acc 0.96875\n",
      "2017-11-07T00:00:16.725923: step 3280, loss 0.231485, acc 0.875\n",
      "2017-11-07T00:00:20.685737: step 3281, loss 0.0680598, acc 0.96875\n",
      "2017-11-07T00:00:24.677572: step 3282, loss 0.397263, acc 0.78125\n",
      "2017-11-07T00:00:28.671412: step 3283, loss 0.357034, acc 0.875\n",
      "2017-11-07T00:00:32.734418: step 3284, loss 0.132166, acc 0.9375\n",
      "2017-11-07T00:00:36.869356: step 3285, loss 0.0824012, acc 0.96875\n",
      "2017-11-07T00:00:40.828169: step 3286, loss 0.226331, acc 0.90625\n",
      "2017-11-07T00:00:44.779976: step 3287, loss 0.113825, acc 0.9375\n",
      "2017-11-07T00:00:48.885894: step 3288, loss 0.243552, acc 0.875\n",
      "2017-11-07T00:00:53.206964: step 3289, loss 0.0503765, acc 0.96875\n",
      "2017-11-07T00:00:57.132755: step 3290, loss 0.220095, acc 0.90625\n",
      "2017-11-07T00:01:01.085564: step 3291, loss 0.198264, acc 0.9375\n",
      "2017-11-07T00:01:05.009351: step 3292, loss 0.0925879, acc 0.9375\n",
      "2017-11-07T00:01:08.963160: step 3293, loss 0.190376, acc 0.90625\n",
      "2017-11-07T00:01:12.893952: step 3294, loss 0.392711, acc 0.78125\n",
      "2017-11-07T00:01:16.860773: step 3295, loss 0.246786, acc 0.90625\n",
      "2017-11-07T00:01:20.815582: step 3296, loss 0.144069, acc 0.9375\n",
      "2017-11-07T00:01:24.798412: step 3297, loss 0.338311, acc 0.84375\n",
      "2017-11-07T00:01:28.729205: step 3298, loss 0.184674, acc 0.875\n",
      "2017-11-07T00:01:32.678012: step 3299, loss 0.105638, acc 0.96875\n",
      "2017-11-07T00:01:36.701871: step 3300, loss 0.298615, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T00:01:39.350752: step 3300, loss 0.890683, acc 0.716667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-3300\n",
      "\n",
      "2017-11-07T00:01:44.980422: step 3301, loss 0.0926979, acc 1\n",
      "2017-11-07T00:01:49.052315: step 3302, loss 0.491489, acc 0.84375\n",
      "2017-11-07T00:01:53.132214: step 3303, loss 0.159573, acc 0.96875\n",
      "2017-11-07T00:01:57.708466: step 3304, loss 0.294794, acc 0.84375\n",
      "2017-11-07T00:02:01.955484: step 3305, loss 0.168471, acc 0.90625\n",
      "2017-11-07T00:02:06.150464: step 3306, loss 0.125164, acc 0.96875\n",
      "2017-11-07T00:02:10.295410: step 3307, loss 0.304687, acc 0.84375\n",
      "2017-11-07T00:02:14.433349: step 3308, loss 0.0713659, acc 0.96875\n",
      "2017-11-07T00:02:18.683370: step 3309, loss 0.255694, acc 0.875\n",
      "2017-11-07T00:02:22.750259: step 3310, loss 0.299616, acc 0.90625\n",
      "2017-11-07T00:02:26.834162: step 3311, loss 0.121101, acc 0.96875\n",
      "2017-11-07T00:02:29.583114: step 3312, loss 0.216637, acc 0.9\n",
      "2017-11-07T00:02:33.934206: step 3313, loss 0.00250488, acc 1\n",
      "2017-11-07T00:02:38.154204: step 3314, loss 0.220899, acc 0.875\n",
      "2017-11-07T00:02:42.458262: step 3315, loss 0.0493948, acc 1\n",
      "2017-11-07T00:02:46.544165: step 3316, loss 0.0405597, acc 0.96875\n",
      "2017-11-07T00:02:50.692920: step 3317, loss 0.173656, acc 0.875\n",
      "2017-11-07T00:02:54.916921: step 3318, loss 0.350624, acc 0.78125\n",
      "2017-11-07T00:02:59.118907: step 3319, loss 0.0794268, acc 0.96875\n",
      "2017-11-07T00:03:03.765208: step 3320, loss 0.190396, acc 0.9375\n",
      "2017-11-07T00:03:07.863120: step 3321, loss 0.0985989, acc 0.96875\n",
      "2017-11-07T00:03:12.026077: step 3322, loss 0.274767, acc 0.90625\n",
      "2017-11-07T00:03:16.178028: step 3323, loss 0.198161, acc 0.90625\n",
      "2017-11-07T00:03:20.262930: step 3324, loss 0.217832, acc 0.90625\n",
      "2017-11-07T00:03:24.446904: step 3325, loss 0.235189, acc 0.875\n",
      "2017-11-07T00:03:28.630876: step 3326, loss 0.0753308, acc 0.96875\n",
      "2017-11-07T00:03:32.724784: step 3327, loss 0.131327, acc 0.9375\n",
      "2017-11-07T00:03:36.876735: step 3328, loss 0.204557, acc 0.90625\n",
      "2017-11-07T00:03:41.016677: step 3329, loss 0.233657, acc 0.84375\n",
      "2017-11-07T00:03:45.125596: step 3330, loss 0.277488, acc 0.875\n",
      "2017-11-07T00:03:49.580761: step 3331, loss 0.0633494, acc 0.96875\n",
      "2017-11-07T00:03:54.293111: step 3332, loss 0.300944, acc 0.84375\n",
      "2017-11-07T00:03:58.986445: step 3333, loss 0.317118, acc 0.875\n",
      "2017-11-07T00:04:03.276493: step 3334, loss 0.0423585, acc 0.96875\n",
      "2017-11-07T00:04:07.754675: step 3335, loss 0.253804, acc 0.875\n",
      "2017-11-07T00:04:12.657159: step 3336, loss 0.216922, acc 0.90625\n",
      "2017-11-07T00:04:17.237413: step 3337, loss 0.16788, acc 0.875\n",
      "2017-11-07T00:04:21.667562: step 3338, loss 0.300611, acc 0.84375\n",
      "2017-11-07T00:04:26.527014: step 3339, loss 0.153418, acc 0.9375\n",
      "2017-11-07T00:04:31.238361: step 3340, loss 0.226775, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-07T00:04:35.885727: step 3341, loss 0.0682701, acc 0.96875\n",
      "2017-11-07T00:04:40.761191: step 3342, loss 0.318846, acc 0.90625\n",
      "2017-11-07T00:04:45.648665: step 3343, loss 0.228955, acc 0.875\n",
      "2017-11-07T00:04:50.448074: step 3344, loss 0.225128, acc 0.90625\n",
      "2017-11-07T00:04:55.149414: step 3345, loss 0.102955, acc 0.9375\n",
      "2017-11-07T00:04:59.860762: step 3346, loss 0.308044, acc 0.875\n",
      "2017-11-07T00:05:04.470037: step 3347, loss 0.248639, acc 0.90625\n",
      "2017-11-07T00:05:07.247011: step 3348, loss 0.132271, acc 0.85\n",
      "2017-11-07T00:05:11.819259: step 3349, loss 0.0535419, acc 0.96875\n",
      "2017-11-07T00:05:16.529606: step 3350, loss 0.0676004, acc 0.96875\n",
      "2017-11-07T00:05:20.689562: step 3351, loss 0.0696388, acc 0.96875\n",
      "2017-11-07T00:05:25.107702: step 3352, loss 0.118773, acc 0.9375\n",
      "2017-11-07T00:05:29.540852: step 3353, loss 0.391324, acc 0.8125\n",
      "2017-11-07T00:05:33.979006: step 3354, loss 0.242333, acc 0.875\n",
      "2017-11-07T00:05:38.286065: step 3355, loss 0.284573, acc 0.9375\n",
      "2017-11-07T00:05:42.988406: step 3356, loss 0.04413, acc 1\n",
      "2017-11-07T00:05:47.747788: step 3357, loss 0.0373757, acc 1\n",
      "2017-11-07T00:05:52.594981: step 3358, loss 0.117646, acc 0.9375\n",
      "2017-11-07T00:05:57.208259: step 3359, loss 0.0778117, acc 0.9375\n",
      "2017-11-07T00:06:01.471287: step 3360, loss 0.272701, acc 0.90625\n",
      "2017-11-07T00:06:05.443110: step 3361, loss 0.0943572, acc 0.96875\n",
      "2017-11-07T00:06:09.410930: step 3362, loss 0.174774, acc 0.9375\n",
      "2017-11-07T00:06:13.324710: step 3363, loss 0.101011, acc 0.9375\n",
      "2017-11-07T00:06:17.497675: step 3364, loss 0.0369109, acc 1\n",
      "2017-11-07T00:06:22.004877: step 3365, loss 0.0971688, acc 0.96875\n",
      "2017-11-07T00:06:26.233883: step 3366, loss 0.220843, acc 0.875\n",
      "2017-11-07T00:06:30.370822: step 3367, loss 0.24493, acc 0.90625\n",
      "2017-11-07T00:06:34.718912: step 3368, loss 0.0893373, acc 0.9375\n",
      "2017-11-07T00:06:38.899281: step 3369, loss 0.4678, acc 0.84375\n",
      "2017-11-07T00:06:43.676674: step 3370, loss 0.494565, acc 0.84375\n",
      "2017-11-07T00:06:48.125835: step 3371, loss 0.113909, acc 0.96875\n",
      "2017-11-07T00:06:52.365848: step 3372, loss 0.168467, acc 0.9375\n",
      "2017-11-07T00:06:56.354701: step 3373, loss 0.0284615, acc 1\n",
      "2017-11-07T00:07:00.308491: step 3374, loss 0.110703, acc 0.96875\n",
      "2017-11-07T00:07:04.216268: step 3375, loss 0.395779, acc 0.8125\n",
      "2017-11-07T00:07:08.171078: step 3376, loss 0.185725, acc 0.90625\n",
      "2017-11-07T00:07:12.152908: step 3377, loss 0.154206, acc 0.9375\n",
      "2017-11-07T00:07:16.076695: step 3378, loss 0.279391, acc 0.875\n",
      "2017-11-07T00:07:20.049519: step 3379, loss 0.202595, acc 0.875\n",
      "2017-11-07T00:07:24.432633: step 3380, loss 0.273812, acc 0.875\n",
      "2017-11-07T00:07:28.591589: step 3381, loss 0.212013, acc 0.90625\n",
      "2017-11-07T00:07:32.595434: step 3382, loss 0.148559, acc 0.9375\n",
      "2017-11-07T00:07:36.561252: step 3383, loss 0.188712, acc 0.9375\n",
      "2017-11-07T00:07:39.089047: step 3384, loss 0.355195, acc 0.75\n",
      "2017-11-07T00:07:43.035851: step 3385, loss 0.214844, acc 0.84375\n",
      "2017-11-07T00:07:47.118752: step 3386, loss 0.0616656, acc 0.96875\n",
      "2017-11-07T00:07:51.156622: step 3387, loss 0.140638, acc 0.96875\n",
      "2017-11-07T00:07:55.067400: step 3388, loss 0.262737, acc 0.875\n",
      "2017-11-07T00:07:59.085255: step 3389, loss 0.036911, acc 1\n",
      "2017-11-07T00:08:03.006041: step 3390, loss 0.242895, acc 0.90625\n",
      "2017-11-07T00:08:06.956848: step 3391, loss 0.225101, acc 0.84375\n",
      "2017-11-07T00:08:10.955690: step 3392, loss 0.135613, acc 0.90625\n",
      "2017-11-07T00:08:14.933516: step 3393, loss 0.0474007, acc 1\n",
      "2017-11-07T00:08:18.947368: step 3394, loss 0.080792, acc 0.96875\n",
      "2017-11-07T00:08:23.072299: step 3395, loss 0.291853, acc 0.875\n",
      "2017-11-07T00:08:27.153198: step 3396, loss 0.100339, acc 0.96875\n",
      "2017-11-07T00:08:31.625376: step 3397, loss 0.25912, acc 0.875\n",
      "2017-11-07T00:08:35.805346: step 3398, loss 0.170831, acc 0.875\n",
      "2017-11-07T00:08:39.894251: step 3399, loss 0.106847, acc 0.96875\n",
      "2017-11-07T00:08:43.808034: step 3400, loss 0.129722, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T00:08:46.453913: step 3400, loss 1.05089, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-3400\n",
      "\n",
      "2017-11-07T00:08:51.825007: step 3401, loss 0.300171, acc 0.875\n",
      "2017-11-07T00:08:56.258156: step 3402, loss 0.162596, acc 0.9375\n",
      "2017-11-07T00:09:00.498169: step 3403, loss 0.188705, acc 0.90625\n",
      "2017-11-07T00:09:04.606089: step 3404, loss 0.152636, acc 0.9375\n",
      "2017-11-07T00:09:08.785058: step 3405, loss 0.244544, acc 0.875\n",
      "2017-11-07T00:09:12.949015: step 3406, loss 0.12765, acc 0.96875\n",
      "2017-11-07T00:09:17.212045: step 3407, loss 0.163695, acc 0.875\n",
      "2017-11-07T00:09:21.188870: step 3408, loss 0.162124, acc 0.875\n",
      "2017-11-07T00:09:25.448898: step 3409, loss 0.0944008, acc 0.96875\n",
      "2017-11-07T00:09:29.456745: step 3410, loss 0.206735, acc 0.90625\n",
      "2017-11-07T00:09:33.745792: step 3411, loss 0.0666309, acc 0.96875\n",
      "2017-11-07T00:09:38.294024: step 3412, loss 0.0229492, acc 1\n",
      "2017-11-07T00:09:42.404946: step 3413, loss 0.218576, acc 0.875\n",
      "2017-11-07T00:09:46.526874: step 3414, loss 0.158953, acc 0.90625\n",
      "2017-11-07T00:09:50.712849: step 3415, loss 0.356456, acc 0.90625\n",
      "2017-11-07T00:09:54.728702: step 3416, loss 0.515954, acc 0.8125\n",
      "2017-11-07T00:09:58.831618: step 3417, loss 0.197766, acc 0.90625\n",
      "2017-11-07T00:10:03.063624: step 3418, loss 0.0681406, acc 0.96875\n",
      "2017-11-07T00:10:06.979407: step 3419, loss 0.0241312, acc 1\n",
      "2017-11-07T00:10:09.499198: step 3420, loss 0.451945, acc 0.8\n",
      "2017-11-07T00:10:13.485029: step 3421, loss 0.11343, acc 0.9375\n",
      "2017-11-07T00:10:17.467859: step 3422, loss 0.270084, acc 0.8125\n",
      "2017-11-07T00:10:21.466701: step 3423, loss 0.269335, acc 0.875\n",
      "2017-11-07T00:10:25.506572: step 3424, loss 0.116037, acc 0.9375\n",
      "2017-11-07T00:10:29.458379: step 3425, loss 0.0539605, acc 1\n",
      "2017-11-07T00:10:33.593317: step 3426, loss 0.316684, acc 0.875\n",
      "2017-11-07T00:10:37.683223: step 3427, loss 0.0939271, acc 1\n",
      "2017-11-07T00:10:42.034391: step 3428, loss 0.164904, acc 0.9375\n",
      "2017-11-07T00:10:46.028228: step 3429, loss 0.454193, acc 0.8125\n",
      "2017-11-07T00:10:49.994046: step 3430, loss 0.131594, acc 0.9375\n",
      "2017-11-07T00:10:54.013903: step 3431, loss 0.254822, acc 0.875\n",
      "2017-11-07T00:10:57.952701: step 3432, loss 0.127306, acc 0.90625\n",
      "2017-11-07T00:11:01.874487: step 3433, loss 0.137746, acc 0.90625\n",
      "2017-11-07T00:11:05.813287: step 3434, loss 0.0281207, acc 1\n",
      "2017-11-07T00:11:09.747082: step 3435, loss 0.182457, acc 0.96875\n",
      "2017-11-07T00:11:13.705894: step 3436, loss 0.251868, acc 0.90625\n",
      "2017-11-07T00:11:17.670712: step 3437, loss 0.169002, acc 0.96875\n",
      "2017-11-07T00:11:21.595500: step 3438, loss 0.258792, acc 0.9375\n",
      "2017-11-07T00:11:25.548309: step 3439, loss 0.443627, acc 0.875\n",
      "2017-11-07T00:11:29.511125: step 3440, loss 0.389372, acc 0.875\n",
      "2017-11-07T00:11:33.465934: step 3441, loss 0.315383, acc 0.84375\n",
      "2017-11-07T00:11:37.396728: step 3442, loss 0.185182, acc 0.875\n",
      "2017-11-07T00:11:41.345534: step 3443, loss 0.270674, acc 0.875\n",
      "2017-11-07T00:11:45.507491: step 3444, loss 0.189857, acc 0.9375\n",
      "2017-11-07T00:11:49.807316: step 3445, loss 0.129129, acc 0.9375\n",
      "2017-11-07T00:11:53.819167: step 3446, loss 0.342512, acc 0.875\n",
      "2017-11-07T00:11:57.791989: step 3447, loss 0.340575, acc 0.84375\n",
      "2017-11-07T00:12:01.763812: step 3448, loss 0.214846, acc 0.875\n",
      "2017-11-07T00:12:05.750644: step 3449, loss 0.156037, acc 0.90625\n",
      "2017-11-07T00:12:09.693446: step 3450, loss 0.233676, acc 0.875\n",
      "2017-11-07T00:12:13.634246: step 3451, loss 0.202724, acc 0.90625\n",
      "2017-11-07T00:12:17.611072: step 3452, loss 0.0970279, acc 0.9375\n",
      "2017-11-07T00:12:21.522851: step 3453, loss 0.246644, acc 0.84375\n",
      "2017-11-07T00:12:25.465653: step 3454, loss 0.18832, acc 0.9375\n",
      "2017-11-07T00:12:29.410456: step 3455, loss 0.154567, acc 0.9375\n",
      "2017-11-07T00:12:31.934268: step 3456, loss 0.316357, acc 0.85\n",
      "2017-11-07T00:12:36.241310: step 3457, loss 0.0641087, acc 0.96875\n",
      "2017-11-07T00:12:40.180110: step 3458, loss 0.112708, acc 0.96875\n",
      "2017-11-07T00:12:44.173946: step 3459, loss 0.19351, acc 0.9375\n",
      "2017-11-07T00:12:48.110743: step 3460, loss 0.235903, acc 0.90625\n",
      "2017-11-07T00:12:52.423808: step 3461, loss 0.247632, acc 0.875\n",
      "2017-11-07T00:12:56.706851: step 3462, loss 0.114487, acc 0.9375\n",
      "2017-11-07T00:13:00.762733: step 3463, loss 0.095803, acc 0.96875\n",
      "2017-11-07T00:13:04.820616: step 3464, loss 0.131833, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-07T00:13:08.957555: step 3465, loss 0.139893, acc 0.90625\n",
      "2017-11-07T00:13:12.875339: step 3466, loss 0.0857062, acc 0.96875\n",
      "2017-11-07T00:13:16.853166: step 3467, loss 0.28057, acc 0.875\n",
      "2017-11-07T00:13:20.850007: step 3468, loss 0.108653, acc 0.96875\n",
      "2017-11-07T00:13:24.905889: step 3469, loss 0.126923, acc 0.9375\n",
      "2017-11-07T00:13:29.184928: step 3470, loss 0.201179, acc 0.90625\n",
      "2017-11-07T00:13:33.132733: step 3471, loss 0.309095, acc 0.875\n",
      "2017-11-07T00:13:37.076535: step 3472, loss 0.0705928, acc 0.9375\n",
      "2017-11-07T00:13:41.038351: step 3473, loss 0.208421, acc 0.9375\n",
      "2017-11-07T00:13:45.002167: step 3474, loss 0.16096, acc 0.90625\n",
      "2017-11-07T00:13:48.971987: step 3475, loss 0.284774, acc 0.84375\n",
      "2017-11-07T00:13:52.924797: step 3476, loss 0.0947229, acc 0.96875\n",
      "2017-11-07T00:13:57.180820: step 3477, loss 0.142329, acc 0.9375\n",
      "2017-11-07T00:14:01.406823: step 3478, loss 0.408273, acc 0.8125\n",
      "2017-11-07T00:14:05.372641: step 3479, loss 0.00605733, acc 1\n",
      "2017-11-07T00:14:09.346464: step 3480, loss 0.278007, acc 0.9375\n",
      "2017-11-07T00:14:13.314284: step 3481, loss 0.144026, acc 0.96875\n",
      "2017-11-07T00:14:17.270094: step 3482, loss 0.134611, acc 0.9375\n",
      "2017-11-07T00:14:21.210895: step 3483, loss 0.280349, acc 0.875\n",
      "2017-11-07T00:14:25.166705: step 3484, loss 0.239357, acc 0.90625\n",
      "2017-11-07T00:14:29.123518: step 3485, loss 0.235591, acc 0.90625\n",
      "2017-11-07T00:14:33.205418: step 3486, loss 0.134526, acc 0.875\n",
      "2017-11-07T00:14:37.298325: step 3487, loss 0.121996, acc 0.875\n",
      "2017-11-07T00:14:41.219111: step 3488, loss 0.0771317, acc 0.96875\n",
      "2017-11-07T00:14:45.209988: step 3489, loss 0.231625, acc 0.84375\n",
      "2017-11-07T00:14:49.130542: step 3490, loss 0.176974, acc 0.90625\n",
      "2017-11-07T00:14:53.107367: step 3491, loss 0.240038, acc 0.90625\n",
      "2017-11-07T00:14:55.630160: step 3492, loss 0.0575672, acc 1\n",
      "2017-11-07T00:14:59.592977: step 3493, loss 0.332139, acc 0.90625\n",
      "2017-11-07T00:15:03.866012: step 3494, loss 0.166193, acc 0.9375\n",
      "2017-11-07T00:15:07.993945: step 3495, loss 0.17676, acc 0.90625\n",
      "2017-11-07T00:15:11.916732: step 3496, loss 0.106793, acc 0.9375\n",
      "2017-11-07T00:15:15.868540: step 3497, loss 0.0936334, acc 0.9375\n",
      "2017-11-07T00:15:19.797332: step 3498, loss 0.125277, acc 0.9375\n",
      "2017-11-07T00:15:23.736131: step 3499, loss 0.165827, acc 0.90625\n",
      "2017-11-07T00:15:27.696945: step 3500, loss 0.127524, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T00:15:30.240753: step 3500, loss 1.05972, acc 0.766667\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-3500\n",
      "\n",
      "2017-11-07T00:15:35.475875: step 3501, loss 0.280992, acc 0.8125\n",
      "2017-11-07T00:15:39.440692: step 3502, loss 0.0773676, acc 0.96875\n",
      "2017-11-07T00:15:43.395503: step 3503, loss 0.277728, acc 0.875\n",
      "2017-11-07T00:15:47.345309: step 3504, loss 0.178303, acc 0.90625\n",
      "2017-11-07T00:15:51.294115: step 3505, loss 0.0635809, acc 0.96875\n",
      "2017-11-07T00:15:55.258933: step 3506, loss 0.106735, acc 0.9375\n",
      "2017-11-07T00:15:59.256773: step 3507, loss 0.133687, acc 0.9375\n",
      "2017-11-07T00:16:03.251612: step 3508, loss 0.201407, acc 0.9375\n",
      "2017-11-07T00:16:07.378543: step 3509, loss 0.406724, acc 0.84375\n",
      "2017-11-07T00:16:11.762659: step 3510, loss 0.273023, acc 0.90625\n",
      "2017-11-07T00:16:15.698456: step 3511, loss 0.0938232, acc 0.9375\n",
      "2017-11-07T00:16:19.678283: step 3512, loss 0.066265, acc 0.96875\n",
      "2017-11-07T00:16:23.634094: step 3513, loss 0.196875, acc 0.9375\n",
      "2017-11-07T00:16:27.624929: step 3514, loss 0.219213, acc 0.875\n",
      "2017-11-07T00:16:31.589748: step 3515, loss 0.158659, acc 0.9375\n",
      "2017-11-07T00:16:35.896811: step 3516, loss 0.127473, acc 0.90625\n",
      "2017-11-07T00:16:39.850616: step 3517, loss 0.0427615, acc 1\n",
      "2017-11-07T00:16:43.785414: step 3518, loss 0.161588, acc 0.90625\n",
      "2017-11-07T00:16:47.777271: step 3519, loss 0.145648, acc 0.9375\n",
      "2017-11-07T00:16:51.713068: step 3520, loss 0.346265, acc 0.84375\n",
      "2017-11-07T00:16:55.697899: step 3521, loss 0.246211, acc 0.90625\n",
      "2017-11-07T00:16:59.699742: step 3522, loss 0.0418781, acc 1\n",
      "2017-11-07T00:17:03.662558: step 3523, loss 0.0956195, acc 0.96875\n",
      "2017-11-07T00:17:07.588347: step 3524, loss 0.128651, acc 0.90625\n",
      "2017-11-07T00:17:11.602200: step 3525, loss 0.230091, acc 0.875\n",
      "2017-11-07T00:17:16.048358: step 3526, loss 0.145655, acc 0.9375\n",
      "2017-11-07T00:17:20.068215: step 3527, loss 0.18066, acc 0.9375\n",
      "2017-11-07T00:17:22.691080: step 3528, loss 0.215322, acc 0.9\n",
      "2017-11-07T00:17:26.779983: step 3529, loss 0.08998, acc 0.96875\n",
      "2017-11-07T00:17:30.811848: step 3530, loss 0.0485603, acc 0.96875\n",
      "2017-11-07T00:17:34.775664: step 3531, loss 0.182259, acc 0.90625\n",
      "2017-11-07T00:17:38.813534: step 3532, loss 0.143592, acc 0.90625\n",
      "2017-11-07T00:17:42.758336: step 3533, loss 0.284694, acc 0.90625\n",
      "2017-11-07T00:17:46.658108: step 3534, loss 0.276362, acc 0.875\n",
      "2017-11-07T00:17:50.740777: step 3535, loss 0.202331, acc 0.90625\n",
      "2017-11-07T00:17:54.733616: step 3536, loss 0.114805, acc 0.9375\n",
      "2017-11-07T00:17:58.686423: step 3537, loss 0.195126, acc 0.875\n",
      "2017-11-07T00:18:02.666251: step 3538, loss 0.0259572, acc 1\n",
      "2017-11-07T00:18:06.619060: step 3539, loss 0.240787, acc 0.875\n",
      "2017-11-07T00:18:10.636915: step 3540, loss 0.216381, acc 0.90625\n",
      "2017-11-07T00:18:14.807878: step 3541, loss 0.097364, acc 0.96875\n",
      "2017-11-07T00:18:19.031880: step 3542, loss 0.128564, acc 0.90625\n",
      "2017-11-07T00:18:23.409990: step 3543, loss 0.0937775, acc 0.96875\n",
      "2017-11-07T00:18:27.478881: step 3544, loss 0.134548, acc 0.90625\n",
      "2017-11-07T00:18:31.450704: step 3545, loss 0.0593736, acc 0.96875\n",
      "2017-11-07T00:18:35.738750: step 3546, loss 0.201744, acc 0.90625\n",
      "2017-11-07T00:18:39.678550: step 3547, loss 0.152106, acc 0.90625\n",
      "2017-11-07T00:18:43.633360: step 3548, loss 0.250714, acc 0.875\n",
      "2017-11-07T00:18:47.602180: step 3549, loss 0.102318, acc 0.9375\n",
      "2017-11-07T00:18:51.611259: step 3550, loss 0.130708, acc 0.9375\n",
      "2017-11-07T00:18:55.588084: step 3551, loss 0.102902, acc 0.9375\n",
      "2017-11-07T00:18:59.535889: step 3552, loss 0.0567721, acc 0.96875\n",
      "2017-11-07T00:19:03.501707: step 3553, loss 0.159511, acc 0.90625\n",
      "2017-11-07T00:19:07.495545: step 3554, loss 0.0443279, acc 1\n",
      "2017-11-07T00:19:11.453357: step 3555, loss 0.0923322, acc 0.96875\n",
      "2017-11-07T00:19:15.485224: step 3556, loss 0.184821, acc 0.9375\n",
      "2017-11-07T00:19:19.485064: step 3557, loss 0.0827428, acc 0.96875\n",
      "2017-11-07T00:19:23.627007: step 3558, loss 0.317089, acc 0.875\n",
      "2017-11-07T00:19:28.343358: step 3559, loss 0.0291693, acc 1\n",
      "2017-11-07T00:19:32.358210: step 3560, loss 0.21459, acc 0.9375\n",
      "2017-11-07T00:19:36.349047: step 3561, loss 0.121242, acc 0.96875\n",
      "2017-11-07T00:19:40.297853: step 3562, loss 0.258236, acc 0.875\n",
      "2017-11-07T00:19:44.224643: step 3563, loss 0.19452, acc 0.90625\n",
      "2017-11-07T00:19:46.761445: step 3564, loss 0.110073, acc 0.95\n",
      "2017-11-07T00:19:50.716255: step 3565, loss 0.0691521, acc 0.96875\n",
      "2017-11-07T00:19:54.672068: step 3566, loss 0.117547, acc 0.90625\n",
      "2017-11-07T00:19:58.634882: step 3567, loss 0.0638516, acc 0.96875\n",
      "2017-11-07T00:20:02.904917: step 3568, loss 0.159623, acc 0.90625\n",
      "2017-11-07T00:20:06.866732: step 3569, loss 0.331431, acc 0.8125\n",
      "2017-11-07T00:20:10.790521: step 3570, loss 0.157168, acc 0.90625\n",
      "2017-11-07T00:20:14.756337: step 3571, loss 0.166934, acc 0.875\n",
      "2017-11-07T00:20:18.753177: step 3572, loss 0.209956, acc 0.90625\n",
      "2017-11-07T00:20:22.763046: step 3573, loss 0.0395648, acc 0.96875\n",
      "2017-11-07T00:20:26.772875: step 3574, loss 0.279656, acc 0.84375\n",
      "2017-11-07T00:20:30.946841: step 3575, loss 0.123697, acc 0.9375\n",
      "2017-11-07T00:20:35.484065: step 3576, loss 0.389931, acc 0.8125\n",
      "2017-11-07T00:20:39.541948: step 3577, loss 0.0819453, acc 0.96875\n",
      "2017-11-07T00:20:43.532784: step 3578, loss 0.155494, acc 0.90625\n",
      "2017-11-07T00:20:47.502604: step 3579, loss 0.200933, acc 0.875\n",
      "2017-11-07T00:20:51.465213: step 3580, loss 0.340115, acc 0.84375\n",
      "2017-11-07T00:20:55.501080: step 3581, loss 0.141164, acc 0.9375\n",
      "2017-11-07T00:20:59.457892: step 3582, loss 0.0744975, acc 0.96875\n",
      "2017-11-07T00:21:03.437719: step 3583, loss 0.0474865, acc 0.96875\n",
      "2017-11-07T00:21:07.384524: step 3584, loss 0.140815, acc 0.875\n",
      "2017-11-07T00:21:11.324323: step 3585, loss 0.222314, acc 0.875\n",
      "2017-11-07T00:21:15.320162: step 3586, loss 0.0639887, acc 1\n",
      "2017-11-07T00:21:19.274973: step 3587, loss 0.106144, acc 0.9375\n",
      "2017-11-07T00:21:23.237788: step 3588, loss 0.00678154, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-07T00:21:27.254642: step 3589, loss 0.151909, acc 0.96875\n",
      "2017-11-07T00:21:31.178430: step 3590, loss 0.156224, acc 0.9375\n",
      "2017-11-07T00:21:35.270338: step 3591, loss 0.107172, acc 0.96875\n",
      "2017-11-07T00:21:39.636440: step 3592, loss 0.280706, acc 0.875\n",
      "2017-11-07T00:21:43.626275: step 3593, loss 0.276355, acc 0.9375\n",
      "2017-11-07T00:21:47.560070: step 3594, loss 0.0278866, acc 1\n",
      "2017-11-07T00:21:51.489863: step 3595, loss 0.103432, acc 0.96875\n",
      "2017-11-07T00:21:55.478697: step 3596, loss 0.29396, acc 0.78125\n",
      "2017-11-07T00:21:59.467533: step 3597, loss 0.208032, acc 0.9375\n",
      "2017-11-07T00:22:03.444358: step 3598, loss 0.144037, acc 0.96875\n",
      "2017-11-07T00:22:07.438195: step 3599, loss 0.398014, acc 0.8125\n",
      "2017-11-07T00:22:09.938971: step 3600, loss 0.475766, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2017-11-07T00:22:12.500792: step 3600, loss 1.06058, acc 0.75\n",
      "\n",
      "Saved model checkpoint to D:\\ITB\\S2\\TESIS\\cnn\\runs\\9\\1509974316\\checkpoints\\model-3600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FLAGS = tf.flags.FLAGS\n",
    "\n",
    "for i in range (10):\n",
    "\n",
    "    positive_data_file = \"data101617/dataNoClassCategory-\" + str(i) + \".txt\"\n",
    "    negative_data_file = \"data101617/dataNoClassNegCategory-\" + str(i) + \".txt\"\n",
    "    \n",
    "    train(i, positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/ITB/S2/TESIS/cnn/runs/0/051117/checkpoints/model-3200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Fetch argument 'w1:0' cannot be interpreted as a Tensor. (\"The name 'w1:0' refers to a Tensor which does not exist. The operation, 'w1', does not exist in the graph.\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    266\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 267\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    268\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2413\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2414\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2455\u001b[0m                          \u001b[1;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2456\u001b[1;33m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[0;32m   2457\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The name 'w1:0' refers to a Tensor which does not exist. The operation, 'w1', does not exist in the graph.\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-05425e17c120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/ITB/S2/TESIS/cnn/runs/0/051117/checkpoints/model-3200.meta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D:/ITB/S2/TESIS/cnn/runs/0/051117/checkpoints/model-3200'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w1:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m##Model has been restored. Above statement will print the saved value of w1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m     \u001b[0mfetch_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \"\"\"\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    275\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[1;32m--> 277\u001b[1;33m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[0;32m    278\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_contraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Fetch argument 'w1:0' cannot be interpreted as a Tensor. (\"The name 'w1:0' refers to a Tensor which does not exist. The operation, 'w1', does not exist in the graph.\")"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('D:/ITB/S2/TESIS/cnn/runs/0/051117/checkpoints/model-3200.meta')\n",
    "    saver.restore(sess, 'D:/ITB/S2/TESIS/cnn/runs/0/051117/checkpoints/model-3200')\n",
    "    print(sess.run('w1:0'))\n",
    "##Model has been restored. Above statement will print the saved value of w1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
