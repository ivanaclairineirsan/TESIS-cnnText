{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_helpers import load_data\n",
    "from data_helpers import load_data_embeddings_vocab\n",
    "from data_helpers import get_embeddings\n",
    "from data_helpers import load_test_data_separate_files\n",
    "import h5py\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doSMOTE(X_train, y_train):\n",
    "    print(\"y_train: \" + str(len(y_train)))\n",
    "    \n",
    "    y_train_bin = []\n",
    "    for y in y_train:\n",
    "        if (y[0] < y[1]):\n",
    "            y_train_bin.append(1)\n",
    "        else:\n",
    "            y_train_bin.append(0)\n",
    "    y_train_bin = np.array(y_train_bin)\n",
    "    \n",
    "    print('Original dataset shape {}'.format(Counter(y_train_bin)))\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_sample(X_train, y_train_bin)\n",
    "    print('Resampled dataset shape {}'.format(Counter(y_res)))\n",
    "    X_res = np.array(X_res, dtype='int')\n",
    "    y_res_mod = []\n",
    "    for y in y_res:\n",
    "        if y == 0:\n",
    "            y_res_mod.append([1, 0])\n",
    "        else:\n",
    "            y_res_mod.append([0, 1])\n",
    "    y_res_mod=np.array(y_res_mod)\n",
    "    return [X_res, y_res_mod]\n",
    "\n",
    "def countF1(predicted, actual):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if(predicted[i] == 1 and actual[i] == 1):\n",
    "            true_pos = true_pos+1\n",
    "        if(predicted[i] == 1 and actual[i] == 0):\n",
    "            false_pos = false_pos+1\n",
    "        if(predicted[i] == 0 and actual[i] == 1):\n",
    "            false_neg = false_neg+1\n",
    "        if(predicted[i] == 0 and actual[i] == 0):\n",
    "            true_neg = true_neg+1\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    print(\"true positive: \" + str(true_pos))\n",
    "    print(\"true negative: \" + str(true_neg))\n",
    "    print(\"false positive: \" + str(false_pos))\n",
    "    print(\"false negative: \" + str(false_neg))\n",
    "    if (precision + recall > 0):\n",
    "        return (2*precision*recall) / (precision+recall)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def startTraining(pos_data, neg_data, model_path, class_id):\n",
    "    print('Loading data - class: ' + str(class_id))\n",
    "#     x, y, vocabulary, vocabulary_inv = load_data(\"../data101617/dataNoClassCategory-0.txt\", \"../data101617/dataNoClassNegCategory-0.txt\")\n",
    "#     x, y, vocabulary, vocabulary_inv = load_data(pos_data, neg_data)\n",
    "    x, y, vocabulary, vocabulary_inv = load_data_embeddings_vocab(pos_data, neg_data, \"../word2vec/wiki.id.vec\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.08)\n",
    "#     X_test, y_test = load_test_data_separate_files(pos_data, neg_data, vocabulary, x.shape[1])\n",
    "#     print(X_train)\n",
    "    print(len(vocabulary))\n",
    "    \n",
    "    write_train_test(X_train, X_test, y_train, y_test, model_path)\n",
    "    write_vocabulary(vocabulary, model_path)\n",
    "    \n",
    "    sequence_length = x.shape[1]\n",
    "    vocabulary_size = len(vocabulary_inv)\n",
    "    embedding_dim = 300\n",
    "    filter_sizes = [3,4,5]\n",
    "    num_filters = 100\n",
    "    drop = 0.5\n",
    "    epochs = 10\n",
    "    batch_size = 50\n",
    "    \n",
    "    # this returns a tensor\n",
    "    print(\"Creating Model... - class: \" + str(class_id))\n",
    "    inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "    print(inputs)\n",
    "    embedding_matrix = get_embeddings(\"../word2vec/wiki.id.vec\", vocabulary_inv, embedding_dim)\n",
    "    embedding = Embedding(input_dim=vocabulary_size+1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=sequence_length, trainable=False)(inputs)\n",
    "    reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    output = Dense(units=2, activation='softmax')(dropout)\n",
    "\n",
    "    # this creates a model that includes\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_path + '/weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=False, mode='auto')\n",
    "#     adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    adam = Adam()\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Traning Model...\")\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training\n",
    " \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_path + \"/model.json\", \"w\") as json_file:\n",
    "        json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n",
    "    \n",
    "    model.save_weights(model_path + \"/model_weights.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    print(\"Start validating -- #test data: \" + str(len(X_test)))\n",
    "    y_prob = model.predict(X_test) \n",
    "    y_classes=[]\n",
    "    for cls in y_prob:\n",
    "        if(cls[0] > cls[1]):\n",
    "            y_classes.append(0)\n",
    "        else:\n",
    "            y_classes.append(1)\n",
    "    print(y_classes)\n",
    "    \n",
    "    y_test_classes=[]\n",
    "    for cls in y_test:\n",
    "        if(cls[0] > cls[1]):\n",
    "            y_test_classes.append(0)\n",
    "        else:\n",
    "            y_test_classes.append(1)\n",
    "    print(y_test_classes)\n",
    "    print(countF1(y_classes, y_test_classes))\n",
    "    \n",
    "    print(\"Start validating -- #train data: \" + str(len(X_train)))\n",
    "    y_prob = model.predict(X_train) \n",
    "    y_classes=[]\n",
    "    for cls in y_prob:\n",
    "        if(cls[0] > cls[1]):\n",
    "            y_classes.append(0)\n",
    "        else:\n",
    "            y_classes.append(1)\n",
    "#     print(y_classes)\n",
    "    y_test_classes=[]\n",
    "    for cls in y_train:\n",
    "        if(cls[0] > cls[1]):\n",
    "            y_test_classes.append(0)\n",
    "        else:\n",
    "            y_test_classes.append(1)\n",
    "#     print(y_test_classes)\n",
    "    print(countF1(y_classes, y_test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_vocabulary(vocabulary, model_path):\n",
    "    pickle_out = open(model_path + \"/vocabulary.pickle\",\"wb\")\n",
    "    pickle.dump(vocabulary, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "def load_vocabulary(model_path):\n",
    "    pickle_in = open(model_path + \"/vocabulary.pickle\",\"rb\")\n",
    "    vocabulary = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    return vocabulary\n",
    "    \n",
    "def write_train_test(x_train, x_test, y_train, y_test, model_path):\n",
    "    file_name = {\n",
    "        \"x_train\":x_train,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_train\":y_train,\n",
    "        \"y_test\":y_test\n",
    "    }\n",
    "    \n",
    "    for name in file_name:\n",
    "        pickle_out = open(model_path + '/' + name + \".pickle\",\"wb\")\n",
    "        pickle.dump(file_name.get(name), pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data - class: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-870d6c58ee33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnegative_data_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data070218/dataAnnotate-\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-neg.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model110218_newsagg/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstartTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive_data_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_data_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a890ac09d6ca>\u001b[0m in \u001b[0;36mstartTraining\u001b[1;34m(pos_data, neg_data, model_path, class_id)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     x, y, vocabulary, vocabulary_inv = load_data(\"../data101617/dataNoClassCategory-0.txt\", \"../data101617/dataNoClassNegCategory-0.txt\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     x, y, vocabulary, vocabulary_inv = load_data(pos_data, neg_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_embeddings_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../word2vec/wiki.id.vec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.08\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     X_test, y_test = load_test_data_separate_files(pos_data, neg_data, vocabulary, x.shape[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ITB\\S2\\TESIS\\cnn\\keras_cnn\\data_helpers.py\u001b[0m in \u001b[0;36mload_data_embeddings_vocab\u001b[1;34m(pos_data, neg_data, embeddings_file)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data_and_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[0msentences_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_join_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_input_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_inv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ITB\\S2\\TESIS\\cnn\\keras_cnn\\data_helpers.py\u001b[0m in \u001b[0;36mbuild_join_vocab\u001b[1;34m(sentences, embeddings_file)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mdata_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_vocab_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m     \u001b[0membeddings_vocab_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_embedding_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ITB\\S2\\TESIS\\cnn\\keras_cnn\\data_helpers.py\u001b[0m in \u001b[0;36mload_embeddings\u001b[1;34m(word_embedding_file)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embedding_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ivana clairine\\appdata\\local\\programs\\python\\python35\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range (7,11):\n",
    "    positive_data_file = \"../data070218/dataAnnotate-\" + str(i) + \"-pos.txt\"\n",
    "    negative_data_file = \"../data070218/dataAnnotate-\" + str(i) + \"-neg.txt\"\n",
    "    model_path = \"model110218_newsagg/\" + str(i)\n",
    "    startTraining(positive_data_file, negative_data_file, model_path, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
